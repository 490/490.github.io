<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java基础知识]]></title>
    <url>%2FJava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[零散的面试常考基础知识 String 和 StringBuffer、StringBuilder 的区别是什么？可变性简单的来说：String 类中使用 final 关键字字符数组保存字符串， private final char value[]，所以 String 对象是不可变的。而StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串 char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 StringBuilder 与 StringBuffer 的构造方法都是调用父类构造方法也就是 AbstractStringBuilder 实现的，大家可以自行查阅源码。 AbstractStringBuilder.java 123456789abstract class AbstractStringBuilder implements Appendable, CharSequence &#123; char[] value; int count; AbstractStringBuilder() &#123; &#125; AbstractStringBuilder(int capacity) &#123; value = new char[capacity]; &#125;&#125; 线程安全性String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的，如果一个StringBuffer对象在字符串缓冲区被多个线程使用时，StringBuffer中很多方法可以带有synchronized关键字。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。 StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StirngBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据 = String 单线程操作字符串缓冲区下操作大量数据 = StringBuilder 多线程操作字符串缓冲区下操作大量数据 = StringBuffer 关于 final 关键字的一些总结final关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为fianl Object类的常见方法总结123456789101112131415161718192021public final native Class&lt;?&gt; getClass()//native方法，用于返回当前运行时对象的Class对象，使用了final关键字修饰，故不允许子类重写。public native int hashCode() //native方法，用于返回对象的哈希码，主要使用在哈希表中，比如JDK中的HashMap。public boolean equals(Object obj)//用于比较2个对象的内存地址是否相等，String类对该方法进行了重写用户比较字符串的值是否相等。protected native Object clone() throws CloneNotSupportedException//naitive方法，用于创建并返回当前对象的一份拷贝。一般情况下，对于任何对象 x，表达式 x.clone() != x 为true，x.clone().getClass() == x.getClass() 为true。Object本身没有实现Cloneable接口，所以不重写clone方法并且进行调用的话会发CloneNotSupportedException异常。public String toString()//返回类的名字@实例的哈希码的16进制的字符串。建议Object所有的子类都重写这个方法。public final native void notify()//native方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。public final native void notifyAll()//native方法，并且不能重写。跟notify一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。public final native void wait(long timeout) throws InterruptedException//native方法，并且不能重写。暂停线程的执行。注意：sleep方法没有释放锁，而wait方法释放了锁 。timeout是等待时间。public final void wait(long timeout, int nanos) throws InterruptedException//多了nanos参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上nanos毫秒。public final void wait() throws InterruptedException//跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念protected void finalize() throws Throwable &#123; &#125;//实例被垃圾回收器回收的时候触发的操作 Java的异常体系 Error（错误）是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（VirtualMachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 Exception（异常）unchecked Exception（RuntimeException）RuntimeException 异常由Java虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以0时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。 对未检查的异常(unchecked exception )的几种处理方式： 捕获 继续抛出 不处理 checked Exception（非RuntimeException）对于不是你犯的错，我们统称为非RuntimeException，也叫checked Exception。对检查的异常的几种处理方式： 继续抛出，消极的方法，一直可以抛到java虚拟机来处理 用try…catch捕获 对于检查的异常必须处理，或者必须捕获或者必须抛出 异常处理完成以后，Exception对象会在下一个垃圾回收过程中被回收掉。 注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。 Throwable类常用方法public string getMessage():返回异常发生时的详细信息public string toString():返回异常发生时的简要描述public string getLocalizedMessage():返回异常对象的本地化信息。使用Throwable的子类覆盖这个方法，可以声称本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与getMessage（）返回的结果相同 public void printStackTrace():在控制台上打印Throwable对象封装的异常信息 异常处理总结 try 块：用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。 catch 块：用于处理try捕获到的异常。 finally 块：无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。 在以下4种特殊情况下，finally块不会被执行： 在finally语句块中发生了异常。 在前面的代码中用了System.exit()退出程序。 程序所在的线程死亡。 关闭CPU 获取用键盘输入常用的的两种方法方法1：通过 Scanner 123Scanner input = new Scanner(System.in);String s = input.nextLine();input.close(); 方法2：通过 BufferedReader 12BufferedReader input = new BufferedReader(new InputStreamReader(System.in));String s = input.readLine(); 接口和抽象类的区别是什么 接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），抽象类可以有非抽象的方法 接口中的实例变量默认是 final 类型的，而抽象类中则不一定 一个类可以实现多个接口，但最多只能实现一个抽象类 一个类实现接口的话要实现接口的所有方法，而抽象类不一定 接口不能用 new 实例化，但可以声明，但是必须引用一个实现该接口的对象 。从设计层面来说，抽象是对类的抽象，是一种模板设计，接口是行为的抽象，是一种行为的规范。 备注:在JDK8中，接口也可以定义静态方法，可以直接用接口名调用。实现类和实现是不可以调用的。如果同时实现两个接口，接口中定义了一样的默认方法，必须重写，不然会报错。 深拷贝与浅拷贝Java深拷贝浅拷贝将一个对象的引用复制给另外一个对象，一共有三种方式。第一种方式是直接赋值，第二种方式是浅拷贝，第三种是深拷贝。 直接赋值：A a1 = a2，复制的是引用，也就是说a1和a2指向的是同一个对象。因此，当a1变化的时候，a2里面的成员变量也会跟着变化。 浅：属性不一样，是独立的，对象（方法）什么的一样，是同一份。clone()主要做了些什么，创建一个新对象，然后将当前对象的非静态字段复制到该新对象，如果字段是值类型的，那么对该字段执行复制；如果该字段是引用类型的话，则复制引用但不复制引用的对象。因此，原始对象及其副本引用同一个对象。 深：都不一样 new Integer(123) 与 Integer.valueOf(123) 的区别在于，new Integer(123) 每次都会新建一个对象，而 Integer.valueOf(123) 可能会使用缓存对象，因此多次使用 Integer.valueOf(123) 会取得同一个对象的引用。 反射、泛型泛型泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。 泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。 12345List arrayList = new ArrayList();arrayList.add("aaaa");List&lt;String&gt; arrayList = new ArrayList&lt;String&gt;();//arrayList.add(100); 在编译阶段，编译器就会报错 泛型只在编译阶段有效。见“反射”，用方法的反射绕过编译 通配符的出现是为了指定泛型中的类型范围。 &lt;?&gt;被称作无限定的通配符。 &lt;? extends T&gt;被称作有上限的通配符。 &lt;? super T&gt;被称作有下限的通配符。 泛型类、泛型接口、泛型方法 泛型擦除： Java 编译器生成的字节码文件不包含有泛型信息，泛型信息将在编译时被擦除，这个过程称为泛型擦除。其主要过程为 将所有泛型参数用其最左边界（最顶级的父类型）类型替换； 移除 all 的类型参数。 反射javap 原生的 看class文件 类是java.lang.class类的实例对象 12345678910111213141516171819202122232425262728//第一种表示方式---&gt;实际在告诉我们任何一个类都有一个隐含的静态成员变量class Class c1 = Foo.class; //第二中表达方式 已经知道该类的对象通过getClass方法 Class c2 = foo1.getClass(); /*官网 c1 ,c2 表示了Foo类的类类型(class type) 万事万物皆对象，类也是对象，是Class类的实例对象 * 这个对象我们称为该类的类类型 */ //不管c1 or c2都代表了Foo类的类类型，一个类只可能是Class类的一个实例对象 System.out.println(c1 == c2);相等 //第三种表达方式 Class c3 = null; try &#123; c3 = Class.forName("com.imooc.reflect.Foo"); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println(c2==c3);相等 //我们完全可以通过类的类类型创建该类的对象实例----&gt;通过c1 or c2 or c3创建Foo的实例对象 try &#123; Foo foo = (Foo)c1.newInstance();//需要有无参数的构造方法 foo.print(); &#125; catch (InstantiationException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; new创建对象是静态加载类，编译时刻就需要加载所有可能用到的类 12345678Class c = obj.getClass();/** 成员变量也是对象* java.lang.reflect.Field* Field类封装了关于成员变量的操作* getFields()方法获取的是所有的public的成员变量的信息* getDeclaredFields获取的是该类自己声明的成员变量的信息 *///Field[] fs = c.getFields();Field[] fs = c.getDeclaredFields(); 要获取一个方法就是获取类的信息，获取类的信息首先要获取类的类类型. 获取方法 名称和参数列表来决定 getMethod获取的是public的方法、getDelcaredMethod自己声明的方法 123456"Method m = c.getMethod(""print"", int.class,int.class);//方法的反射操作 //a1.print(10, 20);方法的反射操作是用m对象来进行方法调用 和a1.print调用的效果完全相同//方法如果没有返回值返回null,有返回值返回具体的返回值//Object o = m.invoke(a1,new Object[]&#123;10,20&#125;);Object o = m.invoke(a1, 10,20); Java WebServlet的生命周期与工作原理Servlet运行在Servlet容器中，其生命周期由容器来管理。Servlet的生命周期通过javax.servlet.Servlet接口中的init()、service()和destroy()方法来表示,Servlet的生命周期包含了下面4个阶段： 加载和实例化 初始化 请求处理 服务终止 Web Client 向Servlet容器（Tomcat）发出Http请求 Servlet容器接收Web Client的请求 Servlet容器创建一个HttpRequest对象，将Web Client请求的信息封装到这个对象中。 Servlet容器创建一个HttpResponse对象 Servlet容器调用HttpServlet对象的service方法，把HttpRequest对象与HttpResponse对象作为参数传给HttpServlet 对象。 HttpServlet调用HttpRequest对象的有关方法，获取Http请求信息。 HttpServlet调用HttpResponse对象的有关方法，生成响应数据。 Servlet容器把HttpServlet的响应结果传给Web Client。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式---基于Java详解(全)]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[创建型模式： 对象实例化的模式，创建型模式用于解耦对象的实例化过程。 单例模式：某个类只能有一个实例，提供一个全局的访问点。 简单工厂模式：一个工厂类根据传入的参量决定创建出那一种产品类的实例。 工厂方法模式：定义一个创建对象的接口，让子类决定实例化那个类。 抽象工厂模式：创建相关或依赖对象的家族，而无需明确指定具体类。 建造者模式：封装一个复杂对象的构建过程，并可以按步骤构造。 原型模式：通过复制现有的实例来创建新的实例。 结构型模式： 把类或对象结合在一起形成一个更大的结构。 适配器模式：将一个类的方法接口转换成客户希望的另外一个接口。 外观模式：对外提供一个统一的方法，来访问子系统中的一群接口。 组合模式：将对象组合成树形结构以表示“”部分-整体“”的层次结构。 装饰模式：动态的给对象添加新的功能。 代理模式：为其他对象提供一个代理以便控制这个对象的访问。 享元模式：通过共享技术来有效的支持大量细粒度的对象。 桥接模式：将抽象部分和它的实现部分分离，使它们都可以独立的变化。 行为型模式： 类和对象如何交互，及划分责任和算法。 模板模式：定义一个算法结构，而将一些步骤延迟到子类实现。 迭代器模式：一种遍历访问聚合对象中各个元素的方法，不暴露该对象的内部结构。 策略模式：定义一系列算法，把他们封装起来，并且使它们可以相互替换。 状态模式：允许一个对象在其对象内部状态改变时改变它的行为。 观察者模式：对象间的一对多的依赖关系。 解释器模式：给定一个语言，定义它的文法的一种表示，并定义一个解释器。 备忘录模式：在不破坏封装的前提下，保持对象的内部状态。 中介者模式：用一个中介对象来封装一系列的对象交互。 命令模式：将命令请求封装为一个对象，使得可以用不同的请求来进行参数化。 访问者模式：在不改变数据结构的前提下，增加作用于一组对象元素的新功能。 责任链模式：将请求的发送者和接收者解耦，使的多个对象都有处理这个请求的机会。 创建者模式单例模式单例模式是一种对象创建型模式，使用单例模式，可以保证为一个类只生成唯一的实例对象。也就是说，在整个程序空间中，该类只存在一个实例对象。GoF对单例模式的定义是：保证一个类、只有一个实例存在，同时提供能对该实例加以访问的全局访问方法。 应用场景： 在多个线程之间，比如servlet环境，共享同一个资源或者操作同一个对象 在整个程序空间使用全局变量，共享资源 大规模系统中，为了性能的考虑，需要节省对象的创建时间等等。 因为Singleton模式可以保证为一个类只生成唯一的实例对象，所以这些情况，Singleton模式就派上用场了。 懒汉式（线程不安全）1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 懒汉式（线程安全）123456public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance;&#125; 双重检验锁123456789101112131415public class Singleton &#123; private volatile static Singleton instance; // 声明成 volatile private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 饿汉式 static final field12345678public class Singleton&#123; private static final Singleton instance = new Singleton(); // 类加载时就初始化 private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return instance; &#125;&#125; 静态内部类 static nested class1234567891011public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 这种写法仍然使用 JVM 本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒汉式的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。 枚举 Enum123public enum EasySingleton&#123; INSTANCE;&#125; 通过 EasySingleton.INSTANCE 来访问实例。 简单工厂简单工厂并不是一个设计模式，而是一种编程习惯。 logger、calendar等类是这个模式 工厂方法模式 作为抽象工厂模式的孪生兄弟，工厂方法模式定义了一个创建对象的接口，但由子类决定要实例化的类是哪一个，也就是说工厂方法模式让实例化推迟到子类。 工厂方法模式非常符合“开闭原则”，当需要增加一个新的产品时，我们只需要增加一个具体的产品类和与之对应的具体工厂即可，无须修改原有系统。同时在工厂方法模式中用户只需要知道生产产品的具体工厂即可，无须关系产品的创建过程，甚至连具体的产品类名称都不需要知道。虽然他很好的符合了“开闭原则”，但是由于每新增一个新产品时就需要增加两个类，这样势必会导致系统的复杂度增加。 Collection的Iterator方法是工厂方法模式。 抽象工厂模式 所谓抽象工厂模式就是提供一个接口，用于创建相关或者依赖对象的家族，而不需要明确指定具体类。他允许客户端使用抽象的接口来创建一组相关的产品，而不需要关系实际产出的具体产品是什么。这样一来，客户就可以从具体的产品中被解耦。 它的优点是隔离了具体类的生成，使得客户端不需要知道什么被创建了，而缺点就在于新增新的行为会比较麻烦，因为当添加一个新的产品对象时，需要更加需要更改接口及其下所有子类。 建造者模式 对于建造者模式而已，它主要是将一个复杂对象的构建与表示分离，使得同样的构建过程可以创建不同的表示。适用于那些产品对象的内部结构比较复杂。 建造者模式将复杂产品的构建过程封装分解在不同的方法中，使得创建过程非常清晰，能够让我们更加精确的控制复杂产品对象的创建过程，同时它隔离了复杂产品对象的创建和使用，使得相同的创建过程能够创建不同的产品。但是如果某个产品的内部结构过于复杂，将会导致整个系统变得非常庞大，不利于控制，同时若几个产品之间存在较大的差异，则不适用建造者模式，毕竟这个世界上存在相同点大的两个产品并不是很多，所以它的使用范围有限。 原型模式 结构型模式适配器模式 在我们的应用程序中我们可能需要将两个不同接口的类来进行通信，在不修改这两个的前提下我们可能会需要某个中间件来完成这个衔接的过程。这个中间件就是适配器。所谓适配器模式就是将一个类的接口，转换成客户期望的另一个接口。它可以让原本两个不兼容的接口能够无缝完成对接。 作为中间件的适配器将目标类和适配者解耦，增加了类的透明性和可复用性。 外观模式 组合模式 装饰模式 代理模式为另一个对象提供一个替身或占位符以控制对这个对象的访问。 Proxy 中有一个 RealSubject 对象，我们拿不到 RealSubject 对象，只能拿到 Proxy 对象； Proxy 和 RealSubject 都实现了 Subject 接口，它们有相同的方法； 我们通过 Proxy 对象调用 RealSubject 对象的方法，不过在调用前，Proxy 会先检查一下这个调用合不合理，不合理它就不调用 RealSubject 对象的方法。 享元模式 桥接模式 行为型模式模板模式 迭代器模式 策略模式 状态模式 观察者模式 解释器模式 备忘录模式 中介者模式 命令模式 访问者模式 责任链模式 参考： 图说设计模式 JAVA设计模式总结之23种设计模式 如何正确地写出单例模式]]></content>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启发式算法]]></title>
    <url>%2F%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[遗传算法遗传算法思想 借鉴生物进化论，遗传算法将要解决的问题模拟成一个生物进化的过程，通过复制、交叉、突变等操作产生下一代的解，并逐步淘汰掉适应度函数值低的解，增加适应度函数值高的解。这样进化N代后就很有可能会进化出适应度函数值很高的个体。 举个例子，使用遗传算法解决“0-1背包问题”的思路：0-1背包的解可以编码为一串0-1字符串（0：不取，1：取） ；首先，随机产生M个0-1字符串，然后评价这些0-1字符串作为0-1背包问题的解的优劣；然后，随机选择一些字符串通过交叉、突变等操作产生下一代的M个字符串，而且较优的解被选中的概率要比较高。这样经过G代的进化后就可能会产生出0-1背包问题的一个“近似最优解”。 编码：需要将问题的解编码成字符串的形式才能使用遗传算法。最简单的一种编码方式是二进制编码，即将问题的解编码成二进制位数组的形式。例如，问题的解是整数，那么可以将其编码成二进制位数组的形式。将0-1字符串作为0-1背包问题的解就属于二进制编码。 遗传算法有3个最基本的操作：选择，交叉，变异。 选择：选择一些染色体来产生下一代。一种常用的选择策略是 “比例选择”，也就是个体被选中的概率与其适应度函数值成正比。假设群体的个体总数是M，那么那么一个体Xi被选中的概率为f(Xi)/( f(X1) + f(X2) + …….. + f(Xn) )。比例选择实现算法就是所谓的“轮盘赌算法”( Roulette Wheel Selection ) ，轮盘赌算法的一个简单的实现如下： 12345678910111213141516171819轮盘赌算法/** 按设定的概率，随机选中一个个体* P[i]表示第i个个体被选中的概率*/int RWS()&#123; m =0; r =Random(0,1); //r为0至1的随机数 for(i=1;i&lt;=N; i++) &#123; /* 产生的随机数在m~m+P[i]间则认为选中了i * 因此i被选中的概率是P[i] */ m = m + P[i]; if(r&lt;=m) return i; &#125;&#125; 交叉(Crossover)：2条染色体交换部分基因，来构造下一代的2条新的染色体。例如： 交叉前： 00000|==011100000000==|1000011100|000001111110|00101 交叉后： 00000|000001111110|1000011100|==011100000000==|00101 染色体交叉是以一定的概率发生的，这个概率记为Pc 。 变异(Mutation)：在繁殖过程，新产生的染色体中的基因会以一定的概率出错，称为变异。变异发生的概率记为Pm 。例如： 变异前：000001110000==0==00010000 变异后：000001110000==1==00010000 适应度函数 ( Fitness Function )：用于评价某个染色体的适应度，用f(x)表示。有时需要区分染色体的适应度函数与问题的目标函数。例如：0-1背包问题的目标函数是所取得物品价值，但将物品价值作为染色体的适应度函数可能并不一定适合。适应度函数与目标函数是正相关的，可对目标函数作一些变形来得到适应度函数。 基本遗传算法的伪代码12345678910111213141516171819202122232425262728基本遗传算法伪代码/** Pc：交叉发生的概率* Pm：变异发生的概率* M：种群规模* G：终止进化的代数* Tf：进化产生的任何一个个体的适应度函数超过Tf，则可以终止进化过程*/初始化Pm，Pc，M，G，Tf等参数。随机产生第一代种群Popdo&#123; 计算种群Pop中每一个体的适应度F(i)。 初始化空种群newPop do &#123; 根据适应度以比例选择算法从种群Pop中选出2个个体 if ( random ( 0 , 1 ) &lt; Pc ) &#123; 对2个个体按交叉概率Pc执行交叉操作 &#125; if ( random ( 0 , 1 ) &lt; Pm ) &#123; 对2个个体按变异概率Pm执行变异操作 &#125; 将2个新个体加入种群newPop中 &#125; until ( M个子代被创建 ) 用newPop取代Pop&#125;until ( 任何染色体得分超过Tf， 或繁殖代数超过G ) 基本遗传算法优化 下面的方法可优化遗传算法的性能。 精英主义(Elitist Strategy)选择：是基本遗传算法的一种优化。为了防止进化过程中产生的最优解被交叉和变异所破坏，可以将每一代中的最优解原封不动的复制到下一代中。 插入操作：可在3个基本操作的基础上增加一个插入操作。插入操作将染色体中的某个随机的片段移位到另一个随机的位置。 爬山算法 ( Hill Climbing )介绍模拟退火前，先介绍爬山算法。爬山算法是一种简单的贪心搜索算法，该算法每次从当前解的临近解空间中选择一个最优解作为当前解，直到达到一个局部最优解。 爬山算法实现很简单，其主要缺点是会陷入局部最优解，而不一定能搜索到全局最优解。如图1所示：假设C点为当前解，爬山算法搜索到A点这个局部最优解就会停止搜索，因为在A点无论向那个方向小幅度移动都不能得到更优的解。 模拟退火(SA,Simulated Annealing)爬山法是完完全全的贪心法，每次都鼠目寸光的选择一个当前最优解，因此只能搜索到局部的最优值。模拟退火其实也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法以一定的概率来接受一个比当前解要差的解，因此有可能会跳出这个局部的最优解，达到全局的最优解。以图1为例，模拟退火算法在搜索到局部最优解A后，会以一定的概率接受到E的移动。也许经过几次这样的不是局部最优的移动后会到达D点，于是就跳出了局部最大值A。 模拟退火算法描述： 若J( Y(i+1) )&gt;= J( Y(i) ) (即移动后得到更优解)，则总是接受该移动 若J( Y(i+1) )&lt; J( Y(i) ) (即移动后的解比当前解要差)，则以一定的概率接受移动，而且这个概率随着时间推移逐渐降低（逐渐降低才能趋向稳定） 这里的“一定的概率”的计算参考了金属冶炼的退火过程，这也是模拟退火算法名称的由来。 根据热力学的原理，在温度为T时，出现能量差为dE的降温的概率为P(dE)，表示为：P(dE) = exp( dE/(kT) ) 其中k是一个常数，exp表示自然指数，且dE&lt;0。这条公式说白了就是：温度越高，出现一次能量差为dE的降温的概率就越大；温度越低，则出现降温的概率就越小。又由于dE总是小于0（否则就不叫退火了），因此dE/kT &lt; 0 ，所以P(dE)的函数取值范围是(0,1) 。 随着温度T的降低，P(dE)会逐渐降低。 我们将一次向较差解的移动看做一次温度跳变过程，我们以概率P(dE)来接受这样的移动。 关于爬山算法与模拟退火，有一个有趣的比喻： 爬山算法：兔子朝着比现在高的地方跳去。它找到了不远处的最高山峰。但是这座山不一定是珠穆朗玛峰。这就是爬山算法，它不能保证局部最优值就是全局最优值。 模拟退火：兔子喝醉了。它随机地跳了很长时间。这期间，它可能走向高处，也可能踏入平地。但是，它渐渐清醒了并朝最高方向跳去。这就是模拟退火。 下面给出模拟退火的伪代码表示。 模拟退火算法伪代码12345678910111213141516171819202122232425262728代码/** J(y)：在状态y时的评价函数值* Y(i)：表示当前状态* Y(i+1)：表示新的状态* r： 用于控制降温的快慢* T： 系统的温度，系统初始应该要处于一个高温的状态* T_min ：温度的下限，若温度T达到T_min，则停止搜索*/while( T &gt; T_min )&#123; dE = J( Y(i+1) ) - J( Y(i) ) ; if ( dE &gt;=0 ) //表达移动后得到更优解，则总是接受移动Y(i+1) = Y(i) ; //接受从Y(i)到Y(i+1)的移动 else &#123;// 函数exp( dE/T )的取值范围是(0,1) ，dE/T越大，则exp( dE/T )也if ( exp( dE/T ) &gt; random( 0 , 1 ) )Y(i+1) = Y(i) ; //接受从Y(i)到Y(i+1)的移动 &#125; T = r * T ; //降温退火 ，0&lt;r&lt;1 。r越大，降温越慢；r越小，降温越快 /* * 若r过大，则搜索到全局最优解的可能会较高，但搜索的过程也就较长。若r过小，则搜索的过程会很快，但最终可能会达到一个局部最优值 */ i ++ ;&#125; 使用模拟退火算法解决旅行商问题 旅行商问题 ( TSP , Traveling Salesman Problem ) ：有N个城市，要求从其中某个问题出发，唯一遍历所有城市，再回到出发的城市，求最短的路线。 旅行商问题属于所谓的NP完全问题，精确的解决TSP只能通过穷举所有的路径组合，其时间复杂度是O(N!) 。 使用模拟退火算法可以比较快的求出TSP的一条近似最优路径。（使用遗传算法也是可以的，我将在下一篇文章中介绍）模拟退火解决TSP的思路： 产生一条新的遍历路径P(i+1)，计算路径P(i+1)的长度L( P(i+1) ) 若L(P(i+1)) &lt; L(P(i))，则接受P(i+1)为新的路径，否则以模拟退火的那个概率接受P(i+1) ，然后降温 重复步骤1，2直到满足退出条件 产生新的遍历路径的方法有很多，下面列举其中3种： 随机选择2个节点，交换路径中的这2个节点的顺序。 随机选择2个节点，将路径中这2个节点间的节点顺序逆转。 随机选择3个节点m，n，k，然后将节点m与n间的节点移位到节点k后面。 算法评价模拟退火算法是一种随机算法，并不一定能找到全局的最优解，可以比较快的找到问题的近似最优解。 如果参数设置得当，模拟退火算法搜索效率比穷举法要高。 原文]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程(4)]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B4%2F</url>
    <content type="text"><![CDATA[COMPLETED RESTORING OFF-LINE CONSISTENT DATABASE完成将片段副本还原到一致的全局检查点后，我们现在将根据还原的数据开始重建有序索引。 在重建有序索引之后，我们准备将START_RECCONF发送到起始DBDIH。 START_RECCONF通过DBLQH代理发送，因此在所有DBLQH实例完成此阶段并使用START_RECCONF响应之前，它不会传递到DBDIH。 此时，在DBLQH实例中，我们已恢复节点中所有数据的一致但旧的变体。 仍然没有有序索引，仍然有很多工作要让节点再次与其他节点同步。 对于群集重新启动，可能是节点已完全准备就绪，但是可能某些节点仍需要与已恢复更新的全局检查点的节点同步。 然后，起始节点的DBDIH将启动接管过程，因为起始节点具有一致的片段副本。 我们将通过为我们将复制的每个片段副本发送PREPARE_COPY_FRAG_REQ来为复制阶段准备起始节点的DBLQH。 这是一个可以并行化的顺序过程。 接管片段副本的过程非常复杂。 它首先将PREPARE_COPY_FRAGREQ / CONF发送到起始DBLQH，然后我们将UPDATE_TOREQ / CONF发送到主DBDIH，以确保我们在接管开始之前锁定片段信息。 在接收到该片段锁定的确认之后，起始节点向所有节点发送UPDATE_FRAG_STATEREQ / CONF以将新节点包括在片段上的所有操作中。 完成此操作后，我们再次向主节点发送UPDATE_TOREQ / CONF以通知新状态并解锁片段信息上的锁定。 然后我们准备执行片段的实际复制。 这是通过将COPY_FRAGREQ / CONF发送到将复制数据的节点来完成的。 完成此复制后，我们将COPY_ACTIVEREQ / CONF发送到起始节点以激活片段副本。 接下来，我们再次向主服务器发送UPDATE_TOREQ / CONF，通知我们即将安装接管新片段副本的提交。 接下来，我们通过向所有节点发送UPDATE_FRAG_STATEREQ / CONF来提交新的片段副本，通知它们片段副本的复制完成。 最后，我们使用UPDATE_TOREQ / CONF向主节点发送另一个更新。 现在我们终于完成了这个片段的复制。 这个方案的想法是第一个UPDATE_FRAG_STATEREQ确保我们是片段上所有事务的一部分。 在执行COPY_FRAGREQ以逐行地将起始节点的片段副本与活动节点的片段副本同步之后，我们确信两个片段副本完全同步，我们可以执行新的UPDATE_FRAG_STATEREQ以确保所有节点都知道我们 完成了同步。 COMPLETED RESTORING ON-LINE NOT RECOVERABLE DATABASE此时，我们通过在线一次添加一个片段来恢复数据库的在线变体。 数据库仍然无法恢复，因为我们尚未启用日志记录，并且起始节点中没有数据的本地检查点。 下一步是启用所有片段的日志记录，完成此步骤后，我们将END_TOREQ发送到主DBDIH。 此时，我们将等到本地检查点完成，其中涉及此节点。 最后，当本地检查点完成后，我们将END_TOCONF发送到起始节点，然后我们将发送START_COPYCONF，这将完成重启的这个阶段。 COMPLETED RESTORING ON-LINE RECOVERABLE DATABASE此时我们已经设法恢复所有数据，并且我们已经将它带到了在线状态，现在我们还在启用日志记录时执行了本地检查点，因此现在起始节点中的数据也是可恢复的。 所以这意味着数据库现在再次完全联机。 完成NDB_STTOR阶段5之后，此处等待点中已同步的所有节点将再次启动，NDBCNTR将继续运行NDB_STTOR的阶段6。 在此阶段DBLQH，DBDICT和DBTC设置一些状态变量，指示现在启动已完成（它尚未完全完成，但这些模块运行所需的所有服务都已完成.DBDIH还启动全局检查点协议以进行集群启动/重启 它已成为主节点。 现在，在群集启动/重启的情况下，所有节点还有一个等待点。 STTOR阶段5的最后一步是SUMA，它读取已配置的节点，获取节点组成员，如果有节点重新启动，它会要求另一个节点重新创建它的订阅。 STTOR Phase 6我们现在进入STTOR阶段6.在此阶段，NDBCNTR获取节点的节点组，DBUTIL获取systable id，准备一组操作供以后使用，并连接到TC以使其能够代表其他模块运行关键操作 稍后的。 STTOR Phase 7接下来我们进入STTOR阶段7.DBDICT现在启动索引统计循环，该循环将在节点存在时运行。 QMGR将启动仲裁处理，以处理我们面临网络分区风险的情况。 BACKUP将更新磁盘检查点速度（重启期间有一个配置变量用于速度，一个用于正常操作，这里我们安装正常运行速度）。 如果初始启动BACKUP也将通过DBUTIL创建备份序列。 如果SUMA在主节点中运行并且它是初始启动，它将创建一个序列。 SUMA还将始终计算它负责处理的桶。 最后，DBTUX将开始监控有序索引。 STTOR Phase 8然后我们转到STTOR阶段8.这里首先要运行NDB_STTOR的第7阶段，其中DBDICT启用外键。 如果我们正在进行集群启动/重启，下一个NDBCNTR也将等待所有节点到达此处。 下一个CMVMI将状态设置为STARTED，QMGR将启用与所有API节点的通信。 STTOR Phase 101在此阶段之后，唯一剩下的阶段是STTOR阶段101，其中SUMA接管它负责异步复制处理的桶的责任。 目前为止主要的潜在消费者：内存分配中的所有步骤（READ_CONFIG_REQ的所有步骤）。 CMVMI STTOR第1阶段可以锁定内存。 运行节点包含协议的QMGR阶段1。 NDBCNTR STTOR阶段2等待CNTR_START_REQ，DBLQH REDO日志初始化为STTOR阶段2中发生的初始启动类型。鉴于每次只有一个节点可以处于此阶段，这可能会被另一个节点的本地检查点等待停顿 开始。 所以这等待可能相当长。 DBLQH建立与DBACC和DBTUP的连接，这是NDB_STTOR阶段2.NDB_STTOR阶段2中的DBDIH也可以等待元数据被锁定，它可以等待对START_PERMREQ的响应。 对于初始启动，等待DBLQH完成NDB_STTOR阶段3，在此阶段初始化REDO日志的设置。 完成NDB_STTOR阶段3后，在STTOR阶段4中用于集群启动/重启的NDBCNTR必须等待所有节点到达此点，然后它必须等待NDB_STARTREQ完成。 对于节点重启，我们在等待对START_MEREQ信号和START_COPYREQ的响应时有延迟，这实际上是重启的大部分实际工作完成的地方。 重新订阅订阅的SUMA STTOR第5阶段是另一个潜在的时间消费者。 所有等待点都是潜在的时间消费者。 这些主要位于NDBCNTR（等待点5.2,5,1和6）。 Historical anecdotes: 1）NDB内核运行时环境最初是为AX虚拟机设计的。在AX中，开始使用模块MISSRA来驱动各种启动阶段的STTOR / STTORRY信号。 MISSRA后来被并入NDBCNTR，现在是NDBCNTR的子模块。 STTOR和STTORRY的名称在早期的AX系统命名信号方式中有一些基础，但现在已经被遗忘了。 ST至少可以通过启动/重启来完成任务。 2）引入NDB_STTOR的原因是我们设想了一个系统，其中NDB内核只是运行时环境中的一个子系统。因此，我们为NDB子系统引入了单独的启动阶段。随着时间的推移，对这种子系统启动阶段的需求不再存在，但软件已经为此设计，因此它以这种方式保存。 3）数据库启动的分布式部分的责任也是分开的。 QMGR负责发现节点何时上下。 NDBCNTR维护用于故障处理和节点配置的其他更改的协议。最后，DBDIH负责数据库部分的分布式启动。它与DBLQH交互很多，DBLQH负责按照DBDIH的指示启动一个节点数据库部分。 Local checkpoint processing in MySQL Cluster此注释试图描述MySQL Cluster中发生的检查点处理。 它还阐明了潜在的瓶颈所在。 此注释主要用作MySQL Cluster开源代码的内部文档。 MySQL Cluster中本地检查点的原因是为了确保我们在磁盘上有数据副本，可用于运行REDO日志以在崩溃后恢复MySQL Cluster中的数据。 我们首先在MySQL Cluster中引入不同的重启变体。第一个变体是正常节点重启，这意味着节点已经短时间丢失，但现在又重新上线。我们首先安装所有表的检查点版本（包括执行REDO日志的正确部分）。下一步是使用仍在线的副本使检查点版本保持最新。副本始终按节点组进行组织，节点组的最常见大小是两个节点。因此，当节点启动时，它使用同一节点组中的另一个节点来使在线版本的表重新联机。在正常的节点重启中，我们首先恢复了所有表的稍微旧版本，然后再使用其他节点进行同步。这意味着我们只需要发送自节点重启之前节点失败以来已更新的最新版本的行。我们还有初始节点重启的情况，其中所有数据都必须从另一个节点恢复，因为起始节点中的检查点太旧而无法重用，或者当一个全新的节点启动时它根本不存在。 重新启动的第三个变体是所谓的系统重启，这意味着整个群集在群集崩溃后或在群集受控停止后启动。 在此重新启动类型中，我们首先在运行REDO日志之前在所有节点上恢复检查点，以使系统处于一致且最新的状态。 如果任何节点还原到较旧的全局检查点而不是重新启动的节点，则必须使用节点重新启动中使用的相同代码将这些节点置于联机状态。 系统重启将恢复所谓的全局检查点。 一组事务被组合在一起成为一个全局检查点，当这个全局检查点完成后，属于它的事务是安全的并且将在集群崩溃后继续存在。 我们在第二级运行全局检查点，本地检查点将整个数据集写入磁盘，并且是一个耗时至少几分钟的较长过程。 在可以将起始节点声明为完全恢复之前，它必须参与本地检查点。 崩溃节点错过了恢复群集所需的一组REDO日志记录，因此节点未完全恢复，直到它可用于恢复系统重启时拥有的所有数据。 因此，当执行滚动节点重新启动时，群集中的所有节点都重新启动（例如，升级MySQL群集中的软件），一次重启一组节点是有意义的，因为我们只能在一个节点重新启动一组节点。 时间。 这是了解本地检查站需求的一个先决条件。 我们现在转到如何处理本地检查点的描述。 本地检查点是一个分布式进程。 它由名为DBDIH（简称DIH，DIstribution Handler）的软件模块控制。 DIH包含有关每个片段（与分区的同义词）的各种副本放置在何处以及这些副本上的各种数据的所有信息。 DIH将分发信息存储在每个表的一个文件中。 这个文件实际上是两个文件，这是为了确保我们可以仔细编写文件。 我们首先写文件0，当这个完成后，我们写文件1，这样我们就可以在编写表描述时轻松处理任何崩溃。 当本地检查点完成后，DIH立即启动该过程以启动下一个检查点。 自从我们启动新的本地检查点之前启动本地检查点以来，必须至少完成一个全局检查点。 下一个本地检查点的第一步是检查我们是否已准备好运行它。 这是通过将消息TCGETOPSIZEREQ发送到集群中的所有TC来执行的。 这将通过检查TC中收到的所有写入事务的信息来报告生成的REDO日志信息量。 该消息将由主DIH发送。 主服务器的角色被分配给最旧的幸存数据节点，这使得当目前充当主数据节点的数据节点死亡时，可以轻松选择新主服务器。 所有节点都同意进入群集的节点的顺序，因此节点的年龄在群集中的所有节点中都是一致的。 当所有消息都将REDO日志写入大小返回到主DIH时，我们将它与配置变量TimeBetweenLocalCheckpoints进行比较（此变量以大小的对数设置，因此例如25表示我们等待2 ^ 25个单词的REDO日志已创建于 该集群是128 MByte的REDO日志信息）。当生成足够数量的REDO日志时，我们启动下一个本地检查点，第一步是清除所有TC计数器，这是通过将TC_CLOPSIZEREQ发送到集群中的所有TC来完成的。 下一步是计算保持GCI（这是需要在REDO日志中保留的最早的全局检查点ID）。 这个数字非常重要，因为我们可以向前移动REDO日志的尾部。 如果我们用完REDO日志空间，我们将无法运行任何写入事务，直到我们启动下一个本地检查点，从而向前移动REDO日志尾部。 我们通过检查每个片段需要恢复的GCI来计算这个数字。 我们目前保留两个旧的本地检查点仍然有效，因此我们不会将GCI移回以使每个片段的两个最旧的本地检查点无效。 完成此计算后可恢复的GCI是循环遍历所有片段时发现的最小GCI。 接下来，我们在集群中所有节点的Sysfile中写下此编号和新的本地检查点ID以及其他一些信息。 在系统重新启动时开始恢复群集时，我们首先看到此Sysfile，因此在此文件中使此类信息正确非常重要。 完成此操作后，我们将计算将参与本地检查点的节点（当前执行重启的早期部分的节点不是本地检查点的一部分，显然也不是死节点）。 我们将有关起始本地检查点的信息发送给系统中的所有其他DIH。我们必须始终保持所有其他DIH的最新状态，以确保在主DIH崩溃或在本地检查点过程中停止时也很容易继续本地检查点。每个DIH记录参与本地检查点的节点集。他们还在每个副本记录上设置一个标志，指示本地检查点正在进行中，在每个片段记录上我们还设置了作为此本地检查点一部分的副本数。 现在我们已经完成了本地检查点的准备工作，现在是时候开始实际检查点写入实际数据了。主DIH通过为应检查点的每个片段副本发送LCP_FRAG_ORD来控制此过程。 DIH目前每个节点有2个这样的LCP_FRAG_ORD未完成，排队的2个片段副本。每个LDM线程可以一次处理一个片段副本的写入，并且可以对排队的下一个片段副本有一个请求。扩展此数字非常简单，以便可以并行写入更多的片段副本，并且可以对更多片段副本进行排队。 当片段副本的本地检查点完成时，LCP_FRAG_REP被发送到所有DIH。 当DIH发现表的所有片段副本都已完成本地检查点时，则应该将表描述写入文件系统。 这将记录所有片段副本的有趣的本地检查点信息。 有两件事可以导致这种情况等待。 首先编写和读取整个表描述只能一次发生一次，这主要发生在正在处理本地检查点时正在进行某些节点故障处理的情况。 可以阻止写表描述的第二件事是，目前最多可以并行写入4个表描述。 这可能很容易成为瓶颈，因为每次写入文件可能需要大约50毫秒。 所以这意味着我们目前每秒只能写出大约80个这样的表。 在具有许多表和少量数据的系统中，这可能成为瓶颈。 然而，它应该不是一个困难的瓶颈。 当主DIH已将所有请求发送到检查点所有片段副本时，它将向所有节点发送一个特殊的LCP_FRAG_ORD，指示不再发送任何片段副本。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字典树简介(Trie)]]></title>
    <url>%2Ftrie%2F</url>
    <content type="text"><![CDATA[在 NLP 中一般会用其存储大量的字典字符以用于文本的快速分词；除此之外，典型应用场景还包括大批量文本的：词频统计、字符串查询和模糊匹配（比如关键词的模糊匹配）、字符串排序等任务；由于 Trie 大幅降低了无谓的字符串比较，因此在执行上述任务时，其效率非常的高。 Trie 树的简介Trie 树中文名叫字典树、前缀树（个人比较喜欢这个名字，看完下文就会明白）等等。这些名字暗示其与字符的处理有关，事实也确实如此，它主要用途就是将字符串（当然也可以不限于字符串）整合成树形。我们先来看一下由“清华”、“清华大学”、“清新”、“中华”、“华人”五个中文词构成的 Trie 树形（为了便于叙述，下文提到该实例，以“例树”简称）： 这个树里面每一个方块代表一个节点，其中 ”Root” 表示根节点，不代表任何字符；紫色代表分支节点；绿色代表叶子节点。除根节点外每一个节点都只包含一个字符。从根节点到叶子节点，路径上经过的字符连接起来，构成一个词。而叶子节点内的数字代表该词在字典树中所处的链路（字典中有多少个词就有多少条链路），具有共同前缀的链路称为串。除此之外，还需特别强调 Trie 树的以下几个特点： 具有相同前缀的词必须位于同一个串内；例如“清华”、“清新”两个词都有“清”这个前缀，那么在 Trie 树上只需构建一个“清”节点，“华”和“新”节点共用一个父节点即可，如此两个词便只需三个节点便可存储，这在一定程度上减少了字典的存储空间。 Trie 树中的词只可共用前缀，不可共用词的其他部分；例如“中华”、“华人”这两个词虽然前一个词的后缀是后一个词的前缀，但在树形上必须是独立的两条链路，而不可以通过首尾交接构建这两个词，这也说明 Trie 树仅能依靠公共前缀压缩字典的存储空间，并不能共享词中的所有相同的字符；当然，这一点也有“例外”，对于复合词，可能会出现两词首尾交接的假象，比如“清华大学”这个词在上例 Trie 树中看起来似乎是由“清华”、“大学”两词首尾交接而成，但是叶子节点的标识已经明确说明 Trie 树里面只有”清华“和”清华大学“两个词，它们之间共用了前缀，而非由“清华”和”大学“两词首尾交接所得，因此上例 Trie 树中若需要“大学”这个词则必须从根节点开始重新构建该词。 Trie 树中任何一个完整的词，都必须是从根节点开始至叶子节点结束，这意味着对一个词进行检索也必须从根节点开始，至叶子节点才算结束。 搜索 Trie 树的时间复杂度在 Trie 树中搜索一个字符串，会从根节点出发，沿着某条链路向下逐字比对字符串的每个字符，直到抵达底部的叶子节点才能确认字符串为该词，这种检索方式具有以下两个优点： 公共前缀的词都位于同一个串内，查词范围因此被大幅缩小（比如首字不同的字符串，都会被排除）。 Trie 树实质是一个有限状态自动机（(Definite Automata, DFA），这就意味着从 Trie 树的一个节点（状态）转移到另一个节点（状态）的行为完全由状态转移函数控制，而状态转移函数本质上是一种映射，这意味着：逐字搜索 Trie 树时，从一个字符到下一个字符比对是不需要遍历该节点的所有子节点的。对于确定性有限自动机感兴趣的同学，可以看看以下引用： 1234567891011确定的有限自动机 M 是一个五元组：M = (Σ, Q, δ, q0, F)其中，Σ 是输入符号的有穷集合；Q 是状态的有限集合；δ 是 Q 与 Σ 的直积 Q × Σ 到Q (下一个状态) 的映射。它支配着有限状态控制的行为，有时也称为状态&gt;转移函数。q0 ∈ Q 是初始状态；F 是终止状态集合，F ⊆ Q；可以把DFA想象成一个单放机，插入一盘磁带，随着磁带的转动，DFA读取一个符号，依靠状态转移函数&gt;改变自己的状态，同时磁带转到下一个字符。 这两个优点相结合可以最大限度地减少无谓的字符比较，使得搜索的时间复杂度理论上仅与检索词的长度有关：O(m)，其中 m 为检索词的长度。 Trie 树的缺点综上可知， Trie 树主要是利用词的公共前缀缩小查词范围、通过状态间的映射关系避免了字符的遍历，从而达到高效检索的目的。这一思想有赖于字符在词中的前后位置能够得到表达，因此其设计哲学是典型的“以信息换时间”，当然，这种优势同样是需要付出代价的： 由于结构需要记录更多的信息，因此 Trie 树的实现稍显复杂。好在这点在大多数情况下并非不可接受。 Trie 型词典不仅需要记录词，还需要记录字符之间、词之间的相关信息，因此字典构建时必须对每个词和字逐一进行处理，而这无疑会减慢词典的构建速度。对于强调实时更新的词典而言，这点可能是致命的，尤其是采用双数组实现的 Trie 树，更新词典很大概率会造成词典的全部重构，词典构建过程中还需处理各种冲突，因此重构的时间非常长，这导致其大多用于离线；不过也有一些 Trie 可以实现实时更新，但也需付出一定的代价，因此这个缺点一定程度上影响了 Trie 树的应用范围。 公共前缀虽然可以减少一定的存储空间，但 Trie 树相比普通字典还需表达词、字之间的各种关系，其实现也更加复杂，因此实际空间消耗相对更大（大多少，得根据具体实现而定）。尤其是早期的“Array Trie”，属于典型的以空间换时间的实现，（其实 Trie 本身的实现思想是是以信息换时间，而非以空间换时间，这就给 Trie 树的改进提供了可能），然而 Trie 树现今已经得到了很好的改进，总体来说，对于类似词典这样的应用，Trie 是一个优秀的数据结构。 Trie 树的几种实现Array Trie 树很多文章里将这种实现称为“标准 Trie 树”，但其实它只是 Trie 众多实现中的一种而已，由于这种实现结构简单，检索效率很好，作为讲解示例很不错，因此特地改称其为“经典 Trie 树”，这里引用一下别人家的示例图： 如上图，abc、d、da、dda 四个字符串构成的 Trie 树，如果是字符串会在节点的尾部进行标记。没有后续字符的 branch 分支指向NULL。这种实现的特点是：每个节点都由指针数组存储，每个节点的所有子节点都位于一个数组之中，每个数组都是完全一样的。对于英文而言，每个数组有27个指针，其中一个作为词的终结符，另外 26 个依次代表字母表中的一个字母，对应指针指向下一个状态，若没有后续字符则指向NULL。由于数组取词的复杂度为O(1)，因此这种实现的 Trie 树效率非常的高，比如要在一个节点中写入字符“c”,则直接在相应数组的第三个位置标入状态即可，而要确定字母“b”是否在现有节点的子节点之中，检查子节点所在数组第二个元素是否为空即可，这种实现巧妙的利用了等长数组中元素位置和值的一一对应关系，完美的实现了了寻址、存值、取值的统一。但其缺点也很明显，它强制要求链路每一层都要有一个数组，每个数组都必须等长，这在实际应用中会造成大多数的数组指针空置（从上图就可以看出），事实上，对于真实的词典而言，公共前缀相对于节点数量而言还是太少，这导致绝大多数节点下并没有太多子节点。而对于中文这样具有大量单字的语言，若采取这样的实现，空置指针的数量简直不可想象。因此，经典 Trie 树是一种典型的以“空间换时间”的实现方式。一般只是拿来用于课程设计和新手练习，很少实际应用。 List Trie 树由于数组的长度是不可变，因此经典 Trie 树存在着明显的空间浪费。但是如果将每一层都换成可变数组（不同语言对这种数据结构称呼不同，比如在 Python 中为List，C# 称为 LinkedList）来存储节点（见下图），每层可以根据节点的数量动态调整数组的长度，就可以避免大量的空间浪费。 但是可变长数组的取词复杂度是O(d),其中 d 为数组的长度，这意味着状态转移函数无法通过映射转移到下一节点，必须先遍历数组，找到节点后再做转移，因此Trie 树实际时间复杂度变为O(m*n)(其中n为每层数组中节点的数量)。这显然降低了查询效率,因此还算不上完善。 Hash Trie 树可变数组取词速度太慢，于是就有人想起用一组键值对（Java中可用HashMap类型，Python 中为 dict 类型，C#为Dictionary类型）代替可变数组：其中每个节点包含一组 Key-Value，每个 Key 对应该节点下的一个子节点字符，value 则指向相应的后一个状态。这种方式可以有效的减少空间浪费，同时由于键值对本质上就是一个哈希实现，因此理论上其查词效率也很高（理想状态下取词复杂度为O(1)）。 但是哈希有的缺点，这种实现的 Trie 树也会有： 为了尽可能的避免键值冲突，哈希表需要额外的空间避开碰撞，因此仍有一部分的空间会被浪费； 哈希表很难做到完美，尤其是数据体量增大之后，其查词复杂度常常难以维持在O(1)，同时，对哈希值的计算也需要额外的时间，因此实际查询效率要比经典实现低，其具体复杂度由相应的哈希实现来定。 与数组和可变数组实现相比，这种实现做到了空间和时间上的一种平衡，这个结果并不意外，因为哈希表本身就是平衡数组（查寻迅速、增删悲剧）和可变数组（增删迅速，查询悲剧）相应优点和缺点的一种数据结构。总体而言，Hash Trie 结构简单，性能堪用，而且由于哈希实现可以为每个节点分配唯一的id,因此可以做到节点的实时动态添加（这点是非常大的优势）因此对于中小规模的词典或者对词典的实时更新有需求的应用，该实现非常适合。 Double-array Trie 树双数组 Trie 树是目前 Trie 树各种实现中性能和存储空间均达到很好效果的实现。但其完整的实现比较复杂，对于新手而言入手相对较难，因此本节将花费较多的篇幅对其解读。 Base Array 的作用双数组 Trie 树和经典 Trie 树一样，也是用数组实现 Trie 树。只不过它是将所有节点的状态都记录到一个数组之中（Base Array），以此避免数组的大量空置。以行文开头的示例为例，每个字符在 Base Array 中的状态可以是这样子的： 为了能使单个数组承载更多的信息，Base Array 仅仅会通过数组的位置记录下字符的状态（节点），比如用数组中的位置 2 指代“清”节点、 位置 7 指代 “中”节点；而数组中真正存储的值其实是一个整数，这个整数我们称之为“转移基数”，比如位置2的转移基数为 base[2]=3位置7的转移基数为base[7]=2，因此在不考虑叶子节点的情况下， Base Array 是这样子的： 转移基数是为了在一维数组中实现 Trie 树中字符的链路关系而设计的，举例而言，如果我们知道一个词中某个字符节点的转移基数，那么就可以据此推断出该词下一个节点在 Base Array 中的位置：比如知道 “清华”首字的转移基数为base[2]=3，那么“华”在数组中的位置就为base[2]+code(&quot;华&quot;)，这里的code(&quot;华&quot;)为字符表中“华”的编码，假设例树的字符编码表为： 清-1，华-2，大-3，学-4，新-5，中-6，人-7 那么“华”的位置应该在Base Array 中的的第 5 位（base[2]+code(&quot;华&quot;)=3+2=5）： 而所有词的首字，则是通过根节点的转移基数推算而来。因此，对于字典中已有的词，只要我们每次从根节点出发，根据词典中各个字符的编码值，结合每个节点的转移基数，通过简单的加法，就可以在Base Array 中实现词的链路关系。以下是“清华”、“清华大学”、“清新”、“中华”、“华人”五个词在 Base Array 中的链路关系： Base Array 的构造可见 Base Array 不仅能够表达词典中每个字符的状态，而且还能实现高效的状态转移。那么，Base Array 又是如何构造的呢？ 事实上，同样一组词和字符编码，以不同的顺序将字符写入 Trie 树中，获得的 Base Array 也是不同的，以“清华”、“清华大学”、“清新”、“中华”、“华人”五个词，以及字符编码：[清-1，华-2，大-3，学-4，新-5，中-6，人-7] 为例，在不考虑叶子节点的情况下，两种处理方式获得的 base array 为： 首先依次处理“清华”、“清华大学”、“清新”、“中华”、“华人”五个词的首字，然后依次处理所有词的第二个字…直到依次处理完所有词的最后一个字，得到的 Base Array 为： 依次处理“清华”、“清华大学”、“清新”、“中华”、“华人”五个词中的每个字，得到的 Base Array 为： 可以发现，不同的字符处理顺序，得到的 Base Array 存在极大的差别：两者各状态的转移基数不仅完全不同，而且 Base Array 的长度也有差别。然而，两者获得的方法却是一致的，下面以第一种字符处理顺序讲解一下无叶子节点的 Base Array 构建： 首先人为赋予根节点的转移基数为1（可自定义，详见下文），然后依次将五个词中的首字”清”、“中”、“华”写入数组之中，写入的位置由base[1]+code(字符)确定，每个位置的转移基数（base[i]）等于上一个状态的转移基数（此例也即base[1]），这个过程未遇到冲突，最终结果见下图： 然后依次处理每个词的第二个字，首先需要处理的是“清华”这个词的“华”字，程序先从根节点出发，通过base[1]+code(“清”)=2找到“清”节点，然后以此计算“华”节点应写入的位置，通过计算base[2]+code(“华”)=3寻找到位置 3,却发现位置3已有值，于是后挪一位，在位置4写入“华”节点，由于“华”节点未能写入由前驱节点“清”预测的位置，因此为了保证通过“清”能够找到“华”，需要重新计算“清”节点的转移基数，计算公式为4-code(“华”)=2,获得新的转移基数后，改写“清”节点的转移基数为2，然后将“华”节点的转移基数与“清”节点保持一致，最终结果为： 重复上面的步骤，最终获得整个 Base Array： 通过以上步骤，可以发现 base array 的构造重点在于状态冲突的处理，对于双数组 Trie 而言，词典构造过程中的冲突是不可避免的，冲突的产生来源于多词共字的情况，比如“中华”、“清华”、“华人”三个词中都有“华”，虽然词在 Trie 树中可以共用前缀，但是对于后缀同字或者后缀与前缀同字的情况却只能重新构造新的节点，这势必会导致冲突。一旦产生冲突，那么父节点的转移基数必须改变，以保证基于前驱节点获得的位置能够容纳下所有子节点（也即保证 base[i]+code(n1)、base[i]+code(n2)、base[i]+code(n3)….都为空，其中n1、n2、n3...为父节点的所有子节点字符，base[i]为父节点新的转移基数，i为父节在数组中的位置）这意味着其他已经构造好的子节点必须一并重构。 因此，双数组 Trie 树的构建时间比较长，有新词加入，运气不好的话，还可能能导致全树的重构：比如要给词典添加一个新词，新词的首字之前未曾写入过，现在写入时若出现冲突，就需要改写根节点的转移基数，那么之前构建好的词都需要重构（因为所有词的链路都是从根节点开始）。上例中，第二种字符写入顺序就遇到了这个问题，导致在词典构造过程中，根节点转移基数被改写了两次，全树也就被重构了三次： 可见不同的节点构建顺序，对 Base Aarry 的构建速度、空间利用率都有影响。建议实际应用中应首先构建所有词的首字，然后逐一构建各个节点的子节点，这样一旦产生冲突，可以将冲突的处理局限在单个父节点和子节点之间，而不至于导致大范围的节点重构。 叶子节点的处理上面关于 Base Array 的叙述，只涉及到了根节点、分支节点的处理，事实上，Base Array 同样也需要负责叶子节点的表达，但是由于叶子节点的处理，具体的实现各不一致，因此特地单列一节予以论述。 一般词的最后一个字都不需要再做状态转移，因此有人建议将词的最后一个节点的转移基数统一改为某个负数（比如统一设置为-2），以表示叶子节点，按照这种处理，对于示例而言，base array 是这样的： 但细心的童鞋可能会发现，“清华” 和 “清华大学” 这两个词中，只有“清华大学”有叶子节点，既是公共前缀又是单个词的“清华”实际上无法用这种方法表示出叶子节点。 也有人建议为词典中所有的词增加一个特殊词尾（比如将“清华”这个词改写为“清华\0”），再将这些词构建为树，特殊字符词尾节点的转移基数统一设置设为-2，以此作为每个词的叶子节点[4]。这种方法的好处是不用对现有逻辑做任何改动，坏处是增加了总节点的数量，相应的会增加词典构建的时长和空间的消耗。 最后，个人给出一个新的处理方式：直接将现有 base array 中词尾节点的转移基数取负，而数组中的其他信息不用改变。 以树例为例，处理叶子节点前，Base Array 是这样子的： 处理叶子节点之后，Base Array 会是这样子的： 每个位置的转移基数绝对值与之前是完全相同的，只是叶子节点的转移基数变成了负数，这样做的好处是：不仅标明了所有的叶子节点，而且程序只需对状态转移公式稍加改变，便可对包括“清华”、“清华大学”这种情况在内的所有状态转移做一致的处理，这样做的代价就是需要将状态转移函数base[s]+code(字符)改为|base[s]|+code(字符)，意味着每次转移需要多做一次取绝对值运算，不过好在这种处理对性能的影响微乎其微。 对此，其他童鞋若有更好的想法， 欢迎在底部留言！ Check Array 的构造“双数组 Trie 树”，必定是两个数组，因此单靠 Base Array 是玩不起来的….上面介绍的 Base Array 虽然解决了节点存储和状态转移两个核心问题，但是单独的 Base Array 仍然有个问题无法解决： Base Array 仅仅记录了字符的状态，而非字符本身，虽然在 Base Array，字典中已有的任意一个词，其链路都是确定的、唯一的，因此并不存在歧义；但是对于一个新的字符串（不管是检索字符串还是准备为字典新增的词），Base Array 是不能确定该词是否位于词典之中的。对于这点，我们举个例子就知道了： 如果我们要在例树中确认外部的一个字符串“清中”是否是一个词，按照 Trie 树的查找规则，首先要查找“清”这个字，我们从根节点出发，获得|base[1]|+code(“清”)=3，然后转移到“清”节点，确认清在数组中存在，我们继续查找“中”，通过|base[3]|+code(“中”)=9获得位置9，字符串此时查询完毕，根据位置9的转移基数base[9]=-2确定该词在此终结，从而认为字符串“清中”是一个词。而这显然是错误的！事实上我们知道 “清中”这个词在 base array 中压根不存在，但是此时的 base array 却不能为此提供更多的信息。 为了解决这些问题，双数组 Trie 树专门设计了一个 check 数组： check array 与 base array 等长，它的作用是标识出 base array 中每个状态的前一个状态，以检验状态转移的正确性。 因此， 例树的 check array 应为： 如图，check array 元素与 base array 一一对应，每个 check array 元素标明了base array 中相应节点的父节点位置，比如“清”节点对应的check[2]=0，说明“清”节点的父节点在 base array 的0 位（也即根节点）。对于上例，程序在找到位置9之后，会检验 check[9]==2，以检验该节点是否与“清”节点处于同一链路，由于check[9]!=2，那么就可以判定字符串“清中”并不在词典之中。 综上，check array 巧妙的利用了父子节点间双向关系的唯一性（公式化的表达就是base[s]+c=t &amp; check[t]=s是唯一的，其中 s为父节点位置，t为子节点位置），避免了 base array 之中单向的状态转移关系所造成的歧义（公式化的表达就是base[s]+c=t）。 Trie 树的压缩双数组 Trie 树虽然大幅改善了经典 Trie 树的空间浪费，但是由于冲突发生时，程序总是向后寻找空地址，导致数组不可避免的出现空置，因此空间上还是会有些浪费。另外， 随着节点的增加，冲突的产生几率也会越来越大，字典构建的时间因此越来越长，为了改善这些问题，有人想到对双数组 Trie 进行尾缀压缩，具体做法是：将非公共前缀的词尾合并为一个节点（tail 节点），以此大幅减少节点总数，从而改善树的构建速度；同时将合并的词尾单独存储在另一个数组之中（Tail array）， 并通过 tail 节点的 base 值指向该数组的相应位置，以 {baby#, bachelor#, badge#, jar# }四词为例，其实现示意图如下： 对于这种改进的效果，看一下别人家的测试就知道了： 速度减少了base， check的状态数，以及冲突的概率，提高了插入的速度。在本地做了一个简单测试，随机插入长度1-100的随机串10w条，no tail的算法需120秒，而tail的算法只需19秒。查询速度没有太大差别。 内存状态数的减少的开销大于存储tail的开销，节省了内存。对于10w条线上URL，匹配12456条前缀，内存消耗9M，而no tail的大约16M 删除能很方便的实现删除，只需将tail删除即可。 对于本文的例树，若采用tail 改进，其最终效果是这一子的： 总结 Trie 树是一种以信息换时间的数据结构，其查询的复杂度为O(m) Trie 的单数组实现能够达到最佳的性能，但是其空间利用率极低，是典型的以空间换时间的实现 Trie 树的哈希实现可以很好的平衡性能需求和空间开销，同时能够实现词典的实时更新 Trie 树的双数组实现基本可以达到单数组实现的性能，同时能够大幅降低空间开销；但是其难以做到词典的实时更新 对双数组 Trie 进行 tail 改进可以明显改善词典的构建速度，同时进一步减少空间开销 原文]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程(3) START OF DATABASE RECOVERY]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B3%2F</url>
    <content type="text"><![CDATA[LCP：本地检查点，在NDB中，这意味着主内存中的所有数据都写入磁盘，我们还将更改的磁盘页写入磁盘，以确保磁盘上某个点之前的所有更改都可用。 执行REDO日志：这意味着我们一次读取REDO日志一条REDO日志记录，并在需要时执行REDO日志记录中的操作。 Apply the REDO log：执行REDO日志的同义词。 Prepare REDO log record: 这是一个REDO日志记录，包含有关数据库更改的信息（插入/删除/更新/写入）。 COMMIT REDO log：这是一个REDO日志记录，指定要实际执行Prepare REDO日志记录。 COMMIT REDO日志记录包含Prepare REDO日志记录的后向引用。 ABORT REDO log record：与COMMIT REDO日志记录类似，但此处事务已中止，因此无需应用REDO日志记录。 Database:在此上下文中表示当节点重新启动时驻留在集群或节点中的所有数据。 Off-line Database:意味着我们节点中的数据库不在线，因此不能用于读取。 这是恢复LCP后但在应用REDO日志之前的数据库状态。 Off-line Consistent database:这是一个数据库状态，它与最新的更改不是最新的，但它表示先前存在的数据库中的旧状态。 恢复LCP并执行REDO日志后，即可实现此状态。 On-line Database:这是一个最新的数据库状态，任何可用于读取数据的节点都有其数据库在线（实际上片段是逐个联机的）。 On-line Recoverable Database:这是一个也可以恢复的在线数据库。 在节点重启中，我们首先到达状态在线数据库，但是我们需要运行LCP，然后数据库也可以恢复到其当前状态。 可恢复的数据库也是持久的，这意味着当我们达到此状态时，我们将ACID中的D添加到数据库中。 Node:有API节点，数据节点和管理服务器节点。 数据节点是ndbd / ndbmtd进程，它运行所有数据库逻辑并包含数据库数据。 管理服务器节点是运行包含群集配置的ndb_mgmd并且还执行许多管理服务的进程。 API节点是应用程序进程的一部分，在mysqld中。 每个应用程序进程可以有多个API节点。 每个API节点通过套接字（或其他通信介质）连接到每个数据节点和管理服务器节点。 当一个人引用本文中的节点时，我们主要暗示的是我们在谈论一个数据节点。 Node Group:一组数据节点，它们都包含相同的数据。 节点组中的节点数等于我们在集群中使用的副本数。 Fragment:表的一部分，完全存储在一个节点组中。 Partition:Synonym of fragment. Fragment replica:这是一个节点中的一个片段。 一个片段最多可以有4个副本（因此节点组中最多可以有4个节点）。 Distribution information: 这是有关表的分区（片段的同义词）以及它们驻留在哪些节点上的信息，以及有关在每个片段副本上执行的LCP的信息。 Metadata:这是有关表，索引，触发器，外键，哈希映射，文件，日志文件组，表空间的信息。 字典信息：元数据的同义词。 LDM:代表本地数据管理器，这些是执行处理一个数据节点内处理的数据的代码的块。 它包含处理元组存储的块，哈希索引，T树索引，页面缓冲区管理器，表空间管理器，写入LCP的块和恢复LCP的块，磁盘数据的日志管理器。 作为START_COPYREQ的一部分，真正的数据库恢复过程是什么。 这里执行大多数重要的数据库恢复算法以使数据库再次联机。 仍需要早期阶段来恢复元数据和设置通信，设置内存并将起始节点作为数据节点集群中的完整公民。 START_COPYREQ遍历所有分发信息，并将START_FRAGREQ发送到拥有的DBLQH模块实例，以便在节点上恢复每个片段副本。 DBLQH将立即启动以恢复这些片段副本，它将对片段副本进行排队并一次恢复一个。 这发生在两个阶段，首先需要恢复本地检查点的所有片段副本开始这样做。 在发送了所有要恢复的片段副本之后，我们已经从存储在磁盘上的本地检查点恢复了所有片段（或者有时通过从活动节点获取整个片段），然后是运行磁盘数据UNDO日志的时候了。 最后，在运行此UNDO日志之后，我们已准备好通过应用REDO日志将片段副本恢复到最新的磁盘持久状态。 DBDIH将所有片段副本的所有必需信息发送到DBLQH，然后它将START_RECREQ发送到DBLQH以指示现在已发送所有片段副本信息。 START_RECREQ通过DBLQH代理模块发送，并且该部分被并行化，以便所有LDM实例并行执行以下部分。 如果我们正在进行初始节点重启，我们不需要恢复任何本地检查点，因为初始节点重启意味着我们在没有文件系统的情况下启动。 所以这意味着我们必须从节点组中的其他节点恢复所有数据。 在这种情况下，当我们收到START_FRAGREQ时，我们立即开始在DBLQH中应用片段副本的复制。 在这种情况下，我们不需要运行任何撤消或重做日志，因为没有本地检查点来恢复片段。 完成此操作后，DBDIH报告已通过将START_RECREQ发送到DBLQH发送了所有要启动的片段副本，我们将向TSMAN发送START_RECREQ，之后我们将完成数据的恢复。 我们将指定要作为REDO日志执行的一部分进行恢复的所有片段副本。 这是通过信号EXEC_FRAGREQ完成的。 当所有这些信号都被发送后，我们发送EXEC_SRREQ表示我们已经准备好在DBLQH中执行下一个REDO日志执行阶段。 当发送所有这些信号时，我们已经完成了所谓的DBLQH的第2阶段，DBLQH中的阶段1是在NDB_STTOR阶段3中开始准备REDO日志以进行读取。 因此，当这两个阶段都完成时，我们就可以开始在DBLQH中开始所谓的阶段3。 这些DBLQH阶段与启动阶段无关，这些阶段是DBLQH模块中启动的内部阶段。 DBLQH中的第3阶段是读取REDO日志并将其应用于从本地检查点恢复的片段副本。 这是创建在特定全局检查点上同步的数据库状态所必需的。 因此，我们首先为所有片段安装本地检查点，接下来我们应用REDO日志将片段副本与某个全局检查点同步。 在执行REDO日志之前，我们需要通过检查我们将恢复到所需全局检查点的所有片段副本的限制来计算要在REDO日志中应用的起始GCI和最后一个GCI。 DBDIH已存储有关片段副本的每个本地检查点的信息，这些信息是从REDO日志运行所需的全局检查点范围，以使其进入某个全局检查点的状态。 此信息已在START_FRAGREQ信号中发送。 DBLQH会将每个片段副本的所有这些限制合并到全局范围的全局检查点，以便为此LDM实例运行。 因此每个片段副本都有自己的GCP id范围来执行，这意味着所有这些起始范围的最小值和所有结束范围的最大值是我们需要在REDO日志中执行以引入集群的全局GCP ID范围 再次在线。 下一步是使用start和stop全局检查点id计算每个日志部分的REDO日志中的开始和停止兆字节。 计算这个所需的所有信息已经在内存中，所以这是一个纯粹的计算。 当我们执行REDO日志时，实际上我们只在正确的全局检查点范围内应用COMMIT记录。 COMMIT记录和实际更改记录位于REDO日志中的不同位置，因此对于每兆字节的REDO日志，我们记录REDO日志中我们必须返回多长时间才能找到更改记录。 在运行REDO日志时，我们维护一个相当大的REDO日志缓存，以避免在事务运行很长时间的情况下我们必须执行磁盘读取。 这意味着长时间运行和大型事务可能会对重新启动时间产生负面影响。 在所有日志部分完成此计算后，我们现在准备开始执行REDO日志。 在执行REDO日志完成后，我们还会在REDO日志中写入一些内容，以表明我们之前使用的任何信息都不会在以后使用。 我们现在需要等待所有其他日志部分也完成其REDO日志部分的执行。 REDO日志执行的设计使我们可以在多个阶段执行REDO日志，这适用于我们可以从多个活动节点重建节点的情况。 目前绝不应使用此代码。 因此，下一步是检查REDO日志部件的新头部和尾部。 这是通过使用启动和停止全局检查点来计算此数字的相同代码完成的。 代码的这一阶段还通过确保正确的REDO日志文件打开来准备REDO日志部分以编写新的REDO日志记录。 它还涉及一些相当棘手的代码，以确保正确处理已脏的页面。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程 (2) READ_CONFIG_REQ]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B2%2F</url>
    <content type="text"><![CDATA[READ_CONFIG_REQ对所有软件模块或多或少都相同。 它分配软件模块所需的内存并初始化内存（创建各种空闲列表等）。 它还读取模块感兴趣的各种配置参数（这些参数通常会影响我们分配的内存大小）。原文 它从CMVMI开始，分配大部分全局内存池，接下来我们有NDBFS为磁盘数据创建必要的文件目录，它还创建一次可以由一个文件使用的绑定IO线程（初始线程数可配置） 通过InitalNoOpenFiles），然后它创建了一些磁盘数据文件（用于处理磁盘数据的所有文件）使用的空闲线程（它们可通过IOThreadPool配置的数量），每个这样的线程可用于打开/读取/写入/关闭 磁盘数据文件。 最后，NDBFS还会创建从文件系统线程返回到其他线程的通信通道。 所有其他模块遵循相同的标准，它们根据硬编码定义或通过配置变量计算多个大小，它们为这些变量分配内存，最后它们初始化那些分配的内存结构。 STTOR Phase 0执行的第一个STTOR阶段是STTOR阶段0.在此阶段执行任何操作的唯一模块是NDBCNTR，如果启动是初始启动，则清除文件系统，CMVMI创建文件系统目录。 STTOR Phase 1下一阶段执行的是STTOR阶段1，在此阶段，大多数模块初始化一些更多数据，必要时设置对相邻模块的引用。 此外，DBDIH创建了一些特殊的互斥锁，确保一次只能在代码的某些部分中涉及一个进程。 NDBCNTR在阶段2开始初始化与运行NDB_STTOR相关的一些数据。如果配置为这样，CMVMI将锁定内存，此后它会安装正常的监视程序超时，因为现在执行所有大内存分配。 CMVMI还启动定期内存报告。 QMGR是此阶段中最活跃的模块。 它初始化一些数据，它从DBDIH获得重启类型（初始启动或正常启动），它打开与集群中所有节点的通信，它开始检查包含节点处理的节点故障。 最后，它运行协议以将新节点包括在心跳协议中。 这可能需要一段时间，因为节点包含过程一次只能引入一个节点，并且协议包含一些延迟。然后，BACKUP模块启动磁盘速度检查循环，该循环将在节点启动并运行时运行。 STTOR Phase 2下一步是执行STTOR阶段2.在STTOR阶段2中执行任何操作的唯一模块是NDBCNTR，它要求DIH执行重启类型，它从配置中读取节点，它初始化控制多长时间的部分超时变量 在我们执行部分启动之前等待。 NDBCNTR将信号CNTR_START_REQ发送到当前主节点中的NDBCNTR，该信号使主节点能够在必要时由于其他起始节点或某些其他条件而延迟该节点的启动。 对于集群启动/重启，它还使主节点有机会确保在启动节点之前等待足够的节点启动。 主设备一次只接受一个接收CNTR_START_CONF的节点，下一个节点只能在前一个起始节点完成复制元数据并释放元数据锁并锁定DIH信息后接收CNTR_START_CONF，这在STTOR阶段5中发生。 因此，在滚动重启中，第一个节点将获得CNTR_START_CONF，然后在DICT锁定上被阻塞，等待LCP完成，这是很常见的。 并行启动的其他节点将在CNTR_START_CONF上等待，因为一次只有一个节点可以通过它。 收到CNTR_START_CONF后，NDBCNTR继续运行NDB_STTOR阶段1.此处DBLQH初始化节点记录，它启动报告服务。 它还初始化有关REDO日志的数据，这还包括在所有类型的初始启动时初始化磁盘上的REDO日志（可能非常耗时）。 DBDICT初始化模式文件（包含已在集群中创建的表和其他元数据对象）。 DBTUP初始化一个默认值片段，DBTC和DBDIH初始化一些数据变量。 完成NDBCNTR中的NDB_STTOR阶段后，STTOR阶段2中没有更多工作。 STTOR Phase 3下一步是运行STTOR阶段3.大多数需要集群中节点列表的模块在此阶段读取此信息。 DBDIH在此阶段读取节点，DBDICT设置重启类型。 下一个NDBCNTR接收此阶段并启动NDB_STTOR阶段2.在此阶段，DBLQH设置从其操作记录到DBACC和DBTUP中的操作记录的连接。 这是针对所有DBLQH模块实例并行完成的。 DBDIH现在通过锁定元数据来准备节点重启过程。 这意味着我们将等待任何正在进行的元数据操作完成，并且当它完成时，我们将锁定元数据，这样在我们完成复制元数据信息的阶段之前，不能进行元数据更改。 锁定的原因是所有元数据和分发信息都是完全复制的。 因此，在将数据从主节点复制到起始节点时，我们需要锁定此信息。 虽然我们保留了此锁，但我们无法通过元数据事务更改元数据。 在稍后复制元数据之前，我们还需要确保没有运行本地检查点，因为这也会更新分发信息。 锁定后，我们需要请求从主节点启动节点的权限。 启动节点的许可请求由发送START_PERMREQ到主节点的起始节点处理。 如果另一个节点已在处理节点重启，则可能会收到否定答复，如果需要初始启动，则可能会失败。 如果另一个节点已经启动，我们将等待3秒钟再试一次。 这在DBDIH中作为NDB_STTOR阶段2的一部分执行。 在完成NDB_STTOR阶段2之后，STTOR阶段3继续由CMVMI模块激活对扫描和密钥操作使用的发送打包数据的检查。 接下来，BACKUP模块读取配置的节点。 接下来，SUMA模块设置对页面池的引用，以便它可以重用此全局内存池中的页面，然后DBTUX设置重新启动类型。 最后，PGMAN启动一个stats循环和一个清理循环，只要节点启动并运行，它就会运行。 如果我们的节点仍然涉及主节点中正在进行的某些进程，我们可能会崩溃节点。 这是相当正常的，只会触发一次崩溃，然后是天使进程正常的新启动。 权限请求由主服务器向所有节点发送信息来处理。 对于初始启动，权限请求可能非常耗时，因为我们必须使所有节点上元数据中所有表的所有本地检查点无效。 目前没有此失效过程的并行化，因此它将一次使一个表无效。 STTOR Phase 4完成STTOR阶段3后，我们进入STTOR阶段4.此阶段由DBLQH在BACKUP模块中获取备份记录开始，该备份记录将用于本地检查点处理。 下一个NDBCNTR启动NDB_STTOR阶段3.这也在DBLQH中开始，我们在其中读取已配置的节点。 然后我们开始阅读REDO日志以进行设置（我们将在后台设置它，它将通过稍后描述的集群重启/节点重启的另一部分进行同步），对于所有类型的初始启动，我们将等到 REDO日志的初始化已经完成，直到报告此阶段完成为止。 下一个DBDICT将读取已配置的节点，然后DBTC将读取已配置的节点并启动事务计数器报告。 接下来在NDB_STTOR阶段3中，DBDIH初始化重启数据以进行初始启动。 在完成STTOR第4阶段的工作之前，NDBCNTR将设置一个等待点，使所有起始节点在继续之前达到此点。 这仅适用于群集启动/重新启动，因此不适用于节点重新启动。 主节点控制此等待点，并在集群重启的所有节点都达到此点时将信号NDB_STARTREQ发送到DBDIH。 稍后将详细介绍此信号。 STTOR阶段4中发生的最后一件事是DBSPJ读取配置的节点。 STTOR Phase 5我们现在进入STTOR阶段5.这里做的第一件事是运行NDB_STTOR阶段4.只有DBDIH在这里做了一些工作，它只在节点重启时做了一些事情。 在这种情况下，它要求当前主节点通过向其发送START_MEREQ信号来启动它。 START_MEREQ的工作原理是从主DBDIH节点复制分发信息，然后从主DBDICT复制元数据信息。 它一次复制一个分发信息表，这使得该过程有点慢，因为它包括将表写入起始节点中的磁盘。 跟踪此事件的唯一方法是在起始节点中的DBDIH中将每个表的表分布信息写入。 我们可以跟踪在起始节点DBDICT中接收的DICTSTARTREQ的接收。 当复制DBDIH和DBDICT信息时，我们需要阻止全局检查点，以便从现在开始将新节点包含在元数据和分发信息的所有更改中。 这是通过将INCL_NODEREQ发送到所有节点来执行的。 在此之后，我们可以释放由DBDIH在STTOR阶段2中设置的元数据锁。 完成NDB_STTOR阶段4后，NDBCNTR以下列方式再次同步启动： 如果初始集群启动和主节点然后创建系统表如果集群启动/重新启动，则等待所有节点到达此点。 等待集群启动/重启中的节点后，在主节点中运行NDB_STTOR阶段5（仅发送到DBDIH）。 如果节点重新启动，则运行NDB_STTOR阶段5（仅发送到DBDIH）。 DBDIH中的NDB_STTOR阶段5正在等待本地检查点的完成（如果它是主设备）并且我们正在运行集群启动/重启。 对于节点重启，我们将信号START_COPYREQ发送到起始节点，要求将数据复制到我们的节点。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程 (1) Restart Phases in MySQL Cluster]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B1%2F</url>
    <content type="text"><![CDATA[在MySQL Cluster中，重新启动是分阶段处理的，节点的重启由一组阶段驱动。此外，节点重启也与已启动的节点以及与我们的节点并行启动的其他节点同步。此注释将描述所使用的各个阶段。 原文启动节点的第一步是创建数据节点运行时环境。数据节点进程通常使用angel进程运行，此angel进程确保数据节点万一失败时能够自动重新启动。因此，再次运行数据节点的唯一原因是在OS崩溃之后或运营商关闭之后或作为软件升级的一部分。 当启动数据节点时，数据节点需要一个节点id，这是通过设置参数–ndb-nodeid在启动datanode时，或者在检索配置时由管理服务器分配。对于数据节点的所有重新启动，angel进程将确保所分配的节点id将相同。 在分配数据节点进程之后，启动进程保持为angel进程并且新进程成为实际的数据节点进程。实际的数据节点进程首先从managementserver检索配置。 在这个阶段我们已经阅读了选项，我们已经分配了一个节点ID，我们从管理服务器加载了配置。我们将在数据节点日志中打印一些关于我们的线程配置和其他一些重要信息。为了确保我们找到正确的文件并在正确的位置创建文件，我们设置了数据节点进程的数据文件夹。 接下来我们必须启动看门狗线程，因为我们现在开始执行操作，我们希望确保我们不要由于某些软件错误而卡住了。 接下来我们将分配全局内存池的内存，这是分配最多内存的地方，我们仍然有相当多的内存分配，作为NDB内核中各种软件模块初始化的部分，但是我们正在逐步使用全局内存池。 分配内存可能是一个相当耗时的过程，其中操作系统可以为每个分配的GByte内存提供长达一秒的时间（自然是OS依赖的并且会随着时间的推移而变化）。 实际上在这里消耗时间实际上是我们还触摸每个页面以确保分配的内存也被映射到真实物理内存以避免在我们运行该过程时页面未命中。为了加快这个过程，我们已经触及了内存多线程。实际上，大多数内存的分配是可配置的，配置变量LateAlloc可用于延迟大多数内存分配到重启的早期阶段。 分配全局内存池后，我们初始化运行时环境使用的所有数据。 这可确保我们准备好在数据节点进程启动后立即在线程之间发送和接收数据。 在这一点上，我们只启动了看门狗进程并且线程在创建进程的过程中启动（如果我们运行ndbmtd，这个线程稍后将被转换为第一个接收线程，如果我们正在运行，则该线程将被转换为唯一的执行线程NDBD）。 下一步是加载所有软件模块并初始化它们，以确保在消息开始到达执行时正确设置它们。 在我们启动运行时环境之前，我们还需要激活发送和接收服务。 这涉及创建一个套接字客户端线程，该线程试图连接到集群中其他节点的套接字服务器部分，并创建一个线程来监听用于我们作为套接字服务器通信的那些数据节点的套接字服务器。 默认行为是nodeid最低的节点是通信设置中的套接字服务器。 这可以在数据节点配置中更改。 在我们继续并启动数据节点环境之前，我们将运行时环境的启动信号放在其正确的作业缓冲区中。 实际上，为了启动系统，需要在作业缓冲区中放置两个相等的信号。 第一个启动信号开始与其他节点的通信，并设置状态以等待下一个信号实际启动系统。 第二个将开始运行启动阶段。 最后，我们启动运行时环境的所有线程。 这些当前可以包括主线程，代表线程，多个tc线程，多个发送线程，多个接收线程和多个ldm线程。 鉴于已预先分配了所有线程的通信缓冲区，我们可以在这些线程启动时立即开始发送信号。 接收线程一旦到达其线程启动代码中的那一点就会开始处理其接收到的信号。 有两个相同的启动信号，第一个启动定期发送的重复信号，以跟踪数据节点中的时间。 只有第二个开始执行各种启动阶段。 数据节点的启动在一组阶段中处理。 第一阶段是将信号READ_CONFIG_REQ发送到内核中的所有软件模块，然后将STTOR类似地发送到256个阶段的所有软件模块，编号从0到255.这些模块的编号从0到255，我们不使用全部 这些阶段，但代码是灵活的，以便任何这些阶段可以现在使用或在将来的某个时间使用。 此外，我们还有6个模块，这些模块涉及另外一组启动阶段。 在这些阶段发送的信号称为NDB_STTOR。 最初的想法是将此消息视为NDB子系统的本地启动。 这些信号由NDBCNTR发送和处理，并作为NDBCNTR中STTOR处理的一部分发送。 这意味着它成为启动阶段的连续部分。 在开始阶段之前，我们确保任何管理节点都可以连接到我们的节点，并且所有其他节点都已断开连接，并且它们只能向QMGR模块发送消息。 管理服务器接收关于数据节点中的各种事件的报告，并且QMGR模块负责将数据节点包括在集群中。 在我们被包含在集群中之前，我们无法以任何方式与其他节点通信。 开始总是从主线程开始，其中每个软件模块至少由所有多线程模块包含的代理模块表示。 代理模块使用一条消息和一条回复，可以轻松地向一组相同类型的模块发送和接收消息。 READ_CONFIG_REQ信号始终以相同的顺序发送。 它首先发送到CMVMI，这是接收启动顺序的块，它执行许多功能，软件模块可以从这些功能影响运行时环境。 它通常会分配进程的大部分内存并触及所有内存。 它是主线程的一部分。 接收READ_CONFIG_REQ的下一个模块是NDBFS，这是控制文件系统线程的模块，该模块位于主线程中。 下一个模块是DBINFO，该模块支持ndbinfo数据库，用于以表格格式获取有关数据节点内部的信息，该模块位于主线程中。 接下来是DBTUP，这是存储实际数据的模块。 下一个DBACC，存储主键和唯一键哈希索引的模块以及我们控制行锁的位置。 这两个块都包含在ldm线程中。 接下来是DBTC，即管理事务协调的模块，该模块是tc线程的一部分。 接下来是DBLQH，该模块通过键操作和扫描控制对数据的操作，并且还处理REDO日志。 这是ldm线程的主要模块。 接下来是DBTUX，它操作有序索引重用页面，用于在DBTUP中存储行，也是ldm线程的一部分。 接下来是DBDICT，这是一个字典模块，用于存储和处理有关表和列，表空间，日志文件等的所有元数据信息。 DICT是主线程的一部分。 接下来是DBDIH，用于存储和处理有关所有表，表分区和每个分区的所有副本的分发信息的模块。 它控制本地检查点进程，全局检查点进程并控制重新启动处理的主要部分。 DIH模块是主线程的一部分。 接下来是控制重启阶段的NDBCNTR，它是主线程的一部分。 接下来是QMGR，它负责处理心跳协议以及包含和排除集群中的节点。 它是主线程的一部分。 接下来是TRIX，它执行与有序索引和其他基于触发器的服务相关的一些服务。 它是tc线程的一部分。 接下来是BACKUP，它用于备份和本地检查点，是ldm线程的一部分。 接下来是DBUTIL，它提供了许多服务，例如代表模块中的代码执行密钥操作。 它是主线程的一部分。 接下来是负责复制事件的SUMA模块，这是由rep线程处理的模块。 接下来是TSMAN，然后是LGMAN，然后是PGMAN，它们都是磁盘数据处理的一部分，负责处理表空间，UNDO日志记录和页面管理。 它们都是ldm线程的一部分。 RESTORE是一个用于在启动时恢复本地检查点的模块。 该模块也是ldm线程的一部分。 最后，我们有DBSPJ模块来处理向下推送到数据节点的连接查询，它作为tc线程的一部分执行。 DBTUP，DBACC，DBLQH，DBTUX，BACKUP，TSMAN，LGMAN，PGMAN，RESTORE都是紧密集成的模块，它们在每个节点中本地处理数据和索引。 这组模块形成一个LDM实例，每个节点可以有多个LDM实例，这些实例可以分布在一组线程上。 每个LDM实例都拥有自己的数据分区。 我们还有两个不属于重启处理的模块，这是TRPMAN模块，它执行许多与传输相关的功能（与其他节点通信）。 它在接收线程中执行。 最后，我们有THRMAN在每个线程中执行并执行一些线程管理功能。 所有模块都接收READ_CONFIG_REQ，所有模块也接收STTOR用于阶段0和阶段1.在阶段1中，他们报告他们希望获得更多信息的起始阶段。 在READ_CONFIG_REQ期间，线程可以在模块中执行很长时间，因为我们可以分配和触摸大尺寸的存储器。 这意味着我们的监视程序线程在此阶段有一个特殊的超时，以确保我们不会因为长时间初始化内存而导致进程崩溃。 在正常操作中，每个信号应仅执行少量微秒。 通过将消息STTOR发送到所有模块来同步启动阶段，逻辑上每个模块从0到255获得每个启动阶段的该信号。然而，响应消息STTORRY包含模块真正感兴趣的启动阶段列表。 处理起始相位信号的NDBCNTR模块可以优化掉不需要的任何信号。 模块接收STTOR消息的顺序对于所有阶段都是相同的： 1) NDBFS2) DBTC3) DBDIH4) DBLQH5) DBACC6) DBTUP7) DBDICT8) NDBCNTR9) CMVMI10) QMGR11) TRIX12) BACKUP13) DBUTIL14) SUMA15) DBTUX16) TSMAN17) LGMAN18) PGMAN19) RESTORE20) DBINFO21) DBSPJ 此外，还有一个由NDBCNTR控制的特殊启动阶段处理，因此当NDBCNTR收到自己的STTOR消息时，它会启动涉及模块的本地启动阶段处理，DBLQH，DBDICT，DBTUP，DBACC，DBTC和DBDIH。 对于阶段2到8，会发生这种情况。在这些启动阶段发送的消息是NDB_STTOR和NDB_STTORRY，它们的处理方式与STTOR和STTORRY类似。 模块还以相同的顺序接收所有阶段的启动阶段，此顺序为： 1) DBLQH2) DBDICT3) DBTUP4) DBACC5) DBTC6) DBDIH 对于那些多线程的模块，STTOR和NDB_STTOR消息始终由在主线程中执行的代理模块接收。 然后，代理模块将STTOR和NDB_STTOR消息发送到模块的每个单独实例（实例数通常与线程数相同，但有时可能不同）。 它并行执行，因此所有实例并行执行STTOR。 因此，有效地，模块的每个实例将在逻辑上首先接收READ_CONFIG_REQ，然后为每个启动阶段接收一组STTOR消息，并且一些模块也将按特定顺序接收NDB_STTOR。 所有这些消息都按特定顺序发送并按顺序发送。 因此，这意味着我们能够通过在正确的启动阶段执行操作来控制何时完成任务。 接下来，我们将逐步描述节点重启（或节点作为集群启动/重启的一部分启动）中发生的情况。 启动目前是一个顺序过程，除非声明它并行发生。 以下描述因此描述了当前实际发生的顺序。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库中的undo日志、redo日志、检查点]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84undo%E6%97%A5%E5%BF%97%E3%80%81redo%E6%97%A5%E5%BF%97%E3%80%81%E6%A3%80%E6%9F%A5%E7%82%B9%2F</url>
    <content type="text"><![CDATA[数据库存放数据的文件，本文称其为data file。数据库的内容在内存里是有缓存的，这里命名为db buffer。某次操作，我们取了数据库某表格中的数据，这个数据会在内存中缓存一些时间。对这个数据的修改在开始时候也只是修改在内存中的内容。当db buffer已满或者遇到其他的情况，这些数据会写入data file。 undo，redo日志在内存里也是有缓存的，这里将其叫做log buffer。磁盘上的日志文件称为log file。log file一般是追加内容，可以认为是顺序写，顺序写的磁盘IO开销要小于随机写。 Undo日志记录某数据被修改前的值，可以用来在事务失败时进行rollback；Redo日志记录某数据块被修改后的值，可以用来恢复未写入data file的已成功事务更新的数据。 例如某一事务的事务序号为T1，其对数据X进行修改，设X的原值是5，修改后的值为15，那么Undo日志为&lt;T1, X, 5&gt;，Redo日志为&lt;T1, X, 15&gt;。 当用户生成一个数据库事务时，undo log buffer会记录被修改的数据的原始值，redo会记录被修改的数据的更新后的值。 redo日志应首先持久化在磁盘上，然后事务的操作结果才写入db buffer，（此时，内存中的数据和data file对应的数据不同，我们认为内存中的数据是脏数据），db buffer再选择合适的时机将数据持久化到data file中。这种顺序可以保证在需要故障恢复时恢复最后的修改操作。先持久化日志的策略叫做Write Ahead Log，即预写日志。 在很多系统中，undo日志并非存到日志文件中，而是存放在数据库内部的一个特殊段中。本文中就把这些存储行为都泛化为undo日志存储到undo log file中。 对于某事务T，在log file的记录中必须开始于事务开始标记（比如“start T”），结束于事务结束标记（比如“end T”、”commit T”）。在系统恢复时，如果在log file中某个事务没有事务结束标记，那么需要对这个事务进行undo操作，如果有事务结束标记，则redo。 在db buffer中的内容写入磁盘数据库文件之前，应当把log buffer的内容写入磁盘日志文件。 有一个问题，redo log buffer和undo log buffer存储的事务数量是多少，是按照什么规则将日志写入log file？如果存储的事务数量都是1个，也就意味着是将日志立即刷入磁盘，那么数据的一致性很好保证。在执行事T时，突然断电，如果未对磁盘上的redo log file发生追加操作，可以把这个事务T看做未成功。如果redo log file被修改，则认为事务是成功了，重启数据库使用redo log恢复数据到db buffer和 data file即可。 如果存储多个的话，其实也挺好解释的。就是db buffer写入data file之前，先把日志写入log file。这种方式可以减少磁盘IO，增加吞吐量。不过，这种方式适用于一致性要求不高的场合。因为如果出现断电等系统故障，log buffer、db buffer中的完成的事务会丢失。以转账为例，如果用户的转账事务在这种情况下丢失了，这意味着在系统恢复后用户需要重新转账。 检查点checkpointcheckpoint是为了定期将db buffer的内容刷新到data file。当遇到内存不足、db buffer已满等情况时，需要将db buffer中的内容/部分内容（特别是脏数据）转储到data file中。在转储时，会记录checkpoint发生的”时刻“。在故障回复时候，只需要redo/undo最近的一次checkpoint之后的操作。 幂等性问题在日志文件中的操作记录应该具有幂等性。幂等性，就是说同一个操作执行多次和执行一次，结果是一样的。例如，5*1 = 5*1*1*1，所以对5的乘1操作具有幂等性。日志文件在故障恢复中，可能会回放多次（比如第一次回放到一半时系统断电了，不得不再重新回放），如果操作记录不满足幂等性，会造成数据错误。 InnoDB Redo Flush及脏页刷新机制深入分析我们知道InnoDB采用Write Ahead Log策略来防止宕机数据丢失，即事务提交时，先写重做日志，再修改内存数据页，这样就产生了脏页。既然有重做日志保证数据持久性，查询时也可以直接从缓冲池页中取数据，那为什么还要刷新脏页到磁盘呢？如果重做日志可以无限增大，同时缓冲池足够大，能够缓存所有数据，那么是不需要将缓冲池中的脏页刷新到磁盘。但是，通常会有以下几个问题： 服务器内存有限，缓冲池不够用，无法缓存全部数据 重做日志无限增大成本要求太高 宕机时如果重做全部日志恢复时间过长事实上，当数据库宕机时，数据库不需要重做所有的日志，只需要执行上次刷入点之后的日志。这个点就叫做Checkpoint，它解决了以上的问题： 缩短数据库恢复时间 缓冲池不够用时，将脏页刷新到磁盘 重做日志不可用时，刷新脏页 InnoDB引擎通过LSN(Log Sequence Number)来标记版本，LSN是日志空间中每条日志的结束点，用字节偏移量来表示。每个page有LSN，redo log也有LSN，Checkpoint也有LSN。可以通过命令show engine innodb status来观察：]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机性能监控及调优]]></title>
    <url>%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%8F%8A%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[常用虚拟机性能监控工具JDK 命令行工具其中的重中之重是 jstat 命令！而它最常用的参数就是 -gcutil，使用格式如下：1jstat -gcutil [pid] [intervel] [count] 输出如下： S0：堆上 Survivor space 0 区已使用空间的百分比 S1：堆上 Survivor space 1 区已使用空间的百分比 E：堆上 Eden 区已使用空间的百分比 O：堆上 Old space 区已使用空间的百分比 P：堆上 Perm space 区已使用空间的百分比 YGC：从程序启动到采样时发生的 Minor GC 次数 YGCT：从程序启动到采样时 Minor GC 所用的时间 FGC：从程序启动到采样时发生的 Full GC 次数 FGCT：从程序启动到采样时 Full GC 所用的时间 GCT：从程序启动到采样时 GC 的总时间ps 命令 (Linux)对于 jps 命令，其实没必要使用，一般使用 Linux 里的 ps 就够了，ps 为我们提供了当前进程状态的一次性的查看，它所提供的查看结果并不动态连续的，如果想对进程时间监控，应该用 top 工具。 Linux 上进程的 5 种状态 运行 [R, Runnable]：正在运行或者在运行队列中等待； 中断 [S, Sleep]：休眠中, 受阻, 在等待某个条件的形成或接受到信号； 不可中断 [D]：收到信号不唤醒和不可运行, 进程必须等待直到有中断发生； 僵死 [Z, zombie]：进程已终止, 但进程描述符存在, 直到父进程调用 wait4() 系统调用后释放； 停止 [T, Traced or stop]：进程收到 SIGSTOP, SIGSTP, SIGTIN, SIGTOU 信号后停止运行运行。 1234567891011ps -A # 列出所有进程信息（非详细信息）ps aux # 列出所有进程的信息ps aux | grep zshps -ef # 显示所有进程信息，连同命令行ps -ef | grep zsh ps -u root # 显示指定用户信息ps -l # 列出这次登录bash相关信息ps axjf # 同时列出进程树状信息 JVM 常见参数设置内存设置参数 -Xms：初始堆大小，JVM 启动的时候，给定堆空间大小。 -Xmx：最大堆大小，如果初始堆空间不足的时候，最大可以扩展到多少。 -Xmn：设置年轻代大小。整个堆大小 = 年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为 64M，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun 官方推荐配置为整个堆的 3/8。 -Xss： 设置每个线程的 Java 栈大小。JDK 5 后每个线程 Java 栈大小为 1M。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000 左右。 -XX:NewRatio=n：设置年轻代和年老代的比值。如为 3，表示年轻代与年老代比值为 1:3。 -XX:MaxTenuringThreshold：设置垃圾最大年龄。如果设置为 0 的话，则年轻代对象不经过 Survivor 区，直接进入年老代。对于年老代比较多的应用（即 Minor GC 过后有大量对象存活的应用），可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在 Survivor 区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概率。 设置经验 开发过程的测试应用，要求物理内存大于 4G 1234-Xmx3550m-Xms3550m -Xmn2g-Xss128k 高并发本地测试使用，大对象相对较多（如 IO 流） 1234567-Xmx3550m-Xms3550m-Xss128k-XX:NewRatio=4-XX:SurvivorRatio=4-XX:MaxPermSize=160m-XX:MaxTenuringThreshold=0 环境： 16G 物理内存，高并发服务，重量级对象中等（线程池，连接池等），常用对象比例为 40%（即运行过程中产生的对象 40% 是生命周期较长的） 1234567-Xmx10G-Xms10G-Xss1M-XX:NewRatio=3-XX:SurvivorRatio=4 -XX:MaxPermSize=2048m-XX:MaxTenuringThreshold=5 收集器设置参数 收集器设置 -XX:+UseSerialGC：设置串行收集器，年轻带收集器。 -XX:+UseParallelGC：设置并行收集器。 -XX:+UseParNewGC：设置年轻代为并行收集。可与 CMS 收集同时使用。JDK 5.0 以上，JVM 会根据系统配置自行设置，所以无需再设置此值。 -XX:+UseParallelOldGC：设置并行年老代收集器，JDK6.0 支持对年老代并行收集。 -XX:+UseConcMarkSweepGC：设置年老代并发收集器，测试中配置这个以后，-XX:NewRatio 的配置失效，原因不明。所以，此时年轻代大小最好用 -Xmn 设置。 -XX:+UseG1GC：设置 G1 收集器。 并行收集器参数设置 -XX:ParallelGCThreads=n：设置并行收集器收集时最大线程数使用的 CPU 数。并行收集线程数。 -XX:MaxGCPauseMillis=n：设置并行收集最大暂停时间，单位毫秒。 -XX:GCTimeRatio=n：设置垃圾回收时间占程序运行时间的百分比。 -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的 Survivor 区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。 -XX:CMSFullGCsBeforeCompaction=n：由于 CMS 不对内存空间进行压缩、整理，所以运行一段时间以后会产生”碎片”，使得运行效率降低。此值设置运行多少次 GC 以后对内存空间进行压缩、整理。 -XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片。虚拟机调优案例分析高性能硬件上的程序部署策略补充：64 位虚拟机在 Java EE 方面，企业级应用经常需要使用超过 4GB 的内存，此时，32 位虚拟机将无法满足需求，可是 64 位虚拟机虽然可以设置更大的内存，却存在以下缺点： 内存问题： 由于指针膨胀和各种数据类型对齐补白的原因，运行于 64 位系统上的 Java 应用程序需要消耗更多的内存，通常要比 32 位系统额外增加 10% ~ 30% 的内存消耗。 性能问题： 64 位虚拟机的运行速度在各个测试项中几乎全面落后于 32 位虚拟机，两者大概有 15% 左右的性能差距。 服务系统经常出现卡顿（Full GC 时间太长）首先 jstat -gcutil 观察 GC 的耗时，jstat -gccapacity 检查内存用量（也可以加上 -verbose:gc 参数获取 GC 的详细日志），发现卡顿是由于 Full GC 时间太长导致的，然后 jinfo -v pid，查看虚拟机参数设置，发现 -XX:NewRatio=9，这就是原因： 新生代太小，对象提前进入老年代，触发 Full GC 老年代较大，一次 Full GC 时间较长 可以调小 NewRatio 的值，尽肯能让比较少的对象进入老年代。 除了 Java 堆和永久代之外，会占用较多内存的区域 区域 大小调整 / 说明 内存不足时抛出的异常 直接内存 -XX:MaxDirectMemorySize OutOfMemoryError: Direct buffer memory 线程堆栈 -Xss StackOverflowError 或 OutOfMemoryError: unable to create new native thread Socket 缓存区 每个 Socket 连接都有 Receive(37KB) 和 Send(25KB) 两个缓存区 IOException: Too many open files JNI 代码 如果代码中使用 JNI 调用本地库，那本地库使用的内存也不在堆中 虚拟机和 GC 虚拟机、GC 代码执行要消耗一定内存 从 GC 调优角度解决新生代存活大量对象问题（Minor GC 时间太长） 将 Survivor 空间去除，让新生代中存活的对象在第一次 Minor GC 后立刻进入老年代，等到 Full GC 时再清理。 参数调整方法： -XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0 -XX:AlwaysTenure]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络基础知识]]></title>
    <url>%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[一次完整的 HTTP 请求所经历的步骤即在浏览器中输入 URL 地址 &gt;&gt; 显示主页的过程。总体来说分为以下几个过程： DNS 解析 TCP 连接 发送 HTTP 请求 服务器处理请求并返回 HTTP 报文 浏览器解析渲染页面 连接结束 简单的来说，就是先经过下图的过程，将客户端的请求传到服务器，再经过下图过程的逆过程，将客户端请求的数据返回给客户端，然后客户端浏览器对返回的数据进行渲染，最终得到了我们看到的页面。 DNS 解析DNS 解析的过程就是寻找哪台机器上有你需要资源的过程。也就是说，把你输入的 URL 地址转换为 IP 地址。 TCP 连接客户端 (浏览器) 通过 TCP 传输协议建立到服务器的 TCP 连接，详见后面 TCP 是如何传输数据，以及三次握手和四次挥手等。 发送 HTTP 请求发送 HTTP 请求的过程就是构建 HTTP 请求报文并通过 TCP 协议中发送到服务器指定端口 (HTTP 协议 80/8080，HTTPS 协议443)。HTTP 请求报文是由三部分组成： 请求行 、 请求报头 、 请求正文 。 服务器处理请求并返回 HTTP 报文后端从固定的端口接收到 TCP 报文（这一部分对应于编程语言中的 socket），它会对 TCP 连接进行处理，对 HTTP 协议进行解析，并按照报文格式进一步封装成 HTTP Request 对象，供上层使用。这一部分工作一般是由 Web 服务器去进行，我使用过的 Web 服务器有 Tomcat，Jetty。 HTTP 响应报文也是由三部分组成： 状态码 、 响应报头 、 响应报文 。 状态码： 1xx：指示信息–表示请求已接收，继续处理。 2xx：成功–表示请求已被成功接收、理解、接受。 3xx：重定向–要完成请求必须进行更进一步的操作。 4xx：客户端错误–请求有语法错误或请求无法实现。 5xx：服务器端错误–服务器未能实现合法的请求。 浏览器解析渲染页面即浏览器收到 HTML、CSS、JS 文件后，把页面呈现到屏幕上的过程。 DNS 解析DNS 解析的过程就是寻找哪台机器上有你需要资源的过程。也就是说，把你输入的 URL 地址转换为 IP 地址。 DNS 域名解析过程如下图所示，简单来说就是先查自己的本地域名服务器，如果自己就有缓存，直接从缓存里面读就可以，如果缓存里没有，本地域名服务器会发出递归连环问去查找。 DNS 负载均衡 DNS 可以根据每台机器的负载量，该机器离用户地理位置的距离等等，返回一个合适的机器的 IP 给用户，这个过程就是 DNS 负载均衡，又叫做 DNS 重定向。大家耳熟能详的 CDN (Content Delivery Network) 就是利用 DNS 的重定向技术实现的，DNS 服务器会返回一个跟用户最接近的服务器的 IP 地址给用户，CDN 节点的服务器负责响应用户的请求。 CDN（Content Distribute Network）CDN，内容分发网络。最简单的 CDN 网络由一个 DNS 服务器和几台缓存服务器组成： 当用户点击网站页面上的内容 URL，经过本地 DNS 系统解析，DNS 系统会最终将域名的解析权交给 CNAME 指向的 CDN 专用 DNS 服务器。 CDN 的 DNS 服务器将 CDN 的全局负载均衡设备 IP 地址返回用户。 用户向 CDN 的全局负载均衡设备发起内容 URL 访问请求。 CDN 全局负载均衡设备根据用户 IP 地址，以及用户请求的内容 URL，选择一台用户所属区域的区域负载均衡设备，告诉用户这台服务器的 IP 地址，让用户向这台设备发起请求。选择的依据包括： 根据用户 IP 地址，判断哪一台服务器距用户最近； 根据用户所请求的 URL 中携带的内容名称，判断哪一台服务器上有用户所需内容； 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力； 用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端。 如果这台缓存服务器上并没有用户想要的内容，而区域均衡设备依然将它分配给了用户，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。 TCP 是如何传输数据的？TCP (Transmission Control Protocol, TCP)，是一种面向连接、确保数据在端到端间可靠传输的协议。在传输前需要先建立一条可靠的传输链路，然后让数据在这条链路上流动，完成传输。简单来说就是，TCP 在想尽各种办法保证数据传输的可靠性，为了可靠性 TCP 会这样进行数据传输： 三次握手建立连接； 对发出的每一个字节进行编号确认，校验每一个数据包的有效性，在出现超时进行重传； 通过流量控制（通过滑动窗口协议实现）和拥塞控制（慢启动和拥塞避免、快重传和快恢复）等机制，避免网络状况恶化而影响数据传输； 四次挥手断开连接。 TCP 报头结构TCP 头部长度的前 20 字节是固定的，后面部分长度不定，但最多 40 字节 ，因此 TCP 头部一般在 20 ~ 60 字节之间。它的结构图如下： 它的每一字段的说明如下： 0 ~ 32 比特：源端口和目的端口 ，各占 16 比特（2 字节）。 32 ~ 64 比特：序列号 seq ，当前 TCP 数据报数据部分的第一个字节的序号（4 字节）。 64 ~ 96 比特：确认序号 ack ，表示当前主机作为接收端时，下一个希望接收的序列号是多少，确认号 = 当前主机已经正确接收的最后一个字节的序列号 + 1 96 ~ 112 比特：数据报报头长度，保留字段，标识符。 标识符：用于表示 TCP 报文的性质，只能是 0 或 1。TCP 的常用标识符： URG=1：紧急指针有效性标志，表示本数据报的数据部分包含紧急信息，紧急数据一定位于当前数据包数据部分的最前面，后面的紧急指针则标明了紧急数据的尾部。 ACK=1：在连接建立后传送的所有报文段都必须把 ACK 置 1，也就是说 ACK=1 后确认号字段才有效。 PSH=1：接收方应尽快将报文段提交至应用层，不会等到缓冲区满后再提交，一些交互式应用需要这样的功能，降低命令的响应时间。 RST=1：当该值为 1 时，表示当前 TCP 连接出现严重问题，必须要释放重连。 SYN=1：用在建立连接时 SYN=1, ACK=0：当前报文段是一个连接请求报文。 SYN=1, ACK=1：表示当前报文段是一个同意建立连接的应答报文。 FIN=1：表示此报文段是一个释放连接的请求报文。 112 ~ 128 比特：接收窗口大小 ，该字段用于实现 TCP 的流量控制。 它表示当前接收方的接收窗口的剩余大小，发送方收到该值后会将发送窗口调整成该值的大小（收到一个数据报就调整一次）。发送窗口的大小又决定了发送速率，所以接收方通过设置该值就可以控制发送放的发送速率。 128 ~ 144 比特：校验和 ，用于接收端检验整个数据包在传输过程中是否出错。 144 ~ 160 比特：紧急指针 ， 用来标明紧急数据的尾部，和 URG 标识符一起使用。 TCP 三次握手、四次挥手 TCP 的三次握手：为了建立可靠的通信信道，即双方确认自己与对方的发送和接收都是正常的。 TCP 的四次挥手：确保双方都在没有想说的内容之后，释放 TCP 连接。 三次握手三次握手的流程 第一次握手：A 向 B 发送建立连接请求（A 对 B 说：“我们在一起吧！”） 第二次握手：B 收到 A 的建立连接请求后，发给 A 一个同意建立连接的应答报文（B 对 A 说：“好的，同意和你在一起啦”） 第三次握手：A 向 B 发送个报文，表示我已经收到你的应答了（A 对 B 说：“亲爱的，你同意真是太好了，我们可以互相砸数据了”） 为什么 TCP 连接需要三次握手，两次不可以吗？首先，我们要知道，这三次握手是为了让双方确认自己与对方的发送和接收都是正常的。但是，只成功完成两次握手的时候，B 不知道自己的发送能力是否正常，也不知道 A 的收报能力是否正常，所以它们需要第三次握手。 同时，如果没有第三次连接，很有可能导致 B 建立一个脏连接。 脏连接建立的过程： A 发送的第一个建立连接的请求，这个连接好久好久都没有达到 B 那里； 所以，A 又重新发送了一个新的建立连接请求给 B，这个请求成功了，A 和 B 愉快的交换完了数据并且断开了连接； 此时，A 第一个发的建立连接的请求终于穿越 n 个路由器到达了 B，B 以为这时 A 发来的新的建立连接的请求，愉快的返回了同意建立连接的请求； 如果只需要两次握手，此时 B 会单方面的建立起与 A 的连接，而 A 根本就不在 SYN_SENT 状态，它会把 B 的应答请求直接丢掉，不会建立连接。此时，B单方面创建的这个连接就是脏连接。 四次挥手四次挥手的流程 A：我们分手吧。 B：好的，我收到了你的分手请求，再等一会，我把你剩我这的东西打包给你。（此时 A 已经不能再给 B 发东西了） B：好了，东西发完了，分吧。（此时 B 也不能再给 A 发东西了） A：好的，知道你东西都发完了，我在等 2MSL，然后就消失了。 TIME_WAIT 存在的必要性如果 A 发送完最后一个 ACK=1 后，立即进入 CLOSED 状态，可能会导致 B 无法进入 CLOSED 状态。 原因：假设 A 最后的 ACK 在网络传输中丢失了，B 会认为 A 根本没收到自己发的 FIN=1, ACK=1 报文，会导致 B 超时重发 FIN=1, ACK=1 报文。A 第二次收到 FIN=1, ACK=1 报文后，会再发一次 ACK，并重新开始 TIME_WAIT 的计时。如果 A 发完最后一个 ACK 后立即关闭，B 可能会永远接收不到最后一个 ACK，也就无法进入 CLOSED 状态。 在高并发上，可以将 TIME_WAIT 调到小于 30s 为宜。 设置方法：改变服务器的配置文件 /etc/sysctl.conf 中的 net.ipv4.tcp_fin_timeout=30 注：seq 表示序列号，ack 表示确认号，2MSL 是报文在网络中生存的最长时间。 TCP 流量控制、拥塞控制流量控制产生原因： 如果发送方数据发送的过快，接收方可能来不及接收，这会造成数据的丢失。 解决方法： 通过滑动窗口实现，接收端告诉发送发自己的接收窗口有多大，发送端会调整自己的发送窗口不超过接收端的接收窗口大小。 流量控制引发的死锁： 当发送者收到了一个窗口为 0 的应答后，发送者会停止发送，等待接收者的下一个应答。但是如果这个窗口不为 0 的应答在传输过程丢失，发送者一直等待下去，而接收者以为发送者已经收到该应答，等待接收新数据，这样双方就相互等待，从而产生死锁。 拥塞控制慢启动和拥塞避免首先，发送方维持一个叫做 拥塞窗口 cwnd（congestion window） 的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于 min{拥塞窗口, 接收窗口}。 慢启动就是： 不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。具体过程如下： 慢启动的时候，拥塞窗口每次是呈 2 的指数次方增长的，因为开始的时候需要比较快速的将拥塞窗口的大小增长到一个合适值。如果我们一直使用慢启动的方法确认拥塞窗口 cwnd 的大小，cwnd 会飞速增大，而且增长的粒度会越来越粗，一不小心就增的过大了，就会导致网络的拥塞。 为了避免这种情况，我们设定了一个慢开始门限 ssthresh，令 cwnd 大于一定值之后就采用拥塞避免算法，拥塞避免算法和慢启动算法的区别在于：拥塞避免算法每次只将 cwnd 增加 1，也就是呈加法增长的。ssthresh 的用法如下： cwnd &lt; ssthresh 时，使用慢开始算法 cwnd &gt; ssthresh 时，改用拥塞避免算法 cwnd = ssthresh时，慢开始与拥塞避免算法任意 拥塞避免算法会让拥塞窗口缓慢增长，即每经过一个往返时间 RTT 就把发送方的拥塞窗口 cwnd 加 1，而不是加倍。这样拥塞窗口按线性规律缓慢增长。 无论是在 慢启动阶段 还是在 拥塞避免阶段 ，只要发送方判断 网络出现拥塞 （其根据就是没有收到确认，虽然没有收到确认可能是其他原因的分组丢失，但是因为无法判定，所以都当做拥塞来处理）， 就把慢开始门限设置为出现拥塞时的发送窗口大小的一半（乘法减小算法）。然后把拥塞窗口设置为 1，执行慢开始算法。 通过使用慢启动与拥塞避免算法，拥塞窗口的大小变化大致如下图所示： 注：这里只是为了讨论方便而将拥塞窗口大小的单位改为数据报的个数，实际上应当是字节。 快重传和快恢复快重传 要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不等到自己发送数据时捎带确认。 快重传算法规定：发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传时间计时器到期。如下图： 快恢复 要求当发送发连续接收到三个确认时，就执行乘法减小算法，把慢启动开始门限（ssthresh）减半，但是接下来并不执行慢开始算法。而是将 cwnd 设置为 ssthresh 的大小，然后执行拥塞避免算法。 通过使用快重传与快恢复算法，拥塞窗口的大小变化大致如下图所示： TCP 滑动窗口停止等待协议（ARQ 协议，滑动窗口协议的简易版）原理： A 向 B 每发送一个分组，都要停止发送，等待 B 的确认应答；A 只有收到了 B 的确认应答后才能发送下一个分组。 A 发送的分组丢失或出错 丢失：发送者 A 拥有超时计时器。每发送一个分组便会启动超时计时器，等待 B 的应答。若超时仍未收到应答，A 就会重发刚才的分组。 出错：若 B 收到分组，但通过检查和字段发现分组在运输途中出现差错，它会直接丢弃该分组，并且不会有任何其他动作。A 超时后便会重新发送该分组，直到 B 正确接收为止。 B 发送的确认应答丢失或迟到 丢失：A 迟迟收不到 B 的确认应答，会进行超时重传，B 收到重复的分组后会立即补发一个确认应答。 迟到：A 会根据分组号得知该分组已被接收，会直接丢弃该应答。 滑动窗口协议（连续 ARQ 协议）ARQ 协议的缺点： 每次只发送一个分组，在该分组的应答到来前只能等待。为了解决这个问题，我们改成一次发送一堆，也就是我们有个窗口，在发送端没有收到确认应答时，可以继续发送窗口中的分组，而不是干等着。 累计确认： 接收端不用为每一个分组发送一个应答了，改为为一组分组发送一个确认应答。这个应答会通过 TCP 头中的 ack（确认号）来告诉发送端它下一个希望接收的分组号是多少。 发送窗口： 发送端收到接收端发来的一个确认应答后，会根据确认应答的 TCP 头中的各种信息移动 P1、P2、P3 三个指针： 根据 ack 的值移动 P1 指针，确认哪些分组被成功接收了； 然后根据窗口大小移动 P3 = P1 + 窗口大小； 然后 P2 从 P1 开始向 P3 移动，向接收端发送分组数据。 接收窗口： 接收者收到的字节会存入接收窗口，接收者会对已经正确接收的有序字节进行累计确认，发送完确认应答后，接收窗口就可以向前移动指定字节。 如果某些字节并未按序收到，接收者只会确认最后一个有序的字节，从而乱序的字节就会被重新发送。 注意： 同一时刻发送窗口的大小并不一定和接收窗口一样大（因为时延和拥塞窗口）。 TCP 标准并未规定未按序到达的字节的处理方式。但 TCP 一般都会缓存这些字节，等缺少的字节到达后再交给应用层处理（应用层可以对它进行排序）。这比直接丢弃乱序的字节要节约带宽。 TCP 与 UDP 的区别从特点上看： TCP 是面向连接的，UDP 是无连接的，即 TCP 在传输数据前要先通过三次握手建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制建立连接，数据传输结束后要释放连接。 UDP 在传输数据前不需要建立连接，远地主机在接收到报文之后也无需确认。 所以 TCP 是可靠传输，UDP 是不可靠传输。 TCP 因为有连接，所以数据以字节流的形式传输，UDP 则以数据报文段形式传输，而且 TCP 只能是一对一的，而 UDP 可以各种通信。 从性能上看： TCP 传输效率慢，需要资源多，但可靠。UDP 传输效率快，需要资源少，但不可靠。 应用场景： TCP 应用在要求传输数据可靠的情况下，如文件传输、邮件传输等。UDP 应用在要求通信速度但对可靠性要求比较低的场景，如 QQ 语音、视频等。 首部字节： TCP 首部有 20 ~ 60 个字节，UDP 首部由 8 个字节 4 个字段组成。 怎么用 UDP 实现 TCP？ 在传输层 UDP 是不可靠的，所以需要在应用层自己实现一些保证可靠传输的机制，简单来说，就是使用 UDP 来构建可靠的面向连接的数据传输，就是在应用层实现类似于 TCP 的超时重传（定时器），拥塞控制（滑动窗口），有序接收（添加包序号），应答确认（ack 和 seq）等。目前已经有了实现 UDP 可靠运输的机制 —— UDT：主要用于高速广域网海量数据传输，是应用层协议。 HTTP 长连接、短连接短连接在 HTTP/1.0 中默认使用 短连接：客户端和服务器每进行一次 HTTP 操作，就建立一次连接，任务结束就中断连接。 当客户端浏览器访问的某个 HTML 或其他类型的 Web 页中包含有其他的 Web 资源（如：JavaScript 文件、图像文件、CSS 文件等）时，浏览器就会重新建立一个 HTTP 会话。 应用： WEB 网站的 http 服务一般都用短链接，因为并发量大，但每个用户无需频繁操作。 长连接而从 HTTP/1.1 起，默认使用长连接。使用长连接的HTTP协议，会在响应头加入这行代码： 1Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。 Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接，有关 TCP 长连接、短连接的介绍请看下一小节。 应用： 适用于操作频繁的点对点通讯，而且连接数不能太多。 TCP 长连接、短连接短链接短连接过程： client 向 server 发起连接请求，server 接到请求，然后双方建立连接。client 向 server 发送消息，server 回应 client，然后一次请求就完成了。这时候双方任意都可以发起 close 操作，不过一般都是 client 先发起 close 操作。也就是说，短连接一般只会在 client 和 server 间进行一次请求操作。 优点： 管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。 缺点： 如果客户请求频繁，将在 TCP 的建立和关闭操作上浪费时间和带宽。 长连接长连接过程： client 向 server 发起连接，server 接受 client 连接，双方建立连接，client 与 server完成一次请求后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 TCP 的保活功能主要为服务器应用提供。如果客户端已经消失而连接未断开，则会使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，此时服务器将永远等待客户端的数据。保活功能就是试图在服务端器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何动作，服务器就向客户发送一个探测报文段，根据客户端主机响应探测 4 个客户端状态： 客户主机依然正常运行，且服务器可达。此时客户的TCP响应正常，服务器将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。上述情况下客户端都不能响应 TCP。服务端将无法收到客户端对探测的响应。服务器总共发送 10 个这样的探测，每个间隔 75 秒。若服务器没有收到任何一个响应，它就认为客户端已经关闭并终止连接。 客户端崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户机正常运行，但是服务器不可达。这种情况与第二种状态类似。 优点： 对于请求比较频繁客户来说，可以节省在 TCP 的建立和关闭操作上浪费时间和带宽。 缺点： 存活探测周期太长，而且 client 端一般不会主动关闭它与服务器之间的连接，如果 client 与 server 之间的连接一直不关闭的话，随着客户端连接越来越多，server 早晚有扛不住的时候 ，这时候 server 端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，以避免一些恶意连接导致 server 端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。 HTTP、HTTPS 区别HTTPS 的全称为：HTTP over SSL，简单理解就是在之前的 HTTP 传输上增加了 SSL 协议加密。 1234567|------| | HTTP ||------| &lt;-- HTTPS 在 HTTP 和 TCP 间加了一层 SSL or TLS| TCP ||------|| IP ||------| HTTP 通信存在的问题 容易被监听： http 通信都是明文，数据在客户端与服务器通信过程中，任何一点都可能被劫持，如果明文保存的密码被截取了是很危险的。 被伪装： http 通信时无法保证双方是合法的。比如你请求 www.taobao.com， 你无法知道返回的数据就是来自淘宝，还是中间人伪装的淘宝。 被篡改： 中间人将发给你的信息篡改了你也不知道。 因为 http 不安全，所以 https 出现了！ 区别 HTTPS 需要到 CA 申请证书，HTTP 不需要 HTTPS 密文传输，HTTP 明文传输 连接方式不同，HTTPS 默认使用 443 端口，HTTP 使用 80 端口 HTTPS = HTTP + 加密 + 认证 + 完整性保护，比 HTTP 安全 HTTP 1.1 与 HTTP 1.0 的区别 1.0 需要设置 keep-alive 参数来告知服务器建立长连接，1.1 默认建立长连接。 1.1 支持只发 header 不带 body，如果服务器认为客户端有权利访问，返回 100，否则返回 401，客户端可以接到 100 后再把 body 发过去，接到 401 就不发了，这样比较节省带宽。 1.1 有 host 域，1.0 没有。 host 域用于处理一个IP地址对应多个域名的情况，假设我的虚拟机服务器 IP 是 111.111.111.111，我们可以把 www.qiniu.com，www.taobao.com 和 www.jd.com 这些网站都架设那台虚拟机上面，但是这样会有一个问题，我们每次访问这些域名其实都是解析到服务器 IP 111.111.111.111，那么如何来区分每次根据域名显示出不同的网站的内容呢？就是通过 Host 域的设置，可以在 Tomcat 的 conf 目录下的 server.xml 进行配置。 1.1 会进行带宽优化。1.0 存在浪费带宽的现象，而且不支持断点续传，1.1 在请求头中引入了 range 域，允许只请求某个资源的某个部分，即返回状态码 206。 如果客户端不断的发送请求连接会怎样服务器端会为每个请求创建一个链接，然后向 client 端发送创建连接时的回复，然后进行等待客户端发送第三次握手数据包，这样会白白浪费资源。DDos 攻击就是基于这一点达到的。 DDos 攻击简单的说就是不停的向服务器发送建立连接请求，但不发送第三次握手的数据包。 客户端向服务器端发送连接请求数据包 服务器向客户端回复连接请求数据包，然后服务器等待客户端发送tcp/ip链接的第三步数据包 如果客户端不向服务器端发送最后一个数据包，服务器须等待 30s 到 2min 才能将此连接关闭。当大量的请求只进行到第二步，而不进行第三步，服务器将有大量的资源在等待第三个数据包，造成DDos攻击。 DDos 预防 DDoS清洗：对用户请求数据进行实时监控，及时发现异常流量，封掉异常流量的 IP，使用的命令：iptables。 用 iptables 屏蔽单个 IP 的命令：iptables -I INPUT -s ***.***.***.*** -j DROP 用 iptables 屏蔽整个 IP 段命令：iptables -I INPUT -s 121.0.0.0/8 -j DROP 用 iptables 解禁 IP 命令：iptables -D INPUT -s ***.***.***.*** -j DROP 相当于在 /etc/iptables.conf 配置文件中写入：-A INPUT -s ***.***.***.*** -j DROP CDN 加速：在现实中，CDN 服务将网站访问流量分配到了各个节点中，这样一方面隐藏网站的真实 IP，另一方面即使遭遇 DDoS 攻击，也可以将流量分散到各个节点中，防止源站崩溃。 GET 和 POST 区别 Http 报文层面：GET 将请求信息放在 URL 中，POST 方法报文中，所以 POST 方法更安全，毕竟数据在地址栏上不可见。 数据库层面：GET 符合幂等性，POST 不符合。 缓存层面：GET 可以被缓存、被存储（书签），而 POST 不行。 301、302 重定向HTTP 返回码中 301 和 302 的区别： 301，302 都是 HTTP 的状态码，都代表着某个 URL 发生了转移，不同之处在于： 301 代表永久性转移 (Permanently Moved)。 302 代表暂时性转移(Temporarily Moved )。搜索引擎会抓取新的内容而保留旧的网址，因为服务器返回 302 代码，搜索引擎认为新的网址只是暂时的。 使用场景： 域名到期不想续费； 在搜索引擎的搜索结果中出现了不带 www 的域名，而带 www 的域名却没有收录，这个时候可以用 301 重定向来告诉搜索引擎我们目标的域名是哪一个； 空间服务器不稳定，换空间的时候。 注意：尽量使用 301 跳转！原因：网址劫持！ 从网址 A 做一个 302 重定向到网址 B 时，主机服务器的隐含意思是网址 A 随时有可能改主意，重新显示本身的内容或转向其他的地方。大部分的搜索引擎在大部分情况下，当收到 302 重定向时，一般只要去抓取目标网址就可以了，也就是说网址 B。如果搜索引擎在遇到 302 转向时，百分之百的都抓取目标网址 B 的话，就不用担心网址 URL 劫持了。问题就在于，有的时候搜索引擎，尤其是 Google，并不能总是抓取目标网址。比如说，有的时候 A 网址很短，但是它做了一个 302 重定向到 B 网址，而 B 网址是一个很长的乱七八糟的 URL 网址，甚至还有可能包含一些问号之类的参数。很自然的，A 网址更加用户友好，而 B 网址既难看，又不用户友好。这时 Google 很有可能会仍然显示网址A。 这就造成了网址 URL 劫持的可能性。也就是说，一个不道德的人在他自己的网址 A 做一个 302 重定向到你的网址 B，出于某种原因， Google 搜索结果所显示的仍然是网址 A，但是所用的网页内容却是你的网址 B 上的内容，这种情况就叫做网址 URL 劫持。你辛辛苦苦所写的内容就这样被别人偷走了。 302 重定向很容易被搜索引擎误认为是利用多个域名指向同一网站，那么你的网站就会被封掉，罪名是 “利用重复的内容来干扰 Google 搜索结果的网站排名”。 URL 和 URIURI 叫做统一资源标志符，就是在某一规则下能把一个资源独一无二地标识出来。 URL 叫做统一资源定位符，是以描述资源的位置来唯一确定一个资源的。所以URL 是 URI 的子集。 NAT网络地址转换常用于私有地址与公有地址的转换，以解决 IP 地址匮乏的问题。 NAT 的基本工作原理是：当私有网主机和公共网主机通信的 IP 包经过 NAT 网关时，将 IP 包中的源 IP 或目的 IP 在私有 IP 和 NAT 的公共 IP 之间进行转换。]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机的执行机制]]></title>
    <url>%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E6%89%A7%E8%A1%8C%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[类文件结构Class 文件是一组以 8 位字节为基础单位的二进制流，各个数据项目严格按照顺序紧凑地排列在 Class 文件中，中间没有任何分隔符。Java 虚拟机规范规定 Class 文件采用一种类似 C 语言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：无符号数和表，我们之后也主要对这两种类型的数据类型进行解析。 无符号数： 无符号数属于基本数据类型，以 u1、u2、u4、u8 分别代表 1 个字节、2 个字节、4 个字节和 8 个字节的无符号数，可以用它来描述数字、索引引用、数量值或 utf-8 编码的字符串值。 表： 表是由多个无符号数或其他表为数据项构成的复合数据类型，名称上都以 _info 结尾。 魔数与版本号Class 文件的头 8 个字节是魔数和版本号，其中头 4 个字节是魔数，也就是 0xCAFEBABE，它可以用来确定这个文件是否为一个能被虚拟机接受的 Class 文件（这通过扩展名来识别文件类型要安全，毕竟扩展名是可以随便修改的）。 后 4 个字节则是当前 Class 文件的版本号，其中第 5、6 个字节是次版本号，第 7、8 个字节是主版本号。 常量池 从第 9 个字节开始，就是常量池的入口，常量池是 Class 文件中： 与其他项目关联最多的的数据类型； 占用 Class 文件空间最大的数据项目； Class 文件中第一个出现的表类型数据项目。 常量池的开始的两个字节，也就是第 9、10 个字节，放置一个 u2 类型的数据，标识常量池中常量的数量 cpc (constant_pool_count)，这个计数值有一个十分特殊的地方，就是它是从 1 开始而不是从 0 开始的，也就是说如果 cpc = 22，那么代表常量池中有 21 项常量，索引值为 1 ~ 21，第 0 项常量被空出来，为了满足后面某些指向常量池的索引值的数据在特定情况下需要表达“不引用任何一个常量池项目”时，将让这个索引值指向 0 即可。 常量池中记录的是代码出现过的所有 token（类名，成员变量名等，也是我们接下来要修改的地方）以及符号引用（方法引用，成员变量引用等），主要包括以下两大类常量： 字面量： 接近于 Java 语言层面的常量概念，包括 文本字符串 声明为 final 的常量值 符号引用： 以一组符号来描述所引用的目标，包括 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 常量池中的每一项常量都通过一个表来存储。目前一共有 14 种常量，不过麻烦的地方就在于，这 14 种常量类型每一种都有自己的结构，我们在这里只详细介绍两种：CONSTANT_Class_info 和 CONSTANT_Utf8_info。 CONSTANT_Class_info 的存储结构为： 12... [ tag=7 ] [ name_index ] ...... [ 1位 ] [ 2位 ] ... 其中，tag 是标志位，用来区分常量类型的，tag = 7 就表示接下来的这个表是一个 CONSTANT_Class_info，name_index 是一个索引值，指向常量池中的一个 CONSTANT_Utf8_info 类型的常量所在的索引值，CONSTANT_Utf8_info 类型常量一般被用来描述类的全限定名、方法名和字段名。它的存储结构如下： 12... [ tag=1 ] [ 当前常量的长度 len ] [ 常量的符号引用的字符串值 ] ...... [ 1位 ] [ 2位 ] [ len位 ] ... 访问标志 常量池之后的两个字节代表访问标志，即这个class是类还是接口，是否为public等的信息。不同的含义有不同的标志值（没有用到的标志位一律为0。），具体信息如下： 类索引、父类索引、接口索引集合 类索引占两个字节，分别指向常量池中的CONSTANT_Class_info类型的常量，这个类型的常量结构见常量池中的图表，其中包含一个指向全限定名常量项的索引。 因为java只允许单继承，所以只有一个父类，具体内容同上-类索引。 接口索引开始两个字节用来表示接口的数量，之后的每两个字节表示一个接口索引，用法同类索引与父类索引。 字段表集合字段用于描述接口或者类中声明的变量，包括类级变量以及实例变量，但不包括局部变量。 字段域的开始两个字节表示字段数量，之后为紧密排列的字段结构体数据，其结构如下： 其中的字段和方法的描述符，对于字段来说用来描述字段的数据类型；而对于方法来说，描述的就是方法的参数列表（包括数量、类型以及顺序）和返回值，这个描述顺序也是固定的，必须是参数列表在前，返回值在后，参数列表必须放在一组小括号内。同时为了节省空间，各种数据类型都使用规定的一个字母来表示，具体如下： 对象使用L加上对象的全限定名来表示，而数组则是在每一个维度前添加一个&quot;[&quot;来描述。属性表在之后进行介绍。 方法表集合class文件中对方法的描述与以前对字段的描述几乎采用了完全一致的方式，唯一的区别就是访问类型不完全一致。 属性表集合java7中预定义了21项属性，具体内容限于篇幅不再列出。对于每个属性的结构，没有特别严格的要求，并且可以自定义属性信息，jvm运行时会忽略不认识的属性。符合规范的属性表基本结构如下： 其中前两个字节为指向常量池中的CONSTANT_Utf8_info类型的属性名称，之后4个字节表示属性值所占用的位数，最后就是具体属性了。 其中有一个比较重要的名称为「Code」的属性为方法的代码，即字节码指令。Code属性表结构如下： 虚拟机的类加载机制类加载的时机JVM 会在程序第一次主动引用类的时候，加载该类，被动引用时并不会引发类加载的操作。也就是说，JVM 并不是在一开始就把一个程序就所有的类都加载到内存中，而是到不得不用的时候才把它加载进来，而且只加载一次。那么什么是主动引用，什么是被动引用呢？ 主动引用 遇到 new、getstatic、putstatic、invokestatic 字节码指令，例如： 使用 new 实例化对象； 读取或设置一个类的 static 字段（被 final 修饰的除外）； 调用类的静态方法。 对类进行反射调用； 初始化一个类时，其父类还没初始化（需先初始化父类）； 这点类与接口具有不同的表现，接口初始化时，不要求其父接口完成初始化，只有真正使用父接口时才初始化，如引用父接口中定义的常量。 虚拟机启动，先初始化包含 main() 函数的主类； JDK 1.7 动态语言支持：一个 java.lang.invoke.MethodHandle 的解析结果为 REF_getStatic、REF_putStatic、REF_invokeStatic。 被动引用 通过子类引用父类静态字段，不会导致子类初始化； Array[] arr = new Array[10]; 不会触发 Array 类初始化； static final VAR 在编译阶段会存入调用类的常量池，通过 ClassName.VAR 引用不会触发 ClassName 初始化。 也就是说，只有发生主动引用所列出的 5 种情况，一个类才会被加载到内存中，也就是说类的加载是 lazy-load 的，不到必要时刻是不会提前加载的，毕竟如果将程序运行中永远用不到的类加载进内存，会占用方法区中的内存，浪费系统资源。 类的显式加载和隐式加载 显示加载： 调用 ClassLoader#loadClass(className) 或 Class.forName(className)。 两种显示加载 .class 文件的区别： Class.forName(className) 加载 class 的同时会初始化静态域，ClassLoader#loadClass(className) 不会初始化静态域； Class.forName 借助当前调用者的 class 的 ClassLoader 完成 class 的加载。 隐式加载： new 类对象； 使用类的静态域； 创建子类对象； 使用子类的静态域； 其他的隐式加载，在 JVM 启动时： BootStrapLoader 会加载一些 JVM 自身运行所需的 Class； ExtClassLoader 会加载指定目录下一些特殊的 Class； AppClassLoader 会加载 classpath 路径下的 Class，以及 main 函数所在的类的 Class 文件。 类加载的过程类的生命周期123加载 --&gt; 验证 --&gt; 准备 --&gt; 解析 --&gt; 初始化 --&gt; 使用 --&gt; 卸载 |&lt;------- 连接 -------&gt;||&lt;------------- 类加载 ----------------&gt;| 类的生命周期一共有 7 个阶段，其中前五个阶段较为重要，统称为类加载，第 2 ~ 4 阶段统称为连接，加载和连接中的三个过程开始的顺序是固定的，但是执行过程中是可以交叉执行的。接下来，我们将对类加载的 5 个阶段进行一一讲解。 加载加载的 3 个阶段 通过类的全限定名获取二进制字节流（将 .class 文件读进内存）； 将字节流的静态存储结构转化为运行时的数据结构； 在内存中生成该类的 Class 对象； HotSpot 虚拟机把这个对象放在方法区，非 Java 堆。 分类 非数组类 系统提供的引导类加载器 用户自定义的类加载器 数组类 不通过类加载器，由 Java 虚拟机直接创建 创建动作由 newarray 指令触发，new 实际上触发了 [L全类名 对象的初始化 规则 数组元素是引用类型 加载：递归加载其组件 可见性：与引用类型一致 数组元素是非引用类型 加载：与引导类加载器关联 可见性：public 验证 目的： 确保 .class 文件中的字节流信息符合虚拟机的要求。 4 个验证过程： 文件格式验证：是否符合 Class 文件格式规范，验证文件开头 4 个字节是不是 “魔数” 0xCAFEBABE 元数据验证：保证字节码描述信息符号 Java 规范（语义分析） 字节码验证：程序语义、逻辑是否正确（通过数据流、控制流分析） 符号引用验证：对类自身以外的信息（常量池中的符号引用）进行匹配性校验 这个操作虽然重要，但不是必要的，可以通过 -Xverify:none 关掉。 准备 描述： 为 static 变量在方法区分配内存。 static 变量准备后的初始值： public static int value = 123; 准备后为 0，value 的赋值指令 putstatic 会被放在 &lt;client&gt;() 方法中，&lt;client&gt;()方法会在初始化时执行，也就是说，value 变量只有在初始化后才等于 123。 public static final int value = 123; 准备后为 123，因为被 static final 赋值之后 value 就不能再修改了，所以在这里进行了赋值之后，之后不可能再出现赋值操作，所以可以直接在准备阶段就把 value 的值初始化好。 解析 描述： 将常量池中的 “符号引用” 替换为 “直接引用”。 在此之前，常量池中的引用是不一定存在的，解析过之后，可以保证常量池中的引用在内存中一定存在。 什么是 “符号引用” 和 “直接引用” ？ 符号引用：以一组符号描述所引用的对象（如对象的全类名），引用的目标不一定存在于内存中。 直接引用：直接指向被引用目标在内存中的位置的指针等，也就是说，引用的目标一定存在于内存中。 初始化 描述： 执行类构造器 &lt;client&gt;() 方法的过程。 &lt;client&gt;() 方法 包含的内容： 所有 static 的赋值操作； static 块中的语句； &lt;client&gt;() 方法中的语句顺序： 基本按照语句在源文件中出现的顺序排列； 静态语句块只能访问定义在它前面的变量，定义在它后面的变量，可以赋值，但不能访问。 与 &lt;init&gt;() 的不同： 不需要显示调用父类的 &lt;client&gt;() 方法； 虚拟机保证在子类的 &lt;client&gt;() 方法执行前，父类的 &lt;client&gt;() 方法一定执行完毕。 也就是说，父类的 static 块和 static 字段的赋值操作是要先于子类的。 接口与类的不同： 执行子接口的 &lt;client&gt;() 方法前不需要先执行父接口的 &lt;client&gt;() 方法（除非用到了父接口中定义的 public static final 变量）； 执行过程中加锁： 同一时刻只能有一个线程在执行 &lt;client&gt;() 方法，因为虚拟机要保证在同一个类加载器下，一个类只被加载一次。 非必要性： 一个类如果没有任何 static 的内容就不需要执行 &lt;client&gt;() 方法。 注：初始化时，才真正开始执行类中定义的 Java 代码。 类加载器如何判断两个类 “相等” “相等” 的要求 同一个 .class 文件 被同一个虚拟机加载 被同一个类加载器加载 判断 “相等” 的方法 instanceof 关键字 Class 对象中的方法： equals() isInstance() isAssignableFrom() 类加载器的分类 启动类加载器（Bootstrap）：Bootstrp加载器是用C++语言写的，它是在Java虚拟机启动后初始化的，它主要负责加载%JAVA_HOME%/jre/lib,-Xbootclasspath参数指定的路径以及%JAVA_HOME%/jre/classes中的类。 扩展类加载器（Extension）：&lt;JAVA_HOME&gt;/lib/ext、java.ext.dirs系统变量指定的路径 应用程序类加载器（Application）： -classpath 参数 双亲委派模型 工作过程 当前类加载器收到类加载的请求后，先不自己尝试加载类，而是先将请求委派给父类加载器 因此，所有的类加载请求，都会先被传送到启动类加载器 只有当父类加载器加载失败时，当前类加载器才会尝试自己去自己负责的区域加载 实现 检查该类是否已经被加载 将类加载请求委派给父类 如果父类加载器为 null，默认使用启动类加载器 parent.loadClass(name, false) 当父类加载器加载失败时 catch ClassNotFoundException 但不做任何处理 调用自己的 findClass() 去加载 我们在实现自己的类加载器时只需要 extends ClassLoader，然后重写 findClass() 方法而不是 loadClass() 方法，这样就不用重写 loadClass() 中的双亲委派机制了 优点 Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。 其次是考虑到安全因素，java核心api中定义类型不会被随意替换，假设通过网络传递一个名为java.lang.Integer的类，通过双亲委托模式传递到启动类加载器，而启动类加载器在核心Java API发现这个名字的类，发现该类已被加载，并不会重新加载网络传递的过来的java.lang.Integer，而直接返回已加载过的Integer.class，这样便可以防止核心API库被随意篡改。 虚拟机字节码执行引擎运行时栈帧结构 局部变量表 存放方法参数和方法内部定义的局部变量； Java 程序编译为 class 文件时，就确定了每个方法需要分配的局部变量表的最大容量。 最小单位：Slot； 一个 Slot 中可以存放：boolean，byte，char，short，int，float，reference，returnAddress (少见)； 虚拟机可通过局部变量表中的 reference 做到： 查找 Java 堆中的实例对象的起始地址； 查找方法区中的 Class 对象。 局部变量表的空间分配 Slot 的复用定义： 如果当前位置已经超过某个变量的作用域时，例如出了定义这个变量的代码块，这个变量对应的 Slot 就可以给其他变量使用了。但同时也说明，只要其他变量没有使用这部分 Slot 区域，这个变量就还保存在那里，这会对 GC 操作产生影响。 对 GC 操作的影响： 123456public static void main(String[] args) &#123; &#123; byte[] placeholder = new byte[64 * 1024 * 1024]; &#125; System.gc();&#125; -verbose:gc 输出： 12[GC (System.gc()) 68813K-&gt;66304K(123904K), 0.0034797 secs][Full GC (System.gc()) 66304K-&gt;66204K(123904K), 0.0086225 secs] // 没有被回收 进行如下修改： 1234567public static void main(String[] args) &#123; &#123; byte[] placeholder = new byte[64 * 1024 * 1024]; &#125; int a = 1; // 新加一个赋值操作 System.gc();&#125; -verbose:gc 输出： 12[GC (System.gc()) 68813K-&gt;66320K(123904K), 0.0017394 secs][Full GC (System.gc()) 66320K-&gt;668K(123904K), 0.0084337 secs] // 被回收了 第二次修改后，placeholder 能被回收的原因？ placeholder 能否被回收的关键：局部变量表中的 Slot 是否还存在关于 placeholder 的引用； 出了 placeholder 所在的代码块后，还没有进行其他操作，所以 placeholder 所在的 Slot 还没有被其他变量复用，也就是说，局部变量表的 Slot 中依然存在着 placeholder 的引用； 第二次修改后，int a 占用了原来 placeholder 所在的 Slot，所以可以被 GC 掉了。 操作数栈 元素可以是任意 Java 类型，32 位数据占 1 个栈容量，64 位数据占 2 个栈容量； Java 虚拟机的解释执行称为：基于栈的执行引擎，其中 “栈” 指的就是操作数栈； 动态连接 指向运行时常量池中该栈帧所属方法的引用； 为了支持方法调用过程中的动态连接，什么是动态连接会在下一篇文章进行讲解，先知道有这么个东西就行。 方法返回地址 两种退出方法的方式： 遇到 return； 遇到异常。 退出方法时可能执行的操作： 恢复上层方法的局部变量表和操作数栈； 把返回值压入调用者栈帧的操作数栈； 调整 PC 计数器指向方法调用后面的指令。 方法调用Java 的方法的执行分为两个部分： 方法调用：确定被调用的方法是哪一个； 基于栈的解释执行：真正的执行方法的字节码。 在本节中我们将对方法调用进行详细的讲解，我们知道，一切方法的调用在 Class 文件中存储的都是常量池中的符号引用，而不是方法实际运行时的入口地址（直接引用），直到类加载的时候，甚至是实际运行的时候才回去会去确定要被运行的方法的直接引用，而确定要被运行的方法的直接引用的过程就叫做方法调用。 方法调用字节码指令Java 虚拟机提供了 5 个职责不同的方法调用字节码指令： invokestatic：调用静态方法； invokespecial：调用构造器方法、私有方法、父类方法； invokevirtual：调用所有虚方法，除了静态方法、构造器方法、私有方法、父类方法、final 方法的其他方法叫虚方法； invokeinterface：调用接口方法，会在运行时确定一个该接口的实现对象； invokedynamic：在运行时动态解析出调用点限定符引用的方法，再执行该方法。 除了 invokedynamic，其他 4 种方法的第一个参数都是被调用的方法的符号引用，是在编译时确定的，所以它们缺乏动态类型语言支持，因为动态类型语言只有在运行期才能确定接收者的类型，即变量的类型检查的主体过程在运行期，而非编译期。 final 方法虽然是通过 invokevirtual 调用的，但是其无法被覆盖，没有其他版本，无需对接收者进行多态选择，或者说多态选择的结果是唯一的，所以属于非虚方法。 解析调用解析调用，正如其名，就是 在类加载的解析阶段，就确定了方法的调用版本 。我们知道类加载的解析阶段会将一部分符号引用转化为直接引用，这一过程就叫做解析调用。因为是在程序真正运行前就确定了要调用哪一个方法，所以 解析调用能成立的前提就是：方法在程序真正运行前就有一个明确的调用版本了，并且这个调用版本不会在运行期发生改变。 符合这两个要求的只有以下两类方法： 通过 invokestatic 调用的方法：静态方法； 通过 invokespecial 调用的方法：私有方法、构造器方法、父类方法； 这两类方法根本不可能通过继承或者别的方式重写出来其他版本，也就是说，在运行前就可以确定调用版本了，十分适合在类加载阶段就解析好。它们会在类加载的解析阶被解析为直接引用，即确定调用版本。 分派调用在介绍分派调用前，我们先来介绍一下 Java 所具备的面向对象的 3 个基本特征：封装，继承，多态。 其中多态最基本的体现就是重载和重写了，重载和重写的一个重要特征就是方法名相同，其他各种不同： 重载：发生在同一个类中，入参必须不同，返回类型、访问修饰符、抛出的异常都可以不同； 重写：发生在子父类中，入参和返回类型必须相同，访问修饰符大于等于被重写的方法，不能抛出新的异常。 相同的方法名实际上给虚拟机的调用带来了困惑，因为虚拟机需要判断，它到底应该调用哪个方法，而这个过程会在分派调用中体现出来。其中： 方法重载 —— 静态分派 方法重写 —— 动态分派 静态分派（方法重载）在介绍静态分派前，我们先来介绍一下什么是变量的静态类型和实际类型。 变量的静态类型和实际类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class StaticDispatch &#123; static abstract class Human &#123; &#125; static class Man extends Human &#123; &#125; static class Woman extends Human &#123; &#125; public void sayHello(Human guy) &#123; System.out.println("Hello guy!"); &#125; public void sayHello(Man man) &#123; System.out.println("Hello man!"); &#125; public void sayHello(Woman woman) &#123; System.out.println("Hello woman!"); &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); StaticDispatch sr = new StaticDispatch(); sr.sayHello(man); sr.sayHello(woman); /* 输出： Hello guy! Hello guy! 因为是根据变量的静态类型，也就是左面的类型：Human 来判断调用哪个方法， 所以调用的都是 public void sayHello(Human guy) */ &#125;&#125;/* 简单讲解 */// 使用Human man = new Man();// 实际类型发生变化Human man = new Man();man = new Woman();// 静态类型发生变化sr.sayHello((Man) man); // 输出：Hello man!sr.sayHello((Woman) man); // 输出：Hello woman! 其中 Human 称为变量的静态类型，Man 称为变量的实际类型。 在重载时，编译器是通过方法参数的静态类型，而不是实际类型，来判断应该调用哪个方法的。 通俗的讲，静态分派就是通过方法的参数（类型 &amp; 个数 &amp; 顺序）这种静态的东西来判断到底调用哪个方法的过程。 重载方法匹配优先级，例如一个字符 ‘a’ 作为入参 基本类型 char int long float double Character Serializable（Character 实现的接口） 同时出现两个优先级相同的接口，如 Serializable 和 Comparable，会提示类型模糊，拒绝编译。 Object char…（变长参数优先级最低） 动态分派（方法重写）动态分派就是在运行时，根据实际类型确定方法执行版本的分派过程。 12345678910111213141516171819202122232425262728293031public class DynamicDispatch &#123; static abstract class Human &#123; protected abstract void sayHello(); &#125; static class Man extends Human &#123; protected void sayHello() &#123; System.out.println("Hello man"); &#125; &#125; static class Woman extends Human &#123; protected void sayHello() &#123; System.out.println("Hello woman"); &#125; &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); man.sayHello(); woman.sayHello(); man = woman; man.sayHello(); /* 输出 Hello man Hello woman Hello woman */ &#125;&#125; 字节码分析： 123456789101112131415161718192021222324public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=3, args_size=1 0: new #2 // class com/jvm/ch8/DynamicDispatch$Man 3: dup 4: invokespecial #3 // Method com/jvm/ch8/DynamicDispatch$Man.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: new #4 // class com/jvm/ch8/DynamicDispatch$Woman 11: dup 12: invokespecial #5 // Method com/jvm/ch8/DynamicDispatch$Woman.&quot;&lt;init&gt;&quot;:()V 15: astore_2 16: aload_1 // 把刚创建的对象的引用压到操作数栈顶， // 供之后执行sayHello时确定是执行哪个对象的sayHello 17: invokevirtual #6 // 方法调用 20: aload_2 // 把刚创建的对象的引用压到操作数栈顶， // 供之后执行sayHello时确定是执行哪个对象的sayHello 21: invokevirtual #6 // 方法调用 24: aload_2 25: astore_1 26: aload_1 27: invokevirtual #6 // Method com/jvm/ch8/DynamicDispatch$Human.sayHello:()V 30: return 通过字节码分析可以看出，invokevirtual 指令的运行过程大致为： 去操作数栈顶取出将要执行的方法的所有者，记作 C； 查找此方法： 在 C 中查找此方法； 在 C 的各个父类中查找； 查找过程： 查找与常量的描述符和简单名称都相同的方法； 进行访问权限验证，不通过抛出：IllegalAccessError 异常； 通过访问权限验证则返回直接引用； 没找到则抛出：AbstractMethodError 异常，即该方法没被实现。 动态分派在虚拟机种执行的非常频繁，而且方法查找的过程要在类的方法元数据中搜索合适的目标，从性能上考虑，不太可能进行如此频繁的搜索，需要进行性能上的优化。 常用优化手段： 在类的方法区中建立一个虚方法表。 虚方法表中存放着各个方法的实际入口地址，如果某个方法没有被子类方法重写，那子类方法表中该方法的入口地址 = 父类方法表中该方法的入口地址； 使用这个方法表索引代替在元数据中查找； 该方法表会在类加载的连接阶段初始化好。 通俗的讲，动态分派就是通过方法的接收者这种动态的东西来判断到底调用哪个方法的过程。 总结一下：静态分派看左面，动态分派看右面。 单分派与多分派除了静态分派和动态分派这种分派分类方式，还有一种根据宗量分类的方式，可以将方法分派分为单分派和多分派。 宗量：方法的接收者 &amp; 方法的参数。 Java 语言的静态分派属于多分派，根据 方法接收者的静态类型 和 方法参数类型 两个宗量进行选择。 Java 语言的动态分派属于单分派，只根据 方法接收者的实际类型 一个宗量进行选择。 动态类型语言支持什么是动态类型语言？ 就是类型检查的主体过程在运行期，而非编译期的编程语言。 动/静态类型语言各自的优点？ 动态类型语言：灵活性高，开发效率高。 静态类型语言：编译器提供了严谨的类型检查，类型相关的问题能在编码的时候就发现。 Java虚拟机层面提供的动态类型支持： invokedynamic 指令 java.lang.invoke 包 java.lang.invoke 包目的： 在之前的依靠符号引用确定调用的目标方法的方式之外，提供了 MethodHandle 这种动态确定目标方法的调用机制。 MethodHandle 的使用 获得方法的参数描述，第一个参数是方法返回值的类型，之后的参数是方法的入参： 1MethodType mt = MethodType.methodType(void.class, String.class); 获取一个普通方法的调用： 12345678/** * 需要的参数： * 1. 被调用方法所属类的类对象 * 2. 方法名 * 3. MethodType 对象 mt * 4. 调用该方法的对象 */MethodHandle.lookup().findVirtual(receiver.getClass(), "方法名", mt).bindTo(receiver); 获取一个父类方法的调用： 12345678/** * 需要的参数： * 1. 被调用方法所属类的类对象 * 2. 方法名 * 3. MethodType 对象 mt * 4. 调用这个方法的类的类对象 */MethodHandle.lookup().findSpecial(GrandFather.class, "方法名", mt, getClass()); 通过 MethodHandle mh 执行方法： 1234567/* invoke() 和 invokeExact() 的区别：- invokeExact() 要求更严格，要求严格的类型匹配，方法的返回值类型也在考虑范围之内- invoke() 允许更加松散的调用方式*/mh.invoke("Hello world");mh.invokeExact("Hello world"); 使用示例： 12345678910111213141516171819202122232425262728293031323334353637public class MethodHandleTest &#123; static class ClassA &#123; public void println(String s) &#123; System.out.println(s); &#125; &#125; public static void main(String[] args) throws Throwable &#123; /* obj的静态类型是Object，是没有println方法的，所以尽管obj的实际类型都包含println方法， 它还是不能调用println方法 */ Object obj = System.currentTimeMillis() % 2 == 0 ? System.out : new ClassA(); /* invoke()和invokeExact()的区别： - invokeExact()要求更严格，要求严格的类型匹配，方法的返回值类型也在考虑范围之内 - invoke()允许更加松散的调用方式 */ getPrintlnMH(obj).invoke("Hello world"); getPrintlnMH(obj).invokeExact("Hello world"); &#125; private static MethodHandle getPrintlnMH(Object receiver) throws NoSuchMethodException, IllegalAccessException &#123; /* MethodType代表方法类型，第一个参数是方法返回值的类型，之后的参数是方法的入参 */ MethodType mt = MethodType.methodType(void.class, String.class); /* lookup()方法来自于MethodHandles.lookup， 这句的作用是在指定类中查找符合给定的方法名称、方法类型，并且符合调用权限的方法句柄 */ /* 因为这里调用的是一个虚方法，按照Java语言的规则，方法第一个参数是隐式的，代表该方法的接收者， 也即是this指向的对象，这个参数以前是放在参数列表中进行传递，现在提供了bindTo()方法来完成这件事情 */ return MethodHandles.lookup().findVirtual(receiver.getClass(), "println", mt).bindTo(receiver); &#125;&#125; MethodHandles.lookup 中 3 个方法对应的字节码指令： findStatic()：对应 invokestatic findVirtual()：对应 invokevirtual &amp; invokeinterface findSpecial()：对应 invokespecial MethodHandle 和 Reflection 的区别 本质区别：它们都在模拟方法调用，但是 Reflection 模拟的是 Java 代码层次的调用； MethodHandle 模拟的是字节码层次的调用。 包含信息的区别： Reflection 的 Method 对象包含的信息多，包括：方法签名、方法描述符、方法的各种属性的Java端表达方式、方法执行权限等； MethodHandle 对象包含的信息比较少，既包含与执行该方法相关的信息。 invokedynamic 指令Lambda 表达式就是通过 invokedynamic 指令实现的。 JAVA类装载方式，有两种:1.隐式装载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中。 2.显式装载， 通过class.forname()等方法，显式加载需要的类类加载的动态性体现:一个应用程序总是由n多个类组成，Java程序启动时，并不是一次把所有的类全部加载后再运行，它总是先把保证程序运行的基础类一次性加载到jvm中，其它类等到jvm用到的时候再加载，这样的好处是节省了内存的开销，因为java最早就是为嵌入式系统而设计的，内存宝贵，这是一种可以理解的机制，而用到时再加载这也是java动态性的一种体现. 基于栈的字节码解释执行引擎这个栈，就是栈帧中的操作数栈。 解释执行先通过 javac 将代码编译成字节码，虚拟机再通过加载字节码文件，解释执行字节码文件生成机器码，解释执行的流程如下： 1词法分析 -&gt; 语法分析 -&gt; 形成抽象语法树 -&gt; 遍历语法树生成线性字节码指令流 指令集分类基于栈的指令集 优点： 可移植：寄存器由硬件直接提供，程序如果直接依赖这些硬件寄存器，会不可避免的受到硬件的约束； 代码更紧凑：字节码中每个字节对应一条指令，多地址指令集中还需要存放参数； 编译器实现更简单：不需要考虑空间分配问题，所需的空间都在栈上操作。 缺点： 执行速度稍慢 完成相同的功能，需要更多的指令，因为出入栈本身就产生相当多的指令； 频繁的栈访问导致频繁的内存访问，对于处理器而言，内存是执行速度的瓶颈。 示例： 两数相加 1234iconst_1 // 把常量1入栈iconst_1iadd // 把栈顶两元素出栈相加，结果入栈istore_0 // 把栈顶值存入第0个Slot中 基于寄存器的指令集示例： 两数相加 12mov eax, 1add eax, 1 执行过程分析1234567891011121314151617181920212223242526272829303132333435public class Architecture &#123; /* calc函数的字节码分析： public int calc(); descriptor: ()I flags: ACC_PUBLIC Code: stack=2, locals=4, args_size=1 // stack=2，说明需要深度为2的操作数栈 // locals=4，说明需要4个Slot的局部变量表 0: bipush 100 // 将单字节的整型常数值push到操作数栈 2: istore_1 // 将操作数栈顶的整型值出栈并存放到第一个局部变量Slot中 3: sipush 200 6: istore_2 7: sipush 300 10: istore_3 11: iload_1 // 将局部变量表第一个Slot中的整型值复制到操作数栈顶 12: iload_2 13: iadd // 将操作数栈中头两个元素出栈并相加，将结果重新入栈 14: iload_3 15: imul // 将操作数栈中头两个元素出栈并相乘，将结果重新入栈 16: ireturn // 返回指令，结束方法执行，将操作数栈顶的整型值返回给此方法的调用者 */ public int calc() &#123; int a = 100; int b = 200; int c = 300; return (a + b) * c; &#125; public static void main(String[] args) &#123; Architecture architecture = new Architecture(); architecture.calc(); &#125;&#125;]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的 I/O机制]]></title>
    <url>%2FJava%E7%9A%84IO%2F</url>
    <content type="text"><![CDATA[Java 的 I/O 大概可以分成以下几类： 磁盘操作：File 字节操作：InputStream 和 OutputStream 字符操作：Reader 和 Writer 对象操作：Serializable：序列化就是将一个对象转换成字节序列，方便存储和传输。 网络操作：Socket 新的输入/输出：NIO Java I/O 使用了装饰者模式来实现 深入分析 Java I/O 的工作机制Linux 网络 I/O 模型简介（图文） UNIX网络编程对I/O模型的分类Linux 的内核将所有外部设备都看做一个文件来操作（一切皆文件），对一个文件的读写操作会调用内核提供的系统命令，返回一个file descriptor（fd，文件描述符）。而对一个socket的读写也会有响应的描述符，称为socket fd（socket文件描述符），描述符就是一个数字，指向内核中的一个结构体（文件路径，数据区等一些属性）。 根据UNIX网络编程对I/O模型的分类，UNIX提供了5种I/O模型。 进程是无法直接操作I/O设备的，其必须通过系统调用请求内核来协助完成I/O动作，而内核会为每个I/O设备维护一个buffer。 整个请求过程为： 用户进程发起请求，内核接受到请求后，从I/O设备中获取数据到buffer中，再将buffer中的数据copy到用户进程的地址空间，该用户进程获取到数据后再响应客户端。 阻塞I/O模型最常用的I/O模型，默认情况下，所有文件操作都是阻塞的。 比如I/O模型下的套接字接口：在进程空间中调用recvfrom，其系统调用直到数据包到达（比如，还没有收到一个完整的UDP包）且被复制到应用进程的缓冲区中或者发生错误时才返回，而在用户进程这边，整个进程会被阻塞。当内核一直等到数据准备好了，它就会将数据从内核中拷贝到用户内存，然后内核返回结果，用户进程才解除block的状态，重新运行起来。 进程在调用recvfrom开始到它返回的整段时间内都是被阻塞的，所以叫阻塞I/O模型。所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 非阻塞I/O模型当用户进程调用recvfrom时，系统不会阻塞用户进程，而是立刻返回一个ewouldblock错误，从用户进程角度讲 ，并不需要等待，而是马上就得到了一个结果。用户进程判断标志是ewouldblock时，就知道数据还没准备好，于是它就可以去做其他的事了，于是它可以再次发送recvfrom，一旦内核中的数据准备好了。并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 当一个应用程序在一个循环里对一个非阻塞调用recvfrom，我们称为轮询。应用程序不断轮询内核，看看是否已经准备好了某些操作。这通常是浪费CPU时间，但这种模式偶尔会遇到。 I/O复用模型单个process就可以同时处理多个网络连接的IO。它的基本原理就是select/epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 Linux提供select/poll，进程通过将一个或多个fd（文件描述符）传递给select或poll系统调用，阻塞在select操作上，这样，select/poll可以帮我们侦测多个fd是否处于就绪状态。 select/poll是顺序扫描fd是否就绪，而且支持的fd数量有限，因此它的使用受到了一些制约。 Linux还提供一个epoll系统调用，epoll使用基于事件驱动方式代替顺序扫描，因此性能更高。当有fd就绪时，立即回调函数rollback。 I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，pselect，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。 IO多路复用之select、poll、epoll详解 当用户进程调用了select，那么整个进程会被block，而同时，内核会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从内核拷贝到用户进程。 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（多说一句。所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 信号驱动I/O模型首先开启套接口信号驱动I/O功能，并通过系统调用sigaction执行一个信号处理函数（此系统调用立即返回，进程继续工作，非阻塞）。当数据准备就绪时，就为改进程生成一个SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主循环函数处理树立。 异步I/O告知内核启动某个操作，并让内核在整个操作完成后（包括数据的复制）通知进程。 信号驱动I/O模型通知的是何时可以开始一个I/O操作，异步I/O模型有内核通知I/O操作何时已经完成。 I/O多路复用技术I/O编程中，需要处理多个客户端接入请求时，可以利用多线程或者I/O多路复用技术进行处理。 正如前面的简介，I/O多路复用技术通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。 与传统的多线程模型相比，I/O多路复用的最大优势就是系统开销小，系统不需要创建新的额外线程，也不需要维护这些线程的运行，降低了系统的维护工作量，节省了系统资源。 主要的应用场景： 服务器需要同时处理多个处于监听状态或多个连接状态的套接字。 服务器需要同时处理多种网络协议的套接字。 支持I/O多路复用的系统调用主要有select、pselect、poll、epoll。 而当前推荐使用的是epoll，优势如下： 支持一个进程打开的socket fd不受限制。 I/O效率不会随着fd数目的增加而线性下将。 使用mmap加速内核与用户空间的消息传递。 epoll拥有更加简单的API。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java容器]]></title>
    <url>%2FJava%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Collection List Arraylist： Object数组 Vector： Object数组 LinkedList： 双向链表(JDK1.6之前为循环链表，JDK1.7取消了循环) Set HashSet（无序，唯一）: 基于 HashMap 实现的，底层采用 HashMap 来保存元素 LinkedHashSet： LinkedHashSet 继承与 HashSet，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的LinkedHashMap 其内部是基于 Hashmap 实现一样，不过还是有一点点区别的。 TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树。) Map HashMap： JDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间 LinkedHashMap: LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。 HashTable: 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap: 红黑树（自平衡的排序二叉树） ArrayList、LinkedList 、VectorArrayList 与 LinkedList 异同 相同点： 都实现了 List 接口。 都是线程不安全的。 不同点： 底层实现： ArrayList 底层使用的是 Object 数组；LinkedList 底层使用的是双向链表数据结构，维护一个 head 指针和一个 tail 指针。（JDK 1.6 之前为循环链表。为啥要改：因为在链表头 / 尾进行插入 / 删除操作时，循环链表需要处理两头的指针，而非循环链表只需要处理一边，更高效，同时在两头（链头 / 链尾）操作是最普遍的。） 基本操作的时间复杂度：ArrayList 可以高效的访问元素 O(1)，但是不能高效的插入和删除元素 O(n)；LinkedList 可以高效的插入和删除元素 O(1)，但是不能高效访问的元素 O(n)。 内存空间占用：ArrayList 的空间浪费主要体现在在 list 列表的结尾会预留一定的容量空间；而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间。 LinkedList包含两个重要的成员：header 和 size。header是双向链表的表头，它是双向链表节点所对应的类Entry的实例。Entry中包含成员变量： previous, next, element。其中，previous是该节点的上一个节点，next是该节点的下一个节点，element是该节点所包含的值。size是双向链表中节点的个数。 补充：RandomAccess 接口public interface RandomAccess {}RandomAccess 接口里啥都没有，和 Serializable 接口一样，是个标识接口。它标识实现这个接口的类具有随机访问功能。 这个标识有啥用？在 Collections.binarySearch() 方法里有用： 123456public static &lt;T&gt; int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) &#123; if (list instanceof RandomAccess || list.size()&lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key); &#125; 在 binarySearch() 方法中，它要判断传入的 list 是否 RamdomAccess 的实例，如果是，调用 indexedBinarySearch() 方法，如果不是，那么调用 iteratorBinarySearch() 方法indexedBinarySearch() 方法和 iteratorBinarySearch() 方法的区别在于：需要使用 indexedBinarySearch() 方法的集合，是直接通过索引 i 取变量的，而需要使用 iteratorBinarySearch() 方法的集合要取到这个集合的迭代器用来取元素： 123456ListIterator&lt;? extends Comparable&lt;? super T&gt;&gt; i = list.listIterator();while (low &lt;= high) &#123; int mid = (low + high) &gt;&gt;&gt; 1; Comparable&lt;? super T&gt; midVal = get(i, mid); // 取元素 ... &#125; ArrayList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的。 实现了RandomAccess接口的list，优先选择普通for循环 ，其次foreach 未实现RandomAccess接口的list， 优先选择iterator遍历（foreach遍历底层也是通过iterator实现的），大 size的数据，千万不要使用普通for循环 ArrayList 与 Vector 区别Vector 类是线程安全的，给所有会出现线程安全问题的方法都加上 synchronized 修饰，所以很慢。 HashMap、HashTable、TreeMap Hash 算法 加法 Hash：把输入元素一个一个的加起来构成最后的结果。 位运算 Hash：这类型 Hash 函做通过利用各种位运算（常见的是移位和异或）来充分的混合输入元素。 乘法 Hash：这种类型的 Hash 函数利用了乘法的不相关性（乘法的这种性质，最有名的莫过于平方取关尾的随机数生成算法，虽然这种算法效果并不好]；jdk5.0 里面的 String 类的 hashCode() 方法也使用乘法Hash；32 位 FNV 算法 除法 Hash：除法和乘法一样，同样具有表面上看起来的不相关性。不过，因为除法太慢，这种方式几乎找不到真正的应用 查表 Hash：查表 Hash 最有名的例子莫过于 CRC 系列算法。虽然 CRC 系列算法本身并不是查表，但是，查表是它的一种最快的实现方式。查表 Hash 中有名的子有：Universal Hashing 和 Zobrist Hashing。他们的表格都是随机生成的。 混合 Hash：混合 Hash 算法利用了以上各种方式。各种常见的 Hash 算法，比如 MD5、Tiger 都属于这个范围。它们一般很少在面向查找的 Hash 函做里面使用 基本区别它们都是最常见的 Map 实现，是以键值对的形式存储数据的容器类型。 HashTable： 线程安全，不支持 null 作为键或值，它的线程安全是通过在所有存在线程安全问题的方法上加 synchronized 实现的，所以性能很差，很少使用。 HashMap： 不是线程安全的，但是支持 null 作为键或值，是绝大部分利用键值对存取场景的首选，put 和 get 基本可以达到常数级别的时间复杂度。 TreeMap： 基于红黑树的一种提供顺序访问的 Map，它的 get，put，remove 等操作是 O(log(n)) 级别的时间复杂度的（因为要保证顺序），具体的排序规则可以由 Comparator 指定：public TreeMap(Comparator&lt;? super K&gt; comparator)。 HashMap 和 HashTable 的区别总结 线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过 synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： HashMap 要比 HashTable 效率高。 对 null key 和 null value 的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同： 创建时不指定容量初始值：HashTable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。 创建时给定了容量初始值：Hashtable 会直接使用你给定的大小，而 HashMap 会使用 tableSizeFor 方法将其扩充为 2 的幂次方大小。 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以减少搜索时间。HashTable 没有这样的机制。 HashMap 的长度为什么要是 2 的幂次方 因为 hashCode 是 -2147483648 到 2147483647 的，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个40亿长度的数组，内存是放不下的。在决定一个元素在哈希表中的真正位置时是要进行 hashCode % n 的运算的（n 是存元素的哈希数组的长度），得到的余数才能用来要存放的位置也就是对应的数组下标，如果 n 是 2 的幂次方的话，这个操作是可以用位运算来解决的：(n - 1) &amp; hash，快. 在对 Map 的顺序没有要求的情况下，HashMap 基本是最好的选择，不过 HashMap 的性能十分依赖于 hashCode 的有效性，所以必须满足： equals 判断相等的对象的 hashCode 一定相等 重写了 hashCode 必须重写 equals hashCode()、equals()、==问题Java hashCode() 和 equals()的若干问题解答 equals() ：定义在JDK的Object.java中。通过判断两个对象的地址是否相等(即，是否是同一个对象)来区分它们是否相等。 若某个类没有覆盖equals()方法，当它的通过equals()比较两个对象时，实际上是比较两个对象是不是同一个对象。这时，等价于通过“==”去比较这两个对象。 我们可以覆盖类的equals()方法，来让equals()通过其它方式比较两个对象是否相等。通常的做法是：若两个对象的内容相等，则equals()方法返回true；否则，返回fasle。 == : 作用是判断两个对象的地址是不是相等。即，判断两个对象是不试同一个对象。 hashCode() ：作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。“将该对象的内部地址转换成一个整数返回”。散列表的本质是通过数组实现的。当我们要获取散列表中的某个“值”时，实际上是要获取数组中的某个位置的元素。而数组的位置，就是通过“键”来获取的；更进一步说，数组的位置，是通过“键”对应的散列码计算得到的。 若重写了equals(Object obj)方法，则有必要重写hashCode()方法。（不然虽然equals认为相等，但是HashSet在添加p1和p2的时候，认为它们不相等） 若两个对象equals(Object obj)返回true，则hashCode（）有必要也返回相同的int数。 若两个对象equals(Object obj)返回false，则hashCode（）不一定返回不同的int数。 （不会在HashSet, Hashtable, HashMap等等这些本质是散列表的数据结构中用到该类，在这种情况下，该类的“hashCode() 和 equals() ”没有半毛钱关系的！） 若两个对象hashCode（）返回相同int数，则equals（Object obj）不一定返回true。（在散列表中，hashCode()相等，即两个键值对的哈希值相等。然而哈希值相等，并不一定能得出键值对相等。补充说一句：“两个不同的键值对，哈希值相等”，这就是哈希冲突。） 若两个对象hashCode（）返回不同int数，则equals（Object obj）一定返回false。 同一对象在执行期间若已经存储在集合中，则不能修改影响hashCode值的相关信息，否则会导致内存泄露问题。 一般一个类的对象如果会存储在HashTable，HashSet,HashMap等散列存储结构中，那么重写equals后最好也重写hashCode，否则会导致存储数据的不唯一性（存储了两个equals相等的数据）。而如果确定不会存储在这些散列结构中，则可以不重写hashCode。 我们注意到，除了 TreeMap，LinkedHashMap 也可以保证某种顺序，它们的 区别 如下： LinkedHashMap：提供的遍历顺序符合插入顺序，是通过为 HashEntry 维护一个双向链表实现的。 TreeMap：顺序由键的顺序决定，依赖于 Comparator。 HashMap 多线程操作导致死循环问题在多线程下，进行 put 操作会导致 HashMap 死循环，原因在于 HashMap 的扩容 resize()方法。由于扩容是新建一个数组，复制原数据到数组。由于数组下标挂有链表，所以需要复制链表，但是多线程操作有可能导致环形链表。jdk1.8已经解决了死循环的问题。 HashMap 源码分析HashMap 的内部结构如下： Java8以后，数组+链表+红黑树。。O(n)-&gt;O(logn) 解决哈希冲突的常用方法： 开放地址法：出现冲突时，以当前哈希值为基础，产生另一个哈希值。 再哈希法：同时构造多个不同的哈希函数，发生冲突就换一个哈希方法。 链地址法：将哈希地址相同的元素放在一个链表中，然后把这个链表的表头放在哈希表的对应位置。 建立公共溢出区：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。 拉链法处理冲突简单，且无堆积现象，即非同义词决不会发生冲突，因此平均查找长度较短； 由于拉链法中各链表上的结点空间是动态申请的，故它更适合于造表前无法确定表长的情况； 开放定址法为减少冲突，要求装填因子α较小，故当结点规模较大时会浪费很多空间。而拉链法中可取α≥1，且结点较大时，拉链法中增加的指针域可忽略不计，因此节省空间； 在用拉链法构造的散列表中，删除结点的操作易于实现。只要简单地删去链表上相应的结点即可。而对开放地址法构造的散列表，删除结点不能简单地将被删结 点的空间置为空，否则将截断在它之后填人散列表的同义词结点的查找路径。这是因为各种开放地址法中，空地址单元（即开放地址）都是查找失败的条件。因此在用开放地址法处理冲突的散列表上执行删除操作，只能在被删结点上做删除标记，而不能真正删除结点。 拉链法的缺点：指针需要额外的空间，故当结点规模较小时，开放定址法较为节省空间，而若将节省的指针空间用来扩大散列表的规模，可使装填因子变小，这又减少了开放定址法中的冲突，从而提高平均查找速度。 HashMap 采用的是链表地址法，不过如果由一个位置的链表比较长了（超过阈值 8 了），链表会被改造为树形结构以提高查找性能。 这个桶数组并没有在 HashMap 的构造函数中初始化好，只是设置了容量（默认初始容量为 16），应该是采用了 lazy-load 原则。 12345public HashMap(int initialCapacity, float loadFactor)&#123; // ... this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 接下来，我们看一下 put 方法：123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 可以看到，put 方法调用了 putVal 方法： 1234567891011121314151617181920212223242526272829303132final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // lazy-load，tab要是空的，用resize初始化它 // resize既要负责初始化，又要负责在容量不够时扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 无哈希冲突，直接new一个节点放到tab[i]就行 // 具体键值对在哈希表中的位置：i = (n - 1) &amp; hash if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 该key存在，直接修改value就行 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 当前hashCode下面挂的已经是颗树了，用树的插入方式插入新节点 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 当前hashCode下面挂的还是个链表，不过保不齐会变成颗树 else &#123; // ... if (binCount &gt;= TREEIFY_THRESHOLD - 1) // 链表要变树啦！ treeifyBin(tab, hash); // ... &#125; &#125; ++modCount; if (++size &gt; threshold) // 容量不够了，扩容 resize();&#125; 分析： key 的 hashCode 用的并不是 key 自己的 hashCode，而是通过 HashMap 内部的一个 hash 方法另算的，这东西叫扰动函数。那么为什么要另算一个 hashCode 呢？这是因为： 有些数据计算出的哈希值差异主要在高位，而 HashMap 里的哈希寻址是忽略容量以上的高位的，这种处理可以有效避免这种情况下的哈希碰撞。 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; resize 方法： （重点！JDK1.7以前有毛病）现在的写法不会出现链表扩容时发生死循环了，以前的写法相当于将 oldTab 上的 Node 一个一个卸下来，然用头插法的方式插入到 newTab 的对应位置，因为用的是头插法，会给链表倒序，这种倒序导致了在多线程时，链表的两个 Node 的 next 可能会互相指向对方，出现死循环 探究HashMap线性不安全（二）——链表成环的详细过程。现在的方法是使用尾插法，即不会改变链表原来在 oldTab 挂着的时候的相对顺序，在 oldTab[j] 处的链表会根据 hash 值分成 lo 和 hi 两个链表，然后分别挂在 newTab 的 newTab[j] 和 newTab[j + oldCap] 两个不同的位置。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; // oldTab 的长度，一定是 2 的幂，也就是说，二进制只有一位为 1 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // MAXIMUM_CAPACITY = 1 &lt;&lt; 30，如果超过这个容量就扩不了容了 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // newCap = oldCap &lt;&lt; 1，容量变成原来的 2 倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 把 oldTab 中的数据移到 newTab 中，这里是要进行 rehash 的 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; // 把 oldTab 中的非 null 元素放到 newTab 去 oldTab[j] = null; // 把链表从 oldTab[j] 上取下来 if (e.next == null) // oldTab[j] 处只有一个元素 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // oldTab[j] 处是一颗树 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // oldTab[j] 处是一个长度不超过 8 链表 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; /* 重点！！！ 下面将根据 (e.hash &amp; oldCap) == 0 将原来 oldTab[j] 处的链表分成 lo 和 hi 两个链表，为什么要这么分呢？ 因为挂在 oldTab[j] 处的节点都是 hash % oldCap == j 的，但是现在， hash % newCap 的结果有了以下两种可能： - hash % newCap == j； - hash % newCap == j + oldCap。 如何区分这两种情况呢？就是通过 (e.hash &amp; oldCap) == 0 来区分的， - 如果 (e.hash &amp; oldCap) == 0，为 hash % newCap == j； - 如果 (e.hash &amp; oldCap) != 0，为 hash % newCap == j + oldCap。 */ if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) // 第一次执行 do-while 循环 loHead = e; // 用 loHead 记录 oldTab[j] 处链表的第一个 Node else // 非第一次执行 do-while 循环 loTail.next = e; // 把当前节点 e 挂到 lo 链表上 loTail = e; // 移动 lo 链表的尾结点指针到当前节点 e &#125; else &#123; // hi 链表的处理方式和上面的 lo 链表一样 if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; // 如果 lo 链表不为空 loTail.next = null; newTab[j] = loHead; // 把 lo 链表挂到 newTab[j] 上 &#125; if (hiTail != null) &#123; // 如果 hi 链表不为空 hiTail.next = null; newTab[j + oldCap] = hiHead; // 把 hi 链表挂到 newTab[j + oldCap] 上 &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 容量、负载因子和树化 容量和负载因子决定了桶数组中的桶数量，如果桶太多了会浪费空间，但桶太少又会影响性能。我们要保证： 负载因子 * 容量 &gt; 元素数量 &amp;&amp; 容量要是 2 的倍数 对于负载因子： 如果没有特别需求，不要轻易更改； 如果需要调整，不要超过 0.75，否则会显著增加冲突； 如果使用太小的负载因子，也要同时调整容量，否则可能会频繁扩容，影响性能。 那么为什么哈希数组的一个位置挂的链表的长度超过 8 要树化呢？ 这本质上是一个安全问题，我们知道如果同一个哈希值对应位置的链表太长，会极大的影响性能，而在现实世界中，构造哈希冲突的数据并不是十分复杂的事情，恶意代码可以利用这些数据与服务端进行交互，会导致服务端 CPU 大量占用，形成哈希碰撞拒绝服务攻击。 HashSetHashSet 底层就是基于 HashMap 实现的。add 的元素会被放在 HashMap 的放 key 的地方，HashMap 放 value 的地方放了一个 private static final Object PRESENT = new Object();。 除了 clone() 方法、writeObject() 方法、readObject() 方法是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法实现的。 TreeSetTreeSet 的底层实现是一颗红黑树，那么什么是红黑树呢？ 红黑树是一颗自平衡的二叉查找树，它从根节点到叶子节点的最长路径不会超过最短路径的 2 倍。除此之外，它还具有如下 5 个特点： 节点分为红色或黑色。 根节点一定是黑色的。 每个叶子节点一定是黑色的 null 节点。 每个红色节点的两个子节点都是黑色，即从每个叶子到根的所有路径上不能有两个连续的红色节点（但黑节点的子节点可以还是黑节点，就红节点事多……）。 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 红黑树在插入和删除节点的时候，可能破坏以上 5 条规则，一旦规则被破坏，红黑树主要依靠以下 3 个操作来恢复： 变色 逆时针旋转 顺时针旋转 红黑树的插入与删除详见：教你透彻了解红黑树。 ConcurrentHashMap特点 ConcorrentHashMap 实现了 ConcorrentMap 接口，能在并发环境实现更高的吞吐量，而在单线程环境中只损失很小的性能； 采用分段锁，使得任意数量的读取线程可以并发地访问 Map，一定数量的写入线程可以并发地修改 Map； 不会抛出 ConcorrentModificationException，它返回迭代器具有“弱一致性”，即可以容忍并发修改，但不保证将修改操作反映给容器； size() 的返回结果可能已经过期，只是一个估计值，不过 size() 和 isEmpty() 方法在并发环境中用的也不多； 提供了许多原子的复合操作： V putIfAbsent(K key, V value);：K 没有相应映射才插入 boolean remove(K key, V value);：K 被映射到 V 才移除 boolean replace(K key, V oldValue, V newValue);：K 被映射到 oldValue 时才替换为 newValue ConcurrentHashMap 内部结构： JDK1.7:在构造的时候，Segment 的数量由所谓的 concurrentcyLevel 决定，默认是 16； Segment 是基于 ReentrantLock 的扩展实现的，在 put 的时候，会对修改的区域加锁。 JDK1.8的实现已经摒弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap，虽然在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本 锁分段实现原理不同线程在同一数据的不同部分上不会互相干扰，例如，ConcurrentHashMap 支持 16 个并发的写入器，是用 16 个锁来实现的。它的实现原理如下： 使用了一个包含 16 个锁的数组，每个锁保护所有散列桶的 1/16，其中第 N 个散列桶由第（N % 16）个锁来保护； 这大约能把对于锁的请求减少到原来的 1/16，也是 ConcurrentHashMap 最多能支持 16 个线程同时写入的原因； 对于 ConcurrentHashMap 的 size() 操作，为了避免枚举每个元素，ConcurrentHashMap 为每个分段都维护了一个独立的计数，并通过每个分段的锁来维护这个值，而不是维护一个全局计数； 代码示例： 123456789101112131415161718192021222324252627282930313233343536public class StripedMap &#123; // 同步策略：buckets[n]由locks[n % N_LOCKS]保护 private static final int N_LOCKS = 16; private final Node[] buckets; private final Object[] locks; // N_LOCKS个锁 private static class Node &#123; Node next; Object key; Object value; &#125; public StripedMap(int numBuckets) &#123; buckets = new Node[numBuckets]; locks = new Object[N_LOCKS]; for (int i = 0; i &lt; N_LOCKS; i++) locks[i] = new Object(); &#125; private final int hash(Object key) &#123; return Math.abs(key.hashCode() % buckets.length); &#125; public Object get(Object key) &#123; int hash = hash(key); synchronized (locks[hash % N_LOCKS]) &#123; // 分段加锁 for (Node m = buckets[hash]; m != null; m = m.next) if (m.key.equals(key)) return m.value; &#125; return null; &#125; public void clear() &#123; for (int i = 0; i &lt; buckets.length; i++) &#123; synchronized (locks[i % N_LOCKS]) &#123; // 分段加锁 buckets[i] = null; &#125; &#125; &#125;&#125; 注意 关于 put 操作： 是否需要扩容 在插入元素前判断是否需要扩容， 比 HashMap 的插入元素后判断是否需要扩容要好，因为可以插入元素后，Map 扩容，之后不再有新的元素插入，Map就进行了一次无效的扩容 如何扩容 先创建一个容量是原来的2倍的数组，然后将原数组中的元素进行再散列后插入新数组中 为了高效，ConcurrentHashMap 只对某个 segment 进行扩容 关于 size 操作： 存在问题：如果不进行同步，只是计算所有 Segment 维护区域的 size 总和，那么在计算的过程中，可能有新的元素 put 进来，导致结果不准确，但如果对所有的 Segment 加锁，代价又过高。 解决方法：重试机制，通过获取两次来试图获取 size 的可靠值，如果没有监控到发生变化，即 Segment.modCount 没有变化，就直接返回，否则获取锁进行操作。 JDK 1.8 的改变ConcurrentHashMap 取消了 Segment 分段锁，采用 CAS 和 synchronized 来保证并发安全。数据结构跟 HashMap1.8 的结构类似，数组 + 链表 / 红黑二叉树。 synchronized 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，效率又提升 N 倍。 ConcurrentHashMap和HashTable的区别 底层数据结构： JDK1.7的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟 HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： ConcurrentHashMap（分段锁）：在JDK1.7的时候， 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本； Hashtable(同一把锁) ：使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低 LinkedHashMap简单的来说，LinkedHashMap 就是在 HashMap 的基础上加了一条双向链表用来维护 LinkedHashMap 中元素的插入顺序。 LinkedHashMap extends HashMap 且 LinkedHashMap.Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt;，它们的结构图如下： LinkedHashMap.Entry&lt;K,V&gt; 的结构： LinkedHashMap 的结构： 迭代器 Iterator对容器进行迭代操作时，我们要考虑它是不是会被其他的线程修改，如果是我们自己写代码，可以考虑通过如下方式对容器的迭代操作加锁： 1234synchronized (vector) &#123; for (int i = 0; i &lt; vector.size(); i++) doSomething(vector.get(i));&#125; 不过 Java 自己的同步容器类并没有考虑并发修改的问题，它主要采用了一种 快速失败 (fail-fast) 的方法，即一旦容器被其他线程修改，它就会抛出异常，例如 Vector 类，它的内部实现是这样的： 12345678910synchronized (Vector.this) &#123; // 类名.this：在内部类中，要用到外围类的 this 对象，使用“外围类名.this” checkForComodification(); // 在进行 next 和 remove 操作前，会先检查以下容器是否被修改 ...&#125;/* checkForComodification()方法 */final void checkForComodification() &#123; if (modCount != expectedModCount) // 在 Itr 的成员变量中有一个：int exceptedModCount = modCount; throw new ConcurrentModificationException(); // 如果容器被修改了，modCount 会变&#125; 因此，我们在调用 Vector 的如下方法时，要小心，因为它们会隐式的调用 Vector 的迭代操作。 toString hashCode equals containsAll removeAll retainAll Iterator 的 安全失败 (fail-safe) 是基于对底层集合做拷贝实现的，因此，它不受源集合上修改的影响。 Enumeration接口Enumeration 接口的作用与 Iterator 接口类似，但只提供了遍历 Vector 和 Hashtable 类型集合元素的功能，不支持元素的移除操作。 例如：遍历Vector v中的元素： 12for (Enumeration&lt;E&gt; e = v.elements();e.hasMoreElements();)System.out.println(e.nextElement()); Iterator 接口添加了一个可选的移除操作，并使用较短的方法名。新的实现应该优先考虑使用 Iterator 接口而不是 Enumeration 接口。 区别：Enumeration速度是Iterator的2倍，同时占用更少的内存。但是，Iterator远远比Enumeration安全，因为其他线程不能够修改正在被iterator遍历的集合里面的对象。同时，Iterator允许调用者删除底层集合里面的元素，这对Enumeration来说是不可能的。 Iterator 接口的用法： 12345Iterator it = list.iterator();while(it.hasNext())&#123; System.out.println(it.next());&#125; 容器中的设计模式迭代器模式：Collection 实现了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历 Collection 中的元素。 适配器模式：java.util.Arrays#asList() 可以把数组类型转换为 List 类型。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot实现基于Token的web后台认证机制]]></title>
    <url>%2FSpringBoot%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EToken%E7%9A%84web%E5%90%8E%E5%8F%B0%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[几种常用的认证机制HTTP Basic Auth、OAuth、Cookie Auth、Token Auth HTTP Basic AuthHTTP Basic Auth简单点说明就是每次请求API时都提供用户的username和password，简言之，Basic Auth是配合RESTful API 使用的最简单的认证方式，只需提供用户名密码即可，但由于有把用户名密码暴露给第三方客户端的风险，在生产环境下被使用的越来越少。因此，在开发对外开放的RESTful API时，尽量避免采用HTTP Basic Auth OAuth2.0OAuth（开放授权）是一个开放的授权标准，允许用户让第三方应用访问该用户在某一web服务上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和密码提供给第三方应用。 Cookie AuthCookie认证机制就是为一次请求认证在服务端创建一个Session对象，同时在客户端的浏览器端创建了一个Cookie对象；通过客户端带上来Cookie对象来与服务器端的session对象匹配来实现状态管理的。默认的，当我们关闭浏览器的时候，cookie会被删除。但可以通过修改cookie 的expire time使cookie在一定时间内有效； Token AuthToken机制相对于Cookie机制又有什么好处呢？ 支持跨域访问: Cookie是不允许垮域访问的，这一点对Token机制是不存在的，前提是传输的用户认证信息通过HTTP头传输. 无状态(也称：服务端可扩展行):Token机制在服务端不需要存储session信息，因为Token 自身包含了所有登录用户的信息，只需要在客户端的cookie或本地介质存储状态信息. 更适用CDN: 可以通过内容分发网络请求你服务端的所有资料（如：javascript，HTML,图片等），而你的服务端只要提供API即可. 去耦: 不需要绑定到一个特定的身份验证方案。Token可以在任何地方生成，只要在你的API被调用的时候，你可以进行Token生成调用即可. 更适用于移动应用: 当你的客户端是一个原生平台（iOS, Android，Windows 8等）时，Cookie是不被支持的（你需要通过Cookie容器进行处理），这时采用Token认证机制就会简单得多。 CSRF:因为不再依赖于Cookie，所以你就不需要考虑对CSRF（跨站请求伪造）的防范。 性能: 一次网络往返时间（通过数据库查询session信息）总比做一次HMACSHA256计算 的Token验证和解析要费时得多. 不需要为登录页面做特殊处理: 如果你使用Protractor 做功能测试的时候，不再需要为登录页面做特殊处理. 基于标准化:你的API可以采用标准化的 JSON Web Token (JWT). 这个标准已经存在多个后端库（.NET, Ruby, Java,Python, PHP）和多家公司的支持（如：Firebase,Google, Microsoft）. 基于Token的身份验证流程如下 客户端使用用户名跟密码请求登录 服务端收到请求，去验证用户名与密码 验证成功后，服务端会签发一个 Token，再把这个 Token 发送给客户端 客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里或者 Local Storage 里 客户端每次向服务端请求资源的时候需要带着服务端签发的 Token 服务端收到请求，然后去验证客户端请求里面带着的 Token，如果验证成功，就向客户端返回请求的数据 Spring Boot 实现实现见UserService，LoginController，PassportInterceptor12345678public class LoginTicket&#123; private int id; private int userId; private Date expired; private int status;//0有效 private String ticket;&#125; 用户先去请求注册或者是登陆，然后服务器去验证他的用户名和密码 验证成功后userservice会生成一个Token，这里是ticket，客户端收到ticket之后呢会把ticket存在Cookie中，登录成功之后会有一个与当前用户对应的ticket 每次访问服务器资源的时候需要带着这个ticket，然后怎么判断是否有呢？就要用拦截器来实现过滤，用拦截器去判断这个ticket当前的状态是什么样的？有没有过期？身份状态是不是有效的？然后根据这个来判断应该赋予什么样的权限？当验证成功之后就把ticket对应的用户的通过下面一段发送给freemaker的上下文，实现页面的正常的渲染 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.zhaole.interceptor;import com.zhaole.dao.LoginTicketDAO;import com.zhaole.dao.UserDAO;import com.zhaole.model.HostHolder;import com.zhaole.model.LoginTicket;import com.zhaole.model.User;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.Date;/** * 拦截器 * @ 用来判断用户的 *1. 当preHandle方法返回false时，从当前拦截器往回执行所有拦截器的afterCompletion方法，再退出拦截器链。也就是说，请求不继续往下传了，直接沿着来的链往回跑。 2.当preHandle方法全为true时，执行下一个拦截器,直到所有拦截器执行完。再运行被拦截的Controller。然后进入拦截器链，运 行所有拦截器的postHandle方法,完后从最后一个拦截器往回执行所有拦截器的afterCompletion方法. */@Componentpublic class PasswordInterceptor implements HandlerInterceptor&#123; @Autowired private LoginTicketDAO loginTicketDAO; @Autowired private UserDAO userDAO; @Autowired private HostHolder hostHolder; @Override public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception &#123; String ticket = null; if(httpServletRequest.getCookies()!=null) &#123; for(Cookie cookie:httpServletRequest.getCookies()) &#123; if(cookie.getName().equals("ticket")) &#123; ticket = cookie.getValue(); break; &#125; &#125; &#125; if(ticket!=null)//说明cookie不空，且name是ticket &#123; //去数据库里把ticket找出来看看是否有效 LoginTicket loginTicket = loginTicketDAO.selectByTicket(ticket); if(loginTicket==null || loginTicket.getExpired().before(new Date()) || loginTicket.getStatus()!=0) &#123; return true; &#125; //有效的话，查这个ticket对应的用户，把这个用户设置到threadlocal里。 User user = userDAO.selectById(loginTicket.getUserId()); hostHolder.setUser(user); &#125; return true; &#125; @Override public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception &#123; if (modelAndView != null &amp;&amp; hostHolder.getUser() != null) &#123; modelAndView.addObject("user", hostHolder.getUser()); &#125; &#125; @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception &#123; hostHolder.clear(); &#125;&#125; 当用户登出的时候就把ticket的身份状态置位为无效状态即可]]></content>
      <tags>
        <tag>Spring Boot</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你如何迅速秒杀掉：99%的海量数据处理面试题]]></title>
    <url>%2F%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E8%BF%85%E9%80%9F%E7%A7%92%E6%9D%80%E6%8E%89%EF%BC%9A99-%E7%9A%84%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[原文地址 分而治之/hash映射 + hash统计 + 堆/快速/归并排序； 双层桶划分 Bloom filter/Bitmap； Trie树/数据库/倒排索引； 外排序； 分布式处理之Hadoop/Mapreduce。 一、从set/map谈到hashtable/hash_map/hash_set稍后本文第二部分中将多次提到hash_map/hash_set，下面稍稍介绍下这些容器，以作为基础准备。一般来说，STL容器分两种， 序列式容器(vector/list/deque/stack/queue/heap)， 关联式容器。关联式容器又分为set(集合)和map(映射表)两大类，以及这两大类的衍生体multiset(多键集合)和multimap(多键映射表)，这些容器均以RB-tree完成。此外，还有第3类关联式容器，如hashtable(散列表)，以及以hashtable为底层机制完成的hash_set(散列集合)/hash_map(散列映射表)/hash_multiset(散列多键集合)/hash_multimap(散列多键映射表)。也就是说，set/map/multiset/multimap都内含一个RB-tree，而hash_set/hash_map/hash_multiset/hash_multimap都内含一个hashtable。 所谓关联式容器，类似关联式数据库，每笔数据或每个元素都有一个键值(key)和一个实值(value)，即所谓的Key-Value(键-值对)。当元素被插入到关联式容器中时，容器内部结构(RB-tree/hashtable)便依照其键值大小，以某种特定规则将这个元素放置于适当位置。 包括在非关联式数据库中，比如，在MongoDB内，文档(document)是最基本的数据组织形式，每个文档也是以Key-Value（键-值对）的方式组织起来。一个文档可以有多个Key-Value组合，每个Value可以是不同的类型，比如String、Integer、List等等。123&#123; &quot;name&quot; : &quot;July&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 23 &#125; set/map/multiset/multimap set，同map一样，所有元素都会根据元素的键值自动被排序，因为set/map两者的所有各种操作，都只是转而调用RB-tree的操作行为，不过，值得注意的是，两者都不允许两个元素有相同的键值。 不同的是：set的元素不像map那样可以同时拥有实值(value)和键值(key)，set元素的键值就是实值，实值就是键值，而map的所有元素都是pair，同时拥有实值(value)和键值(key)，pair的第一个元素被视为键值，第二个元素被视为实值。 至于multiset/multimap，他们的特性及用法和set/map完全相同，唯一的差别就在于它们允许键值重复，即所有的插入操作基于RB-tree的insert_equal()而非insert_unique()。 hash_set/hash_map/hash_multiset/hash_multimap hash_set/hash_map，两者的一切操作都是基于hashtable之上。不同的是，hash_set同set一样，同时拥有实值和键值，且实质就是键值，键值就是实值，而hash_map同map一样，每一个元素同时拥有一个实值(value)和一个键值(key)，所以其使用方式，和上面的map基本相同。但由于hash_set/hash_map都是基于hashtable之上，所以不具备自动排序功能。为什么?因为hashtable没有自动排序功能。 至于hash_multiset/hash_multimap的特性与上面的multiset/multimap完全相同，唯一的差别就是它们hash_multiset/hash_multimap的底层实现机制是hashtable（而multiset/multimap，上面说了，底层实现机制是RB-tree），所以它们的元素都不会被自动排序，不过也都允许键值重复。 所以，综上，说白了，什么样的结构决定其什么样的性质，因为set/map/multiset/multimap都是基于RB-tree之上，所以有自动排序功能，而hash_set/hash_map/hash_multiset/hash_multimap都是基于hashtable之上，所以不含有自动排序功能，至于加个前缀multi_无非就是允许键值重复而已。如下图所示： 二、处理海量数据问题之六把密匙分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序1、海量日志数据，提取出某日访问百度次数最多的那个IP。既然是海量数据处理，那么可想而知，给我们的数据那就一定是海量的。针对这个数据的海量，我们如何着手呢?对的，无非就是分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序： 分而治之/hash映射：针对数据太大，内存受限，只能是：把大文件化成(取模映射)小文件，即16字方针：大而化小，各个击破，缩小规模，逐个解决hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。 具体而论，则是： “首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如%1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map对那1000个文件中的所有IP进行频率统计，然后依次找出各个文件中频率最大的那个IP）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。” 关于本题，还有几个问题，如下： Hash取模是一种等价映射，不会存在同一个元素分散到不同小文件中的情况，即这里采用的是mod1000算法，那么相同的IP在hash取模后，只可能落在同一个文件中，不可能被分散的。因为如果两个IP相等，那么经过Hash(IP)之后的哈希值是相同的，将此哈希值取模（如模1000），必定仍然相等。 那到底什么是hash映射呢？简单来说，就是为了便于计算机在有限的内存中处理big数据，从而通过一种映射散列的方式让数据均匀分布在对应的内存位置(如大数据通过取余的方式映射成小树存放在内存中，或大文件映射成多个小文件)，而这个映射散列方式便是我们通常所说的hash函数，设计的好的hash函数能让数据均匀分布而减少冲突。尽管数据映射到了另外一些不同的位置，但数据还是原来的数据，只是代替和表示这些原始数据的形式发生了变化而已。 一致性hash算法，见此文第五部分 2、寻找热门查询，300万个查询字符串中统计最热门的10个查询原题：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。 解答：由上面第1题，我们知道，数据大则划为小的，如如一亿个Ip求Top 10，可先%1000将ip分到1000个小文件中去，并保证一种ip只出现在一个文件中，再对每个小文件中的ip进行hashmap计数统计并按数量排序，最后归并或者最小堆依次处理每个小文件的top10以得到最后的结。 但如果数据规模比较小，能一次性装入内存呢?比如这第2题，虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把他们都放进内存中去（300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理），而现在只是需要一个合适的数据结构，在这里，HashTable绝对是我们优先的选择。 所以我们放弃分而治之/hash映射的步骤，直接上hash统计，然后排序。So，针对此类典型的TOP K问题，采取的对策往往是：hashmap + 堆。如下所示： hash_map统计：先对这批海量数据预处理。具体方法是：维护一个Key为Query字串，Value为该Query出现次数的HashTable，即hash_map(Query，Value)，每次读取一个Query，如果该字串不在Table中，那么加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加一即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计； 堆排序：第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。即借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O（N） + N’ * O（logK），（N为1000万，N’为300万）。 别忘了这篇文章中所述的堆排序思路：“维护k个元素的最小堆，即用容量为k的最小堆存储最先遍历到的k个数，并假设它们即是最大的k个数，建堆费时O（k），并调整堆(费时O（logk）)后，有k1&gt;k2&gt;…kmin（kmin设为小顶堆中最小元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，若x&gt;kmin，则更新堆（x入堆，用时logk），否则不更新堆。这样下来，总费时O（klogk+（n-k）logk）=O（n*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。”–第三章续、Top K算法问题的实现。 当然，你也可以采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。 3、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词由上面那两个例题，分而治之 + hash统计 + 堆/快速排序这个套路，我们已经开始有了屡试不爽的感觉。下面，再拿几道再多多验证下。请看此第3题：又是文件很大，又是内存受限，咋办?还能怎么办呢?无非还是： 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。 堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。4、海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素： 堆排序：在每台电脑上求出TOP10，可以采用包含10个元素的堆完成（TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大）。 求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。 但如果同一个元素重复出现在不同的电脑中呢，如下例子所述： 这个时候，你可以有两种方法： 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。 或者，暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP10。5、有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。 方案1：直接上： hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。 堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件（记为）。最后，对这10个文件进行归并排序（内排序与外排序相结合）。根据此方案1，这里有一份实现。 方案2：一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。 方案3：与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。6、 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 分而治之/hash映射：遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为，这里漏写个了a1）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为）。这样处理后，所有可能相同的url都在对应的小文件（）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。 OK，此第一种方法：分而治之/hash映射 + hash统计 + 堆/快速/归并排序，再看最后4道题，如下：7、怎么在海量数据中找出重复次数最多的一个？方案：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。8、上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。方案：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后利用堆取出前N个出现次数最多的数据。9、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 方案1：如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。 方案2：通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，也可以用trie树统计每个词出现的次数，时间复杂度是O(nle)（le表示单词的平准长度），最终同样找出出现最频繁的前10个词（可用堆来实现），时间复杂度是O(nlg10)。10、 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？ 方案1：这题用trie树比较合适，hash_map也行。 方案2：from xjbzju:，1000w的数据规模插入操作完全不现实，以前试过在stl下100w元素插入set中已经慢得不能忍受，觉得基于hash的实现不会比红黑树好太多，使用vector+sort+unique都要可行许多，建议还是先hash成小文件分开处理再综合。11、 一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。 方案1：首先根据用hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中10个最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词。12、 100w个数中找出最大的100个数。 方案1：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。 方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。 方案3：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。多层划分适用范围：第k大，中位数，不重复或重复的数字基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。13、2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。14、5亿个int找它们的中位数。 思路一：这个例子比上面那个更明显。首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用direct addr table进行统计了。 思路二@绿色夹克衫：同样需要做两遍统计，如果数据存在硬盘上，就需要读取2次。方法同基数排序有些像，开一个大小为65536的Int数组，第一遍读取，统计Int32的高16位的情况，也就是0-65535，都算作0,65536 - 131071都算作1。就相当于用该数除以65536。Int32 除以 65536的结果不会超过65536种情况，因此开一个长度为65536的数组计数就可以。每读取一个数，数组中对应的计数+1，考虑有负数的情况，需要将结果加32768后，记录在相应的数组内。第一遍统计之后，遍历数组，逐个累加统计，看中位数处于哪个区间，比如处于区间k，那么0- k-1的区间里数字的数量sum应该&lt;n/2（2.5亿）。而k+1 - 65535的计数和也&lt;n/2，第二遍统计同上面的方法类似，但这次只统计处于区间k的情况，也就是说(x / 65536) + 32768 = k。统计只统计低16位的情况。并且利用刚才统计的sum，比如sum = 2.49亿，那么现在就是要在低16位里面找100万个数(2.5亿-2.49亿)。这次计数之后，再统计一下，看中位数所处的区间，最后将高位和低位组合一下就是结果了。Bloom filter/Bitmap海量数据处理之Bloom Filter详解适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集基本原理及要点：对于原理来说很简单，位数组+k个独立hash函数。将hash函数对应的值的位数组置1，查找时如果发现所有hash函数对应位都是1说明存在，很明显这个过程并不保证查找的结果是100%正确的。同时也不支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是 counting Bloom filter，用一个counter数组代替位数组，就可以支持删除了。 还有一个比较重要的问题，如何根据输入元素个数n，确定位数组m的大小及hash函数个数。当hash函数个数k=(ln2)(m/n)时错误率最小。在错误率不大于E的情况下，m至少要等于nlg(1/E)才能表示任意n个元素的集合。但m还应该更大些，因为还要保证bit数组里至少一半为0，则m应该&gt;=nlg(1/E)*lge 大概就是nlg(1/E)1.44倍(lg表示以2为底的对数)。 举个例子我们假设错误率为0.01，则此时m应大概是n的13倍。这样k大概是8个。注意这里m与n的单位不同，m是bit为单位，而n则是以元素个数为单位(准确的说是不同元素的个数)。通常单个元素的长度都是有很多bit的。所以使用bloom filter内存上通常都是节省的。 扩展：Bloom filter将集合中的元素映射到位数组中，用k（k为哈希函数个数）个映射位是否全1表示元素在不在这个集合中。Counting bloom filter（CBF）将位数组中的每一位扩展为一个counter，从而支持了元素的删除操作。Spectral Bloom Filter（SBF）将其与集合元素的出现次数关联。SBF采用counter中的最小值来近似表示元素的出现频率。 可以看下上文中的第6题： 6、给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？ 根据这个问题我们来计算下内存的占用，4G=2^32大概是40亿*8大概是340亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些urlip是一一对应的，就可以转换成ip，则大大简单了。 同时，上文的第5题：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。” Bitmap13、在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。 方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。 方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。” 15、给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？ 方案1：frome oo，用位图/Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。Trie树/数据库/倒排索引Trie树 适用范围：数据量大，重复多，但是数据种类小可以放入内存基本原理及要点：实现方式，节点孩子的表示方式扩展：压缩实现。 问题实例： 上面的第2题：寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。 上面的第5题：有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？ 上面的第8题：一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词。其解决方法是：用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词。 数据库索引 适用范围：大数据量的增删改查基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。 关于数据库索引及其优化，更多可参见此文关于MySQL索引背后的数据结构及算法原理关于B 树、B+ 树、B* 树及R 树 倒排索引(Inverted index) 适用范围：搜索引擎，关键字查询基本原理及要点：为何叫倒排索引？一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。 以英文为例，下面是要被索引的文本： T0 = “it is what it is” T1 = “what is it” T2 = “it is a banana” 我们就能得到下面的反向文件索引： “a”: {2} “banana”: {2} “is”: {0, 1, 2} “it”: {0, 1, 2} “what”: {0, 1} 检索的条件”what”,”is”和”it”将对应集合的交集。 正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。扩展：问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。 外排序适用范围：大数据的排序，去重基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树问题实例：有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1M做hash明显不够，所以可以用来排序。内存可以当输入缓冲区使用。如何给10^7个数据量的磁盘文件排序 分布式处理之Mapreduce MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。但如果你要我再通俗点介绍，那么，说白了，Mapreduce的原理就是一个归并排序。 适用范围：数据量大，但是数据种类小可以放入内存基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。问题实例： The canonical example application of MapReduce is a process to count the appearances of each different word in a set of documents: 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数的中数(median)？ Hadhoop框架与MapReduce模式中谈海量数据处理MapReduce技术的初步了解与学习 其它模式/方法论，结合操作系统知识至此，六种处理海量数据问题的模式/方法已经阐述完毕。据观察，这方面的面试题无外乎以上一种或其变形，然题目为何取为是：秒杀99%的海量数据处理面试题，而不是100%呢。OK，给读者看最后一道题，如下：非常大的文件，装不进内存。每行一个int类型数据，现在要你随机取100个数。我们发现上述这道题，无论是以上任何一种模式/方法都不好做，那有什么好的别的方法呢？我们可以看看：操作系统内存分页系统设计(说白了，就是映射+建索引)。Windows 2000使用基于分页机制的虚拟内存。每个进程有4GB的虚拟地址空间。基于分页机制，这4GB地址空间的一些部分被映射了物理内存，一些部分映射硬盘上的交换文 件，一些部分什么也没有映射。程序中使用的都是4GB地址空间中的虚拟地址。而访问物理内存，需要使用物理地址。 物理地址 (physical address): 放在寻址总线上的地址。放在寻址总线上，如果是读，电路根据这个地址每位的值就将相应地址的物理内存中的数据放到数据总线中传输。如果是写，电路根据这个 地址每位的值就将相应地址的物理内存中放入数据总线上的内容。物理内存是以字节(8位)为单位编址的。 虚拟地址 (virtual address): 4G虚拟地址空间中的地址，程序中使用的都是虚拟地址。 使用了分页机制之后，4G的地址空间被分成了固定大小的页，每一页或者被映射到物理内存，或者被映射到硬盘上的交换文件中，或者没有映射任何东西。对于一 般程序来说，4G的地址空间，只有一小部分映射了物理内存，大片大片的部分是没有映射任何东西。物理内存也被分页，来映射地址空间。对于32bit的 Win2k，页的大小是4K字节。CPU用来把虚拟地址转换成物理地址的信息存放在叫做页目录和页表的结构里。 物理内存分页，一个物理页的大小为4K字节，第0个物理页从物理地址 0x00000000 处开始。由于页的大小为4KB，就是0x1000字节，所以第1页从物理地址 0x00001000 处开始。第2页从物理地址 0x00002000 处开始。可以看到由于页的大小是4KB，所以只需要32bit的地址中高20bit来寻址物理页。 返回上面我们的题目：非常大的文件，装不进内存。每行一个int类型数据，现在要你随机取100个数。针对此题，我们可以借鉴上述操作系统中内存分页的设计方法，做出如下解决方案： 操作系统中的方法，先生成4G的地址表，在把这个表划分为小的4M的小文件做个索引，二级索引。30位前十位表示第几个4M文件，后20位表示在这个4M文件的第几个，等等，基于key value来设计存储，用key来建索引。 更多海里数据处理面试题]]></content>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[freemarker基础知识总结]]></title>
    <url>%2Ffreemarker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Freemaker FTL指令常用标签及语法注意**：使用freemaker，要求所有标签必须闭合，否则会导致freemaker无法解析。 freemaker注释:&lt;#– 注释内容 –&gt;格式部分,不会输出 基础语法1、字符输出12345$&#123;emp.name?if_exists&#125; // 变量存在，输出该变量，否则不输出$&#123;emp.name!&#125; // 变量存在，输出该变量，否则不输出$&#123;emp.name?default(&quot;xxx&quot;)&#125; // 变量不存在，取默认值xxx $&#123;emp.name!&quot;xxx&quot;&#125; // 变量不存在，取默认值xxx 常用内部函数： 12345$&#123;&quot;123&lt;br&gt;456&quot;?html&#125; // 对字符串进行HTML编码，对html中特殊字符进行转义$&#123;&quot;str&quot;?cap_first&#125; // 使字符串第一个字母大写 $&#123;&quot;Str&quot;?lower_case&#125; // 将字符串转换成小写 $&#123;&quot;Str&quot;?upper_case&#125; // 将字符串转换成大写$&#123;&quot;str&quot;?trim&#125; // 去掉字符串前后的空白字符 字符串的两种拼接方式拼接： 12$&#123;&quot;hello$&#123;emp.name!&#125;&quot;&#125; // 输出hello+变量名$&#123;&quot;hello&quot;+emp.name!&#125; // 使用+号来连接，输出hello+变量名 可以通过如下语法来截取子串: 1234567891011&lt;#assign str = &quot;abcdefghijklmn&quot;/&gt;// 方法1$&#123;str?substring(0,4)&#125; // 输出abcd// 方法2$&#123;str[0]&#125;$&#123;str[4]&#125; // 结果是ae$&#123;str[1..4]&#125; // 结果是bcde// 返回指定字符的索引$&#123;str?index_of(&quot;n&quot;)&#125; 2、日期输出1$&#123;emp.date?string(&apos;yyyy-MM-dd&apos;)&#125; //日期格式 3、数字输出(以数字20为例)12345678910111213141516171819202122232425$&#123;emp.name?string.number&#125; // 输出20$&#123;emp.name?string.currency&#125; // ￥20.00 $&#123;emp.name?string.percent&#125; // 20%$&#123;1.222?int&#125; // 将小数转为int，输出1&lt;#setting number_format=&quot;percent&quot;/&gt; // 设置数字默认输出方式(&apos;percent&apos;,百分比)&lt;#assign answer=42/&gt; // 声明变量 answer 42#&#123;answer&#125; // 输出 4,200%$&#123;answer?string&#125; // 输出 4,200%$&#123;answer?string.number&#125; // 输出 42$&#123;answer?string.currency&#125; // 输出 ￥42.00$&#123;answer?string.percent&#125; // 输出 4,200%#&#123;answer&#125; // 输出 42数字格式化插值可采用#&#123;expr;format&#125;形式来格式化数字,其中format可以是:mX:小数部分最小X位MX:小数部分最大X位如下面的例子:&lt;#assign x=2.582/&gt;&lt;#assign y=4/&gt;#&#123;x; M2&#125; // 输出2.58#&#123;y; M2&#125; // 输出4#&#123;x; m2&#125; // 输出2.58#&#123;y; m2&#125; // 输出4.0#&#123;x; m1M2&#125; // 输出2.58#&#123;x; m1M2&#125; // 输出4.0 4、申明变量123456789101112131415&lt;#assign foo=false/&gt; // 声明变量,插入布尔值进行显示,注意不要用引号$&#123;foo?string(&quot;yes&quot;,&quot;no&quot;)&#125; // 当为true时输出&quot;yes&quot;,否则输出&quot;no&quot;申明变量的几种方式&lt;#assign name=value&gt; &lt;#assign name1=value1 name2=value2 ... nameN=valueN&gt; &lt;#assign same as above... in namespacehash&gt;&lt;#assign name&gt; capture this &lt;/#assign&gt;&lt;#assign name in namespacehash&gt; capture this &lt;/#assign&gt; 5、比较运算符1234567表达式中支持的比较运算符有如下几个:= 或 == ：判断两个值是否相等.!= ：判断两个值是否不等.&gt; 或 gt ：判断左边值是否大于右边值&gt;= 或 gte ：判断左边值是否大于等于右边值&lt; 或 lt ：判断左边值是否小于右边值&lt;= 或 lte ：判断左边值是否小于等于右边值 6、算术运算符12345FreeMarker表达式中完全支持算术运算,FreeMarker支持的算术运算符包括:+, - , * , / , %注意：（1）、运算符两边必须是数字（2）、使用+运算符时,如果一边是数字,一边是字符串,就会自动将数字转换为字符串再连接,如:$&#123;3 + “5”&#125;,结果是:35 7、逻辑运算符逻辑运算符有如下几个:逻辑与:&amp;&amp;逻辑或:||逻辑非:!逻辑运算符只能作用于布尔值,否则将产生错误 8、FreeMarker中的运算符优先级如下(由高到低排列):①、一元运算符:!②、内建函数:?③、乘除法:*, / , %④、加减法:- , +⑤、比较:&gt; , &lt; , &gt;= , &lt;= (lt , lte , gt , gte)⑥、相等:== , = , !=⑦、逻辑与:&amp;&amp;⑧、逻辑或:||⑨、数字范围:..实际上,我们在开发过程中应该使用括号来严格区分,这样的可读性好,出错少 9、if 逻辑判断（注意：elseif 不加空格）1234567891011121314151617181920&lt;#if condition&gt;...&lt;#elseif condition2&gt;...&lt;#elseif condition3&gt;...&lt;#else&gt;...&lt;/#if&gt;if 空值判断// 当 photoList 不为空时&lt;#if photoList??&gt;...&lt;/#if&gt; 值得注意的是,$&#123;..&#125;只能用于文本部分,不能用于表达式,下面的代码是错误的:&lt;#if $&#123;isBig&#125;&gt;Wow!&lt;/#if&gt;&lt;#if &quot;$&#123;isBig&#125;&quot;&gt;Wow!&lt;/#if&gt;// 正确写法&lt;#if isBig&gt;Wow!&lt;/#if&gt; 10、switch (条件可为数字，可为字符串)12345678910111213&lt;#switch value&gt; &lt;#case refValue1&gt; ....&lt;#break&gt; &lt;#case refValue2&gt; ....&lt;#break&gt; &lt;#case refValueN&gt; ....&lt;#break&gt; &lt;#default&gt; .... &lt;/#switch&gt; 11、集合 &amp; 循环12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// 遍历集合:&lt;#list empList! as emp&gt; $&#123;emp.name!&#125;&lt;/#list&gt;// 可以这样遍历集合:&lt;#list 0..(empList!?size-1) as i&gt; $&#123;empList[i].name!&#125;&lt;/#list&gt;// 与jstl循环类似,也可以访问循环的状态。empList?size // 取集合的长度emp_index: // int类型，当前对象的索引值 emp_has_next: // boolean类型，是否存在下一个对象// 使用&lt;#break&gt;跳出循环&lt;#if emp_index = 0&gt;&lt;#break&gt;&lt;/#if&gt;// 集合长度判断 &lt;#if empList?size != 0&gt;&lt;/#if&gt; // 判断=的时候,注意只要一个=符号,而不是==&lt;#assign l=0..100/&gt; // 定义一个int区间的0~100的集合，数字范围也支持反递增,如100..2&lt;#list 0..100 as i&gt; // 等效于java for(int i=0; i &lt;= 100; i++) $&#123;i&#125;&lt;/#list&gt;// 截取子集合：empList[3..5] //返回empList集合的子集合,子集合中的元素是empList集合中的第4-6个元素// 创建集合：&lt;#list [&quot;星期一&quot;, &quot;星期二&quot;, &quot;星期三&quot;, &quot;星期四&quot;, &quot;星期五&quot;, &quot;星期六&quot;, &quot;星期天&quot;] as x&gt;// 集合连接运算,将两个集合连接成一个新的集合&lt;#list [&quot;星期一&quot;,&quot;星期二&quot;,&quot;星期三&quot;] + [&quot;星期四&quot;,&quot;星期五&quot;,&quot;星期六&quot;,&quot;星期天&quot;] as x&gt;// 除此之外,集合元素也可以是表达式,例子如下:[2 + 2, [1, 2, 3, 4], &quot;whatnot&quot;]// seq_contains：判断序列中的元素是否存在&lt;#assign x = [&quot;red&quot;, 16, &quot;blue&quot;, &quot;cyan&quot;]&gt; $&#123;x?seq_contains(&quot;blue&quot;)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // yes$&#123;x?seq_contains(&quot;yellow&quot;)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // no$&#123;x?seq_contains(16)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // yes$&#123;x?seq_contains(&quot;16&quot;)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // no// seq_index_of：第一次出现的索引&lt;#assign x = [&quot;red&quot;, 16, &quot;blue&quot;, &quot;cyan&quot;, &quot;blue&quot;]&gt; $&#123;x?seq_index_of(&quot;blue&quot;)&#125; // 2// sort_by：排序（升序）&lt;#list movies?sort_by(&quot;showtime&quot;) as movie&gt;&lt;/#list&gt;// sort_by：排序（降序）&lt;#list movies?sort_by(&quot;showtime&quot;)?reverse as movie&gt;&lt;/#list&gt;// 具体介绍：// 不排序的情况：&lt;#list movies as moive&gt; &lt;a href=&quot;$&#123;moive.url&#125;&quot;&gt;$&#123;moive.name&#125;&lt;/a&gt;&lt;/#list&gt;//要是排序，则用&lt;#list movies?sort as movie&gt; &lt;a href=&quot;$&#123;movie.url&#125;&quot;&gt;$&#123;movie.name&#125;&lt;/a&gt;&lt;/#list&gt;// 这是按元素的首字母排序。若要按list中对象元素的某一属性排序的话，则用&lt;#list moives?sort_by([&quot;name&quot;]) as movie&gt; &lt;a href=&quot;$&#123;movie.url&#125;&quot;&gt;$&#123;movie.name&#125;&lt;/a&gt;&lt;/#list&gt;//这个是按list中对象元素的[name]属性排序的，是升序，如果需要降序的话，如下所示：&lt;#list movies?sort_by([&quot;name&quot;])?reverse as movie&gt; &lt;a href=&quot;$&#123;movie.url&#125;&quot;&gt;$&#123;movie.name&#125;&lt;/a&gt;&lt;/#list&gt; 12、Map对象123456789// 创建map&lt;#assign scores = &#123;&quot;语文&quot;:86,&quot;数学&quot;:78&#125;&gt;// Map连接运算符&lt;#assign scores = &#123;&quot;语文&quot;:86,&quot;数学&quot;:78&#125; + &#123;&quot;数学&quot;:87,&quot;Java&quot;:93&#125;&gt;// Map元素输出emp.name // 全部使用点语法emp[&quot;name&quot;] // 使用方括号 13、FreeMarker支持如下转义字符:1234567891011121314151617\” ：双引号(u0022)\’ ：单引号(u0027)\ ：反斜杠(u005C)\n ：换行(u000A)\r ：回车(u000D)\t ：Tab(u0009)\b ：退格键(u0008)\f ：Form feed(u000C)\l ：&lt;\g ：&gt;\a ：&amp;\&#123; ：&#123;\xCode ：直接通过4位的16进制数来指定Unicode码,输出该unicode码对应的字符.如果某段文本中包含大量的特殊符号,FreeMarker提供了另一种特殊格式:可以在指定字符串内容的引号前增加r标记,在r标记后的文件将会直接输出.看如下代码:r”$foo”//输出&#123;foo&#125;$&#123;r”C:/foo/bar”&#125; // 输出 C:/foo/bar 14、include指令123456// include指令的作用类似于JSP的包含指令:&lt;#include &quot;/test.ftl&quot; encoding=&quot;UTF-8&quot; parse=true&gt;// 在上面的语法格式中,两个参数的解释如下:encoding=&quot;GBK&quot; // 编码格式parse=true // 是否作为ftl语法解析,默认是true，false就是以文本方式引入,注意:在ftl文件里布尔值都是直接赋值的如parse=true,而不是parse=&quot;true&quot; 15、import指令123// 类似于jsp里的import,它导入文件，然后就可以在当前文件里使用被导入文件里的宏组件&lt;#import &quot;/libs/mylib.ftl&quot; as my&gt;// 上面的代码将导入/lib/common.ftl模板文件中的所有变量,交将这些变量放置在一个名为com的Map对象中，&quot;my&quot;在freemarker里被称作namespace 17、compress 压缩12345678// 用来压缩空白空间和空白的行 &lt;#compress&gt; ... &lt;/#compress&gt;&lt;#t&gt; // 去掉左右空白和回车换行 &lt;#lt&gt;// 去掉左边空白和回车换行 &lt;#rt&gt;// 去掉右边空白和回车换行 &lt;#nt&gt;// 取消上面的效果 18、escape,noescape 对字符串进行HTML编码1234567891011// escape指令导致body区的插值都会被自动加上escape表达式,但不会影响字符串内的插值,只会影响到body内出现的插值,使用escape指令的语法格式如下:&lt;#escape x as x?html&gt; First name: $&#123;firstName&#125; &lt;#noescape&gt;Last name: $&#123;lastName&#125;&lt;/#noescape&gt; Maiden name: $&#123;maidenName&#125; &lt;/#escape&gt;// 相同表达式First name: $&#123;firstName?html&#125; Last name: $&#123;lastName&#125; Maiden name: $&#123;maidenName?html&#125;]]></content>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于前缀树图文详解敏感词过滤]]></title>
    <url>%2F%E5%9F%BA%E4%BA%8E%E5%89%8D%E7%BC%80%E6%A0%91%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3%E6%95%8F%E6%84%9F%E8%AF%8D%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[一般设计网站的时候，会有问题发布或者是内容发布的功能，这些功能的有一个很重要的点在于如何实现敏感词过滤，要不然可能会有不良信息的发布，或者发布的内容中有夹杂可能会有恶意功能的代码片段，敏感词过滤的基本的算法是前缀树算法，前缀树也就是字典树，通过前缀树匹配可以加快敏感词匹配的速度。首先是过滤HTML代码，在Spring中有直接的函数可以使用：1question.setContent(HtmlUtils.htmlEscape(question.getContent())); 实现的功能就是将html的代码进行转义后显示出来，使其失效。举一个具体的例子：如果有一串字符串为xwabfabcff,敏感词为abc、bf、bc，若这个字符串中包含敏感词，则使用***代替敏感词，实现一个算法。1.使用三个指针，指针1指向根节点，指针2指向字符串下标起始值，指针3指向字符串当前下标值。指针1为tempnode=rootnode，指针2为begin=0，指针3为position=0，创建stringbuffer sb来保存结果；2.遍历x，tempnode未找到子节点x，将x保存到sb中，1begin=begin+1;position=begin，tempnode=rootnode; 3.遍历w，tempnode未找到子节点w，将w保存到sb中，1begin=begin+1;position=begin，tempnode=rootnode; 同上4.遍历a，tempnode找到子节点a，tempnode指向a节点，则position++；5.遍历b，tempnode发现a节点下有b这个子节点，所以，tempnode指向b节点，则position++；6.遍历f，tempnode发现b节点下没有f这个子节点，所以，代表以begin开头的字符串，不会有敏感字符，因此，将a存入sb中。1position=begin+1;bigin=position;tempnode=rootnode 7.遍历b,tempnode找到子节点b，tempnode指向b节点，则position++；8.遍历f，tempnode发现b节点下有f这个子节点，而且f值敏感词结尾标记，所以，打码。将写入sb中，同时，begin=position+1;position=begin;tempnode=rootnode;9.遍历a，tempnode找到子节点a，tempnode指向a节点，则position++；10.遍历b，tempnode发现a节点下有b这个子节点，所以，tempnode指向b节点，则position++；11.遍历c，tempnode发现b节点下有c这个子节点，而且c值敏感词结尾标记，所以，打码。将*写入sb中，同时，begin=position+1;position=begin;tempnode=rootnode;12.遍历f，tempnode发现根节点下没有f这个节点，因此，将f存入sb中。position=begin+1;bigin=position;tempnode=rootnode；13.遍历f，tempnode发现根节点下没有f这个节点，因此，将f存入sb中。position=begin+1;bigin=position;tempnode=rootnode；因此，最后sb中为：xwa**ff;这里每次是将position指向的字符挨个的与tempnode的子节点进行比较，因此，代码中的while条件应该是1while (position &lt; text.length())&#123;&#125; 同时，需要思考：如果字符串为xwabfabcfb，则最后，begin指向b下标，position指向b下标，tempnode发现根节点下有b节点，因此position++;然后就退出循环了。而此时，sb中还只保存了xwa**f,所以，我们在循环的最后，还要将最后一串字符串加进来。1result.append(text.substring(begin)); 完整的代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158package com.springboot.springboot.service;import org.apache.commons.lang3.CharUtils;import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Service; import java.io.BufferedReader;import java.io.InputStream;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Map;@Servicepublic class SensitiveService implements InitializingBean &#123; private static final Logger logger = LoggerFactory.getLogger(SensitiveService.class); /** * 默认敏感词替换符 */ private static final String DEFAULT_REPLACEMENT = "敏感词"; private class TrieNode &#123; /** * true 关键词的终结 ； false 继续 */ private boolean end = false; /** * key下一个字符，value是对应的节点 */ private Map&lt;Character, TrieNode&gt; subNodes = new HashMap&lt;&gt;(); /** * 向指定位置添加节点树 */ void addSubNode(Character key, TrieNode node) &#123; subNodes.put(key, node); &#125; /** * 获取下个节点 */ TrieNode getSubNode(Character key) &#123; return subNodes.get(key); &#125; boolean isKeywordEnd() &#123; return end; &#125; void setKeywordEnd(boolean end) &#123; this.end = end; &#125; public int getSubNodeCount() &#123; return subNodes.size(); &#125; &#125; /** * 根节点 */ private TrieNode rootNode = new TrieNode(); /** * 判断是否是一个符号 */ private boolean isSymbol(char c) &#123; int ic = (int) c; // 0x2E80-0x9FFF 东亚文字范围 return !CharUtils.isAsciiAlphanumeric(c) &amp;&amp; (ic &lt; 0x2E80 || ic &gt; 0x9FFF); &#125; /** * 过滤敏感词 */ public String filter(String text) &#123; if (StringUtils.isBlank(text)) &#123; return text; &#125; String replacement = DEFAULT_REPLACEMENT; StringBuilder result = new StringBuilder(); TrieNode tempNode = rootNode; //指向树的根节点 int begin = 0; // 回滚数，指向字符串的指针，与树进行交互的 int position = 0; // 当前比较的位置，指向字符串 while (position &lt; text.length()) &#123; char c = text.charAt(position); // 空格直接跳过 if (isSymbol(c)) &#123; if (tempNode == rootNode) &#123; result.append(c); ++begin; &#125; ++position; continue; &#125; tempNode = tempNode.getSubNode(c); // 当前位置的匹配结束 if (tempNode == null) &#123; // 以begin开始的字符串不存在敏感词 result.append(text.charAt(begin)); // 跳到下一个字符开始测试 position = begin + 1; begin = position; // 回到树初始节点 tempNode = rootNode; &#125; else if (tempNode.isKeywordEnd()) &#123; // 发现敏感词， 从begin到position的位置用replacement替换掉 result.append(replacement); position = position + 1; begin = position; tempNode = rootNode; &#125; else &#123; ++position; &#125; &#125; //将最后一次的比较结果添加进去 result.append(text.substring(begin)); return result.toString(); &#125; private void addWord(String lineTxt) &#123; TrieNode tempNode = rootNode; // 循环每个字节 for (int i = 0; i &lt; lineTxt.length(); ++i) &#123; Character c = lineTxt.charAt(i); // 过滤空格 if (isSymbol(c)) &#123; continue; &#125; TrieNode node = tempNode.getSubNode(c); if (node == null) &#123; // 没初始化 node = new TrieNode(); tempNode.addSubNode(c, node); &#125; tempNode = node; if (i == lineTxt.length() - 1) &#123; // 关键词结束， 设置结束标志 tempNode.setKeywordEnd(true); &#125; &#125; &#125; @Override public void afterPropertiesSet() throws Exception &#123; rootNode = new TrieNode(); try &#123; InputStream is = Thread.currentThread().getContextClassLoader() .getResourceAsStream("SensitiveWords.txt"); InputStreamReader read = new InputStreamReader(is); BufferedReader bufferedReader = new BufferedReader(read); String lineTxt; while ((lineTxt = bufferedReader.readLine()) != null) &#123; lineTxt = lineTxt.trim(); addWord(lineTxt); &#125; read.close(); &#125; catch (Exception e) &#123; logger.error("读取敏感词文件失败" + e.getMessage()); &#125; &#125; public static void main(String[] argv) &#123; SensitiveService s = new SensitiveService(); s.addWord("色情"); s.addWord("赌博"); System.out.print(s.filter("你好赌博")); &#125;&#125;]]></content>
      <tags>
        <tag>Spring Boot</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring]]></title>
    <url>%2FSpring%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Redis基础知识]]></title>
    <url>%2FRedis%2F</url>
    <content type="text"><![CDATA[简单来说 Redis 就是一个数据库，不过与传统数据库不同的是 Redis 的数据是存在内存中的，所以存写速度非常快，因此 Redis 被广泛应用于缓存方向。此外，redis 也经常用来做分布式锁。 为什么要用 Redis高性能 假如用户第一次访问数据库中的某些数据。因为是从硬盘上读取的，所以这个过程会比较慢。我们可以将该用户这次访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发 高并发系统挂掉，多挂在数据库读写处，因为磁盘操作这个慢呀。直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 为什么要用 Redis 而不用 map 做缓存?缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 JVM 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 Redis 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 Redis 服务的高可用，会使整个程序的架构变的较为复杂。 Redis 常见数据结构以及使用场景分析StringString 数据结构是简单的 key-value 类型，value 其实不仅可以是 String，也可以是数字。常规 key-value 缓存应用；常规计数：微博数，粉丝数等。 List双向链表，应用场景：微博的关注列表，粉丝列表，消息列表等。 常用命令1234567891011lpush #在key对应list的头部添加rpush #尾部lpoprpopblpop # 阻塞 pop，bl：blocklrange # LRANGE mylist 0 1，取出 list 0~1 的元素llenlrem #lrem key count value从key对应 list 中删除 count(&gt;0从头到尾,=0所有) 个和 value 相同的元素lindex # 按 index get list 种的元素linsert # LINSERT mylist BEFORE &quot;World&quot; &quot;There&quot;lset # LSET mylist 0 &quot;four&quot;，把第 0 个位置改成 &quot;four&quot; 底层实现链表。 HashHash 是一个 string 类型的 field 和 value 的映射表，又名字典，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以Hash数据结构来存储用户信息，商品信息等等。 比如可以用 hash 类型存放： 1234567key=JavaUser293847value=&#123; "id": 1, "name": "SnailClimb", "age": 22, "location": "Wuhan, Hubei"&#125; 常用命令12345678hset # HSET myhash field1 &quot;Hello&quot;，field1 是 key，&quot;Hello&quot; 是 valuehgethdel # 就 hash 特别，删除叫 del，别人都叫 remhgetAll # 返回所有的 field 和 value，顺序：field1，value1，field2，value2，field3，value3 ...hexistshkeyshvalshsetnx # 字段不存在时才 set，字段存在不 set 底层实现 哈希表，一个字典有两个哈希表（ht[0] &amp; ht[1]），一个是平时用的，一个是 rehash 时用的。 插入一个新的键值对时，会先根据 key 计算出哈希值和索引值，然后把键值对发到对应索引处。 哈希算法：MurmurHash2，该算法即使输入的键是由规律的，也能给出一个很好的随机性，并且速度快。 解决键冲突：链地址法，每个哈希表节点有一个 next 指针，冲突的键会形成一个单链表。 为了让哈希表的负载因子维持在一个合理的范围，当哈希表保存的键值对数量太多或太少时，程序会对哈希表的大小进行相应的扩展或收缩，这个过程叫 rehash。步骤如下： ht[0] 是现在正在用的哈希表，Redis 会根据 ht[0] 中当前包含的键值对个数（ht[0].used）为 ht[1] 分配空间，空间大小取决于： 扩展操作：ht[1].size = 第一个大于 ht[0].used * 2 的 2^n 收缩操作：ht[1].size = 第一个大于 ht[0].used 的 2^n 将 ht[0] 上的键值对 rehash 到 ht[1] 上； 将 ht[1] 设置为 ht[0]，并在 ht[1] 新创建一个空白哈希表用于下一次 rehash。 Set适用于无顺序的集合，点赞点踩，抽奖，已读，共同好友（适合用来去重） 常用命令12345678910sadd # SADD myset one two three，可以一次 add 一坨sinter # 两个集合的交集，SINTER key1 key2，其中 key1 和 key2 是两个 set 名sunion # 两个集合的并集sdiff # 第一个集合 - 交集smemberssismembersrem # 删除元素smove # 把一个set中的元素移动到另一个集合scard # 集合的sizesrandmember # SRANDMEMBER myset n，随机取 n 个，可以用来做抽奖，不写 n 就是随机取一个 底层实现Set 有两种类型，一种是 intset，就是整数集合，另一种是对象集合。 Sorted Set排行榜，优先队列（适合用来排序） 多个节点可以包含相同的 score，不过成员对象必须是唯一的。元素先按照 score 大小进行排序，score 相同时，按照成员对象大小（字典序之类的）进行排序。 常用命令12345678910111213zadd # ZADD myzset 2 &quot;two&quot; 3 &quot;three&quot;zcard # 集合的 sizezcount # 可以计算一个范围内数的 sizezscore # 查询 key 的值zincrby # 做加减法，对不存在的值加分会默认新建该值，并且初始值为 0zrange # ZRANGE myzset 0 -1 WITHSCORES，排行功能，打印排行后的结果zrevrange # 反转 Setzrank # key 的正向排名zrevrank # key 的反向排名# 遍历for (Tuple tuple : jedis.zrangeByScoreWithScores(rankKey, &quot;60&quot;, &quot;100&quot;)) &#123; print(37, tuple.getElement() + &quot;:&quot; + String.valueOf(tuple.getScore()));&#125; 底层实现（跳跃表）传说中的跳跃表。跳跃表（skiplist）是一种随机化的数据，跳跃表以有序的方式在层次化的链表中保存元素， 效率和平衡树媲美 —— 查找、删除、添加等操作都可以在对数期望时间下完成， 并且比起平衡树来说， 跳跃表的实现要简单直观得多。 首先说一下我们的需求，我们要一个有序的列表，因为一个有序的列表搜索起来可以用二分法，快啊！ 所以当我们要插入新元素的时候，就不能直接往表尾一放，我们需要保证把这个节点放进去之后，这个表还是有序的，所以我们的插入操作要分两步来进行： 找：把新节点插哪我们的表还有序 插：把新节点插到我们上一步找到的位置处 这需求一看就是平衡树了，可是红黑树之流实现起来有多复杂，大家也是有目共睹的，所以，Redis 用了一种叫跳跃表的数据结构。我们先来介绍一下 跳跃表（这个小灰讲的超好，我就是在下面挑我关心的重点复述一下）： 首先，本质上来讲，跳跃表还是一个链表的，这样插入和删除就很快啦！不过存链表的查找很慢呀，根本用不了二分法之流，只能从头到尾一个一个的遍历，也就是说，它的查找移动步长只能是 1，那么怎么解决这个问题呢？我们首先想到的方法应该就是想尽一切办法能让查找操作以比较大的步长移动。 那么如何实现大步长移动呢？这就想到了 MySQL 的 InnoDB 的索引的实现方式，我们知道 InnoDB 的一个数据页中的数据和一个链表差不多，不过它有一个叫页目录的东西，这个页目录就是从它所在的页中，以一定的间隔抽出一些节点，作为这个页的目录，这样我们就能大跨度的在单链表中进行查找了。 跳跃表就是基于这个方法实现的大跨度的查找，它从真正的数据中抽出了一些作为一级索引，又从一级索引中抽出一部分作为二级索引，就这样抽抽抽，直到最高层只有两个节点为止（就剩俩了也没有必要继续抽了……）。抽完之后大概是这样的： 那么我们抽索引的时候，抽谁呢？跳跃表是个选择障碍症患者，所以它抛硬币，每插入一个新节点，它都有 50% 的概率被选为索引，所以跳跃表的插入操作的是这样滴，比如我们要在上面的那个表里插个 9： 那么删除呢？删除超简单，比如说我们要删个 5，那么我们从最高层开始找 5： 第一层有 5，删了，然后这层就剩 1，然后干脆把这层都删了…… 第二层有 5，删了 第三层有 5，删了，好了，删光了！ 不过呢，Redis 的实现方式和上面描述的过程还是有区别的，Redis 实现跳跃表用了两种结构体： zskiplistNode：表示跳跃表节点； zskiplist：保存跳跃表的相关信息； 先来看一下 zskiplistNode 的定义： 1234567891011121314151617181920typedef struct zskiplistNode &#123; /* 下面俩货是我们的数据，不多说了 */ robj *obj; // member 对象 double score; // 分值 /* 后退指针，指向当前节点的前一个节点，用于从表尾向表头遍历跳跃表中的所有节点 */ struct zskiplistNode *backward; /* 层数组，实现大幅度跳跃的关键 */ /* 每个节点的层数组长度不一定，是一个 1~32 的随机数，是根据幂次定理随机的，就是说数越大，出现的概率越小 */ /* 每一个节点的同一层组成一个单链表，就是那个前进指针，比如 level[3] 吧，它就指向下一个有 level[3] 的节点 */ struct zskiplistLevel &#123; struct zskiplistNode *forward; // 前进指针 unsigned int span; // 这个层跨越的节点数量 // 不要小看这个东西，这个东西可以拿来计算排位 rank // 在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来， // 得到的结果就是目标节点在跳跃表中的排位 // 要是没有这个东西，鬼知道你在进行大步跨越时，跨越了多少节点啊 &#125; level[];&#125; zskiplistNode; 再来看一下 zskiplist 的定义： 12345typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; // 头节点，尾节点 unsigned long length; // 节点数量 int level; // 目前表内节点的最大层数&#125; zskiplist; 所以 Redis 的表看起来是这样子滴：如果我们想要遍历整个跳跃表，就是把 L1 层的链表从头遍历到尾，过程如下图虚线所示：如果我们想查找一个节点，比如 o2，我们的查找过程如下图虚线所示： Redis 设置过期时间Redis 中有个设置时间过期的功能，即对存储在 Redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 对于这种数据，我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。这样就不需要我们再在应用中手动判断这个 key 是否过期，而是将这个任务交给 Redis 来做。 过期时间设置方式：EXPIRE &lt;key&gt; &lt;ttl&gt;原子性的命令：我们是不管了，那么 Redis 到底是怎么删除这个过期数据的呢？ Redis 删除过期数据的方式： 定期删除 + 惰性删除 。 定期删除： Redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。 为什么要随机呢？你想一想假如 Redis 存了几十万个 key ，如果每隔 100ms 就遍历所有的设置过期时间的 key 的话，CPU怕是要废了…… 惰性删除： 定期删除可能会导致很多过期 key 到了时间并没有被删除掉，所以就有了惰性删除。惰性删除就是：假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，这时候你的系统去查了一下那个 key，Redis 会先检查下数据过期没，如果过期了，就会被 Redis 删除掉。 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？当然会有好多过期的 key 堆积在内存中了，如果大量过期 key 堆积在内存里，是导致 Redis 内存块耗尽的。那么怎么解决这个问题呢？这就要依靠 Redis 的内存淘汰机制了。 Redis 内存淘汰机制即 MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，那么如何保证 Redis 中的数据都是热点数据呢？ Redis 提供了如下 8 种数据淘汰策略： 策略 描述 volatile-lru 从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 allkeys-lru 从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 volatile-lfu 从已设置过期时间的数据集（server.db[i].expires）中挑选使用频率最小的数据淘汰 allkeys-lfu 从数据集（server.db[i].dict）中挑选挑选使用频率最小的数据淘汰 volatile-random 从已设置过期时间的数据集（server.db[i].expires）中挑选任意选择数据淘汰 allkeys-random 从数据集（server.db[i].dict）中挑选任意选择数据淘汰 volatile-ttl 从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 noeviction (Default) 禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错 设置方式：在配置文件 redis.conf 中写入 maxmemory-policy 策略名。 记忆： LRU：Least Recently Used LFU：Least Frequently Used volatile：已设置过期时间的数据集（server.db[i].expires） allkeys：数据集（server.db[i].dict） Redis 持久化机制即怎么保证 Redis 挂掉之后再重启数据可以进行恢复？ Redis 是支持持久化的，而且还支持两种，它们分别是： 快照（snapshotting，RDB） 只追加文件（append-only file，AOF） RDB 持久化Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis 主从结构，主要用来提高 Redis 性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是 Redis 默认采用的持久化方式，在 redis.conf 配置文件中默认有此下配置： 123save 900 1 # 在 900 秒(15分钟)之后，如果至少有 1 个 key 发生变化，Redis 就会自动触发 BGSAVE 命令创建快照save 300 10 # 在 300 秒(5分钟)之后，如果至少有 10 个 key 发生变化，Redis 就会自动触发 BGSAVE 命令创建快照save 60 10000 # 在 60 秒(1分钟)之后，如果至少有 10000 个 key 发生变化，Redis 就会自动触发 BGSAVE 命令创建快照 BGSAVE原理：copy-on-write AOF 持久化与快照持久化相比，AOF 持久化的实时性更好，因此已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF 方式的持久化，需要通过 appendonly 参数开启：appendonly yes开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入硬盘中的 AOF 文件。AOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。 在 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是：123appendfsync always # 每次有数据修改发生时都会写入 AOF 文件，这样会严重降低 Redis 的速度appendfsync everysec # 每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no # 让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis 还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 Redis 事务Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 一个事务从开始到结束会经历以下三个阶段： 事务开始 命令入队 事务执行 流程如下： 123456MULTI # 事务开始SET "name" "xxx" # 放到事务队列的 index 0 处GET "name" # 放到事务队列的 index 1 处SET "author" "Bean" # 放到事务队列的 index 2 处GET "author" # 放到事务队列的 index 3 处EXEC # 从 index 0 到 index 3 执行事务队列中的命令，并将 4 个命令的结果返回客户端 那么 WATCH 命令是干嘛的呢？它是一个乐观锁，可以在 EXEC 命令执行前，监视任意数量的数据库键，在 EXEC 命令执行时，只要有一个被监视的键发生了变化，服务器就会拒绝执行事务，并返回：(nil) 表示事务执行失败。 在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的可靠性和安全性。在 Redis 中，事务总是具有原子性（Atomicity)、一致性（Consistency）和隔离性（Isolation），并且当 Redis 运行在某种特定的持久化模式下时，事务也具有持久性（Durability）。 缓存穿透、缓存雪崩、缓存击穿问题的解决方案缓存穿透问题描述： 缓存穿透是指查询一个一定不存在的数据，由于缓存是命中时被动写的，这个数据不存在，所以缓存肯定没有，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能 DB 就挂掉了，要是有人利用不存在的 key 频繁攻击我们的应用，这就是漏洞。 解决方法的思路： 要能快速的判断出一个 key 在我们的系统到底存不存在，数据不存在，就不去 DB 查了。 解决方法：布隆过滤器。 将所有可能存在的数据哈希到一个足够大的 bitmap 中，要查询一个 key 之前，都先用这个 bitmap 判断一下存不存在，数据不存在就不去 DB 查了，从而避免了对底层存储系统的查询压力。 另外也有一个更为简单粗暴的方法，即如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 缓存雪崩问题描述： 缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到 DB，DB 瞬时压力过重雪崩。 解决方法： 将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如 1-5 分钟的随机时间，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 缓存击穿问题描述： 对于一些设置了过期时间的 key，如果这些 key 可能会在某些时间点被超高并发地访问，是一种非常 “热点” 的数据。但当缓存在某个时间点过期的时候，如果恰好在这个时间点有对这个 key 的大量并发请求过来，这些请求发现缓存过期了，就会选择从后端 DB 加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端 DB 压垮。 有如下 3 种解决方法： 使用互斥锁（mutex key）就是在缓存失效的时候（判断拿出来的值为空），不是立即去 load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如 Redis 的 SETNX）去 set 一个 mutex key，当操作返回成功时，再进行 load db 的操作并回设缓存；否则，就重试整个 get 缓存的方法（此时其他成功的请求可能已经把缓存更新好了，这个请求就可以成功的 get 到了）。 也就是说，我们只放一个请求去 load DB，把其他的请求都拦在了缓存层。 “提前” 使用互斥锁在这个 key 的 value 内部设置 1 个超时值 (timeout1)，timeout1 比实际的 timeout (timeout2) 要小。当从 cache 读取到 timeout1 发现它已经过期时候，马上延长 timeout1 并重新设置到 cache。然后再从数据库加载数据并设置到 cache 中。 也就是说，我们根本不让这个热点数据在缓存中不存在，热点数据快过期了就更新一下它，不让它真正的过期。 永远不过期缓存在 Redis 中的热点数据根本不设置过期时间，把过期时间存在 key 对应的 value 里，如果发现要过期了，通过一个后台的异步线程更新缓存。 如何解决 Redis 的并发竞争 Key 问题所谓 Redis 的并发竞争 key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（但如果不存在 Redis 的并发竞争 key 问题，不要使用分布式锁，会影响性能）。 分布式锁应该是怎么样的： setnx k v 互斥性：可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。 这把锁要是一把可重入锁（避免死锁） 不会发生死锁：有一个客户端在持有锁的过程中崩溃而没有解锁，也能保证其他客户端能够加锁。 获取锁和释放锁的性能要好 基于数据库表基于 Redis 的分布式锁基于 Zookeeper 实现分布式锁基于 Zookeeper 临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在 Zookeeper 上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 如何保证缓存与数据库双写时的数据一致性？你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，就是读的时候肯定不写，写的时候肯定不读，这样就可以保证一定不会出现不一致的情况，但是这会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 最经典的缓存 + 数据库读写的模式：Cache Aside Pattern。 即： 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候， 先更新数据库，然后再删除缓存 。（一个 lazy 计算的思想） 为什么是删除缓存，而不是更新缓存？ 很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值，而可能是多个数据库中的结果计算出来的。比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 数据库频繁更新的数据不一定是被频繁访问的数据，这种情况下，数据库更新一次就修改一次缓存是很不值得的。 Cache Aside Pattern 可能会出现的问题： 1. 先修改数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。 解决思路： 先删除缓存，再修改数据库。如果数据库修改失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中。虽然数据还是旧的，不过至少数据库和缓存是一致的。 2. 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了… 更新数据的时候，根据 数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据 + 更新缓存的操作，根据唯一标识路由之后，也发送同一个 jvm 内部队列中。 一个队列对应一个工作线程，每个工作线程 串行 拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时即使一个读请求过来，读到了空的缓存，它也会先将缓存更新的请求发送到队列中，等待缓存更新完成，而不是拿了旧数据就走，走前还不忘把旧数据放缓存里…… 优化点： 一个队列中，多个更新缓存请求串在一起是没意义的，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值，但不会把这个旧值设置到缓存中去。 用Redis做消息队列列表。rpush推入、lpop取出缺点：没有等待队列里有值就直接消费。弥补：应用层sleep，然后lpop重试 或blpop list seconds 发布订阅：subscribe、publish。但是消息无状态，不保证可达。需要用kafka什么的 参考： 如何保证缓存与数据库双写时的数据一致性？ 分布式锁解决并发的三种实现方式 缓存穿透，缓存击穿，缓存雪崩解决方案分析]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker简介]]></title>
    <url>%2FDocker%2F</url>
    <content type="text"><![CDATA[一句话概括容器：容器就是将软件打包成标准化单元，以用于开发、交付和部署。 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。 容器化软件适用于基于Linux和Windows的应用，在任何环境中都能够始终如一地运行。 容器赋予了软件独立性 ，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。 拿内存举例，虚拟机是利用Hypervisor去虚拟化内存，整个调用过程是虚拟内存-&gt;虚拟物理内存-&gt;真正物理内存，但是Docker是利用Docker Engine去调用宿主的的资源，这时候过程是虚拟内存-&gt;真正物理内存。 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便.虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker通常用于隔离不同的应用 ，例如前端，后端以及数据库。 镜像（Image）——一个特殊的文件系统Docker 镜像（Image）就是一个只读的模板。例如：一个镜像可以包含一个完整的操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而Docker 镜像（Image），就相当于是一个 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 Docker 设计时，就充分利用 Union FS的技术，将其设计为 分层存储的架构 。 镜像实际是由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。 比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 容器（Container)——镜像运行时的实体镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。 仓库（Repository）——集中存放镜像文件的地方镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。 一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是Docker用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。 这里补充一下Docker Registry 公开服务和私有 Docker Registry的概念： Docker Registry 公开服务 是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 常用命令docker ps列出所有运行中容器。 进入应用内部的系统 4f是编号docker exec -it 4f bash docker网络类型：bridge（和主机独立） host none bridge需要端口映射 docker run -d -p 8888:8080 test/tomcat -d：表示指定容器后台运行 -p：表示宿主机的8080端口对外映射暴露为8888端口 docker rmdocker rm [options “o”&gt;] “o”&gt;[container…]docker rm nginx-01 nginx-02 db-01 db-02sudo docker rm -l /webapp/redis-f 强行移除该容器，即使其正在运行；-l 移除容器间的网络连接，而非容器本身；-v 移除与容器关联的空间。 docker start|stop|restart docker imagesdocker images [options “o”&gt;] [name]列出本地所有镜像。其中 [name] 对镜像名称进行关键词查询。]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ基础知识简介]]></title>
    <url>%2FRabbitMQ%2F</url>
    <content type="text"><![CDATA[AMQP，即 Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。消息中间件主要用于组件之间的解耦和通讯。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。RabbitMQ是一个开源的AMQP实现，服务器端用 Erlang 语言编写，支持多种客户端，如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持AJAX。用于在分布式系统中存储转发消息，具有很高的易用性和可用性。 ConnectionFactory、Connection、Channel 都是RabbitMQ对外提供的API中最基本的对象。 ConnectionFactory：ConnectionFactory为Connection的制造工厂。 Connection：Connection是RabbitMQ的socket链接，它封装了socket协议相关部分逻辑。 Channel(信道)：信道是建立在“真实的”TCP连接上的虚拟连接，在一条TCP链接上创建多少条信道是没有限制的，把他想象成光纤就是可以了。它是我们与RabbitMQ打交道的最重要的一个接口，我们大部分的业务操作是在Channel这个接口中完成的，包括定义Queue、定义Exchange、绑定Queue与Exchange、发布消息等。 queue队列生产者Send Message “A”被传送到Queue中，消费者发现消息队列Queue中有订阅的消息，就会将这条消息A读取出来进行一些列的业务操作。这里只是一个消费正对应一个队列Queue，也可以多个消费者订阅同一个队列Queue，当然这里就会将Queue里面的消息平分给其他的消费者，但是会存在一个一个问题就是如果每个消息的处理时间不同，就会导致某些消费者一直在忙碌中，而有的消费者处理完了消息后一直处于空闲状态，因为前面已经提及到了Queue会平分这些消息给相应的消费者。这里我们就可以使用prefetchCount来限制每次发送给消费者消息的个数。详情见下图所示：生产者在将消息发送给 Exchange 的时候，一般会指定一个 routing key，来指定这个消息的路由规则。 Exchange 会根据 routing key 和 Exchange Type（交换器类型） 以及 Binding key 的匹配情况来决定把消息路由到哪个 Queue。RabbitMQ为routing key设定的长度限制为255 bytes。 RabbitMQ常用的Exchange Type有 Fanout、 Direct、 Topic、 Headers 这四种。 Fanout 这种类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中，这时 Routing key 不起作用。 Direct 这种类型的Exchange路由规则也很简单，它会把消息路由到那些 binding key 与 routing key完全匹配的Queue中。 当生产者（P）发送消息时Rotuing key=booking时，这时候将消息传送给Exchange，Exchange获取到生产者发送过来消息后，会根据自身的规则进行与匹配相应的Queue，这时发现Queue1和Queue2都符合，就会将消息传送给这两个队列，如果我们以Rotuing key=create和Rotuing key=confirm发送消息时，这时消息只会被推送到Queue2队列中，其他Routing Key的消息将会被丢弃。 Topic 这种类型的Exchange的路由规则支持 binding key 和 routing key 的模糊匹配，会把消息路由到满足条件的Queue。 binding key 中可以存在两种特殊字符 *与 #，用于做模糊匹配，其中 *用于匹配一个单词，# 用于匹配0个或多个单词，单词以符号.为分隔符。 Headers 这种类型的Exchange不依赖于 routing key 与 binding key 的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配,对比其中的键值对是否完全匹配Queue与Exchange绑定时指定的键值对；如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。 Message durability（消息的持久化） 如果我们希望即使在RabbitMQ服务重启的情况下，也不会丢失消息，我们可以将Queue与Message都设置为可持久化的（durable），这样可以保证绝大部分情况下我们的RabbitMQ消息不会丢失。但依然解决不了小概率丢失事件的发生（比如RabbitMQ服务器已经接收到生产者的消息，但还没来得及持久化该消息时RabbitMQ服务器就断电了），如果我们需要对这种小概率事件也要管理起来，那么我们要用到事务。由于这里仅为RabbitMQ的简单介绍，所以这里将不讲解RabbitMQ相关的事务。具体可以参考 RabbitMQ之消息确认机制（事务+Confirm） 接口优化===同步下单改为异步下单收到请求后由redis先预检库存，不足的话直接返回，减少数据库访问如果有库存，则请求放到消息队列。返回一个排队中请求出队。客户端轮询是否秒杀成功]]></content>
      <tags>
        <tag>消息队列</tag>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库基础知识]]></title>
    <url>%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[索引事务隔离级别锁SQL joinInnodb和MyISAM引擎MyCAT 索引数据结构： 二叉树：O(logn),深度会深，io多B树：B+树：底下的链接可做范围统计。磁盘代价低、查询效率稳定、有利于对数据库扫描Hash： innoDB密集索引：每个搜索码值都对应一个索引值Myisam稀疏索引：只为索引码的某些值建立索引项 如何定位并优化慢查询sql：根据慢日志定位慢查询SQL使用explain等工具分析SQL修改SQL或尽量让SQL走索引 事务的隔离级别、锁深入理解乐观锁与悲观锁 事务的隔离级别 锁在关系数据库管理系统里，悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）是一种并发控制的方法。它可以阻止一个事务以影响其他用户的方式来修改数据。如果一个事务执行的操作都某行数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 上面我们提到，使用select…for update会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁把整张表锁住，这点需要注意。 乐观锁（ Optimistic Locking ） 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。只能防止脏读后数据的提交，不能解决脏读。相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 行级锁开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 页级锁开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 在不通过索引条件查询的时候,InnoDB 确实使用的是表锁,而不是行锁。MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。 行锁TODO….gap锁 ANSI/ISO SQL定义的标准隔离级别有四种，从高到底依次为：可序列化(Serializable)、可重复读(Repeatable reads)、提交读(Read committed)、未提交读(Read uncommitted)。 未提交读(Read uncommitted)事务在读数据的时候并未对数据加锁。事务在修改数据的时候只对数据增加行级共享锁。所以，未提交读会导致脏读可以读到其他事务未提交的结果 提交读(Read committed)提交读(READ COMMITTED)也可以翻译成读已提交，通过名字也可以分析出，在一个事务修改数据过程中，如果事务还没提交，其他事务不能读该数据。提交读的数据库锁情况事务对当前被读取的数据加 行级共享锁（当读到时才加锁），一旦读完该行，立即释放该行级共享锁；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加 行级排他锁，直到事务结束才释放。简而言之，提交读这种隔离级别保证了读到的任何数据都是提交的数据，避免了脏读(dirty reads)。但是不保证事务重新读的时候能读到相同的数据，因为在每次数据读完之后其他事务可以修改刚才读到的数据。 可重复读(Repeatable reads)事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加 行级共享锁，直到事务结束才释放；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加 行级排他锁，直到事务结束才释放。可重复读隔离级别可以解决不可重复读的读现象。但是可重复读这种隔离级别中，还有另外一种读现象他解决不了，那就是幻读。 可序列化(Serializable)事务在读取数据时，必须先对其加 表级共享锁 ，直到事务结束才释放；事务在更新数据时，必须先对其加 表级排他锁 ，直到事务结束才释放。 脏读 脏读又称无效数据的读出，是指在数据库访问中，事务T1将某一值修改，然后事务T2读取该值，此后T1因为某种原因撤销对该值的修改，这就导致了T2所读取到的数据是无效的。 不可重复读 一种更易理解的说法是：在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 幻读 幻读是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中的“全部数据行”。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入“一行新数据”。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样.一般解决幻读的方法是增加范围锁RangeS，锁定检锁范围为只读，这样就避免了幻读。 幻读(phantom read)”是不可重复读(Non-repeatable reads)的一种特殊场景：当事务没有获取范围锁的情况下执行SELECT … WHERE操作可能会发生“幻影读(phantom read)”。 当前读、快照读 DB_TRX_ID ，，DB_ROLL_PTR ，， DB_ROW_ID字段 undo日志 SQL JOIN123456789101112Id_P LastName FirstName Address City1 Adams John Oxford Street London2 Bush George Fifth Avenue New York3 Carter Thomas Changan Street BeijingId_O OrderNo Id_P1 77895 32 44678 33 22456 14 24562 15 34764 65 SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName 结果： LastName FirstName OrderNoAdams John 22456Adams John 24562Carter Thomas 77895Carter Thomas 44678 在表中存在至少一个匹配时（来自两个表的所有字段都不为空），INNER JOIN 关键字返回行。与 JOIN 是相同的LEFT JOIN 关键字会从左表 (table_name1) 那里返回所有的行，即使在右表 (table_name2) 中没有匹配的行。 LEFT OUTER JOIN。SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName LastName FirstName OrderNoAdams John 22456Adams John 24562Carter Thomas 77895Carter Thomas 44678Bush GeorgeRIGHT JOIN 关键字会右表 (table_name2) 那里返回所有的行，即使在左表 (table_name1) 中没有匹配的行。只要其中某个表存在匹配，FULL JOIN 关键字就会返回行。LastName FirstName OrderNoAdams John 22456Adams John 24562Carter Thomas 77895Carter Thomas 44678Bush George 34764 Innodb与Myisam引擎1. 区别：（1）事务处理：MyISAM是非事务安全型的，而InnoDB是事务安全型的（支持事务处理等高级处理）；（2）锁机制不同：MyISAM是表级锁，而InnoDB是行级锁；（3）select ,update ,insert ,delete 操作：MyISAM：如果执行大量的SELECT，MyISAM是更好的选择InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表（4）查询表的行数不同：MyISAM：select count() from table,MyISAM只要简单的读出保存好的行数，注意的是，当count()语句包含 where条件时，两种表的操作是一样的InnoDB ： InnoDB 中不保存表的具体行数，也就是说，执行select count(*) from table时，InnoDB要扫描一遍整个表来计算有多少行（5）外键支持：mysiam表不支持外键，而InnoDB支持 2. 为什么MyISAM会比Innodb 的查询速度快。INNODB在做SELECT的时候，要维护的东西比MYISAM引擎多很多；1）数据块，INNODB要缓存，MYISAM只缓存索引块， 这中间还有换进换出的减少；2）innodb寻址要映射到块，再到行，MYISAM 记录的直接是文件的OFFSET，定位比INNODB要快3）INNODB还需要维护MVCC一致；虽然你的场景没有，但他还是需要去检查和维护MVCC ( Multi-Version Concurrency Control )多版本并发控制 3. 应用场景MyISAM适合：(1)做很多count 的计算；(2)插入不频繁，查询非常频繁；(3)没有事务。InnoDB适合：(1)可靠性要求比较高，或者要求事务；(2)表更新和查询都相当的频繁，并且行锁定的机会比较大的情况。 MVCC 【mysql】关于innodb中MVCC的一些理解通过加锁，让所有的读者等待写者工作完成，但是这样效率会很差。MVCC 使用了一种不同的手段，每个连接到数据库的读者，在某个瞬间看到的是数据库的一个快照，写者写操作造成的变化在写操作完成之前（或者数据库事务提交之前）对于其他的读者来说是不可见的。 innodb存储的最基本row中包含一些额外的存储信息 DATA_TRX_ID，DATA_ROLL_PTR，DB_ROW_ID，DELETE BIT 6字节的DATA_TRX_ID 标记了最新更新这条行记录的transaction id，每处理一个事务，其值自动+1 7字节的DATA_ROLL_PTR 指向当前记录项的rollback segment的undo log记录，找之前版本的数据就是通过这个指针 6字节的DB_ROW_ID，当由innodb自动产生聚集索引时，聚集索引包括这个DB_ROW_ID的值，否则聚集索引中不包括这个值.，这个用于索引当中 DELETE BIT位用于标识该记录是否被删除，这里的不是真正的删除数据，而是标志出来的删除。真正意义的删除是在commit的时候 InnoDB的MVCC,是通过在每行记录后面保存两个隐藏的列来实现的,这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值,而是系统版本号(可以理解为事务的ID)，每开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID. SELECTInnoDB会根据以下两个条件检查每行记录: a.InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的. b.行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除.只有a,b同时满足的记录，才能返回作为查询结果.DELETEInnoDB会为删除的每一行保存当前系统的版本号(事务的ID)作为删除标识. innodb索引和数据在一起 myisam不在一起 MyCAThttps://www.jianshu.com/p/21b1e133dd9b分布式数据库系统中间层 应用场景：需要读写分离，需要分库分表，多租户，数据统计系统，HBASE的一种替代方案 支持全局表支持ER的分片策略支持一致性hash分片 使用MySQL客户端管理mycat动态加载配置文件：reload @@config;查看数据节点：show @@datanode;查看后端数据库：show @@datasource;]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++知识]]></title>
    <url>%2FC-C-%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[构造函数 析构函数析构函数和构造函数是一对。构造函数用于创建对象，而析构函数是用来撤销对象。 内联函数inline1.内联函数在运行时可调试，而宏定义不可以;2.编译器会对内联函数的参数类型做安全检查或自动类型转换（同普通函数），而宏定义则不会；3.内联函数可以访问类的成员变量，宏定义则不能；4.在类中声明同时定义的成员函数，自动转化为内联函数。 C语言中switch 的查找实现原理 if…else结果的查找当case语句是小于3句的时候，switch语句底层的实现和if…else的实现方式相同。 线性查找当case语句大于等于4的时候，且每两个case之间产生的间隔之和不超过6时，就按线性结构查找。即，如下图的汇编里面的jmp dword ptr [edx*4+11B1428h]该指令里面的11B1428h地址里面，其存放着各个case语句的首地址。由于内存中下标是从0开始的，因此，通过对其进行减一操作，在判断其是否大于11B1428h地址的数组长度，如果大于直接跳出，否则通过计算直接定位到该数组上的地址进行跳转。 树形查找当最大case值和最小case值之差大于255的情况下，此时，编译器会采用树形查找。即，将数据由小到大排列，并取中间值（如果是偶数，就取中间两个靠右的那一个），在左右两边继续取中间值划分（左右两边划分不需要将中间值算进去），直到小于等于3个数据的时候不在划分。​​]]></content>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程与并发]]></title>
    <url>%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[线程池基本组成1、线程池管理器（ThreadPool）：用于创建并管理线程池，包括 创建线程池，销毁线程池，添加新任务；2、工作线程（PoolWorker）：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务；3、任务接口（Task）：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等；4、任务队列（taskQueue）：用于存放没有处理的任务。提供一种缓冲机制。 降低资源消耗。 通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。 当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。使用线程池可以进行统一的分配，调优和监控，延时执行、定时循环执行的策略等。 java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类 构造参数 1234567891011java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类构造参数public ThreadPoolExecutor(int ==corePoolSize==,该线程池中核心线程数最大值。线程池新建线程的时候，如果当前线程总数小于corePoolSize，则新建的是核心线程，如果超过corePoolSize，则新建的是非核心线程 核心线程默认情况下会一直存活在线程池中，即使这个核心线程啥也不干(闲置状态)。 如果指定ThreadPoolExecutor的allowCoreThreadTimeOut这个属性为true，那么核心线程如果不干活(闲置状态)的话，超过一定时间(时长下面参数决定)，就会被销毁掉 int ==maximumPoolSize==,（线程不够用时能够创建的最大线程数）线程总数 = 核心线程数 + 非核心线程数long ==keepAliveTime==,非核心线程闲置超时时长TimeUnit ==unit==,keepAliveTime的单位，TimeUnit是一个枚举类型BlockingQueue&lt;Runnable&gt; ==workQueue==,任务队列：维护着等待执行的Runnable对象 当所有的核心线程都在干活时，新添加的任务会被添加到这个队列中等待处理，如果队列满了，则新建非核心线程执行任务 ThreadFactory ==threadFactory==, 创建新线程，Executors.defaultThreadFactory()RejectedExecutionHandler ==handler ==线程池的饱和策略) 通过ThreadPoolExecutor.execute(Runnable command)方法即可向线程池内添加一个任务 当一个任务被添加进线程池时： 1、线程数量未达到corePoolSize，则新建一个线程(核心线程)执行任务2、线程数量达到了corePools，则将任务移入队列BlockingQueue等待3、如果无法将任务加入BlockingQueue（队列已满），则在非corePool中创建新的线程来处理任务（注意，执行这一步骤需要获取全局锁）。4、队列已满，总线程数又达到了maximumPoolSize，(RejectedExecutionHandler)抛出异常 四种java实现好的线程池CachedThreadPool() 可缓存线程池：线程数无限制有空闲线程则复用空闲线程，若无空闲线程则新建线程一定程序减少频繁创建/销毁线程，减少系统开销 FixedThreadPool() 定长线程池：可控制线程最大并发数（同时执行的线程数）超出的线程会在队列中等待 ScheduledThreadPool() 定长线程池：支持定时及周期性任务执行。 SingleThreadExecutor() 单线程化的线程池：有且仅有一个工作线程执行任务所有任务按照指定顺序执行，即遵循队列的入队出队规则 123456789101112import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class ThreadPoolExecutorTest &#123;public static void main(String[] args) &#123;ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5);scheduledThreadPool.schedule(new Runnable() &#123;public void run() &#123;System.out.println(&quot;delay 3 seconds&quot;);&#125;&#125;, 3, TimeUnit.SECONDS);&#125;&#125; 线程池的状态 线程池风险：死锁、资源不足、并发错误、 线程泄漏、请求过载 Executor管理多个异步任务的执行，而无需程序员显式地管理线程的生命周期。这里的异步是指多个任务的执行互不干扰，不需要进行同步操作。 主要有三种 Executor： CachedThreadPool：一个任务创建一个线程； FixedThreadPool：所有任务只能使用固定大小的线程； SingleThreadExecutor：相当于大小为 1 的 FixedThreadPool。 1234567public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; executorService.execute(new MyRunnable()); &#125; executorService.shutdown();&#125; 实现Runnable接口和Callable接口的区别如果想让线程池执行任务的话需要实现的Runnable接口或Callable接口。 Runnable接口或Callable接口实现类都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行。两者的区别在于 Runnable 接口不会返回结果但是 Callable 接口可以返回结果。 备注： 工具类 Executors 可以实现 Runnable 对象和 Callable 对象之间的相互转换。（Executors.callable（Runnable task 或Executors.callable（Runnable task，Object resule） ）。 执行execute()方法和submit()方法的区别是什么呢？ execute() 方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit） 方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 volatile关键字如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。 为了解决缓存不一致性问题，通常来说有以下2种解决方法：1）通过在总线加LOCK#锁的方式 2）通过缓存一致性协议 。这2种方式都是硬件层面上提供的方式。 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。 缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 原子性问题，可见性问题，有序性问题 对于可见性，Java提供了volatile关键字来保证可见性。 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 下面这段话摘自《深入理解Java虚拟机》：“观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令” lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效。 下面列举几个Java中使用volatile的几个场景。 状态标记量 double check synchronized 关键字和 volatile 关键字的区别 volatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字要好。但是volatile关键字只能用于变量，而synchronized关键字可以修饰方法以及代码块。synchronized关键字在JavaSE1.6之后，为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁以及其它各种优化，执行效率有了显著提升，实际开发中使用 synchronized 关键字的场景还是更多一些。 多线程访问volatile关键字不会发生阻塞，而synchronized关键字可能会发生阻塞。 volatile关键字能保证数据的可见性，但不能保证数据的原子性。synchronized关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized关键字解决的是多个线程之间访问资源的同步性。 Daemon守护线程是程序运行时在后台提供服务的线程，不属于程序中不可或缺的部分。当所有非守护线程结束时，程序也就终止，同时会杀死所有守护线程。main() 属于非守护线程。使用 setDaemon() 方法将一个线程设置为守护线程。 1234public static void main(String[] args) &#123;Thread thread = new Thread(new MyRunnable());thread.setDaemon(true);&#125; synchronized关键字JVM 实现的 synchronizedJDK 实现的 ReentrantLock synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 synchronized关键字最主要的三种使用方式 修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁。 修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 。也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份，所以对该类的所有对象都加了锁）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 和 synchronized 方法一样，synchronized(this)代码块也是锁定当前对象的。synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。这里再提一下：synchronized关键字加到非 static 静态方法上是给对象实例上锁。另外需要注意的是：尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓冲功能。 双重校验锁实现对象单例（线程安全）12345678910111213141516171819202122public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; //类对象加锁 synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的，uniqueInstance = new Singleton() 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出先问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 synchronized 关键字的底层原理synchronized 同步语句块的情况123456789public class SynchronizedDemo &#123; public void method() &#123; synchronized (this) &#123; System.out.println("synchronized 代码块"); &#125; &#125;&#125; 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行javac SynchronizedDemo.java命令生成编译后的 .class 文件，然后执行 javap -c -s -v -l SynchronizedDemo.class 从上面我们可以看出： synchronized 同步语句块的实现使用的是 monitorenter和 monitorexit指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权。当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 synchronized 修饰方法的的情况12345public class SynchronizedDemo2 &#123; public synchronized void method() &#123; System.out.println("synchronized 方法"); &#125;&#125; synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED标识，该标识指明了该方法是一个同步方法，JVM 通过该访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 synchronized和ReenTrantLock 的区别 两者都是可重入锁两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 synchronized 依赖于 JVM 而 ReenTrantLock 依赖于 APIsynchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReenTrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ReenTrantLock 比 synchronized 增加了一些高级功能相比synchronized，ReenTrantLock增加了一些高级功能。主要来说主要有三点： 等待可中断； 可实现公平锁； 可实现选择性通知（锁可以绑定多个条件） ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock默认情况是非公平的，可以通过 ReenTrantLock类的ReentrantLock(boolean fair) 构造方法来制定是否是公平的。 synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 锁的不是代码。是对象 对象在内存中的布局：对象头、实例数据、对齐填充 锁悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中 synchronized和 ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中 java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁一般会使用版本号机制或CAS算法实现。 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子：假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 12341、操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。2、在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。3、操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。4、操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 CAS算法CAS算法乐观锁的一种表现。 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 自旋锁、自适应自旋锁自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁。 1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125; 乐观锁的缺点1、 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference类就提供了此种能力，其中的 compareAndSet方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 、循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3、 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了 AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用 AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS与synchronized的使用情景简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 Lock锁的使用Lock接口简介锁是用于通过多个线程控制对共享资源的访问的工具。通常，锁提供对共享资源的独占访问：一次只能有一个线程可以获取锁，并且对共享资源的所有访问都要求首先获取锁。 但是，一些锁可能允许并发访问共享资源，如ReadWriteLock的读写锁。 在Lock接口出现之前，Java程序是靠synchronized关键字实现锁功能的。JDK1.5之后并发包中新增了Lock接口以及相关实现类来实现锁功能。 虽然synchronized方法和语句的范围机制使得使用监视器锁更容易编程，并且有助于避免涉及锁的许多常见编程错误，但是有时您需要以更灵活的方式处理锁。例如，用于遍历并发访问的数据结构的一些算法需要使用“手动”或“链锁定”：您获取节点A的锁定，然后获取节点B，然后释放A并获取C，然后释放B并获得D等。在这种场景中synchronized关键字就不那么容易实现了，使用Lock接口容易很多。 Lock的简单使用Lock接口的实现类：ReentrantLock ， ReentrantReadWriteLock.ReadLock ， ReentrantReadWriteLock.WriteLock 123456Lock lock=new ReentrantLock()；lock.lock();try&#123; &#125;finally&#123; lock.unlock();&#125; Lock接口特性 特性 描述 尝试非阻塞地获取锁 当前线程尝试获取锁，如果这一时刻锁没有被其他线程获取到，则成功获取并持有锁 能被中断地获取锁 获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常将会被抛出，同时锁会被释放 超时获取锁 在指定的截止时间之前获取锁， 超过截止时间后仍旧无法获取则返回 ReentrantLock类常见方法： 方法名称 描述 ReentrantLock() 创建一个 ReentrantLock的实例。 ReentrantLock(boolean fair) 创建一个特定锁类型（公平锁/非公平锁）的ReentrantLock的实例 Condition接口简介synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。 Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是有JVM选择的，使用ReentrantLock类结合Condition实例可以实现“选择性通知”，这个功能非常重要，而且是Condition接口默认提供的。 而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程 公平锁与非公平锁Lock锁分为：公平锁 和 非公平锁。 公平锁表示线程获取锁的顺序是按照线程加锁的顺序来分配的，即先来先得的FIFO先进先出顺序。 非公平锁就是一种获取锁的抢占机制，是随机获取锁的，和公平锁不一样的就是先来的不一定先的到锁，这样可能造成某些线程一直拿不到锁，结果也就是不公平的了。 线程间的协作在线程中调用另一个线程的 join() 方法，会将当前线程挂起，而不是忙等待，直到目标线程结束。 wait、notify、notifyAll()调用 wait() 使得线程等待某个条件满足，线程在等待时会被挂起，当其他线程的运行使得这个条件满足时，其它线程会调用 notify() 或者 notifyAll() 来唤醒挂起的线程。 它们都属于 Object 的一部分，而不属于 Thread。 只能用在同步方法或者同步控制块中使用，否则会在运行时抛出 IllegalMonitorStateExeception。 使用 wait() 挂起期间，线程会释放锁。这是因为，如果没有释放锁，那么其它线程就无法进入对象的同步方法或者同步控制块中，那么就无法执行 notify() 或者 notifyAll() 来唤醒挂起的线程，造成死锁。 wait() 是 Object 的方法，而 sleep() 是 Thread 的静态方法； wait() 会释放锁，sleep() 不会，是让出cpu。 await、signal、signalAll1234567891011121314151617线程consumersynchronize(obj)&#123; obj.wait();//没东西了，等待&#125;线程producersynchronize(obj)&#123; obj.notify();//有东西了，唤醒 &#125;lock.lock();condition.await();lock.unlock();lock.lock();condition.signal();lock.unlock; 为了突出区别，省略了若干细节。区别有三点： lock不再用synchronize把同步代码包装起来； 阻塞需要另外一个对象condition； 同步和唤醒的对象是condition而不是lock，对应的方法是await和signal，而不是wait和notify。 为什么需要使用condition呢？简单一句话，lock更灵活。以前的方式只能有一个等待队列，在实际应用时可能需要多个，比如读和写。为了这个灵活性，lock将同步互斥控制和等待队列分离开来，互斥保证在某个时刻只有一个线程访问临界区（lock自己完成），等待队列负责保存被阻塞的线程（condition完成）。 通过查看ReentrantLock的源代码发现，condition其实是等待队列的一个管理者，condition确保阻塞的对象按顺序被唤醒。 在Lock的实现中，LockSupport被用来实现线程状态的改变，后续将更进一步研究LockSupport的实现机制。 线程的状态 yield使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。cpu会从众多的可执行态里选择，也就是说，当前也就是刚刚的那个线程还是有可能会被再次执行到的，并不是说一定会执行其他线程而该线程在下一次中不会执行到了。 用了yield方法后，该线程就会把CPU时间让掉，让其他或者自己的线程执行（也就是谁先抢到谁执行） 中断线程 ThreadLocalThreadLocal 不继承 Thread，也不实现 Runable 接口， ThreadLocal 类为每一个线程都维护了自己独有的变量拷贝。每个线程都拥有自己独立的变量，其作用在于数据独立。ThreadLocal 采用 hash 表的方式来为每个线程提供一个变量的副本 阻塞队列阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 用优先队列实现最大堆最小堆 123456789Comparator&lt;Integer&gt; mycomparator = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return [o2.compareTo](http://o2.compareTo)(o1); &#125; &#125;; maxheap = new PriorityQueue&lt;Integer&gt;(20,mycomparator); minheap = new PriorityQueue&lt;Integer&gt;(20); Atomic 原子类Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。 所以，所谓原子类说简单点就是具有原子/原子操作特征的类。并发包 java.util.concurrent 的原子类都存放在 java.util.concurrent.atomic 下。 JUC 包中的原子类是哪4类 基本类型使用原子的方式更新基本类型 AtomicInteger：整形原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类 数组类型使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray ：引用类型数组原子类 引用类型 AtomicReference：引用类型原子类 AtomicStampedRerence：原子更新引用类型里的字段原子类 AtomicMarkableReference ：原子更新带有标记位的引用类型 对象的属性修改类型 AtomicIntegerFieldUpdater:原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicInteger 类常用方法1234567public final int get() //获取当前的值public final int getAndSet(int newValue)//获取当前的值，并设置新的值public final int getAndIncrement()//获取当前的值，并自增public final int getAndDecrement() //获取当前的值，并自减public final int getAndAdd(int delta) //获取当前的值，并加上预期的值boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update）public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 使用 AtomicInteger 之后，不用对 increment() 方法加锁也可以保证线程安全 AtomicInteger 类的原理1234567891011// setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用）private static final Unsafe unsafe = Unsafe.getUnsafe();private static final long valueOffset;static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField("value")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125; private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址，返回值是 valueOffset。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 AQS（AbstractQueuedSynchronizer)AQS在java.util.concurrent.locks包下面。是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。 并发编程中一些问题多线程就一定好吗？快吗？并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：内存泄漏、上下文切换、死锁还有受限于硬件和软件的资源闲置问题。 多线程就是几乎同时执行多个线程（一个处理器在某一个时间点上永远都只能是一个线程！即使这个处理器是多核的，除非有多个处理器才能实现多个线程同时运行）。CPU通过给每个线程分配CPU时间片来实现伪同时运行，因为CPU时间片一般很短很短，所以给人一种同时运行的感觉。 上下文切换当前任务在执行完CPU时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换会这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。Linux相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 那么我们现在可能会考虑 ：如何减少上下文切换的次数呢？ 减少上下文切换上下文切换又分为2种：让步式上下文切换和抢占式上下文切换。前者是指执行线程主动释放CPU，与锁竞争严重程度成正比，可通过减少锁竞争和使用CAS算法来避免；后者是指线程因分配的时间片用尽而被迫放弃CPU或者被其他优先级更高的线程所抢占，一般由于线程数大于CPU可用核心数引起，可通过适当减少线程数和使用协程来避免。 总结一下： 减少锁的使用。因为多线程竞争锁时会引起上下文切换。 使用CAS算法。这种算法也是为了减少锁的使用。CAS算法是一种无锁算法。 减少线程的使用。人物很少的时候创建大量线程会导致大量线程都处于等待状态。 使用协程（微线程或者说是轻量级的线程，它占用的内存更少并且更灵活）。 我们上面提到了两个名词：“CAS算法” 和 “协程”。可能有些人不是很了解这俩东西，所以这里简单说一下。 避免死锁在操作系统中，死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。 在线程中，如果两个线程同时等待对方释放锁也会产生死锁。 锁是一个好东西，但是使用不当就会造成死锁。一旦死锁产生程序就无法继续运行下去。所以如何避免死锁的产生，在我们使用并发编程时至关重要。根据《Java并发编程的艺术》有下面四种避免死锁的常见方法： 避免一个线程同时获得多个锁 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况 解决资源限制什么是资源限制所谓资源限制就是我们在进行并发编程时，程序的运行速度受限于计算机硬件资源比如CPU,内存等等或软件资源比如软件的质量、性能等等。举个例子：如果说服务器的带宽只有2MB/s，某个资源的下载速度是1MB/s，系统启动10个线程下载该资源并不会导致下载速度编程10MB/s，所以在并发编程时，需要考虑这些资源的限制。硬件资源限制有：带宽的上传和下载速度、硬盘读写速度和CPU处理速度；软件资源限制有数据库的连接数、socket连接数、软件质量和性能等等。 资源限制引发的问题在并发编程中，程序运行加快的原因是运行方式从串行运行变为并发运行，但是如果如果某段程序的并发执行由于资源限制仍然在串行执行的话，这时候程序的运行不仅不会加快，反而会更慢，因为可能增加了上下文切换和资源调度的时间。 如何解决资源限制的问题对于硬件资源限制，可以考虑使用集群并行执行程序。既然单机的资源有限制，那么就让程序在多机上运行。比如使用Hadoop或者自己搭建服务器集群。 对于软件资源的限制，可以考虑使用资源池将资源复用。比如使用连接池将数据库和Socket复用，或者在调用对方webservice接口获取数据时，只建立一个连接。另外还可以考虑使用良好的开源软件。 在资源限制的情况下如何进行并发编程根据不同的资源限制调整程序的并发度，比如下载文件程序依赖于两个资源-带宽和硬盘读写速度。有数据库操作时，设计数据库练连接数，如果SQL语句执行非常快，而线程的数量比数据库连接数大很多，则某些线程会被阻塞，等待数据库连接。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机内存管理机制]]></title>
    <url>%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[整体结构参考原文 程序计数器 当前线程所执行的字节码的行号指示器，字节码解释器工作时就是通过改变这个计数器的值来确定下一条要执行的字节码指令的位置 执行 Java 方法和 native 方法时的区别： 执行 Java 方法时：记录虚拟机正在执行的字节码指令地址； 执行 native 方法时：无定义； 是 5 个区域中唯一不会出现 OOM 的区域。 Java 虚拟机栈 Java 方法执行的内存模型，每个方法执行的过程，就是它所对应的栈帧在虚拟机栈中入栈到出栈的过程； 服务于 Java 方法； 可能抛出的异常： OutOfMemoryError（在虚拟机栈可以动态扩展的情况下，扩展时无法申请到足够的内存）； StackOverflowError（线程请求的栈深度 &gt; 虚拟机所允许的深度）； 虚拟机参数设置：-Xss. 本地方法栈 服务于 native 方法； 可能抛出的异常：与 Java 虚拟机栈一样。 Java堆 唯一的目的：存放对象实例； 垃圾收集器管理的主要区域； 可以处于物理上不连续的内存空间中； 可能抛出的异常： OutOfMemoryError（堆中没有内存可以分配给新创建的实例，并且堆也无法再继续扩展了）。 虚拟机参数设置： 最大值：-Xmx 最小值：-Xms 两个参数设置成相同的值可避免堆自动扩展。 现代的垃圾收集器基本都是采用分代收集算法，该算法的思想是针对不同的对象采取不同的垃圾回收算法，因此虚拟机把 Java 堆分成以下三块： 新生代（Young Generation） 老年代（Old Generation） 永久代（Permanent Generation） 方法区 存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据； 类信息：即 Class 类，如类名、访问修饰符、常量池、字段描述、方法描述等。 垃圾收集行为在此区域很少发生； 不过也不能不清理，对于经常动态生成大量 Class 的应用，如 Spring 等，需要特别注意类的回收状况。 运行时常量池也是方法区的一部分； Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池，用于存放编译器生成的各种字面量（就是代码中定义的 static final 常量）和符号引用，这部分信息就存储在运行时常量池中。 可能抛出的异常： OutOfMemoryError（方法区无法满足内存分配需求时）。 直接内存 JDK 1.4 的 NIO 类可以使用 native 函数库直接分配堆外内存，这是一种基于通道与缓冲区的 I/O 方式，它在 Java 堆中存储一个 DirectByteBuffer 对象作为堆外内存的引用，这样就可以对堆外内存进行操作了。因为可以避免 Java 堆和 Native 堆之间来回复制数据，在一些场景可以带来显著的性能提高。 虚拟机参数设置：-XX:MaxDirectMemorySize 默认等于 Java 堆最大值，即 -Xmx 指定的值。 将直接内存放在这里讲解的原因是它也可能会出现 OutOfMemoryError； 服务器管理员在配置 JVM 参数时，会根据机器的实际内存设置 -Xmx 等信息，但经常会忽略直接内存（默认等于 -Xmx 设置值），这可能会使得各个内存区域的总和大于物理内存限制，从而导致动态扩展时出现 OOM。 Java内存模型中堆和栈的区别元空间metaspace和永久代permgen HotSpot 虚拟机堆中的对象对象的创建（遇到一条 new 指令时） 检查这个指令的参数能否在常量池中定位到一个类的符号引用，并检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，先把这个类加载进内存； 类加载检查通过后，虚拟机将为新对象分配内存，此时已经可以确定存储这个对象所需的内存大小； 在堆中为新对象分配可用内存； 将分配到的内存初始化； 设置对象头中的数据； 此时，从虚拟机的角度看，对象已经创建好了，但从 Java 程序的角度看，对象创建才刚刚开始，构造函数还没有执行。 第 3 步，在堆中为新对象分配可用内存时，会涉及到以下两个问题： 如何在堆中为新对象划分可用的内存？ 指针碰撞（内存分配规整） 用过的内存放一边，没用过的内存放一边，中间用一个指针分隔； 分配内存的过程就是将指针向没用过的内存那边移动所需的长度； 空闲列表（内存分配不规整） 维护一个列表，记录哪些内存块是可用的； 分配内存时，从列表上选取一块足够大的空间分给对象，并更新列表上的记录； 如何处理多线程创建对象时，划分内存的指针的同步问题？ 对分配内存空间的动作进行同步处理（CAS）； 把内存分配动作按照线程划分在不同的空间之中进行； 每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）； 哪个线程要分配内存就在哪个线程的 TLAB 上分配，TLAB 用完需要分配新的 TLAB 时，才需要同步锁定； 通过 -XX:+/-UseTLAB 参数设定是否使用 TLAB。 对象的内存布局 对象头： 第一部分：存储对象自身运行时的数据，HashCode、GC分代年龄等（Mark Word）； 第二部分：类型指针，指向它的类元数据的指针，虚拟机通过这个指针来判断这个对象是哪个类的实例（HotSpot 采用的是直接指针的方式访问对象的）； 如果是个数组对象，对象头中还有一块用于记录数组长度的数据。 实例数据： 默认分配顺序：longs/doubles、ints、shorts/chars、bytes/booleans、oops (Ordinary Object Pointers)，相同宽度的字段会被分配在一起，除了 oops，其他的长度由长到短； 默认分配顺序下，父类字段会被分配在子类字段前面。注：HotSpot VM要求对象的起始地址必须是8字节的整数倍，所以不够要补齐。 对象的访问Java 程序需要通过虚拟机栈上的 reference 数据来操作堆上的具体对象，reference 数据是一个指向对象的引用，不过如何通过这个引用定位到具体的对象，目前主要有以下两种访问方式：句柄访问和直接指针访问。 句柄访问句柄访问会在 Java 堆中划分一块内存作为句柄池，每一个句柄存放着到对象实例数据和对象类型数据的指针。优势：对象移动的时候（这在垃圾回收时十分常见）只需改变句柄池中对象实例数据的指针，不需要修改reference本身。 直接指针访问直接指针访问方式在 Java 堆对象的实例数据中存放了一个指向对象类型数据的指针，在 HotSpot 中，这个指针会被存放在对象头中。优势：减少了一次指针定位对象实例数据的开销，速度更快。 OOM 异常 (OutOfMemoryError)Java 堆溢出 出现标志：java.lang.OutOfMemoryError: Java heap space 解决方法： 先通过内存映像分析工具分析 Dump 出来的堆转储快照，确认内存中的对象是否是必要的，即分清楚是出现了内存泄漏还是内存溢出； 如果是内存泄漏，通过工具查看泄漏对象到 GC Root 的引用链，定位出泄漏的位置； 如果不存在泄漏，检查虚拟机堆参数（-Xmx 和 -Xms）是否可以调大，检查代码中是否有哪些对象的生命周期过长，尝试减少程序运行期的内存消耗。 虚拟机参数： -XX:HeapDumpOnOutOfMemoryError：让虚拟机在出现内存泄漏异常时 Dump 出当前的内存堆转储快照用于事后分析。 Java 虚拟机栈和本地方法栈溢出 单线程下，栈帧过大、虚拟机容量过小都不会导致 OutOfMemoryError，只会导致 StackOverflowError（栈会比内存先爆掉），一般多线程才会出现 OutOfMemoryError，因为线程本身要占用内存； 如果是多线程导致的 OutOfMemoryError，在不能减少线程数或更换 64 位虚拟机的情况，只能通过减少最大堆和减少栈容量来换取更多的线程； 这个调节思路和 Java 堆出现 OOM 正好相反，Java 堆出现 OOM 要调大堆内存的设置值，而栈出现 OOM 反而要调小。 方法区和运行时常量池溢出 测试思路：产生大量的类去填满方法区，直到溢出； 在经常动态生成大量 Class 的应用中，如 Spring 框架（使用 CGLib 字节码技术），方法区溢出是一种常见的内存溢出，要特别注意类的回收状况。 直接内存溢出 出现特征：Heap Dump 文件中看不见明显异常，程序中直接或间接用了 NIO； 虚拟机参数：-XX:MaxDirectMemorySize，如果不指定，则和 -Xmx 一样。 垃圾收集 (GC)垃圾收集（Garbage Collection，GC），它的任务是解决以下 3 件问题： 哪些内存需要回收？ 什么时候回收？ 如何回收？其中第一个问题很好回答，在 Java 中，GC 主要发生在 Java 堆和方法区中，对于后两个问题，我们将在之后的内容中进行讨论，并介绍 HotSpot 的 7 个垃圾收集器。 判断对象的生死 判断对象是否可用的算法引用计数算法 算法描述： 给对象添加一个引用计数器； 每有一个地方引用它，计数器加 1； 引用失效时，计数器减 1； 计数器值为 0 的对象不再可用。 缺点： 很难解决循环引用的问题。即 objA.instance = objB; objB.instance = objA;，objA 和 objB 都不会再被访问后，它们仍然相互引用着对方，所以它们的引用计数器不为 0，将永远不能被判为不可用。 可达性分析算法（主流） 算法描述： 从 “GC Root” 对象作为起点开始向下搜索，走过的路径称为引用链（Reference Chain）； 从 “GC Root” 开始，不可达的对象被判为不可用。 Java 中可作为 “GC Root” 的对象： 栈中（本地变量表中的reference） 虚拟机栈中，栈帧中的本地变量表引用的对象；（a = new Obj()，a销毁之前，obj就是gc root） 本地方法栈中，JNI 引用的对象（native方法）； 方法区中 类的静态属性引用的对象； 常量引用的对象；（常量保存的是某个对象的地址） 活跃线程引用的对象即便如此，一个对象也不是一旦被判为不可达，就立即死去的，宣告一个的死亡需要经过两次标记过程。 四种引用类型JDK 1.2 后，Java 中才有了后 3 种引用的实现。 强引用： 像 Object obj = new Object() 这种，只要强引用还存在，垃圾收集器就永远不会回收掉被引用的对象。 软引用： 被软引用关联的对象，只有在内存不够的情况下才会被回收。对于软引用对象，在 OOM 前，虚拟机会把这些对象列入回收范围中进行第二次回收，如果这次回收后，内存还是不够用，就 OOM。 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 弱引用： 被弱引用引用的对象只能生存到下一次垃圾收集前，一旦发生垃圾收集，被弱引用所引用的对象就会被清掉。实现类：WeakReference。Tomcat 中的 ConcurrentCache 就使用了 WeakHashMap 来实现缓存功能。 虚引用： 又称为幽灵引用或者幻影引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象实例。它唯一的用途就是：当被一个虚引用引用的对象被回收时，系统会收到这个对象被回收了的通知。实现类：PhantomReference。 宣告对象死亡的两次标记过程 当发现对象不可达后，该对象被第一次标记，并进行是否有必要执行 finalize() 方法的判断； 不需要执行：对象没有覆盖 finalize() 方法，或者 finalize() 方法已被执行过（finalize() 只被执行一次）； 需要执行：将该对象放置在一个队列中，稍后由一个虚拟机自动创建的低优先级线程执行。 finalize() 方法是对象逃脱死亡的最后一次机会，不过虚拟机不保证等待 finalize() 方法执行结束，也就是说，虚拟机只触发 finalize() 方法的执行，如果这个方法要执行超久，那么虚拟机并不等待它执行结束，所以最好不要用这个方法。 finalize() 方法能做的，try-finally 都能做，所以忘了这个方法吧！ 方法区的回收因为方法区主要存放永久代对象，而永久代对象的回收率比新生代差很多，因此在方法区上进行回收性价比不高。主要是对常量池的回收和对类的卸载。永久代的 GC 主要回收：废弃常量 和 无用的类。 废弃常量：例如一个字符串 “abc”，当没有任何引用指向 “abc” 时，它就是废弃常量了。 无用的类：同时满足以下 3 个条件的类。 该类的所有实例已被回收，Java 堆中不存在该类的任何实例； 加载该类的 Classloader 已被回收； 该类的 Class 对象没有被任何地方引用，即无法在任何地方通过反射访问该类的方法。 垃圾收集算法 基础：标记 - 清除算法 算法描述： 先标记出所有需要回收的对象（图中深色区域）； 标记完后，统一回收所有被标记对象（留下狗啃似的可用内存区域……）。 不足： 效率问题：标记和清理两个过程的效率都不高。 空间碎片问题：标记清除后会产生大量不连续的内存碎片，导致以后为较大的对象分配内存时找不到足够的连续内存，会提前触发另一次 GC。 解决效率问题：复制算法 算法描述： 将可用内存分为大小相等的两块，每次只使用其中一块； 当一块内存用完时，将这块内存上还存活的对象复制到另一块内存上去，将这一块内存全部清理掉。 不足： 可用内存缩小为原来的一半，适合GC过后只有少量对象存活的新生代。 节省内存的方法： 新生代中的对象 98% 都是朝生夕死的，所以不需要按照 1:1 的比例对内存进行划分； 把内存划分为： 1 块比较大的 Eden 区； 2 块较小的 Survivor 区； 每次使用 Eden 区和 1 块 Survivor 区； 回收时，将以上 2 部分区域中的存活对象复制到另一块 Survivor 区中，然后将以上两部分区域清空； JVM 参数设置：-XX:SurvivorRatio=8 表示 Eden 区大小 / 1 块 Survivor 区大小 = 8。 解决空间碎片问题：标记 - 整理算法 算法描述： 标记方法与 “标记 - 清除算法” 一样； 标记完后，将所有存活对象向一端移动，然后直接清理掉边界以外的内存。 不足： 存在效率问题，适合老年代。 进化：分代收集算法 新生代： GC 过后只有少量对象存活 —— 复制算法 老年代： GC 过后对象存活率高 —— 标记 - 整理算法 当系统创建一个对象的时候，总是在Eden区操作，当这个区满了，那么就会触发一次Minor GC，也就是年轻代的垃圾回收。一般来说这时候不是所有的对象都没用了，所以就会把还能用的对象复制到From区。 这样整个Eden区就被清理干净了，可以继续创建新的对象，当Eden区再次被用完，就再触发一次Minor GC，然后呢，注意，这个时候跟刚才稍稍有点区别。这次触发Minor GC后，会将Eden区与From区还在被使用的对象复制到To区，再下一次Minor GC的时候，则是将Eden区与To区中的还在被使用的对象复制到From区。经过若干次Minor GC后，有些对象在From与To之间来回游荡，这时候From区与To区亮出了底线（阈值），这些家伙要是到现在还没挂掉，对不起，一起滚到（复制）老年代吧。 经历一定minor次数依然存活的对象 survivor区放不下的对象 新生成的大对象（-XX:+PretenuerSizeThreshold）老年代经过这么几次折腾，也就扛不住了（空间被用完），好，那就来次集体大扫除（Full GC），也就是全量回收，一起滚蛋吧。全量回收就好比我们刚才比作的大扫除，毕竟动做比较大，成本高，不能跟平时的小型值日（Minor GC）相比，所以如果Full GC使用太频繁的话，无疑会对系统性能产生很大的影响。所以要合理设置年轻代与老年代的大小，尽量减少Full GC的操作 Minor GC触发机制： 当年轻代满时就会触发Minor GC，这里的年轻代满指的是Eden代满，Survivor满不会引发GC。 Full GC触发机制： 调用System.gc()时，系统建议执行Full GC，但是不必然执行 老年代空间不足 永久代空间不足 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、survivor space1（From Space）区向survivor space2（To Space）区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 使用RMI来进行RPC或管理的JDK应用，每小时执行一次Full GC CMS GC时出现promotion failed, concurrent mode failure 常用调优参数-XX:SurvivorRatio:Eden和Survivor比值，默认8-XX:NewRatio:老年代和年轻代内存大小比例-XX:MaxTenuringThreshold:对象从年轻代晋升到老年代经过gc次数的最大阈值 HotSpot 中 GC 算法的实现通过前两小节对于判断对象生死和垃圾收集算法的介绍，我们已经对虚拟机进行 GC 的流程有了一个大致的了解。但是，在 HotSpot 虚拟机中，高效的实现这些算法也是一个需要考虑的问题。所以，接下来，我们将研究一下 HotSpot 虚拟机到底是如何高效的实现这些算法的，以及在实现中有哪些需要注意的问题。 通过之前的分析，GC 算法的实现流程简单的来说分为以下两步： 找到死掉的对象； 把它清了。 想要找到死掉的对象，我们就要进行可达性分析，也就是从 GC Root 找到引用链的这个操作。 也就是说，进行可达性分析的第一步，就是要枚举 GC Roots，这就需要虚拟机知道哪些地方存放着对象应用。如果每一次枚举 GC Roots 都需要把整个栈上位置都遍历一遍，那可就费时间了，毕竟并不是所有位置都存放在引用呀。所以为了提高 GC 的效率，HotSpot 使用了一种 OopMap 的数据结构，OopMap 记录了栈上本地变量到堆上对象的引用关系，也就是说，GC 的时候就不用遍历整个栈只遍历每个栈的 OopMap 就行了。 在 OopMap 的帮助下，HotSpot 可以快速准确的完成 GC 枚举了，不过，OopMap 也不是万年不变的，它也是需要被更新的，当内存中的对象间的引用关系发生变化时，就需要改变 OopMap 中的相应内容。可是能导致引用关系发生变化的指令非常之多，如果我们执行完一条指令就改下 OopMap，这 GC 成本实在太高了。 因此，HotSpot 采用了一种在 “安全点” 更新 OopMap 的方法，安全点的选取既不能让 GC 等待的时间过长，也不能过于频繁增加运行负担，也就是说，我们既要让程序运行一段时间，又不能让这个时间太长。我们知道，JVM 中每条指令执行的是很快的，所以一个超级长的指令流也可能很快就执行完了，所以 真正会出现 “长时间执行” 的一般是指令的复用，例如：方法调用、循环跳转、异常跳转等，虚拟机一般会将这些地方设置为安全点更新 OopMap 并判断是否需要进行 GC 操作。 此外，在进行枚举根节点的这个操作时，为了保证准确性，我们需要在一段时间内 “冻结” 整个应用，即 Stop The World（传说中的 GC 停顿），因为如果在我们分析可达性的过程中，对象的引用关系还在变来变去，那是不可能得到正确的分析结果的。即便是在号称几乎不会发生停顿的 CMS 垃圾收集器中，枚举根节点时也是必须要停顿的。这里就涉及到了一个问题： 我们让所有线程跑到最近的安全点再停顿下来进行 GC 操作呢？ 主要有以下两种方式： 抢先式中断： 先中断所有线程； 发现有线程没中断在安全点，恢复它，让它跑到安全点。 主动式中断： (主要使用) 设置一个中断标记； 每个线程到达安全点时，检查这个中断标记，选择是否中断自己。 除此安全点之外，还有一个叫做 “安全区域” 的东西，一个一直在执行的线程可以自己 “走” 到安全点去，可是一个处于 Sleep 或者 Blocked 状态的线程是没办法自己到达安全点中断自己的，我们总不能让 GC 操作一直等着这些个 ”不执行“ 的线程重新被分配资源吧。对于这种情况，我们要依靠安全区域来解决。 安全区域是指在一段代码片段之中，引用关系不会发生变化，因此在这个区域中的任意位置开始 GC 都是安全的。 当线程执行到安全区域时，它会把自己标识为 Safe Region，这样 JVM 发起 GC 时是不会理会这个线程的。当这个线程要离开安全区域时，它会检查系统是否在 GC 中，如果不在，它就继续执行，如果在，它就等 GC 结束再继续执行。 本小节我们主要讲述 HotSpot 虚拟机是如何发起内存回收的，也就是如何找到死掉的对象，至于如何清掉这些个对象，HotSpot 将其交给了一堆叫做 ”GC 收集器“ 的东西，这东西又有好多种，不同的 GC 收集器的处理方式不同，适用的场景也不同，我们将在下一小节进行详细讲述。 JVM 运行模式ServerClient 7 个垃圾收集器垃圾收集器就是内存回收操作的具体实现，HotSpot 里足足有 7 种，为啥要弄这么多，因为它们各有各的适用场景。有的属于新生代收集器，有的属于老年代收集器，所以一般是搭配使用的（除了万能的 G1）。关于它们的简单介绍以及分类请见下图。 Serial / ParNew 搭配 Serial Old 收集器 Serial 收集器是虚拟机在 Client 模式下的默认新生代收集器，它的优势是简单高效，在单 CPU 模式下很牛。ParNew 收集器就是 Serial 收集器的多线程版本，虽然除此之外没什么创新之处，但它却是许多运行在 Server 模式下的虚拟机中的首选新生代收集器，因为除了 Serial 收集器外，只有它能和 CMS 收集器搭配使用。 Parallel 搭配 Parallel Scavenge 收集器首先，这俩货肯定是要搭配使用的，不仅仅如此，它俩还贼特别，它们的关注点与其他收集器不同，其他收集器关注于尽可能缩短垃圾收集时用户线程的停顿时间，而 Parallel Scavenge 收集器的目的是达到一个可控的吞吐量。 吞吐量 = 运行用户代码时间 / ( 运行用户代码时间 + 垃圾收集时间 ) 因此，Parallel Scavenge 收集器不管是新生代还是老年代都是多个线程同时进行垃圾收集，十分适合于应用在注重吞吐量以及 CPU 资源敏感的场合。可调节的虚拟机参数： -XX:+UserParallelGC -XX:MaxGCPauseMillis：最大 GC 停顿的秒数； -XX:GCTimeRatio：吞吐量大小，一个 0 ~ 100 的数，最大 GC 时间占总时间的比率 = 1 / (GCTimeRatio + 1)； -XX:+UseAdaptiveSizePolicy：一个开关参数，打开后就无需手工指定 -Xmn，-XX:SurvivorRatio 等参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，自行调整。 CMS 收集器 参数设置： -XX:+UseCMSCompactAtFullCollection：在 CMS 要进行 Full GC 时进行内存碎片整理（默认开启） -XX:CMSFullGCsBeforeCompaction：在多少次 Full GC 后进行一次空间整理（默认是 0，即每一次 Full GC 后都进行一次空间整理） G1 收集器-XX：UserG1GC 复制+标记-整理算法 GC 日志解读 Java 内存分配策略 优先在 Eden 区分配 Eden 空间不够将会触发一次 Minor GC； 虚拟机参数： -Xmx：Java 堆的最大值； -Xms：Java 堆的最小值； -Xmn：新生代大小； -XX:SurvivorRatio=8：Eden 区 / Survivor 区 = 8 : 1 大对象直接进入老年代 大对象定义： 需要大量连续内存空间的 Java 对象。例如那种很长的字符串或者数组。 设置对象直接进入老年代大小限制： -XX:PretenureSizeThreshold：单位是字节； 只对 Serial 和 ParNew 两款收集器有效。 目的： 因为新生代采用的是复制算法收集垃圾，大对象直接进入老年代可以避免在 Eden 区和 Survivor 区发生大量的内存复制。 长期存活的对象将进入老年代 固定对象年龄判定： 虚拟机给每个对象定义一个年龄计数器，对象每在 Survivor 中熬过一次 Minor GC，年龄 +1，达到 -XX:MaxTenuringThreshold 设定值后，会被晋升到老年代，-XX:MaxTenuringThreshold 默认为 15； 动态对象年龄判定： Survivor 中有相同年龄的对象的空间总和大于 Survivor 空间的一半，那么，年龄大于或等于该年龄的对象直接晋升到老年代。 空间分配担保我们知道，新生代采用的是复制算法清理内存，每一次 Minor GC，虚拟机会将 Eden 区和其中一块 Survivor 区的存活对象复制到另一块 Survivor 区，但当出现大量对象在一次 Minor GC 后仍然存活的情况时，Survivor 区可能容纳不下这么多对象，此时，就需要老年代进行分配担保，即将 Survivor 无法容纳的对象直接进入老年代。 这么做有一个前提，就是老年代得装得下这么多对象。可是在一次 GC 操作前，虚拟机并不知道到底会有多少对象存活，所以空间分配担保有这样一个判断流程： 发生 Minor GC 前，虚拟机先检查老年代的最大可用连续空间是否大于新生代所有对象的总空间； 如果大于，Minor GC 一定是安全的； 如果小于，虚拟机会查看 HandlePromotionFailure 参数，看看是否允许担保失败； 允许失败：尝试着进行一次 Minor GC； 不允许失败：进行一次 Full GC； 不过 JDK 6 Update 24 后，HandlePromotionFailure 参数就没有用了，规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会进行 Minor GC，否则将进行 Full GC。 Metaspace 元空间与 PermGem 永久代元空间是方法区的在HotSpot jvm 中的实现，方法区主要用于存储类的信息、常量池、方法数据、方法代码等。方法区逻辑上属于堆的一部分，但是为了与堆进行区分，通常又叫“非堆”。 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。，理论上取决于32位/64位系统可虚拟的内存大小。可见也不是无限制的，需要配置参数。 ==元空间和永久代都是方法区的实现== Java 8 彻底将永久代 (PermGen) 移除出了 HotSpot JVM，将其原有的数据迁移至 Java Heap 或 Metaspace。 移除 PermGem 的原因： PermGen 内存经常会溢出，引发恼人的 java.lang.OutOfMemoryError: PermGen，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM； 移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。移除 PermGem 后，方法区和字符串常量的位置： 方法区：移至 Metaspace； 字符串常量：移至 Java Heap。Metaspace 的位置： 本地堆内存(native heap)。Metaspace 的优点： 永久代 OOM 问题将不复存在，因为默认的类的元数据分配只受本地内存大小的限制，也就是说本地内存剩余多少，理论上 Metaspace 就可以有多大；JVM参数： -XX:MetaspaceSize：分配给类元数据空间（以字节计）的初始大小，为估计值。MetaspaceSize的值设置的过大会延长垃圾回收时间。垃圾回收过后，引起下一次垃圾回收的类元数据空间的大小可能会变大。 -XX:MaxMetaspaceSize：分配给类元数据空间的最大值，超过此值就会触发Full GC，取决于系统内存的大小。JVM会动态地改变此值。 -XX:MinMetaspaceFreeRatio：一次GC以后，为了避免增加元数据空间的大小，空闲的类元数据的容量的最小比例，不够就会导致垃圾回收。 -XX:MaxMetaspaceFreeRatio：一次GC以后，为了避免增加元数据空间的大小，空闲的类元数据的容量的最大比例，不够就会导致垃圾回收。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式：七大原则]]></title>
    <url>%2F%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%83%E5%A4%A7%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[开闭原则 依赖倒置原则 单一职责原则 接口隔离原则 迪米特法则（最少知道原则） 里氏替换原则 合成/复用原则 UML类图#:protected~:默认，包权限下划线：static斜体：抽象方法 开闭原则类应该对扩展开放，对修改关闭。 扩展就是添加新功能的意思，因此该原则要求在添加新功能时不需要修改代码。 符合开闭原则最典型的设计模式是装饰者模式，它可以动态地将责任附加到对象上，而不用去修改类的代码。 依赖倒置原则高层次模块不应该依赖于低层的 例如扩展的时候是面向ICourse接口的，而不是在Geely类中添加study方法，main函数也不需要调用Geely的对象的具体课程方法。 大概是：不要写很多不同名字的实现方法，而是去写很多类去继承接口，然后传不同的参数来调用不同类，这些类里的方法名字相同，但实现不同。 单一职责原则修改一个类的原因应该只有一个。换句话说就是让一个类只负责一件事，当这个类需要做过多事情的时候，就需要分解这个类。如果一个类承担的职责过多，就等于把这些职责耦合在了一起，一个职责的变化可能会削弱这个类完成其它职责的能力。 接口隔离原则不应该强迫客户依赖于它们不用的方法。因此使用多个专门的接口比使用单一的总接口要好。 迪米特原则对象对其他对象保持最少的了解，所以少用public尽量降低类之间的耦合只和朋友交流（入参出参、成员变量。。其他内部出现的类不是朋友，尽量搞出去） 里氏替换原则 子类能够替换父类。重载时，子类的入参要比父类宽松。返回 时，子类要更严格。 合成/复用原则 黑箱复用。 下面类图中描述的例子。“人”被继承到“学生”、“经理”和“雇员”等子类。而实际上，学生”、“经理”和“雇员”分别描述一种角色，而“人”可以同时有几种不同的角色。比如，一个人既然是“经理”，就必然是“雇员”；而“人”可能同时还参加MBA课程，从而也是一个“学生”。使用继承来实现角色，则只能使每一个“人”具有Is-A角色，而且继承是静态的，这会使得一个“人”在成为“雇员”身份后，就永远为“雇员”，不能成为“学生”和“经理”，而这显然是不合理的。 这一错误的设计源自于把“角色”的等级结构和“人”的等级结构混淆起来，把“Has-A”角色误解为“Is -A”角色。因此要纠正这种错误，关键是区分“人”与“角色”的区别。下图所示的的设计就正确的做到了这一点。]]></content>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
</search>
