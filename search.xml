<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SpringBoot中搭建全文检索引擎Solr]]></title>
    <url>%2FSpringBoot%E4%B8%AD%E6%90%AD%E5%BB%BA%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8ESolr%2F</url>
    <content type="text"><![CDATA[Solr介绍Solr 是Apache下的一个顶级开源项目，采用Java开发，它是基于Lucene的全文搜索服务器。Solr可以独立运行在Jetty、Tomcat等这些Servlet容器中。Solr提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展，并对索引、搜索性能进行了优化。 使用Solr 进行创建索引和搜索索引的实现方法很简单，如下： 创建索引：客户端（可以是浏览器可以是Java程序）用 POST 方法向 Solr 服务器发送一个描述 Field 及其内容的 XML 文档，Solr服务器根据xml文档添加、删除、更新索引 。 搜索索引：客户端（可以是浏览器可以是Java程序）用 GET方法向 Solr 服务器发送请求，然后对Solr服务器返回Xml、json等格式的查询结果进行解析，组织页面布局。Solr不提供构建页面UI的功能，但是Solr提供了一个管理界面，通过管理界面可以查询Solr的配置和运行情况。 Slor的安装启动下载地址http://lucene.apache.org/solr/ 下载完成后解压即可 cmd命令行下切换到解压目录下的bin目录下，启动slor服务，启动的是solr的云服务版，启动命令为 solr -e cloud -noprompt默认在8983和7574端口启动两组solr服务 当然也可以用下述命令启动单机版的solr： solr start -p 8983当需要更多的命令时，可以使用 solr -help查看更多的命令 启动服务之后在http://localhost:8983就可以打开启动的solr服务 启动服务之后，需要导入相应的数据，命令为： 1java -Dc=gettingstarted -Dauto -Drecursive -jar example\exampledocs\post.jar docs\ 中文分词搜索：IK-Analyzer（1）关掉上述cloud版的服务，然后打开单机版，默认89893端口打开 （2）创建索引的部署 （3）中文分词 由于slor默认的是英文格式的分词格式，也就是按照空格的方式分词的，因此要实现中文的分词需要做一些修改 下面两个文件是我们所需要了解的两个配置文件 在managed-schema中添加如下代码端作为中文分词的配置1234567891011&lt;fieldTypename=&quot;text_ik&quot; class=&quot;solr.TextField&quot;&gt; &lt;!--索引时候的分词器--&gt; &lt;analyzer type=&quot;index&quot;&gt; &lt;tokenizerclass=&quot;org.wltea.analyzer.util.IKTokenizerFactory&quot; useSmart=“false&quot;/&gt; &lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&gt; &lt;/analyzer&gt; &lt;!--查询时候的分词器--&gt; &lt;analyzer type=&quot;query&quot;&gt; &lt;tokenizerclass=&quot;org.wltea.analyzer.util.IKTokenizerFactory&quot; useSmart=“true&quot;/&gt; &lt;/analyzer&gt;&lt;/fieldType&gt; 索引时候的分词器使用的是useSmart=“false”，这个时候分词分的越细越好查询时候的分词器使用的是useSmart=“true”，这个时候就是越接近查询的内容越好，查询起来越快 后台的配置因为需要用到中文的分词，所以需要你进行在solrconfig.xml中的lib下添加相应的jar包 重启solr，在solr中就可以看见相应的text_ik的分词器，就是刚才自己定义的text_ik使用分词之后可以看到和上述的分词的结果是一样的 然后搜索的话需要有字典和后台的数据库的内容，然后如何从数据库中把数据导入呢？ 数据库相关jar包导入，参考资料：http://wiki.apache.org/solr/DIHQuickStart 首先在solrconfig.xml中添加如下代码行 上图中有显示一个data-config.xml的文件，这个是我们需要自己添加并配置的 在你的自己之前建立的wenda的目录文件夹下新建一个data-config.xml，并在文件中添加如下代码段1234567891011121314&lt;dataConfig&gt; &lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://localhost/wenda&quot; user=&quot;root&quot; password=&quot;123456&quot;/&gt; &lt;document&gt; &lt;entity name=&quot;question&quot; query=&quot;select id,title,content from question&quot;&gt; &lt;field column=&quot;title&quot; name=&quot;question_title&quot;/&gt; &lt;field column=&quot;content&quot; name=&quot;question_content&quot;/&gt; &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt; 其实上面代码段与你的数据库是一一对应的，这个代码段是在里抄来的，然后根据自己的参数进行修改 url 数据库的本机地址user 数据库的用户名password 数据库的密码 与是你数据库的字段，与你的managed-schema中定义的filed的name的定义是相同的 同上 然后重启solr 导入数据库里的数据 到这里就把solr的服务器搭建并测试成功了 Sringboot中集成solr（1）.pom中添加solrj的Maven依赖123456!-- https://mvnrepository.com/artifact/org.apache.solr/solr-solrj --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.solr&lt;/groupId&gt; &lt;artifactId&gt;solr-solrj&lt;/artifactId&gt; &lt;version&gt;6.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;（2）按照dao-service-controller编写服务因为不涉及数据库的读写，所以就不需要dao层 关于solr的集成按照什么方式去配置，怎么去写，参照 service层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.springboot.springboot.service; import com.springboot.springboot.model.Question;import org.apache.ibatis.annotations.Update;import org.apache.solr.client.solrj.SolrQuery;import org.apache.solr.client.solrj.impl.HttpSolrClient;import org.apache.solr.client.solrj.response.QueryResponse;import org.apache.solr.client.solrj.response.UpdateResponse;import org.apache.solr.common.SolrInputDocument;import org.springframework.stereotype.Service; import java.util.ArrayList;import java.util.List;import java.util.Map; /** * @author WilsonSong * solr的搜索服务 * @date 2018/7/25 */@Servicepublic class SearchService &#123; //private static final String SOLR_URL = "http://127.0.0.1:8983/solr/#/wenda"; private static final String SOLR_URL ="http://localhost:8983/solr/wenda"; private HttpSolrClient client = new HttpSolrClient.Builder(SOLR_URL).build(); private static final String QUESTION_TITLE_FIELD = "question_title"; private static final String QUESTION_CONTENT_FIELD = "question_content"; /** * * @param keyword 搜索的关键词 * @param offset 翻页 * @param count 翻页 * @param hlPer 高亮的前缀 * @param hlPos 高亮的后缀 * @return * 搜索 */ public List&lt;Question&gt; searchQuestion(String keyword, int offset, int count, String hlPer, String hlPos) throws Exception &#123; List&lt;Question&gt; questionList = new ArrayList&lt;&gt;(); //solr的相关配置 SolrQuery query = new SolrQuery(keyword); query.setRows(count); query.setStart(offset); query.setHighlight(true); //高亮 query.setHighlightSimplePre(hlPer); //前缀 query.setHighlightSimplePost(hlPos); //后缀 query.set("hl.fl", QUESTION_CONTENT_FIELD + "," +QUESTION_TITLE_FIELD); QueryResponse response = client.query(query); //解析搜索的界面的东西 for (Map.Entry&lt;String, Map&lt;String, List&lt;String&gt;&gt;&gt; entry :response.getHighlighting().entrySet()) &#123; Question q = new Question(); q.setId(Integer.parseInt(entry.getKey())); if (entry.getValue().containsKey(QUESTION_CONTENT_FIELD)) &#123; List&lt;String&gt; contentList = entry.getValue().get(QUESTION_CONTENT_FIELD); if (contentList.size() &gt; 0) &#123; q.setContent(contentList.get(0)); &#125; &#125; if (entry.getValue().containsKey(QUESTION_TITLE_FIELD)) &#123; List&lt;String&gt; titleList = entry.getValue().get(QUESTION_TITLE_FIELD); if (titleList.size() &gt; 0) &#123; q.setTitle(titleList.get(0)); &#125; &#125; questionList.add(q); &#125; return questionList; &#125; /** * 索引 * @param qid * @param title * @param content * @return */ public boolean indexQuestion(int qid, String title, String content) throws Exception &#123; SolrInputDocument doc = new SolrInputDocument(); doc.setField("id", qid); doc.setField(QUESTION_TITLE_FIELD, title); doc.setField(QUESTION_CONTENT_FIELD, content); UpdateResponse response = client.add(doc,1000); return response != null &amp;&amp; response.getStatus() == 0; &#125;&#125; controller层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.springboot.springboot.controller; import com.springboot.springboot.model.EntityType;import com.springboot.springboot.model.Question;import com.springboot.springboot.model.viewObject;import com.springboot.springboot.service.FollowService;import com.springboot.springboot.service.SearchService;import com.springboot.springboot.service.questionService;import com.springboot.springboot.service.userService;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam; import java.util.ArrayList;import java.util.List; /** * 搜索 * @author WilsonSong * @date 2018/7/25/025 */@Controllerpublic class SearchController &#123; private static final Logger logger = LoggerFactory.getLogger(SearchController.class); @Autowired SearchService searchService; @Autowired questionService qService; @Autowired FollowService followService; @Autowired userService uService; @RequestMapping(path = &#123;"/search"&#125;, method = &#123;RequestMethod.GET&#125;) public String Search(Model model,@RequestParam("q") String keyword, @RequestParam(value = "offset",defaultValue = "0") int offset , @RequestParam(value = "count", defaultValue = "10") int count) &#123; try&#123; List&lt;Question&gt; questionList = searchService.searchQuestion(keyword,offset,count, "&lt;strong&gt;","&lt;/strong&gt;"); List&lt;viewObject&gt; vos = new ArrayList&lt;&gt;(); for (Question question:questionList) &#123; Question q = qService.selectQuestionById(question.getId()); viewObject vo = new viewObject(); if (question.getContent() != null) &#123; q.setContent(question.getContent()); &#125; if (question.getTitle() != null) &#123; q.setTitle(question.getTitle()); &#125; vo.set("question",q); //问题关注的数量 vo.set("followCount", followService.getFollowerCount(EntityType.ENTITY_QUESTION, question.getId())); vo.set("user", uService.getUser(q.getUserId())); vos.add(vo); &#125; model.addAttribute("vos", vos); model.addAttribute("keyword", keyword); &#125;catch (Exception e)&#123; logger.error("查询失败" + e.getMessage()); &#125; return "result"; &#125;&#125; 这样后端的代码结合前端就写完了，实现solr的集成，然后就可以在你的网站使用solr搜索引擎来搜索 但是面临一个问题，你的slor的库是从数据库中导入的，然后每次出现新的内容，怎么能够实现实时的搜索呢？ 有两种方式： 使用solr的自动增量导入功能 在增加内容的时候使用异步事件把内容实时添加到solr的搜索内容中，具体实现 首先在你增加内容 的时候先产生事件 1234//添加问题后就产生一个异步的事件，把问题增加进去，然后达到实时搜索的功能 eventProducer.fireEvent(new EventModel(EventType.ADD_QUESTION) .setActorId(question.getUserId()).setEntityId(question.getId()) .setExts("title", question.getTitle()).setExts("content", question.getContent())); 1234567891011121314151617181920212223242526272829303132333435363738package com.springboot.springboot.async.handler; import com.springboot.springboot.async.EventHandler;import com.springboot.springboot.async.EventModel;import com.springboot.springboot.async.EventType;import com.springboot.springboot.service.SearchService;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component; import java.util.Arrays;import java.util.List;/** * 异步事件实现增加问题就直接加入solr的搜索库 * @author WilsonSong * @date 2018/7/26/026 */@Componentpublic class AddQuestionHandler implements EventHandler&#123; private static final Logger logger = LoggerFactory.getLogger(AddQuestionHandler.class); @Autowired SearchService searchService; @Override public void doHander(EventModel model) &#123; try&#123; searchService.indexQuestion(model.getEntityId(),model.getExts("title"),model.getExts("content")); &#125;catch (Exception e)&#123; logger.error("增加问题索引失败"); &#125; &#125; @Override public List&lt;EventType&gt; getSupportEventTypes() &#123; return Arrays.asList(EventType.ADD_QUESTION); &#125;&#125; 这样就通过异步队列实时的将新增添的内容通过service的方式添加到solr的搜索库中，实现实时的搜索。 然后其余的部分，如solr的搜索的原理可以参照 https://blog.csdn.net/awj3584/article/details/16963525 https://blog.csdn.net/vtopqx/article/details/73459439]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化方法]]></title>
    <url>%2F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[SQL 优化1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：select id from t where num=0 3.应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 4.应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num=10 or num=20可以这样查询：select id from t where num=10union allselect id from t where num=20 5.in 和 not in 也要慎用，否则会导致全表扫描，如：select id from t where num in(1,2,3)对于连续的数值，能用 between 就不要用 in 了：select id from t where num between 1 and 3 6.下面的查询也将导致全表扫描：select id from t where name like ‘%abc%’ 7.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where num/2=100应改为:select id from t where num=100*2 8.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where substring(name,1,3)=’abc’–name以abc开头的id应改为:select id from t where name like ‘abc%’ 9.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 10.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 11.不要写一些没有意义的查询，如需要生成一个空表结构：select col1,col2 into #t from t where 1=0这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：create table #t(…) 12.很多时候用 exists 代替 in 是一个好的选择：select num from a where num in(select num from b)用下面的语句替换：select num from a where exists(select 1 from b where num=a.num) 13.并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 14.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 15.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 16.尽可能的使用 varchar 代替 char ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 17.任何地方都不要使用 select from t ，用具体的字段列表代替“”，不要返回用不到的任何字段。 18.避免频繁创建和删除临时表，以减少系统表资源的消耗。 19.临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 20.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 21.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 22.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 23.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 24.与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。25.尽量避免大事务操作，提高系统并发能力。26.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 慢日志定位完整的慢日志格式一般如下：123451 # Time: 130320 7:30:262 # User@Host: db_user[db_database] @ localhost []3 # Query_time: 4.545309 Lock_time: 0.000069 Rows_sent: 219 Rows_examined: 2544 SET timestamp=1363779026; 5 SELECT option_name, option_value FROM wp_options WHERE autoload = &apos;yes&apos;; 第1行，代表记录慢日志的时间，格式是YYMMDD H:M:S. 注: 这是MySQL服务器的时间,可能和你的当地时间不同 第2行，很明显不多解释 第3行，是整个语句的query time, Lock time, 返回或者发送了多少行， 执行的行数 第4行，是语句真正发生的时间 第5行，具体语句]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8新特性]]></title>
    <url>%2FJava8%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[Java 8 (又称为 jdk 1.8) 是 Java 语言开发的一个主要版本。 Oracle 公司于 2014 年 3 月 18 日发布 Java 8 ，它支持函数式编程，新的 JavaScript 引擎，新的日期 API，新的Stream API 等。 Lambda 表达式Lambda 表达式，也可称为闭包，它是推动 Java 8 发布的最重要新特性。 Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中）。 使用 Lambda 表达式可以使代码变的更加简洁紧凑。 语法lambda 表达式的语法格式如下： (parameters) -&gt; expression 或 (parameters) -&gt;{ statements; } 以下是lambda表达式的重要特征: 可选类型声明：不需要声明参数类型，编译器可以统一识别参数值。 可选的参数圆括号：一个参数无需定义圆括号，但多个参数需要定义圆括号。 可选的大括号：如果主体包含了一个语句，就不需要使用大括号。 可选的返回关键字：如果主体只有一个表达式返回值则编译器会自动返回值，大括号需要指定明表达式返回了一个数值。 Lambda 表达式实例Lambda 表达式的简单例子: 12345678910// 1. 不需要参数,返回值为 5 () -&gt; 5 // 2. 接收一个参数(数字类型),返回其2倍的值 x -&gt; 2 * x // 3. 接受2个参数(数字),并返回他们的差值 (x, y) -&gt; x – y // 4. 接收2个int型整数,返回他们的和 (int x, int y) -&gt; x + y // 5. 接受一个 string 对象,并在控制台打印,不返回任何值(看起来像是返回void) (String s) -&gt; System.out.print(s) 方法引用方法引用通过方法的名字来指向一个方法。方法引用可以使语言的构造更紧凑简洁，减少冗余代码。方法引用使用一对冒号 :: 。下面，我们在 Car 类中定义了 4 个方法作为例子来区分 Java 中 4 种不同方法的引用。 1234567891011121314151617181920212223242526package com.runoob.main;@FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123; T get();&#125;class Car &#123; //Supplier是jdk1.8的接口，这里和lamda一起使用了 public static Car create(final Supplier&lt;Car&gt; supplier) &#123; return supplier.get(); &#125; public static void collide(final Car car) &#123; System.out.println("Collided " + car.toString()); &#125; public void follow(final Car another) &#123; System.out.println("Following the " + another.toString()); &#125; public void repair() &#123; System.out.println("Repaired " + this.toString()); &#125;&#125; 构造器引用：它的语法是Class::new，或者更一般的Class&lt; T &gt;::new实例如下：final Car car = Car.create( Car::new ); final List&lt; Car &gt; cars = Arrays.asList( car ); 静态方法引用：它的语法是Class::static_method，实例如下：cars.forEach( Car::collide ); 特定类的任意对象的方法引用：它的语法是Class::method实例如下：cars.forEach( Car::repair ); 特定对象的方法引用：它的语法是instance::method实例如下：final Car police = Car.create( Car::new ); cars.forEach( police::follow ); 方法引用实例 123456789101112131415161718192021222324import java.util.List;import java.util.ArrayList;public class Java8Tester &#123; public static void main(String args[]) &#123; List names = new ArrayList(); names.add("Google"); names.add("Runoob"); names.add("Taobao"); names.add("Baidu"); names.add("Sina"); names.forEach(System.out::println); &#125;&#125;实例中我们将 System.out::println 方法作为静态方法来引用。执行以上脚本，输出结果为：$ javac Java8Tester.java $ java Java8TesterGoogleRunoobTaobaoBaiduSina Optional 类https://www.cnblogs.com/zhangboyu/p/7580262.html Optional 类是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。 Optional 是个容器：它可以保存类型T的值，或者仅仅保存null。Optional提供很多有用的方法，这样我们就不用显式进行空值检测。 Optional 类的引入很好的解决空指针异常。 我们从一个简单的用例开始。在 Java 8 之前，任何访问对象方法或属性的调用都可能导致 NullPointerException： String isocode = user.getAddress().getCountry().getIsocode().toUpperCase(); 在这个小示例中，如果我们需要确保不触发异常，就得在访问每一个值之前对其进行明确地检查：12345678910111213141516if (user != null)&#123; Address address = user.getAddress(); if (address != null) &#123; Country country = address.getCountry(); if (country != null) &#123; String isocode = country.getIsocode(); if (isocode != null) &#123; isocode = isocode.toUpperCase(); &#125; &#125; &#125;&#125; 你看到了，这很容易就变得冗长，难以维护。 为了简化这个过程，我们来看看用 Optional 类是怎么做的。从创建和验证实例，到使用其不同的方法，并与其它返回相同类型的方法相结合，下面是见证 _Optional _奇迹的时刻。 StreamJava 8 API添加了一个新的抽象称为流Stream，可以让你以一种声明的方式处理数据。 Stream 使用一种类似用 SQL 语句从数据库查询数据的直观方式来提供一种对 Java 集合运算和表达的高阶抽象。 Stream API可以极大提高Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。 这种风格将要处理的元素集合看作一种流， 流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。 元素流在管道中经过中间操作（intermediate operation）的处理，最后由最终操作(terminal operation)得到前面处理的结果。 123456List&lt;Integer&gt; transactionsIds = widgets.stream() .filter(b -&gt; b.getColor() == RED) .sorted((x,y) -&gt; x.getWeight() - y.getWeight()) .mapToInt(Widget::getWeight) .sum(); Stream（流）是一个来自数据源的元素队列并支持聚合操作 元素是特定类型的对象，形成一个队列。 Java中的Stream并不会存储元素，而是按需计算。 数据源 流的来源。 可以是集合，数组，I/O channel， 产生器generator 等。 聚合操作 类似SQL语句一样的操作， 比如filter, map, reduce, find, match, sorted等。 和以前的Collection操作不同， Stream操作还有两个基础的特征： Pipelining: 中间操作都会返回流对象本身。 这样多个操作可以串联成一个管道， 如同流式风格（fluent style）。 这样做可以对操作进行优化， 比如延迟执行(laziness)和短路( short-circuiting)。 内部迭代： 以前对集合遍历都是通过Iterator或者For-Each的方式, 显式的在集合外部进行迭代， 这叫做外部迭代。 Stream提供了内部迭代的方式， 通过访问者模式(Visitor)实现。 生成流 stream() − 为集合创建串行流。 parallelStream() − 为集合创建并行流。 1List&lt;String&gt; strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl"); List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList()); Stream 完整实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195import java.util.ArrayList;import java.util.Arrays;import java.util.IntSummaryStatistics;import java.util.List;import java.util.Random;import java.util.stream.Collectors;import java.util.Map; public class Java8Tester &#123; public static void main(String args[]) &#123; System.out.println("使用 Java 7: "); // 计算空字符串 List&lt;String&gt; strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl"); System.out.println("列表: " +strings); long count = getCountEmptyStringUsingJava7(strings); System.out.println("空字符数量为: " + count); count = getCountLength3UsingJava7(strings); System.out.println("字符串长度为 3 的数量为: " + count); // 删除空字符串 List&lt;String&gt; filtered = deleteEmptyStringsUsingJava7(strings); System.out.println("筛选后的列表: " + filtered); // 删除空字符串，并使用逗号把它们合并起来 String mergedString = getMergedStringUsingJava7(strings,", "); System.out.println("合并字符串: " + mergedString); List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); // 获取列表元素平方数 List&lt;Integer&gt; squaresList = getSquares(numbers); System.out.println("平方数列表: " + squaresList); List&lt;Integer&gt; integers = Arrays.asList(1,2,13,4,15,6,17,8,19); System.out.println("列表: " +integers); System.out.println("列表中最大的数 : " + getMax(integers)); System.out.println("列表中最小的数 : " + getMin(integers)); System.out.println("所有数之和 : " + getSum(integers)); System.out.println("平均数 : " + getAverage(integers)); System.out.println("随机数: "); // 输出10个随机数 Random random = new Random(); for(int i=0; i &lt; 10; i++) &#123; System.out.println(random.nextInt()); &#125; System.out.println("使用 Java 8: "); System.out.println("列表: " +strings); count = strings.stream().filter(string-&gt;string.isEmpty()).count(); System.out.println("空字符串数量为: " + count); count = strings.stream().filter(string -&gt; string.length() == 3).count(); System.out.println("字符串长度为 3 的数量为: " + count); filtered = strings.stream().filter(string -&gt;!string.isEmpty()).collect(Collectors.toList()); System.out.println("筛选后的列表: " + filtered); mergedString = strings.stream().filter(string -&gt;!string.isEmpty()).collect(Collectors.joining(", ")); System.out.println("合并字符串: " + mergedString); squaresList = numbers.stream().map( i -&gt;i*i).distinct().collect(Collectors.toList()); System.out.println("Squares List: " + squaresList); System.out.println("列表: " +integers); IntSummaryStatistics stats = integers.stream().mapToInt((x) -&gt;x).summaryStatistics(); System.out.println("列表中最大的数 : " + stats.getMax()); System.out.println("列表中最小的数 : " + stats.getMin()); System.out.println("所有数之和 : " + stats.getSum()); System.out.println("平均数 : " + stats.getAverage()); System.out.println("随机数: "); random.ints().limit(10).sorted().forEach(System.out::println); // 并行处理 count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count(); System.out.println("空字符串的数量为: " + count); &#125; private static int getCountEmptyStringUsingJava7(List&lt;String&gt; strings) &#123; int count = 0; for(String string: strings) &#123; if(string.isEmpty()) &#123; count++; &#125; &#125; return count; &#125; private static int getCountLength3UsingJava7(List&lt;String&gt; strings) &#123; int count = 0; for(String string: strings) &#123; if(string.length() == 3) &#123; count++; &#125; &#125; return count; &#125; private static List&lt;String&gt; deleteEmptyStringsUsingJava7(List&lt;String&gt; strings) &#123; List&lt;String&gt; filteredList = new ArrayList&lt;String&gt;(); for(String string: strings) &#123; if(!string.isEmpty()) &#123; filteredList.add(string); &#125; &#125; return filteredList; &#125; private static String getMergedStringUsingJava7(List&lt;String&gt; strings, String separator) &#123; StringBuilder stringBuilder = new StringBuilder(); for(String string: strings) &#123; if(!string.isEmpty()) &#123; stringBuilder.append(string); stringBuilder.append(separator); &#125; &#125; String mergedString = stringBuilder.toString(); return mergedString.substring(0, mergedString.length()-2); &#125; private static List&lt;Integer&gt; getSquares(List&lt;Integer&gt; numbers) &#123; List&lt;Integer&gt; squaresList = new ArrayList&lt;Integer&gt;(); for(Integer number: numbers) &#123; Integer square = new Integer(number.intValue() * number.intValue()); if(!squaresList.contains(square)) &#123; squaresList.add(square); &#125; &#125; return squaresList; &#125; private static int getMax(List&lt;Integer&gt; numbers) &#123; int max = numbers.get(0); for(int i=1;i &lt; numbers.size();i++) &#123; Integer number = numbers.get(i); if(number.intValue() &gt; max) &#123; max = number.intValue(); &#125; &#125; return max; &#125; private static int getMin(List&lt;Integer&gt; numbers) &#123; int min = numbers.get(0); for(int i=1;i &lt; numbers.size();i++) &#123; Integer number = numbers.get(i); if(number.intValue() &lt; min) &#123; min = number.intValue(); &#125; &#125; return min; &#125; private static int getSum(List numbers) &#123; int sum = (int)(numbers.get(0)); for(int i=1;i &lt; numbers.size();i++) &#123; sum += (int)numbers.get(i); &#125; return sum; &#125; private static int getAverage(List&lt;Integer&gt; numbers) &#123; return getSum(numbers) / numbers.size(); &#125;&#125;]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始实现一个简易的JavaMVC框架]]></title>
    <url>%2F%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%9A%84JavaMVC%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[原文]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase]]></title>
    <url>%2Fhbase%2F</url>
    <content type="text"><![CDATA[深入了解hbase 集群建备份，它是master/slaves结构式的备份，由master推送，这样更容易跟踪现在备份到哪里了，况且region server是都有自己的WAL 和HLog日志，它就像mysql的主从备份结构一样，只有一个日志来跟踪。一个master集群可以向多个slave集群推送，收到推送的集群会覆盖它本地的edits日志。 这个备份操作是异步的，这意味着，有时候他们的连接可能是断开的，master的变化不会马上反应到slave当中。备份个格式在设计上是和mysql的statement-based replication是一样的，全部的WALEdits（多种来自Delete和Put的Cell单元）为了保持原子性，会一次性提交。 HLogs是region server备份的基础，当他们要进行备份时必须保存在hdfs上，每个region server从它需要的最老的日志开始进行备份，并且把当前的指针保存在zookeeper当中来简化错误恢复，这个位置对于每一个slave 集群是不同的，但是对于同一个队列的HLogs是相同的。 下面这个是设计的结构图 下面我们了解一下master和一个slave节点的整个过程。 （1）当客户端通过api发送Put、Delete或者ICV到region server，这些KeyValue被转换成WALEdit，这个过程会被replication检测到，每一个设置了replication的列族，会把scope添加到edit的日志，然后追加到WAL中，并被应用到MemStore中。 （2）在另一个线程当中，edit被从log当中读取来，并且只有可以备份的KeyValues（列族为scoped为GLOBAL的，并且不是catalog，catalog指的是.META. 和 -ROOT-） （3-1）这个edit然后被打上master群集的UUID，当buffer写满的时候或者读完文件，buffer会发到slave集群的随机的一个region server同步的，收到他们的region server把edit分开，一个表一个buffer，当所有的edits被读完之后，每一个buffer会通过HTable来flush，edits里面的master集群的UUID被应用到了备份节点，以此可以进行循环备份。 （4-1）回到master的region server上，当前WAL的位移offset已经被注册到了zookeeper上面。 （3-2）这里面，如果slave的region server没有响应，master的region server会停止等待，并且重试，如果目标的region server还是不可用，它会重新选择别的slave的region server去发送那些buffer。 同时WALs会被回滚，并且保存一个队列在zookeeper当中，那些被region server存档的Logs会更新他们在复制线程中的内存中的queue的地址。 （4-2）当目标集群可用了，master的region server会复制积压的日志。 HBase协处理器简介总体来说其包含两种协处理器：Observers和Endpoint。 其中Observers可以理解问传统数据库的触发器，当发生某一个特定操作的时候出发Observer。 RegionObserver：提供基于表的region上的Get, Put, Delete, Scan等操作，比如可以在客户端进行get操作的时候定义RegionObserver来查询其时候具有get权限等。具体的方法（拦截点）有： preOpen, postOpen: Called before and after the region is reported as online to the master. preFlush, postFlush: Called before and after the memstore is flushed into a new store file. preGet, postGet: Called before and after a client makes a Get request. preExists, postExists: Called before and after the client tests for existence using a Get. prePut and postPut: Called before and after the client stores a value. preDelete and postDelete: Called before and after the client deletes a - value. WALObserver：提供基于WAL的写和刷新WAL文件的操作，一个regionserver上只有一个WAL的上下文。具体的方法（拦截点）有： preWALWrite/postWALWrite: called before and after a WALEdit written to WAL. MasterObserver：提供基于诸如ddl的的操作检查，如create, delete, modify table等，同样的当客户端delete表的时候通过逻辑检查时候具有此权限场景等。其运行于Master进程中。具体的方法（拦截点）有： preCreateTable/postCreateTable: Called before and after the region is reported as online to the master. preDeleteTable/postDeleteTable 以上对于Observer的逻辑以RegionObserver举例来说其时序图如下： Endpoint可以理解为传统数据库的存储过程操作，比如可以进行某族某列值得加和。无Endpoint特性的情况下需要全局扫描表，通过Endpoint则可以在多台分布有对应表的regionserver上同步加和，在将加和数返回给客户端进行全局加和操作，充分利用了集群资源，增加性能。Endpoint基本概念如下图： 两者代码实现细节的差异 在实现两种协处理器的时候稍有区别。无论哪种协处理器都需要运行于Server端的环境中。其中Endpoint还需要通过protocl来定义接口实现客户端代码进行rpc通信以此来进行数据的搜集归并。而Observer则不需要客户端代码，只在特定操作发生的时候出发服务端代码的实现。 Observer协处理器的实现相对来说Observer的实现来的简单点，只需要实现服务端代码逻辑即可。通过实现一个RegionserverObserver来加深了解。 所要实现的功能： 假定某个表有A和B两个列——————————–便于后续描述我们称之为coprocessor_table 当我们向A列插入数据的时候通过协处理器像B列也插入数据。 在读取数据的时候只允许客户端读取B列数据而不能读取A列数据。换句话说A列是只写 B列是只读的。（为了简单起见，用户在读取数据的时候需要制定列名） A列值必须是整数，换句话说B列值也自然都是整数 当删除操作的时候不能指定删除B列 当删除A列的时候同时需要删除B列 对于其他列的删除不做检查 在上述功能点确定后，我们就要开始实现这两个功能。好在HBase API中有BaseRegionObserver，这个类已经帮助我们实现了大部分的默认实现，我们只要专注于业务上的方法重载即可。 代码框架：1234567891011121314151617181920public class 协处理器类名称 extends BaseRegionObserver &#123; private static final Log LOG = LogFactory.getLog(协处理器类名称.class); private RegionCoprocessorEnvironment env = null;// 协处理器是运行于region中的，每一个region都会加载协处理器 // 这个方法会在regionserver打开region时候执行（还没有真正打开） @Override public void start(CoprocessorEnvironment e) throws IOException &#123; env = (RegionCoprocessorEnvironment) e; &#125; // 这个方法会在regionserver关闭region时候执行（还没有真正关闭） @Override public void stop(CoprocessorEnvironment e) throws IOException &#123; // nothing to do here &#125; /** * 出发点，比如可以重写prePut postPut等方法，这样就可以在数据插入前和插入后做逻辑控制了。 */ @Override 业务代码实现 ： 根据上述需求和代码框架，具体逻辑实现如下。 在插入需要做检查所以重写了prePut方法 在删除前需要做检查所以重写了preDelete方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107public class MyRegionObserver extends BaseRegionObserver &#123; private static final Log LOG = LogFactory.getLog(MyRegionObserver.class); private RegionCoprocessorEnvironment env = null; // 设定只有F族下的列才能被操作，且A列只写，B列只读。的语言 private static final String FAMAILLY_NAME = "F"; private static final String ONLY_PUT_COL = "A"; private static final String ONLY_READ_COL = "B"; // 协处理器是运行于region中的，每一个region都会加载协处理器 // 这个方法会在regionserver打开region时候执行（还没有真正打开） @Override public void start(CoprocessorEnvironment e) throws IOException &#123; env = (RegionCoprocessorEnvironment) e; &#125; // 这个方法会在regionserver关闭region时候执行（还没有真正关闭） @Override public void stop(CoprocessorEnvironment e) throws IOException &#123; // nothing to do here &#125; /** * 需求 1.不允许插入B列 2.只能插入A列 3.插入的数据必须为整数 4.插入A列的时候自动插入B列 */ @Override public void prePut(final ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, final Put put, final WALEdit edit, final Durability durability) throws IOException &#123; // 首先查看单个put中是否有对只读列有写操作 List&lt;Cell&gt; cells = put.get(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(ONLY_READ_COL)); if (cells != null &amp;&amp; cells.size() != 0) &#123; LOG.warn("User is not allowed to write read_only col."); throw new IOException("User is not allowed to write read_only col."); &#125; // 检查A列 cells = put.get(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(ONLY_PUT_COL)); if (cells == null || cells.size() == 0) &#123; // 当不存在对于A列的操作的时候则不做任何的处理，直接放行即可 LOG.info("No A col operation, just do it."); return; &#125; // 当A列存在的情况下在进行值得检查，查看是否插入了整数 byte[] aValue = null; for (Cell cell : cells) &#123; try &#123; aValue = CellUtil.cloneValue(cell); LOG.warn("aValue = " + Bytes.toString(aValue)); Integer.valueOf(Bytes.toString(aValue)); &#125; catch (Exception e1) &#123; LOG.warn("Can not put un number value to A col."); throw new IOException("Can not put un number value to A col."); &#125; &#125; // 当一切都ok的时候再去构建B列的值，因为按照需求，插入A列的时候需要同时插入B列 LOG.info("B col also been put value!"); put.addColumn(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(ONLY_READ_COL), aValue); &#125; /** * 需求 1.不能删除B列 2.只能删除A列 3.删除A列的时候需要一并删除B列 */ @Override public void preDelete(final ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, final Delete delete, final WALEdit edit, final Durability durability) throws IOException &#123; // 首先查看是否对于B列进行了指定删除 List&lt;Cell&gt; cells = delete.getFamilyCellMap().get(Bytes.toBytes(FAMAILLY_NAME)); if (cells == null || cells.size() == 0) &#123; // 如果客户端没有针对于FAMAILLY_NAME列族的操作则不用关心，让其继续操作即可。 LOG.info("NO F famally operation ,just do it."); return; &#125; // 开始检查F列族内的操作情况 byte[] qualifierName = null; boolean aDeleteFlg = false; for (Cell cell : cells) &#123; qualifierName = CellUtil.cloneQualifier(cell); // 检查是否对B列进行了删除，这个是不允许的 if (Arrays.equals(qualifierName, Bytes.toBytes(ONLY_READ_COL))) &#123; LOG.info("Can not delete read only B col."); throw new IOException("Can not delete read only B col."); &#125; // 检查是否存在对于A队列的删除 if (Arrays.equals(qualifierName, Bytes.toBytes(ONLY_PUT_COL))) &#123; LOG.info("there is A col in delete operation!"); aDeleteFlg = true; &#125; &#125; // 如果对于A列有删除，则需要对B列也要删除 if (aDeleteFlg) &#123; LOG.info("B col also been deleted!"); delete.addColumn(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(ONLY_READ_COL)); &#125; &#125;&#125; Observer协处理器上传加载 完成实现后需要将协处理器类打包成jar文件，对于协处理器的加载通常有三种方法： 配置文件加载：即通过hbase-site.xml文件配置加载，一般这样的协处理器是系统级别的，全局的协处理器，如权限控制等检查。 shell加载：可以通过alter命令来对表进行scheme进行修改来加载协处理器。 通过API代码实现：即通过API的方式来加载协处理器。 上述加载方法中，1，3都需要将协处理器jar文件放到集群的hbase的classpath中。而2方法只需要将jar文件上传至集群环境的hdfs即可。 下面我们只介绍如何通过2方法进行加载。 步骤1：通过如下方法创建表12hbase(main):001:0&gt; create 'coprocessor_table','F'0 row(s) in 2.7570 seconds =&gt; Hbase::Table - coprocessor_table 步骤2：通过alter命令将协处理器加载到表中1alter 'coprocessor_table' , METHOD =&gt;'table_att','coprocessor'=&gt;'hdfs://ns1/testdata/Test-HBase-Observer.jar|cn.com.newbee.feng.MyRegionObserver|1001' 其中：’coprocessor’=&gt;’jar文件在hdfs上的绝对路径|协处理器主类|优先级|协处理器参数上述协处理器并没有参数，所以未给出参数，对于协处理器的优先级不在此做讨论。步骤3：检查协处理器的加载 123456789hbase(main):021:0&gt; describe 'coprocessor_table' Table coprocessor_table is ENABLED coprocessor_table, &#123;TABLE_ATTRIBUTES =&gt; &#123;coprocessor$1 =&gt; 'hdfs://ns1/testdata/Test-HBase-Observer.jar|cn.com.newbee.feng.MyRegionObserver|1001'&#125; COLUMN FAMILIES DESCRIPTION &#123;NAME =&gt; 'F', DATA_BLOCK_ENCODING =&gt; 'NONE', BLOOMFILTER =&gt; 'ROW', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '1', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', TTL =&gt; 'FOREVER', KEEP_DELETED_CELLS =&gt; 'FALSE', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'&#125; 1 row(s) in 0.0300 seconds 可以看到协处理器被表成功加载，其实内部是将update所有此表的region去加载协处理器的。 Hbase读写数据的原理解析 针对上图的一些解释：这里面数据分区（region）存储是为了查询方便（即因为是集群所以能充分利用磁盘的IO性）。添加数据时，数据先进入Hlog–预写日志（数据只能追加不能修改）&lt;防止数据丢失&gt;，数据在Hlog写完后再写到内存中。HFile：认为是将数据进行序列化。StoreFile：认为是一个文件。DFS:调用HDFS的客户端API来将数据传到HDFS。 写数据的流程（参考上图）：1、客户端向hregionServer请求写数据2、hregionServer将数据先写入hlog中。3、hregionServer将数据后写入memstore中。4、当内存中的数据达到阈值64M的时候，将数据Flush到硬盘中，并同时删除内存和hlog中的历史数据。5、将硬盘中数据通过HFile来序列化，再将数据传输到HDFS进行存储。并对Hlog做一个标记。6、当HDFS中的数据块达到4块的时候，Hmaster将数据加载到本地进行一个合并（如果合并后数据的大小小于256M则当数据块再次达到4块时（包含小于256M的数据块）将最新4块数据块再次进行合并，此时数据块大于256M）。7、若数据块大于256M，则将数据重新拆分，将分配后的region重新分配给不同的hregionServer进行管理。8、当hregionServer宕机后，将hregionServer上的hlog重新分配给不同的hregionServer进行加载（修改.META文件中关于数据所在server的信息）。注意：hlog会同步到HDFS中。 读数据的流程（参考下图）：1、通过zk来获取ROOT表在那个节点上，然后进一步通过-ROOT表和-META表来获取最终的位置信息。2、数据从内存和硬盘合并后返回到客户端。 PS：由上图可看到，当客户端在执行查询语句的时候，会先到zk上寻找对应-ROOT表（主要描述-META表在哪里）的位置信息（由此也知道ZK在Hbase中的作用），接下来根据-ROOT表中数据进而找到对应的HRegionServer，在对应的HRegionServer上的-META表（主要记载表的元数据信息）中找到对应的Table表在哪个HRegionServer上，再到对应的HRegionServer中查找对应的数据。 hbase replication原理分析replicationSource大致工作流程 while(isAlive())进行主体循环 从WAL文件获取List&lt;WAL.Entry&gt; 通过调用shipEdits方法发送数据 调用replicationEndpoint replicate方法发送数据 最终调用admin.replicateWALEntry通过rpc发送数据 regionserver如何从slave cluster中选取regionserver当做复制节点 replication过程需要连接peer（slave cluster），首先要获取这个peer所有活着的regionservers 拿到所有regionservers信息之后，开始选择哪些regionservers作为replication的对象 选哪些regionservers当做sink由peer活着的regionserver个数*ratio（默认值0.1）决定，regionservers先shuffle打乱顺序后再截取 如果选择的sink（regionserver）个数为0，一直等待peer上线，也就是slave cluster没有启动的情况 总结 每个slave cluster对应一个replicationSource线程，各个slave复制互不干扰 每个replicationSource是单线程进行传输数据，改成多线程并发传可能更好 数据是通过rpc发送过去，调用slave cluster regionserver RSRpcServices的replicateWALEntry方法 HBase是强一致性系统Hbase具有以下特点 每个值只出现在一个REGION 同一时间一个Region只分配给一个Region服务器 行内的mutation操作都是原子的(原子性操作是指：如果把一个事务可看作是一个程序,它要么完整的被执行,要么完全不执行)。 put操作要么成功，要么完全失败。 当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。 crash后对于分布式数据库来说，容错处理是非常重要的一个部分。RegionServer是HBase系统中存在最多的节点，所以对于RegionServer的容错处理对于HBase来说至关重要。本文对RegionServer的容错处理进行Step by Step的分析，希望能解释清除整个过程并加以点评。 我们假设在HBase运行的过程中有一个RegionServer突然Crash, 基于这个场景进行分析。 1. RegionServer Crash了Crash的原因可能有很多种，程序自身挂掉，OS挂掉，网络断掉，电源断掉，等等。但从MasterServer的角度看来只有一种现象，那就是RegionServer在Zookeeper上面注册的Node消失了。我们知道当RegionServer启动的时候会产生一个StartCode，并在Zookeeper上面注册一个EPHEMERAL类型的节点。 一旦RS和ZK之间的通信消失，EPHEMERAL的节点就会被自动删除。而MasterServer则会捕获这个Node消失的事件。 捕获这个事件之后，所有的事情就交给ServerManager.expireServer()来处理了。 2. ServerManager处理首先ServerManager会更新自己的列表，包括deadserver list, online server list 以及 server connection list. 当然在我们试图Stop整个Cluster的时候也会收到同样的请求，不予理睬就好了。 并不是所有的RS Crash都做相同的处理，有些RS比较特殊，他们正在管理-ROOT-表或者.META.表。如果我们重新分配Region, 就需要修改.META.表，如果要访问.META.表先要查询-ROOT-表 （参见以前的文章，client如何路由到正确的RS）。问题是现在管理-ROOT-表或者.META.表的RS挂了，显然第一要务是让-ROOT-和.META.有所归属，能够正常的对他们进行读写。 接下来ServerManager通过传递Event的方式将任务交给Excutor线程来处理，具体调用MetaServerShutdownHandler或者ServerShutdownHandler的process函数。 3. ServerShutdownHandler处理MetaServerShutdownHandler是继承于ServerShutdownHandler的，除了打上META的标志以外其他都一样，所以我们只介绍ServerShutdownHandler的处理。 处理的第一步是Split HLog。当RS Crash的时候所有MemCache里面的内容都会被丢掉，这些内容还没有来得及Flush到HFile里面。感谢WAL机制，所有的内容都可以在HLog File里面找到。问题是原来管理这些HLog文件的RS已经挂掉了，需要将这些HLog交给新的RS去处理。往往这些HLog不会交给同一个新的RS去处理，因为HLog可能包含多个Region的内容，而这些Region可能会分配给不同的RS。这样看来最好的方式是让HLog里面的每一个Entry跟随Region，Region被分配给哪个RS，就让那个RS来处理这个Entry。事实上MasterServer也确实是这么做的。具体步骤如下： 1) 从Crash的RS的HLog目录下读取每个HLog文件 2) 根据文件中每个Entry所隶属的Region找到Region文件的存储目录。将Entry写到一个叫做 “recovered.edits” 的文件夹中。Entry写入的格式依然是HLog的格式。 3) 将Region分配给某个具体的RS，剩下的任务由RS处理 4. Assign RegionServer前面有提到过，如果Crash的RS正在Handle -ROOT-或者.META.，需要特殊处理。特殊处理的方式就是先assign -ROOT-和.META.，并等待他们online。这样可以保证后需的assign工作。 具体Assign的过程可以单独用一章来讲，这里不做详细介绍。 5. RegionServer加载新的Region一旦某个RS被assign了一个新的Region, 它就会试图加载这个新Region. 在加载的过程中RS会查看 “recovered.edits” 目录，试图从HLog中恢复丢失的数据。具体过程如下： 1) 扫描目录中的每个HLog文件 2) 跳过那些Sequence ID小于Region MAX Seq ID的Entry，因为这些Entry已经在HFile里面了。 3) 找到丢失的Entry并写入MemStore 4) Flush MemStore到HFile 5) 删除目录下的文件 具体可以参考 HRegion.replayRecoveredEdits 6. 分析后的思考写到这里整个过程已经基本上结束了，但除了分析过程，还有许多我们可以思考的地方。 1) 恢复一个RegionServer需要多少时间？ 等待ZK Node消失 + Split Log + Assign Region + Recover HLog +Region Online 这是从正常情况看来需要花费的时间，Log越多，Region越多，需要花费的时间越长。别忘了如果RS正在Handle -ROOT-或者.META.，需要的时间会更多。 2) RegionServer的问题真的可以都被侦测吗？ 如果RS已经停止服务，但依然存活，ZK Node 就不会消失，这种情况应该发生过。目前只能作为Bug去修理。也许额外的Monitor是一个弥补的办法（定期Scan什么的）。 3) 容错性到底有多强？ 如果是RS接二连三的挂掉，刚刚分配的RS又挂掉，等等极端情况，HBase的容错性到底有多强呢？也许需要针对性的理论分析和详细测试。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解排序算法]]></title>
    <url>%2F%E5%9B%BE%E8%A7%A3%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[转载 希尔排序 希尔排序是希尔（Donald Shell）于1959年提出的一种排序算法。希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序，同时该算法是冲破O(n2）的第一批算法之一。本文会以图解的方式详细介绍希尔排序的基本思想及其代码实现。 基本思想 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。 简单插入排序很循规蹈矩，不管数组分布是怎么样的，依然一步一步的对元素进行比较，移动，插入，比如[5,4,3,2,1,0]这种倒序序列，数组末端的0要回到首位置很是费劲，比较和移动元素均需n-1次。而希尔排序在数组中采用跳跃式分组的策略，通过某个增量将数组元素划分为若干组，然后分组进行插入排序，随后逐步缩小增量，继续按组进行插入排序操作，直至增量为1。希尔排序通过这种策略使得整个数组在初始阶段达到从宏观上看基本有序，小的基本在前，大的基本在后。然后缩小增量，到增量为1时，其实多数情况下只需微调即可，不会涉及过多的数据移动。 我们来看下希尔排序的基本步骤，在此我们选择增量gap=length/2，缩小增量继续以gap = gap/2的方式，这种增量选择我们可以用一个序列来表示，{n/2,(n/2)/2…1}，称为增量序列。希尔排序的增量序列的选择与证明是个数学难题，我们选择的这个增量序列是比较常用的，也是希尔建议的增量，称为希尔增量，但其实这个增量序列不是最优的。此处我们做示例使用希尔增量。 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package sortdemo;import java.util.Arrays;/** * Created by chengxiao on 2016/11/24. */public class ShellSort &#123; public static void main(String []args) &#123; int []arr =&#123;1,4,2,7,9,8,3,6&#125;; sort(arr); System.out.println(Arrays.toString(arr)); int []arr1 =&#123;1,4,2,7,9,8,3,6&#125;; sort1(arr1); System.out.println(Arrays.toString(arr1)); &#125; /** * 希尔排序 针对有序序列在插入时采用交换法 * @param arr */ public static void sort(int []arr) &#123; //增量gap，并逐步缩小增量 for(int gap=arr.length/2;gap&gt;0;gap/=2) &#123; //从第gap个元素，逐个对其所在组进行直接插入排序操作 for(int i=gap;i&lt;arr.length;i++) &#123; int j = i; while(j-gap&gt;=0 &amp;&amp; arr[j]&lt;arr[j-gap]) &#123; //插入排序采用交换法 swap(arr,j,j-gap); j-=gap; &#125; &#125; &#125; &#125; /** * 希尔排序 针对有序序列在插入时采用移动法。 * @param arr */ public static void sort1(int []arr) &#123; //增量gap，并逐步缩小增量 for(int gap=arr.length/2;gap&gt;0;gap/=2) &#123; //从第gap个元素，逐个对其所在组进行直接插入排序操作 for(int i=gap;i&lt;arr.length;i++) &#123; int j = i; int temp = arr[j]; if(arr[j]&lt;arr[j-gap]) &#123; while(j-gap&gt;=0 &amp;&amp; temp&lt;arr[j-gap]) &#123; //移动法 arr[j] = arr[j-gap]; j-=gap; &#125; arr[j] = temp; &#125; &#125; &#125; &#125; /** * 交换数组元素 * @param arr * @param a * @param b */ public static void swap(int []arr,int a,int b) &#123; arr[a] = arr[a]+arr[b]; arr[b] = arr[a]-arr[b]; arr[a] = arr[a]-arr[b]; &#125;&#125; 堆排序预备知识堆排序 堆排序是利用堆这种数据结构而设计的一种排序算法，堆排序是一种选择排序，它的最坏，最好，平均时间复杂度均为O(nlogn)，它也是不稳定排序。首先简单了解下堆结构。 堆 堆是具有以下性质的完全二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆；或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。如下图： 该数组从逻辑上讲就是一个堆结构，我们用简单的公式来描述一下堆的定义就是： 大顶堆：arr[i] &gt;= arr[2i+1] &amp;&amp; arr[i] &gt;= arr[2i+2] 小顶堆：arr[i] &lt;= arr[2i+1] &amp;&amp; arr[i] &lt;= arr[2i+2] ok，了解了这些定义。接下来，我们来看看堆排序的基本思想及基本步骤： 堆排序基本思想及步骤 堆排序的基本思想是：将待排序序列构造成一个大顶堆，此时，整个序列的最大值就是堆顶的根节点。将其与末尾元素进行交换，此时末尾就为最大值。然后将剩余n-1个元素重新构造成一个堆，这样会得到n个元素的次小值。如此反复执行，便能得到一个有序序列了 步骤一 构造初始堆。将给定无序序列构造成一个大顶堆（一般升序采用大顶堆，降序采用小顶堆)。 假设给定无序序列结构如下 此时我们从最后一个非叶子结点开始（叶结点自然不用调整，第一个非叶子结点 arr.length/2-1=5/2-1=1，也就是下面的6结点），从左至右，从下至上进行调整。 找到第二个非叶节点4，由于[4,9,8]中9元素最大，4和9交换。 这时，交换导致了子根[4,5,6]结构混乱，继续调整，[4,5,6]中6最大，交换4和6。 此时，我们就将一个无需序列构造成了一个大顶堆。 步骤二 将堆顶元素与末尾元素进行交换，使末尾元素最大。然后继续调整堆，再将堆顶元素与末尾元素交换，得到第二大元素。如此反复进行交换、重建、交换。 a.将堆顶元素9和末尾元素4进行交换 b.重新调整结构，使其继续满足堆定义 c.再将堆顶元素8与末尾元素5进行交换，得到第二大元素8. 后续过程，继续进行调整，交换，如此反复进行，最终使得整个序列有序 再简单总结下堆排序的基本思路： 将无需序列构建成一个堆，根据升序降序需求选择大顶堆或小顶堆; 将堆顶元素与末尾元素交换，将最大元素”沉”到数组末端; 重新调整结构，使其满足堆定义，然后继续交换堆顶元素与当前末尾元素，反复执行调整+交换步骤，直到整个序列有序。 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package sortdemo;import java.util.Arrays;/** * Created by chengxiao on 2016/12/17. * 堆排序demo */public class HeapSort &#123; public static void main(String []args) &#123; int []arr = &#123;9,8,7,6,5,4,3,2,1&#125;; sort(arr); System.out.println(Arrays.toString(arr)); &#125; public static void sort(int []arr) &#123; //1.构建大顶堆 for(int i=arr.length/2-1;i&gt;=0;i--) &#123; //从第一个非叶子结点从下至上，从右至左调整结构 adjustHeap(arr,i,arr.length); &#125; //2.调整堆结构+交换堆顶元素与末尾元素 for(int j=arr.length-1;j&gt;0;j--) &#123; swap(arr,0,j);//将堆顶元素与末尾元素进行交换 adjustHeap(arr,0,j);//重新对堆进行调整 &#125; &#125; /** * 调整大顶堆（仅是调整过程，建立在大顶堆已构建的基础上） * @param arr * @param i * @param length */ public static void adjustHeap(int []arr,int i,int length) &#123; int temp = arr[i];//先取出当前元素i for(int k=i*2+1;k&lt;length;k=k*2+1) &#123;//从i结点的左子结点开始，也就是2i+1处开始 if(k+1&lt;length &amp;&amp; arr[k]&lt;arr[k+1]) &#123;//如果左子结点小于右子结点，k指向右子结点 k++; &#125; if(arr[k] &gt;temp) &#123;//如果子节点大于父节点，将子节点值赋给父节点（不用进行交换） arr[i] = arr[k]; i = k; &#125;else &#123; break; &#125; &#125; arr[i] = temp;//将temp值放到最终的位置 &#125; /** * 交换元素 * @param arr * @param a * @param b */ public static void swap(int []arr,int a ,int b) &#123; int temp=arr[a]; arr[a] = arr[b]; arr[b] = temp; &#125;&#125; 基于PriorityQueue实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class b_41_medianinstream&#123; public static PriorityQueue&lt;Integer&gt; maxheap,minheap; public static int num = 0; public void addNumber(int value) &#123; maxheap.add(value); System.out.println(minheap); System.out.println(maxheap); if(num % 2 == 0) &#123; if(minheap.isEmpty()) &#123; num++; return; &#125; //新插入的值大于min堆的根（最小值），则应该被插到min堆中 //交换？min堆最小的给max堆 else if(maxheap.peek() &gt; minheap.peek()) &#123; int maxheap_root = maxheap.poll(); int minheap_root = minheap.poll(); System.out.println("maxheap_root="+maxheap_root+",minheap_root="+minheap_root); maxheap.add(minheap_root); minheap.add(maxheap_root); System.out.println(minheap); System.out.println(maxheap); &#125; &#125; else &#123; minheap.add(maxheap.poll()); &#125; num++; &#125; public int getMedian() &#123; if(num % 2 ==0) return maxheap.peek(); else return (maxheap.peek() + minheap.peek())/2; &#125; public static void main(String[] args) &#123; b_41_medianinstream medianinstream = new b_41_medianinstream(); Comparator&lt;Integer&gt; mycomparator = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; /*如果指定的数与参数相等返回0。 如果指定的数小于参数返回 -1。 如果指定的数大于参数返回 1 * */ return o2.compareTo(o1); &#125; &#125;; maxheap = new PriorityQueue&lt;Integer&gt;(20,mycomparator); minheap = new PriorityQueue&lt;Integer&gt;(20); int a[] = &#123;2,4,6,8,10,5,7,9&#125;; for(int i =0; i &lt; 8; i++) &#123; medianinstream.addNumber(a[i]); // minheap.add(a[i]); // maxheap.add(a[i]); // System.out.println(minheap); // System.out.println(maxheap); System.out.println(); &#125; System.out.println(medianinstream.getMedian()); &#125;&#125; 堆排序是一种选择排序，整体主要由构建初始堆+交换堆顶元素和末尾元素并重建堆两部分组成。其中构建初始堆经推导复杂度为O(n)，在交换并重建堆的过程中，需交换n-1次，而重建堆的过程中，根据完全二叉树的性质，[log2(n-1),log2(n-2)…1]逐步递减，近似为nlogn。所以堆排序时间复杂度一般认为就是O(nlogn)级。 归并排序 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package sortdemo;import java.util.Arrays;/** * Created by chengxiao on 2016/12/8. */public class MergeSort &#123; public static void main(String []args) &#123; int []arr = &#123;9,8,7,6,5,4,3,2,1&#125;; sort(arr); System.out.println(Arrays.toString(arr)); &#125; public static void sort(int []arr) &#123; int []temp = new int[arr.length];//在排序前，先建好一个长度等于原数组长度的临时数组，避免递归中频繁开辟空间 sort(arr,0,arr.length-1,temp); &#125; private static void sort(int[] arr,int left,int right,int []temp) &#123; if(left&lt;right) &#123; int mid = (left+right)/2; sort(arr,left,mid,temp);//左边归并排序，使得左子序列有序 sort(arr,mid+1,right,temp);//右边归并排序，使得右子序列有序 merge(arr,left,mid,right,temp);//将两个有序子数组合并操作 &#125; &#125; private static void merge(int[] arr,int left,int mid,int right,int[] temp) &#123; int i = left;//左序列指针 int j = mid+1;//右序列指针 int t = 0;//临时数组指针 while (i&lt;=mid &amp;&amp; j&lt;=right) &#123; if(arr[i]&lt;=arr[j]) &#123; temp[t++] = arr[i++]; &#125;else &#123; temp[t++] = arr[j++]; &#125; &#125; while(i&lt;=mid) &#123;//将左边剩余元素填充进temp中 temp[t++] = arr[i++]; &#125; while(j&lt;=right) &#123;//将右序列剩余元素填充进temp中 temp[t++] = arr[j++]; &#125; t = 0; //将temp中的元素全部拷贝到原数组中 while(left &lt;= right) &#123; arr[left++] = temp[t++]; &#125; &#125;&#125; 归并排序是稳定排序，它也是一种十分高效的排序，能利用完全二叉树特性的排序一般性能都不会太差。java中Arrays.sort()采用了一种名为TimSort的排序算法，就是归并排序的优化版本。从上文的图中可看出，每次合并操作的平均时间复杂度为O(n)，而完全二叉树的深度为|log2n|。总的平均时间复杂度为O(nlogn)。而且，归并排序的最好，最坏，平均时间复杂度均为O(nlogn)。]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[彻底理解KMP]]></title>
    <url>%2F%E5%BD%BB%E5%BA%95%E7%90%86%E8%A7%A3KMP%2F</url>
    <content type="text"><![CDATA[转自从头到尾彻底理解KMP（2014年8月22日版） 暴力匹配算法假设现在我们面临这样一个问题：有一个文本串S，和一个模式串P，现在要查找P在S中的位置，怎么查找呢？ 如果用暴力匹配的思路，并假设现在文本串S匹配到 i 位置，模式串P匹配到 j 位置，则有： 如果当前字符匹配成功（即S[i] == P[j]），则i++,j++，继续匹配下一个字符；如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0。相当于每次匹配失败时，i 回溯，j 被置为0。 理清楚了暴力匹配算法的流程及内在的逻辑，咱们可以写出暴力匹配的代码，如下： 12345678910111213141516171819202122232425262728int ViolentMatch(char* s, char* p)&#123; int sLen = strlen(s); int pLen = strlen(p); int i = 0; int j = 0; while (i &lt; sLen &amp;&amp; j &lt; pLen) &#123; if (s[i] == p[j]) &#123; //①如果当前字符匹配成功（即S[i] == P[j]），则i++，j++ i++; j++; &#125; else &#123; //②如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0 i = i - j + 1; j = 0; &#125; &#125; //匹配成功，返回模式串p在文本串s中的位置，否则返回-1 if (j == pLen) return i - j; else return -1;&#125; KMP算法定义Knuth-Morris-Pratt 字符串查找算法，简称为 “KMP算法”，常用于在一个文本串S内查找一个模式串P 的出现位置，这个算法由Donald Knuth、Vaughan Pratt、James H. Morris三人于1977年联合发表，故取这3人的姓氏命名此算法。 下面先直接给出KMP的算法流程： 假设现在文本串S匹配到 i 位置，模式串P匹配到 j 位置 如果j = -1，或者当前字符匹配成功（即S[i] == P[j]），都令i++，j++，继续匹配下一个字符； 如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j]。此举意味着失配时，模式串P相对于文本串S向右移动了j - next [j] 位。换言之，当匹配失败时，模式串向右移动的位数为：失配字符所在位置 - 失配字符对应的next 值（next 数组的求解会在下文详细阐述），即移动的实际位数为：j - next[j]，且此值大于等于1。 很快，你也会意识到next 数组各值的含义：代表当前字符之前的字符串中，有多大长度的相同前缀后缀。例如如果next [j] = k，代表j 之前的字符串中有最大长度为k 的相同前缀后缀。 此也意味着在某个字符失配时，该字符对应的next 值会告诉你下一步匹配中，模式串应该跳到哪个位置（跳到next [j] 的位置）。如果next [j] 等于0或-1，则跳到模式串的开头字符，若next [j] = k 且 k &gt; 0，代表下次匹配跳到j 之前的某个字符，而不是跳到开头，且具体跳过了k 个字符。 1234567891011121314151617181920212223242526int KmpSearch(char* s, char* p)&#123; int i = 0; int j = 0; int sLen = strlen(s); int pLen = strlen(p); while (i &lt; sLen &amp;&amp; j &lt; pLen) &#123; //①如果j = -1，或者当前字符匹配成功（即S[i] == P[j]），都令i++，j++ if (j == -1 || s[i] == p[j]) &#123; i++; j++; &#125; else &#123; //②如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j] //next[j]即为j所对应的next值 j = next[j]; &#125; &#125; if (j == pLen) return i - j; else return -1;&#125; 当S[10]跟P[6]匹配失败时，KMP不是跟暴力匹配那样简单的把模式串右移一位，而是执行第②条指令：“如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j]”，即j 从6变到2（后面我们将求得P[6]，即字符D对应的next 值为2），所以相当于模式串向右移动的位数为j - next[j]（j - next[j] = 6-2 = 4）。 向右移动4位后，S[10]跟P[2]继续匹配。为什么要向右移动4位呢，因为移动4位后，模式串中又有个“AB”可以继续跟S[8]S[9]对应着，从而不用让i 回溯。相当于在除去字符D的模式串子串中寻找相同的前缀和后缀，然后根据前缀后缀求出next 数组，最后基于next 数组进行匹配。 步骤 寻找前缀后缀最长公共元素长度对于P = p0 p1 ...pj-1 pj，寻找模式串P中长度最大且相等的前缀和后缀。如果存在p0 p1 ...pk-1 pk = pj- k pj-k+1...pj-1 pj，那么在包含pj的模式串中有最大长度为k+1的相同前缀后缀。举个例子，如果给定的模式串为“abab”，那么它的各个子串的前缀后缀的公共元素的最大长度如下表格所示： 比如对于字符串aba来说，它有长度为1的相同前缀后缀a；而对于字符串abab来说，它有长度为2的相同前缀后缀ab（相同前缀后缀的长度为k + 1，k + 1 = 2）。 求next数组next 数组考虑的是除当前字符外的最长相同前缀后缀，所以通过第①步骤求得各个前缀后缀的公共元素的最大长度后，只要稍作变形即可：将第①步骤中求得的值整体右移一位，然后初值赋为-1，如下表格所示： 比如对于aba来说，第3个字符a之前的字符串ab中有长度为0的相同前缀后缀，所以第3个字符a对应的next值为0；而对于abab来说，第4个字符b之前的字符串aba中有长度为1的相同前缀后缀a，所以第4个字符b对应的next值为1（相同前缀后缀的长度为k，k = 1）。 根据next数组进行匹配匹配失配，j = next [j]，模式串向右移动的位数为：j - next[j]。换言之，当模式串的后缀pj-k pj-k+1, ..., pj-1 跟文本串si-k si-k+1, ..., si-1匹配成功，但pj 跟si匹配失败时，因为next[j] = k，相当于在不包含pj的模式串中有最大长度为k 的相同前缀后缀，即p0 p1 ...pk-1 = pj-k pj-k+1...pj-1，故令j = next[j]，从而让模式串右移j - next[j] 位，使得模式串的前缀p0 p1, ..., pk-1对应着文本串 si-k si-k+1, ..., si-1，而后让pk 跟si 继续匹配。如下图所示： 综上，KMP的next 数组相当于告诉我们：当模式串中的某个字符跟文本串中的某个字符匹配失配时，模式串下一步应该跳到哪个位置。如模式串中在j 处的字符跟文本串在i 处的字符匹配失配时，下一步用next [j] 处的字符继续跟文本串i 处的字符匹配，相当于模式串向右移动j - next[j] 位。 接下来，分别具体解释上述3个步骤。 解释寻找最长前缀后缀 如果给定的模式串是：ABCDABD，从左至右遍历整个模式串，其各个子串的前缀后缀分别如下表格所示： 原模式串子串对应的各个前缀后缀的公共元素的最大长度表为（简称《最大长度表》）： 基于《最大长度表》匹配 因为模式串中首尾可能会有重复的字符，故可得出下述结论： 失配时，模式串向右移动的位数为：已匹配字符数 - 失配字符的上一位字符所对应的最大长度值 下面，咱们就结合之前的《最大长度表》和上述结论，进行字符串的匹配。如果给定文本串BBC ABCDAB ABCDABCDABDE，和模式串ABCDABD，现在要拿模式串去跟文本串匹配，如下图所示： 因为模式串中的字符A跟文本串中的字符B、B、C、空格一开始就不匹配，所以不必考虑结论，直接将模式串不断的右移一位即可，直到模式串中的字符A跟文本串的第5个字符A匹配成功： 继续往后匹配，当模式串最后一个字符D跟文本串匹配时失配，显而易见，模式串需要向右移动。但向右移动多少位呢？因为此时已经匹配的字符数为6个（ABCDAB），然后根据《最大长度表》可得失配字符D的上一位字符B对应的长度值为2，所以根据之前的结论，可知需要向右移动6 - 2 = 4 位。 模式串向右移动4位后，发现C处再度失配，因为此时已经匹配了2个字符（AB），且上一位字符B对应的最大长度值为0，所以向右移动：2 - 0 =2 位。 A与空格失配，向右移动1 位。 继续比较，发现D与C 失配，故向右移动的位数为：已匹配的字符数6减去上一位字符B对应的最大长度2，即向右移动6 - 2 = 4 位。 经历第5步后，发现匹配成功，过程结束。 通过上述匹配过程可以看出，问题的关键就是寻找模式串中最大长度的相同前缀和后缀，找到了模式串中每个字符之前的前缀和后缀公共部分的最大长度后，便可基于此匹配。而这个最大长度便正是next 数组要表达的含义。 根据《最大长度表》求next 数组由上文，我们已经知道，字符串“ABCDABD”各个前缀后缀的最大公共元素长度分别为： 而且，根据这个表可以得出下述结论 失配时，模式串向右移动的位数为：已匹配字符数 - 失配字符的上一位字符所对应的最大长度值 上文利用这个表和结论进行匹配时，我们发现，当匹配到一个字符失配时，其实没必要考虑当前失配的字符，更何况我们每次失配时，都是看的失配字符的上一位字符对应的最大长度值。如此，便引出了next 数组。 给定字符串“ABCDABD”，可求得它的next 数组如下： 把next 数组跟之前求得的最大长度表对比后，不难发现，next 数组相当于“最大长度值” 整体向右移动一位，然后初始值赋为-1。意识到了这一点，你会惊呼原来next 数组的求解竟然如此简单：就是找最大对称长度的前缀后缀，然后整体右移一位，初值赋为-1（当然，你也可以直接计算某个字符对应的next值，就是看这个字符之前的字符串中有多大长度的相同前缀后缀）。 换言之，对于给定的模式串：ABCDABD，它的最大长度表及next 数组分别如下： 根据最大长度表求出了next 数组后，从而有 失配时，模式串向右移动的位数为：失配字符所在位置 - 失配字符对应的next 值 而后，你会发现，无论是基于《最大长度表》的匹配，还是基于next 数组的匹配，两者得出来的向右移动的位数是一样的。为什么呢？因为： 根据《最大长度表》，失配时，模式串向右移动的位数 = 已经匹配的字符数 - 失配字符的上一位字符的最大长度值 而根据《next 数组》，失配时，模式串向右移动的位数 = 失配字符的位置 - 失配字符对应的next 值 其中，从0开始计数时，失配字符的位置 = 已经匹配的字符数（失配字符不计数），而失配字符对应的next 值 = 失配字符的上一位字符的最大长度值，两相比较，结果必然完全一致。 所以，你可以把《最大长度表》看做是next 数组的雏形，甚至就把它当做next 数组也是可以的，区别不过是怎么用的问题。 通过代码递推计算next 数组接下来，咱们来写代码求下next 数组。基于之前的理解，可知计算next 数组的方法可以采用递推： 如果对于值k，已有p0 p1, ..., pk-1 = pj-k pj-k+1, ..., pj-1，相当于next[j] = k。此意味着什么呢？究其本质，next[j] = k 代表p[j] 之前的模式串子串中，有长度为k 的相同前缀和后缀。有了这个next 数组，在KMP匹配中，当模式串中j 处的字符失配时，下一步用next[j]处的字符继续跟文本串匹配，相当于模式串向右移动j - next[j] 位。 举个例子，如下图，根据模式串“ABCDABD”的next 数组可知失配位置的字符D对应的next 值为2，代表字符D前有长度为2的相同前缀和后缀（这个相同的前缀后缀即为“AB”），失配后，模式串需要向右移动j - next [j] = 6 - 2 =4位。 向右移动4位后，模式串中的字符C继续跟文本串匹配。 下面的问题是：已知next [0, …, j]，如何求出next [j + 1]呢？ 对于P的前j+1个序列字符： 若p[k] == p[j]，则next[j + 1 ] = next [j] + 1 = k + 1； 若p[k ] ≠ p[j]，如果此时p[ next[k] ] == p[j ]，则next[ j + 1 ] = next[k] + 1，否则继续递归前缀索引k = next[k]，而后重复此过程。 相当于在字符p[j+1]之前不存在长度为k+1的前缀”p0 p1, …, pk-1 pk“跟后缀“pj-k pj-k+1, …, pj-1 pj“相等，那么是否可能存在另一个值t+1 &lt; k+1，使得长度更小的前缀 p0 p1, …, pt-1 pt 等于长度更小的后缀 “pj-t pj-t+1, …, pj-1 pj” 呢？如果存在，那么这个t+1 便是next[ j+1]的值，此相当于利用已经求得的next 数组（next [0, ..., k, ..., j]）进行P串前缀跟P串后缀的匹配。 一般的文章或教材可能就此一笔带过，但大部分的初学者可能还是不能很好的理解上述求解next 数组的原理，故接下来，我再来着重说明下。 如下图所示，假定给定模式串ABCDABCE，且已知next [j] = k（相当于“p0 pk-1” = “pj-k pj-1” = AB，可以看出k为2），现要求next [j + 1]等于多少？因为pk = pj = C，所以next[j + 1] = next[j] + 1 = k + 1（可以看出next[j + 1] = 3）。代表字符E前的模式串中，有长度k+1 的相同前缀后缀。 但如果pk != pj 呢？说明p0 pk-1 pk ≠ pj-k pj-1 pj。换言之，当pk != pj后，字符E前有多大长度的相同前缀后缀呢？很明显，因为C不同于D，所以ABC 跟 ABD不相同，即字符E前的模式串没有长度为k+1的相同前缀后缀，也就不能再简单的令：next[j + 1] = next[j] + 1 。所以，咱们只能去寻找长度更短一点的相同前缀后缀。 结合上图来讲，若能在前缀p0 pk-1 pk 中不断的递归前缀索引k = next [k]，找到一个字符pk’ 也为D，代表pk’ = pj，且满足p0 pk&#39;-1 pk&#39; = pj-k&#39; pj-1 pj，则最大相同的前缀后缀长度为k&#39; + 1，从而next [j + 1] = k’ + 1 = next [k&#39; ] + 1。否则前缀中没有D，则代表没有相同的前缀后缀，next [j + 1] = 0。 那为何递归前缀索引k = next[k]，就能找到长度更短的相同前缀后缀呢？这又归根到next数组的含义。我们拿前缀 p0 pk-1 pk 去跟后缀pj-k pj-1 pj匹配，如果pk 跟pj 失配，下一步就是用p[next[k]] 去跟pj 继续匹配，如果p[ next[k] ]跟pj还是不匹配，则需要寻找长度更短的相同前缀后缀，即下一步用p[ next[ next[k] ] ]去跟pj匹配。此过程相当于模式串的自我匹配，所以不断的递归k = next[k]，直到要么找到长度更短的相同前缀后缀，要么没有长度更短的相同前缀后缀。如下图所示： 引用下一读者wudehua55555于本文评论下留言，以辅助大家从另一个角度理解：“ 一直以为博主在用递归求next数组时没讲清楚，为何要用k = next[k],仔细看了这个红黄蓝分区图才突然恍然大悟，就是找到p[k]对应的next[k]，根据对称性，只需再判断p[next[k]]与p[j]是否相等即可，于是令k = next[k],这里恰好就使用了递归的思路。其实我觉得不要一开始就陷入递归的方法中，换一种思路，直接从考虑对称性入手，可直接得出k = next[k]，而这正好是递归罢了。以上是一些个人看法，非常感谢博主提供的解析，非计算机的学生也能看懂，虽然从昨晚9点看到了现在。高兴。” 所以，因最终在前缀ABC中没有找到D，故E的next 值为0： 模式串的后缀：ABDE模式串的前缀：ABC前缀右移两位：ABC 读到此，有的读者可能又有疑问了，那能否举一个能在前缀中找到字符D的例子呢？OK，咱们便来看一个能在前缀中找到字符D的例子，如下图所示： 给定模式串DABCDABDE，我们很顺利的求得字符D之前的“DABCDAB”的各个子串的最长相同前缀后缀的长度分别为0 0 0 0 1 2 3，但当遍历到字符D，要求包括D在内的“DABCDABD”最长相同前缀后缀时，我们发现pj处的字符D跟pk处的字符C不一样，换言之，前缀DABC的最后一个字符C 跟后缀DABD的最后一个字符D不相同，所以不存在长度为4的相同前缀后缀。 怎么办呢？既然没有长度为4的相同前缀后缀，咱们可以寻找长度短点的相同前缀后缀，最终，因在p0处发现也有个字符D，p0 = pj，所以p[j]对应的长度值为1，相当于E对应的next 值为1（即字符E之前的字符串“DABCDABD”中有长度为1的相同前缀和后缀）。 综上，可以通过递推求得next 数组，代码如下所示： 123456789101112131415161718192021void GetNext(char* p,int next[])&#123; int pLen = strlen(p); next[0] = -1; int k = -1; int j = 0; while (j &lt; pLen - 1) &#123; //p[k]表示前缀，p[j]表示后缀 if (k == -1 || p[j] == p[k]) &#123; ++k; ++j; next[j] = k; &#125; else &#123; k = next[k]; &#125; &#125;&#125; 用代码重新计算下“ABCDABD”的next 数组，以验证之前通过“最长相同前缀后缀长度值右移一位，然后初值赋为-1”得到的next 数组是否正确，计算结果如下表格所示： 从上述表格可以看出，无论是之前通过“最长相同前缀后缀长度值右移一位，然后初值赋为-1”得到的next 数组，还是之后通过代码递推计算求得的next 数组，结果是完全一致的。 基于《next 数组》匹配下面，我们来基于next 数组进行匹配。 还是给定文本串“BBC ABCDAB ABCDABCDABDE”，和模式串“ABCDABD”，现在要拿模式串去跟文本串匹配，如下图所示： 在正式匹配之前，让我们来再次回顾下上文2.1节所述的KMP算法的匹配流程： “假设现在文本串S匹配到 i 位置，模式串P匹配到 j 位置 如果j = -1，或者当前字符匹配成功（即S[i] == P[j]），都令i++，j++，继续匹配下一个字符； 如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j]。此举意味着失配时，模式串P相对于文本串S向右移动了j - next [j] 位。换言之，当匹配失败时，模式串向右移动的位数为：失配字符所在位置 - 失配字符对应的next 值，即移动的实际位数为：j - next[j]，且此值大于等于1。” 最开始匹配时 P[0]跟S[0]匹配失败 所以执行“如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j]”，所以j = -1，故转而执行“如果j = -1，或者当前字符匹配成功（即S[i] == P[j]），都令i++，j++”，得到i = 1，j = 0，即P[0]继续跟S[1]匹配。 P[0]跟S[1]又失配，j再次等于-1，i、j继续自增，从而P[0]跟S[2]匹配。 P[0]跟S[2]失配后，P[0]又跟S[3]匹配。 P[0]跟S[3]再失配，直到P[0]跟S[4]匹配成功，开始执行此条指令的后半段：“如果j = -1，或者当前字符匹配成功（即S[i] == P[j]），都令i++，j++”。 P[1]跟S[5]匹配成功，P[2]跟S[6]也匹配成功, …，直到当匹配到P[6]处的字符D时失配（即S[10] != P[6]），由于P[6]处的D对应的next 值为2，所以下一步用P[2]处的字符C继续跟S[10]匹配，相当于向右移动：j - next[j] = 6 - 2 =4 位。 向右移动4位后，P[2]处的C再次失配，由于C对应的next值为0，所以下一步用P[0]处的字符继续跟S[10]匹配，相当于向右移动：j - next[j] = 2 - 0 = 2 位。 移动两位之后，A 跟空格不匹配，模式串后移1 位。 P[6]处的D再次失配，因为P[6]对应的next值为2，故下一步用P[2]继续跟文本串匹配，相当于模式串向右移动 j - next[j] = 6 - 2 = 4 位。 匹配成功，过程结束。 匹配过程一模一样。也从侧面佐证了，next 数组确实是只要将各个最大前缀后缀的公共元素的长度值右移一位，且把初值赋为-1 即可。 基于《最大长度表》与基于《next 数组》等价我们已经知道，利用next 数组进行匹配失配时，模式串向右移动 j - next [ j ] 位，等价于已匹配字符数 - 失配字符的上一位字符所对应的最大长度值。原因是： j 从0开始计数，那么当数到失配字符时，j 的数值就是已匹配的字符数； 由于next 数组是由最大长度值表整体向右移动一位（且初值赋为-1）得到的，那么失配字符的上一位字符所对应的最大长度值，即为当前失配字符的next 值。 但为何本文不直接利用next 数组进行匹配呢？因为next 数组不好求，而一个字符串的前缀后缀的公共元素的最大长度值很容易求。例如若给定模式串“ababa”，要你快速口算出其next 数组，乍一看，每次求对应字符的next值时，还得把该字符排除之外，然后看该字符之前的字符串中有最大长度为多大的相同前缀后缀，此过程不够直接。而如果让你求其前缀后缀公共元素的最大长度，则很容易直接得出结果：0 0 1 2 3，如下表格所示： 然后这5个数字 全部整体右移一位，且初值赋为-1，即得到其next 数组：-1 0 0 1 2。 Next 数组与有限状态自动机next 负责把模式串向前移动，且当第j位不匹配的时候，用第next[j]位和主串匹配，就像打了张“表”。此外，next 也可以看作有限状态自动机的状态，在已经读了多少字符的情况下，失配后，前面读的若干个字符是有用的。]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群/分布式环境下5种session处理策略]]></title>
    <url>%2F%E9%9B%86%E7%BE%A4%E5%88%86%E5%B8%83%E5%BC%8Fsession%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[原文 在搭建完集群环境后，不得不考虑的一个问题就是用户访问产生的session如何处理。如果不做任何处理的话，用户将出现频繁登录的现象，比如集群中存在A、B两台服务器，用户在第一次访问网站时，Nginx通过其负载均衡机制将用户请求转发到A服务器，这时A服务器就会给用户创建一个Session。当用户第二次发送请求时，Nginx将其负载均衡到B服务器，而这时候B服务器并不存在Session，所以就会将用户踢到登录页面。这将大大降低用户体验度，导致用户的流失，这种情况是项目绝不应该出现的。 我们应当对产生的Session进行处理，通过粘性Session，Session复制或Session共享等方式保证用户的体验度。 以下我将说明5种Session处理策略，并分析其优劣性。 粘性session 原理：粘性Session是指将用户锁定到某一个服务器上，比如上面说的例子，用户第一次请求时，负载均衡器将用户的请求转发到了A服务器上，如果负载均衡器设置了粘性Session的话，那么用户以后的每次请求都会转发到A服务器上，相当于把用户和A服务器粘到了一块，这就是粘性Session机制。 优点：简单，不需要对session做任何处理。 缺点：缺乏容错性，如果当前访问的服务器发生故障，用户被转移到第二个服务器上时，他的session信息都将失效。 适用场景：发生故障对客户产生的影响较小；服务器发生故障是低概率事件。 实现方式：以Nginx为例，在upstream模块配置ip_hash属性即可实现粘性Session。 服务器session复制 原理：任何一个服务器上的session发生改变（增删改），该节点会把这个 session的所有内容序列化，然后广播给所有其它节点，不管其他服务器需不需要session，以此来保证Session同步。 优点：可容错，各个服务器间session能够实时响应。 缺点：会对网络负荷造成一定压力，如果session量大的话可能会造成网络堵塞，拖慢服务器性能。 实现方式： 设置tomcat ，server.xml 开启tomcat集群功能 Address:填写本机ip即可，设置端口号，预防端口冲突。 在应用里增加信息：通知应用当前处于集群环境中，支持分布式在web.xml中添加选项 session共享机制使用分布式缓存方案比如memcached、Redis，但是要求Memcached或Redis必须是集群。 使用Session共享也分两种机制，两种情况如下： 粘性session处理方式原理：不同的 tomcat指定访问不同的主memcached。多个Memcached之间信息是同步的，能主从备份和高可用。用户访问时首先在tomcat中创建session，然后将session复制一份放到它对应的memcahed上。memcache只起备份作用，读写都在tomcat上。当某一个tomcat挂掉后，集群将用户的访问定位到备tomcat上，然后根据cookie中存储的SessionId找session，找不到时，再去相应的memcached上去session，找到之后将其复制到备tomcat上。 非粘性session处理方式原理：memcached做主从复制，写入session都往从memcached服务上写，读取都从主memcached读取，tomcat本身不存储session 优点：可容错，session实时响应。 实现方式：用开源的msm插件解决tomcat之间的session共享：Memcached_Session_Manager（MSM） 复制相关jar包到tomcat/lib 目录下 12345678JAVA memcached客户端：spymemcached.jarmsm项目相关的jar包：1\. 核心包，memcached-session-manager-&#123;version&#125;.jar2\. Tomcat版本对应的jar包：memcached-session-manager-tc&#123;tomcat-version&#125;-&#123;version&#125;.jar序列化工具包：可选kryo，javolution,xstream等，不设置时使用jdk默认序列化。 配置Context.xml ，加入处理Session的Manager 粘性模式配置： session持久化到数据库 优点：服务器出现问题，session不会丢失 缺点：如果网站的访问量很大，把session存储到数据库中，会对数据库造成很大压力，还需要增加额外的开销维护数据库。 terracotta实现session复制 原理：Terracotta的基本原理是对于集群间共享的数据，当在一个节点发生变化的时候，Terracotta只把变化的部分发送给Terracotta服务器，然后由服务器把它转发给真正需要这个数据的节点。可以看成是对第二种方案的优化。 优点：这样对网络的压力就非常小，各个节点也不必浪费CPU时间和内存进行大量的序列化操作。把这种集群间数据共享的机制应用在session同步上，既避免了对数据库的依赖，又能达到负载均衡和灾难恢复的效果。 小结以上讲述的就是集群或分布式环境下，session的5种处理策略。其中就应用广泛性而言，第三种方式，也就是基于第三方缓存框架共享session，应用的最为广泛，无论是效率还是扩展性都很好。而Terracotta作为一个JVM级的开源群集框架，不仅提供HTTP Session复制，它还能做分布式缓存，POJO群集，跨越群集的JVM来实现分布式应用程序协调等，也值得学习一下。]]></content>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git入门]]></title>
    <url>%2FGit%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Git简介 Git 其他版本管理系统的主要差别：对待数据的方式。Git采用的是直接记录快照的方式，而非差异比较。 大部分版本控制系统（CVS、Subversion、Perforce、Bazaar 等等）都是以文件变更列表的方式存储信息，这类系统将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。具体原理如下图所示，理解起来其实很简单，每个我们对提交更新一个文件之后，系统记录都会记录这个文件做了哪些更新，以增量符号Δ(Delta)表示。下图来源于Git官网。 我们怎样才能得到一个文件的最终版本呢？ 很简单，高中数学的基本知识，我们只需要将这些原文件和这些增加进行相加就行了。 这种方式有什么问题呢？ 比如我们的增量特别特别多的话，如果我们要得到最终的文件是不是会耗费时间和性能。 Git 不按照以上方式对待或保存数据。 反之，Git 更像是把数据看作是对小型文件系统的一组快照。 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。 Git 对待数据更像是一个 快照流。下图来源于Git官网。 Git 的三种状态 Git 有三种状态，你的文件可能处于其中之一： •已提交（committed）：数据已经安全的保存在本地数据库中。•已修改（modified）：已修改表示修改了文件，但还没保存到数据库中。•已暂存（staged）：表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 由此引入 Git 项目的三个工作区域的概念：Git 仓库(.git directoty) 、工作目录(Working Directory) 以及 暂存区域(Staging Area) 。下图来源于Git官网。 基本的 Git 工作流程如下： 在工作目录中修改文件。 暂存文件，将文件的快照放入暂存区域。 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。 Git 使用快速入门获取 Git 仓库有两种取得 Git 项目仓库的方法。 •在现有目录中初始化仓库: 进入项目目录运行 git init 命令,该命令将创建一个名为 .git 的子目录。•从一个服务器克隆一个现有的 Git 仓库: git clone [url] 自定义本地仓库的名字: git clone [url] directoryname 记录每次更新到仓库•检测当前文件状态 : git status•提出更改（把它们添加到暂存区）：git add filename (针对特定文件)、git add *(所有文件)、git add *.txt（支持通配符，所有 .txt 文件）•忽略文件：.gitignore 文件•提交更新: git commit -m &quot;代码提交信息&quot; （每次准备提交前，先用 git status 看下，是不是都已暂存起来了， 然后再运行提交命令 git commit）•跳过使用暂存区域更新的方式 : git commit -a -m &quot;代码提交信息&quot;。 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。•移除文件 ：git rm filename （从暂存区域移除，然后提交。）•对文件重命名 ：git mv README.md README(这个命令相当于mv README.md README、git rm README.md、git add README 这三条命令的集合) 推送改动到远程仓库 如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：·git remote add origin &lt;server&gt; ,比如我们要让本地的一个仓库和 Github 上创建的一个仓库关联可以这样git remote add origin https://github.com/Snailclimb/test.git将 将这些改动提交到远端仓库：git push origin master (可以把 master 换成你想要推送的任何分支) 如此你就能够将你的改动推送到所添加的服务器上去了。 远程仓库的移除与重命名•将 test 重命名位 test1：git remote rename test test1•移除远程仓库 test1:git remote rm test1 查看提交历史在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。git log 会按提交时间列出所有的更新，最近的更新排在最上面。 可以添加一些参数来查看自己希望看到的内容： 只看某个人的提交记录： 1git log --author=bob 撤销操作有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。 此时，可以运行带有 --amend 选项的提交命令尝试重新提交： 1git commit --amend 取消暂存的文件 1git reset filename 撤消对文件的修改: 1git checkout -- filename 假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它： 1git fetch origin 分支分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认的”分支。在其他分支上进行开发，完成后再将它们合并到主分支上。 我们通常在开发新功能、修复一个紧急 bug 等等时候会选择创建分支。单分支开发好还是多分支开发好，还是要看具体场景来说。 创建一个名字叫做 test 的分支 1git branch test 切换当前分支到 test（当你切换分支的时候，Git 会重置你的工作目录，使其看起来像回到了你在那个分支上最后一次提交的样子。 Git 会自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样） 1git checkout test 你也可以直接这样创建分支并切换过去(上面两条命令的合写) 1git checkout -b feature_x 切换到主分支 1git checkout master 合并分支(可能会有冲突) 1git merge test 把新建的分支删掉 1git branch -d feature_x 将分支推送到远端仓库（推送成功后其他人可见）： 1git push origin]]></content>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[基本特征 1. 并发 2. 共享 3. 虚拟 4. 异步 基本功能 1. 进程管理 2. 内存管理 3. 文件管理 4. 设备管理 系统调用 大内核和微内核 1. 大内核 2. 微内核 中断分类 1. 外中断 2. 异常 3. 陷入 概述基本特征1. 并发并发是指宏观上在一段时间内能同时运行多个程序，而并行则指同一时刻能运行多个指令。 并行需要硬件支持，如多流水线、多核处理器或者分布式计算系统。 操作系统通过引入进程和线程，使得程序能够并发运行。 2. 共享共享是指系统中的资源可以被多个并发进程共同使用。 有两种共享方式：互斥共享和同时共享。 互斥共享的资源称为临界资源，例如打印机等，在同一时间只允许一个进程访问，需要用同步机制来实现对临界资源的访问。 3. 虚拟虚拟技术把一个物理实体转换为多个逻辑实体。 主要有两种虚拟技术：时分复用技术和空分复用技术。 多个进程能在同一个处理器上并发执行使用了时分复用技术，让每个进程轮流占有处理器，每次只执行一小个时间片并快速切换。 虚拟内存使用了空分复用技术，它将物理内存抽象为地址空间，每个进程都有各自的地址空间。地址空间的页被映射到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法，将该页置换到内存中。 4. 异步异步指进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进。 基本功能1. 进程管理进程控制、进程同步、进程通信、死锁处理、处理机调度等。 2. 内存管理内存分配、地址映射、内存保护与共享、虚拟内存等。 3. 文件管理文件存储空间的管理、目录管理、文件读写管理和保护等。 4. 设备管理完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率。 主要包括缓冲管理、设备分配、设备处理、虛拟设备等。 系统调用如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而陷入内核，由操作系统代为完成。 Linux 的系统调用主要有以下这些： Task Commands 进程控制 fork(); exit(); wait(); 进程通信 pipe(); shmget(); mmap(); 文件操作 open(); read(); write(); 设备操作 ioctl(); read(); write(); 信息维护 getpid(); alarm(); sleep(); 安全 chmod(); umask(); chown(); 大内核和微内核1. 大内核大内核是将操作系统功能作为一个紧密结合的整体放到内核。 由于各模块共享信息，因此有很高的性能。 2. 微内核由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。 在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在用户态。 因为需要频繁地在用户态和核心态之间进行切换，所以会有一定的性能损失。 中断分类1. 外中断由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等。 2. 异常由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。 进程管理进程与线程1. 进程进程是资源分配的基本单位。 进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。 2. 线程线程是独立调度的基本单位。 一个进程中可以有多个线程，它们共享进程资源。 QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。 3. 区别Ⅰ 拥有资源 进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。 Ⅱ 调度 线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。 Ⅲ 系统开销 由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。 Ⅳ 通信方面 线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC。 进程状态的切换 就绪状态（ready）：等待被调度 运行状态（running） 阻塞状态（waiting）：等待资源 应该注意以下内容： 只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。 阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。 进程调度算法不同环境的调度算法目标不同，因此需要针对不同环境来讨论调度算法。 1. 批处理系统批处理系统没有太多的用户操作，在该系统中，调度算法目标是保证吞吐量和周转时间（从提交到终止的时间）。 1.1 先来先服务 first-come first-serverd（FCFS） 按照请求的顺序进行调度。 有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。 1.2 短作业优先 shortest job first（SJF） 按估计运行时间最短的顺序进行调度。 长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。 1.3 最短剩余时间优先 shortest remaining time next（SRTN） 按估计剩余时间最短的顺序进行调度。 2. 交互式系统交互式系统有大量的用户交互操作，在该系统中调度算法的目标是快速地进行响应。 2.1 时间片轮转 将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 CPU 时间分配给队首的进程。 时间片轮转算法的效率和时间片的大小有很大关系： 因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进程切换上就会花过多时间。 而如果时间片过长，那么实时性就不能得到保证。 2.2 优先级调度 为每个进程分配一个优先级，按优先级进行调度。 为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。 2.3 多级反馈队列 一个进程需要执行 100 个时间片，如果采用时间片轮转调度算法，那么需要交换 100 次。 多级队列是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列时间片大小都不同，例如 1,2,4,8,..。进程在第一个队列没执行完，就会被移到下一个队列。这种方式下，之前的进程只需要交换 7 次。 每个队列优先权也不同，最上面的优先权最高。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。 可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。 3. 实时系统实时系统要求一个请求在一个确定时间内得到响应。 分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时。 进程同步1. 临界区对临界资源进行访问的那段代码称为临界区。 为了互斥访问临界资源，每个进程在进入临界区之前，需要先进行检查。 123// entry section// critical section;// exit section 2. 同步与互斥 同步：多个进程按一定顺序执行； 互斥：多个进程在同一时刻只有一个进程能进入临界区。 3. 信号量信号量（Semaphore）是一个整型变量，可以对其执行 down 和 up 操作，也就是常见的 P 和 V 操作。 down : 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0； up ：对信号量执行 +1 操作，唤醒睡眠的进程让其完成 down 操作。 down 和 up 操作需要被设计成原语，不可分割，通常的做法是在执行这些操作的时候屏蔽中断。 如果信号量的取值只能为 0 或者 1，那么就成为了 互斥量（Mutex） ，0 表示临界区已经加锁，1 表示临界区解锁。 12345678910111213typedef int semaphore;semaphore mutex = 1;void P1() &#123; down(&amp;mutex); // 临界区 up(&amp;mutex);&#125;void P2() &#123; down(&amp;mutex); // 临界区 up(&amp;mutex);&#125; 使用信号量实现生产者-消费者问题 问题描述：使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才可以拿走物品。 因为缓冲区属于临界资源，因此需要使用一个互斥量 mutex 来控制对缓冲区的互斥访问。 为了同步生产者和消费者的行为，需要记录缓冲区中物品的数量。数量可以使用信号量来进行统计，这里需要使用两个信号量：empty 记录空缓冲区的数量，full 记录满缓冲区的数量。其中，empty 信号量是在生产者进程中使用，当 empty 不为 0 时，生产者才可以放入物品；full 信号量是在消费者进程中使用，当 full 信号量不为 0 时，消费者才可以取走物品。 注意，不能先对缓冲区进行加锁，再测试信号量。也就是说，不能先执行 down(mutex) 再执行 down(empty)。如果这么做了，那么可能会出现这种情况：生产者对缓冲区加锁后，执行 down(empty) 操作，发现 empty = 0，此时生产者睡眠。消费者不能进入临界区，因为生产者对缓冲区加锁了，消费者就无法执行 up(empty) 操作，empty 永远都为 0，导致生产者永远等待下，不会释放锁，消费者因此也会永远等待下去。 123456789101112131415161718192021222324252627#define N 100typedef int semaphore;semaphore mutex = 1;semaphore empty = N;semaphore full = 0;void producer() &#123; while(TRUE) &#123; int item = produce_item(); down(&amp;empty); down(&amp;mutex); insert_item(item); up(&amp;mutex); up(&amp;full); &#125;&#125;void consumer() &#123; while(TRUE) &#123; down(&amp;full); down(&amp;mutex); int item = remove_item(); consume_item(item); up(&amp;mutex); up(&amp;empty); &#125;&#125; 4. 管程使用信号量机制实现的生产者消费者问题需要客户端代码做很多控制，而管程把控制的代码独立出来，不仅不容易出错，也使得客户端代码调用更容易。 c 语言不支持管程，下面的示例代码使用了类 Pascal 语言来描述管程。示例代码的管程提供了 insert() 和 remove() 方法，客户端代码通过调用这两个方法来解决生产者-消费者问题。 1234567891011121314monitor ProducerConsumer integer i; condition c; procedure insert(); begin // ... end; procedure remove(); begin // ... end;end monitor; 管程有一个重要特性：在一个时刻只能有一个进程使用管程。进程在无法继续执行的时候不能一直占用管程，否者其它进程永远不能使用管程。 管程引入了 条件变量 以及相关的操作：wait() 和 signal() 来实现同步操作。对条件变量执行 wait() 操作会导致调用进程阻塞，把管程让出来给另一个进程持有。signal() 操作用于唤醒被阻塞的进程。 使用管程实现生产者-消费者问题 123456789101112131415161718192021222324252627282930313233343536373839404142// 管程monitor ProducerConsumer condition full, empty; integer count := 0; condition c; procedure insert(item: integer); begin if count = N then wait(full); insert_item(item); count := count + 1; if count = 1 then signal(empty); end; function remove: integer; begin if count = 0 then wait(empty); remove = remove_item; count := count - 1; if count = N -1 then signal(full); end;end monitor;// 生产者客户端procedure producerbegin while true do begin item = produce_item; ProducerConsumer.insert(item); endend;// 消费者客户端procedure consumerbegin while true do begin item = ProducerConsumer.remove; consume_item(item); endend; 经典同步问题生产者和消费者问题前面已经讨论过了。 1. 读者-写者问题允许多个进程同时对数据进行读操作，但是不允许读和写以及写和写操作同时发生。 一个整型变量 count 记录在对数据进行读操作的进程数量，一个互斥量 count_mutex 用于对 count 加锁，一个互斥量 data_mutex 用于对读写的数据加锁。 1234567891011121314151617181920212223242526typedef int semaphore;semaphore count_mutex = 1;semaphore data_mutex = 1;int count = 0;void reader() &#123; while(TRUE) &#123; down(&amp;count_mutex); count++; if(count == 1) down(&amp;data_mutex); // 第一个读者需要对数据进行加锁，防止写进程访问 up(&amp;count_mutex); read(); down(&amp;count_mutex); count--; if(count == 0) up(&amp;data_mutex); up(&amp;count_mutex); &#125;&#125;void writer() &#123; while(TRUE) &#123; down(&amp;data_mutex); write(); up(&amp;data_mutex); &#125;&#125; 以下内容由 @Bandi Yugandhar 提供。 The first case may result Writer to starve. This case favous Writers i.e no writer, once added to the queue, shall be kept waiting longer than absolutely necessary(only when there are readers that entered the queue before the writer). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546int readcount, writecount; //(initial value = 0)semaphore rmutex, wmutex, readLock, resource; //(initial value = 1)//READERvoid reader() &#123;&lt;ENTRY Section&gt; down(&amp;readLock); // reader is trying to enter down(&amp;rmutex); // lock to increase readcount readcount++; if (readcount == 1) down(&amp;resource); //if you are the first reader then lock the resource up(&amp;rmutex); //release for other readers up(&amp;readLock); //Done with trying to access the resource&lt;CRITICAL Section&gt;//reading is performed&lt;EXIT Section&gt; down(&amp;rmutex); //reserve exit section - avoids race condition with readers readcount--; //indicate you&apos;re leaving if (readcount == 0) //checks if you are last reader leaving up(&amp;resource); //if last, you must release the locked resource up(&amp;rmutex); //release exit section for other readers&#125;//WRITERvoid writer() &#123; &lt;ENTRY Section&gt; down(&amp;wmutex); //reserve entry section for writers - avoids race conditions writecount++; //report yourself as a writer entering if (writecount == 1) //checks if you&apos;re first writer down(&amp;readLock); //if you&apos;re first, then you must lock the readers out. Prevent them from trying to enter CS up(&amp;wmutex); //release entry section&lt;CRITICAL Section&gt; down(&amp;resource); //reserve the resource for yourself - prevents other writers from simultaneously editing the shared resource //writing is performed up(&amp;resource); //release file&lt;EXIT Section&gt; down(&amp;wmutex); //reserve exit section writecount--; //indicate you&apos;re leaving if (writecount == 0) //checks if you&apos;re the last writer up(&amp;readLock); //if you&apos;re last writer, you must unlock the readers. Allows them to try enter CS for reading up(&amp;wmutex); //release exit section&#125; We can observe that every reader is forced to acquire ReadLock. On the otherhand, writers doesn’t need to lock individually. Once the first writer locks the ReadLock, it will be released only when there is no writer left in the queue. From the both cases we observed that either reader or writer has to starve. Below solutionadds the constraint that no thread shall be allowed to starve; that is, the operation of obtaining a lock on the shared data will always terminate in a bounded amount of time. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748int readCount; // init to 0; number of readers currently accessing resource// all semaphores initialised to 1Semaphore resourceAccess; // controls access (read/write) to the resourceSemaphore readCountAccess; // for syncing changes to shared variable readCountSemaphore serviceQueue; // FAIRNESS: preserves ordering of requests (signaling must be FIFO)void writer()&#123; down(&amp;serviceQueue); // wait in line to be servicexs // &lt;ENTER&gt; down(&amp;resourceAccess); // request exclusive access to resource // &lt;/ENTER&gt; up(&amp;serviceQueue); // let next in line be serviced // &lt;WRITE&gt; writeResource(); // writing is performed // &lt;/WRITE&gt; // &lt;EXIT&gt; up(&amp;resourceAccess); // release resource access for next reader/writer // &lt;/EXIT&gt;&#125;void reader()&#123; down(&amp;serviceQueue); // wait in line to be serviced down(&amp;readCountAccess); // request exclusive access to readCount // &lt;ENTER&gt; if (readCount == 0) // if there are no readers already reading: down(&amp;resourceAccess); // request resource access for readers (writers blocked) readCount++; // update count of active readers // &lt;/ENTER&gt; up(&amp;serviceQueue); // let next in line be serviced up(&amp;readCountAccess); // release access to readCount // &lt;READ&gt; readResource(); // reading is performed // &lt;/READ&gt; down(&amp;readCountAccess); // request exclusive access to readCount // &lt;EXIT&gt; readCount--; // update count of active readers if (readCount == 0) // if there are no readers left: up(&amp;resourceAccess); // release resource access for all // &lt;/EXIT&gt; up(&amp;readCountAccess); // release access to readCount&#125; 2. 哲学家进餐问题五个哲学家围着一张圆桌，每个哲学家面前放着食物。哲学家的生活有两种交替活动：吃饭以及思考。当一个哲学家吃饭时，需要先拿起自己左右两边的两根筷子，并且一次只能拿起一根筷子。 下面是一种错误的解法，考虑到如果所有哲学家同时拿起左手边的筷子，那么就无法拿起右手边的筷子，造成死锁。 123456789101112#define N 5void philosopher(int i) &#123; while(TRUE) &#123; think(); take(i); // 拿起左边的筷子 take((i+1)%N); // 拿起右边的筷子 eat(); put(i); put((i+1)%N); &#125;&#125; 为了防止死锁的发生，可以设置两个条件： 必须同时拿起左右两根筷子； 只有在两个邻居都没有进餐的情况下才允许进餐。 123456789101112131415161718192021222324252627282930313233343536373839404142#define N 5#define LEFT (i + N - 1) % N // 左邻居#define RIGHT (i + 1) % N // 右邻居#define THINKING 0#define HUNGRY 1#define EATING 2typedef int semaphore;int state[N]; // 跟踪每个哲学家的状态semaphore mutex = 1; // 临界区的互斥semaphore s[N]; // 每个哲学家一个信号量void philosopher(int i) &#123; while(TRUE) &#123; think(); take_two(i); eat(); put_two(i); &#125;&#125;void take_two(int i) &#123; down(&amp;mutex); state[i] = HUNGRY; test(i); up(&amp;mutex); down(&amp;s[i]);&#125;void put_two(i) &#123; down(&amp;mutex); state[i] = THINKING; test(LEFT); test(RIGHT); up(&amp;mutex);&#125;void test(i) &#123; // 尝试拿起两把筷子 if(state[i] == HUNGRY &amp;&amp; state[LEFT] != EATING &amp;&amp; state[RIGHT] !=EATING) &#123; state[i] = EATING; up(&amp;s[i]); &#125;&#125; 进程通信进程同步与进程通信很容易混淆，它们的区别在于： 进程同步：控制多个进程按一定顺序执行； 进程通信：进程间传输信息。 进程通信是一种手段，而进程同步是一种目的。也可以说，为了能够达到进程同步的目的，需要让进程进行通信，传输一些进程同步所需要的信息。 1. 管道管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。 12#include &lt;unistd.h&gt;int pipe(int fd[2]); 它具有以下限制： 只支持半双工通信（单向交替传输）； 只能在父子进程中使用。 2. FIFO也称为命名管道，去除了管道只能在父子进程中使用的限制。 123#include &lt;sys/stat.h&gt;int mkfifo(const char *path, mode_t mode);int mkfifoat(int fd, const char *path, mode_t mode); FIFO 常用于客户-服务器应用程序中，FIFO 用作汇聚点，在客户进程和服务器进程之间传递数据。 3. 消息队列相比于 FIFO，消息队列具有以下优点： 消息队列可以独立于读写进程存在，从而避免了 FIFO 中同步管道的打开和关闭时可能产生的困难； 避免了 FIFO 的同步阻塞问题，不需要进程自己提供同步方法； 读进程可以根据消息类型有选择地接收消息，而不像 FIFO 那样只能默认地接收。 4. 信号量它是一个计数器，用于为多个进程提供对共享数据对象的访问。 5. 共享存储允许多个进程共享一个给定的存储区。因为数据不需要在进程之间复制，所以这是最快的一种 IPC。 需要使用信号量用来同步对共享存储的访问。 多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存。另外 XSI 共享内存不是使用文件，而是使用使用内存的匿名段。 6. 套接字与其它通信机制不同的是，它可用于不同机器间的进程通信。 死锁必要条件 互斥：每个资源要么已经分配给了一个进程，要么就是可用的。 占有和等待：已经得到了某个资源的进程可以再请求新的资源。 不可抢占：已经分配给一个进程的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。 环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源。 处理方法主要有以下四种方法： 鸵鸟策略 死锁检测与死锁恢复 死锁预防 死锁避免 鸵鸟策略把头埋在沙子里，假装根本没发生问题。 因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。 当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。 大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。 死锁检测与死锁恢复不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。 1. 每种类型一个资源的死锁检测 上图为资源分配图，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。 图 a 可以抽取出环，如图 b，它满足了环路等待条件，因此会发生死锁。 每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。 2. 每种类型多个资源的死锁检测 上图中，有三个进程四个资源，每个数据代表的含义如下： E 向量：资源总量 A 向量：资源剩余量 C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量 R 矩阵：每个进程请求的资源数量 进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。 算法总结如下： 每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。 如果没有这样一个进程，算法终止。 3. 死锁恢复 利用抢占恢复 利用回滚恢复 通过杀死进程恢复 死锁预防在程序运行之前预防发生死锁。 1. 破坏互斥条件例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。 2. 破坏占有和等待条件一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。 3. 破坏不可抢占条件4. 破坏环路等待给资源统一编号，进程只能按编号顺序来请求资源。 死锁避免在程序运行时避免发生死锁。 1. 安全状态 图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。 定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。 安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类似，可以结合着做参考对比。 2. 单个资源的银行家算法一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。 上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。 3. 多个资源的银行家算法 上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。 检查一个状态是否安全的算法如下： 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。 重复以上两步，直到所有进程都标记为终止，则状态时安全的。 如果一个状态不是安全的，需要拒绝进入这个状态。 内存管理虚拟内存虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。 为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。 从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。 分页系统地址映射内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。 一个虚拟地址分成两个部分，一部分存储页面号，一部分存储偏移量。 下图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。例如对于虚拟地址（0010 000000000100），前 4 位是存储页面号 2，读取表项内容为（110 1），页表项最后一位表示是否存在于内存中，1 表示存在。后 12 位存储偏移量。这个页对应的页框的地址为 （110 000000000100）。 页面置换算法在程序运行过程中，如果要访问的页面不在内存中，就发生缺页中断从而将该页调入内存中。此时如果内存已无空闲空间，系统必须从内存中调出一个页面到磁盘对换区中来腾出空间。 页面置换算法和缓存淘汰策略类似，可以将内存看成磁盘的缓存。在缓存系统中，缓存的大小有限，当有新的缓存到达时，需要淘汰一部分已经存在的缓存，这样才有空间存放新的缓存数据。 页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。 1. 最佳 OPT, Optimal replacement algorithm 所选择的被换出的页面将是最长时间内不再被访问，通常可以保证获得最低的缺页率。 是一种理论上的算法，因为无法知道一个页面多长时间不再被访问。 举例：一个系统为某进程分配了三个物理块，并有如下页面引用序列： 7，0，1，2，0，3，0，4，2，3，0，3，2，1，2，0，1，7，0，1 开始运行时，先将 7, 0, 1 三个页面装入内存。当进程要访问页面 2 时，产生缺页中断，会将页面 7 换出，因为页面 7 再次被访问的时间最长。 2. 最近最久未使用 LRU, Least Recently Used 虽然无法知道将来要使用的页面情况，但是可以知道过去使用页面的情况。LRU 将最近最久未使用的页面换出。 为了实现 LRU，需要在内存中维护一个所有页面的链表。当一个页面被访问时，将这个页面移到链表表头。这样就能保证链表表尾的页面是最近最久未访问的。 因为每次访问都需要更新链表，因此这种方式实现的 LRU 代价很高。 4，7，0，7，1，0，1，2，1，2，6 3. 最近未使用 NRU, Not Recently Used 每个页面都有两个状态位：R 与 M，当页面被访问时设置页面的 R=1，当页面被修改时设置 M=1。其中 R 位会定时被清零。可以将页面分成以下四类： R=0，M=0 R=0，M=1 R=1，M=0 R=1，M=1 当发生缺页中断时，NRU 算法随机地从类编号最小的非空类中挑选一个页面将它换出。 NRU 优先换出已经被修改的脏页面（R=0，M=1），而不是被频繁使用的干净页面（R=1，M=0）。 4. 先进先出 FIFO, First In First Out 选择换出的页面是最先进入的页面。 该算法会将那些经常被访问的页面也被换出，从而使缺页率升高。 5. 第二次机会算法FIFO 算法可能会把经常使用的页面置换出去，为了避免这一问题，对该算法做一个简单的修改： 当页面被访问 (读或写) 时设置该页面的 R 位为 1。需要替换的时候，检查最老页面的 R 位。如果 R 位是 0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是 1，就将 R 位清 0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续从链表的头部开始搜索。 6. 时钟 Clock 第二次机会算法需要在链表中移动页面，降低了效率。时钟算法使用环形链表将页面连接起来，再使用一个指针指向最老的页面。 分段虚拟内存采用的是分页技术，也就是将地址空间划分成固定大小的页，每一页再与内存进行映射。 下图为一个编译器在编译过程中建立的多个表，有 4 个表是动态增长的，如果使用分页系统的一维地址空间，动态增长的特点会导致覆盖问题的出现。 分段的做法是把每个表分成段，一个段构成一个独立的地址空间。每个段的长度可以不同，并且可以动态增长。 段页式程序的地址空间划分成多个拥有独立地址空间的段，每个段上的地址空间划分成大小相同的页。这样既拥有分段系统的共享和保护，又拥有分页系统的虚拟内存功能。 分页与分段的比较 对程序员的透明性：分页透明，但是分段需要程序员显示划分每个段。 地址空间的维度：分页是一维地址空间，分段是二维的。 大小是否可以改变：页的大小不可变，段的大小可以动态改变。 出现的原因：分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护。 设备管理磁盘结构 盘面（Platter）：一个磁盘有多个盘面； 磁道（Track）：盘面上的圆形带状区域，一个盘面可以有多个磁道； 扇区（Track Sector）：磁道上的一个弧段，一个磁道可以有多个扇区，它是最小的物理储存单位，目前主要有 512 bytes 与 4 K 两种大小； 磁头（Head）：与盘面非常接近，能够将盘面上的磁场转换为电信号（读），或者将电信号转换为盘面的磁场（写）； 制动手臂（Actuator arm）：用于在磁道之间移动磁头； 主轴（Spindle）：使整个盘面转动。 磁盘调度算法读写一个磁盘块的时间的影响因素有： 旋转时间（主轴转动盘面，使得磁头移动到适当的扇区上） 寻道时间（制动手臂移动，使得磁头移动到适当的磁道上） 实际的数据传输时间 其中，寻道时间最长，因此磁盘调度的主要目标是使磁盘的平均寻道时间最短。 1. 先来先服务 FCFS, First Come First Served 按照磁盘请求的顺序进行调度。 优点是公平和简单。缺点也很明显，因为未对寻道做任何优化，使平均寻道时间可能较长。 2. 最短寻道时间优先 SSTF, Shortest Seek Time First 优先调度与当前磁头所在磁道距离最近的磁道。 虽然平均寻道时间比较低，但是不够公平。如果新到达的磁道请求总是比一个在等待的磁道请求近，那么在等待的磁道请求会一直等待下去，也就是出现饥饿现象。具体来说，两端的磁道请求更容易出现饥饿现象。 3. 电梯算法 SCAN 电梯总是保持一个方向运行，直到该方向没有请求为止，然后改变运行方向。 电梯算法（扫描算法）和电梯的运行过程类似，总是按一个方向来进行磁盘调度，直到该方向上没有未完成的磁盘请求，然后改变方向。 因为考虑了移动方向，因此所有的磁盘请求都会被满足，解决了 SSTF 的饥饿问题。 编译链接编译系统以下是一个 hello.c 程序： 1234567#include &lt;stdio.h&gt;int main()&#123; printf("hello, world\n"); return 0;&#125; 在 Unix 系统上，由编译器把源文件转换为目标文件。 1gcc -o hello hello.c 这个过程大致如下： 预处理阶段：处理以 # 开头的预处理命令； 编译阶段：翻译成汇编文件； 汇编阶段：将汇编文件翻译成可重定向目标文件； 链接阶段：将可重定向目标文件和 printf.o 等单独预编译好的目标文件进行合并，得到最终的可执行目标文件。 静态链接静态链接器以一组可重定向目标文件为输入，生成一个完全链接的可执行目标文件作为输出。链接器主要完成以下两个任务： 符号解析：每个符号对应于一个函数、一个全局变量或一个静态变量，符号解析的目的是将每个符号引用与一个符号定义关联起来。 重定位：链接器通过把每个符号定义与一个内存位置关联起来，然后修改所有对这些符号的引用，使得它们指向这个内存位置。 目标文件 可执行目标文件：可以直接在内存中执行； 可重定向目标文件：可与其它可重定向目标文件在链接阶段合并，创建一个可执行目标文件； 共享目标文件：这是一种特殊的可重定向目标文件，可以在运行时被动态加载进内存并链接； 动态链接静态库有以下两个问题： 当静态库更新时那么整个程序都要重新进行链接； 对于 printf 这种标准函数库，如果每个程序都要有代码，这会极大浪费资源。 共享库是为了解决静态库的这两个问题而设计的，在 Linux 系统中通常用 .so 后缀来表示，Windows 系统上它们被称为 DLL。它具有以下特点： 在给定的文件系统中一个库只有一个文件，所有引用该库的可执行目标文件都共享这个文件，它不会被复制到引用它的可执行文件中； 在内存中，一个共享库的 .text 节（已编译程序的机器代码）的一个副本可以被不同的正在运行的进程共享。 设计并实现一个LRU cache, leetcode146来源：牛客网 利用链表和hashmap。当需要插入新的数据项的时候，如果新数据项在链表中存在（一般称为命中），则把该节点移到链表头部，如果不存在，则新建一个节点，放到链表头部，若缓存满了，则把链表最后一个节点删除即可。在访问数据的时候，如果数据项在链表中存在，则把该节点移到链表头部，否则返回-1。这样一来在链表尾部的节点就是最近最久未访问的数据项。 总结一下：根据题目的要求，LRU Cache具备的操作： set(key,value)：如果key在hashmap中存在，则先重置对应的value值，然后获取对应的节点cur，将cur节点从链表删除，并移动到链表的头部；若果key在hashmap不存在，则新建一个节点，并将节点放到链表的头部。当Cache存满的时候，将链表最后一个节点删除即可。 get(key)：如果key在hashmap中存在，则把对应的节点放到链表头部，并返回对应的value值；如果不存在，则返回-1。 12345678910111213141516171819202122232425262728293031323334353637383940414243//leetcode passpublic class LRUCache &#123; private int capacity; private LinkedList&lt;Integer&gt; list; private HashMap&lt;Integer, Integer&gt; map; public LRUCache(int capacity) &#123; this.capacity = capacity; list = new LinkedList&lt;&gt;(); map = new HashMap&lt;&gt;(); &#125; public int get(int key) &#123; if (map.containsKey(key)) &#123; list.removeFirstOccurrence(key); list.addFirst(key); return map.get(key); &#125; else &#123; return -1; &#125; &#125; public void put(int key, int value) &#123; if (!map.containsKey(key)) &#123; if (list.size() == capacity) &#123; int last = list.removeLast(); map.remove(last); &#125; list.addFirst(key); map.put(key, value); &#125; else &#123; list.removeFirstOccurrence(key); list.addFirst(key); map.put(key, value); &#125; &#125;&#125; linkedhashmap:默认是按插入顺序排序，如果指定按访问顺序排序，那么调用get方法后，会将这次访问的元素移至链表尾部，不断访问可以形成按访问顺序排序的链表。 可以重写removeEldestEntry方法返回true值指定插入元素时移除最老的元素。 123456789101112131415161718192021222324class LRUCache &#123; private Map&lt;Integer, Integer&gt; map; public LRUCache(int capacity) &#123; //true：指定迭代的顺序是按照访问顺序(近期访问最少到近期访问最多的元素)来迭代的。 //false：指定迭代的顺序是按照插入顺序迭代，也就是通过插入元素的顺序来迭代所有元素 map = new LinkedHashMap&lt;Integer, Integer&gt;(capacity, 0.75f, true) &#123; protected boolean removeEldestEntry(Map.Entry eldest) &#123; return size() &gt; capacity; &#125; &#125;; &#125; public int get(int key) &#123; return map.getOrDefault(key, -1); &#125; public void put(int key, int value) &#123; map.put(key, value); &#125;&#125;]]></content>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PostgreSQL]]></title>
    <url>%2FPostgreSQL%2F</url>
    <content type="text"><![CDATA[XL, Chapter 26. High Availability, Load Balancing, and Replication26.1. Comparison of Different Solutions Shared Disk Failover 共享磁盘故障转移通过仅拥有一个数据库副本来避免同步开销。 它使用由多个服务器共享的单个磁盘阵列。 如果主数据库服务器出现故障，则备用服务器可以装入并启动数据库，就好像它正在从数据库崩溃中恢复一样。 这允许快速故障转移而不会丢失数据。 共享硬件功能在网络存储设备中很常见。 使用网络文件系统也是可能的，但必须注意文件系统具有完整的POSIX行为（参见第18.2.2节）。 此方法的一个重要限制是，如果共享磁盘阵列出现故障或损坏，则主服务器和备用服务器都不起作用。 另一个问题是备用服务器在主服务器运行时永远不应访问共享存储。 File System (Block Device) Replication 共享硬件功能的修改版本是文件系统复制，其中对文件系统的所有更改都镜像到驻留在另一台计算机上的文件系统。 唯一的限制是镜像必须以确保备用服务器具有文件系统的一致副本的方式完成 - 具体而言，对备用数据库的写入必须按照与主数据库相同的顺序进行。 DRBD是一种流行的Linux文件系统复制解决方案。 Write-Ahead Log Shipping 通过读取预写日志（WAL）记录流，可以使热备用服务器保持最新状态。 如果主服务器出现故障，则备用数据库几乎包含主服务器的所有数据，并且可以快速建立新的主数据库服务器。 这可以是同步的，也可以是异步的，只能对整个数据库服务器执行。 可以使用基于文件的日志传送（第26.2节）或流复制（请参阅第26.2.5节）或两者的组合来实现备用服务器。 有关热备用的信息，请参见第26.5节。 Logical Replication 逻辑复制允许数据库服务器将数据修改流发送到另一个服务器。 PostgreSQL逻辑复制构造来自WAL的逻辑数据修改流。 逻辑复制允许复制各个表的数据更改。 逻辑复制不需要将特定服务器指定为主服务器或副本，但允许数据在多个方向上流动。 有关逻辑复制的更多信息，请参阅第31章。通过逻辑解码接口（第48章），第三方扩展也可以提供类似的功能。 Trigger-Based Master-Standby Replication 主 - 备复制设置将所有数据修改查询发送到主服务器。 主服务器异步将数据更改发送到备用服务器。 备用数据库可以在主服务器运行时应答只读查询。 备用服务器是数据仓库查询的理想选择。 Slony-I是此类复制的一个示例，具有每表粒度，并支持多个备用服务器。 因为它以异步方式（批量）更新备用服务器，故障转移期间可能会丢失数据。 Statement-Based Replication Middleware 使用基于语句的复制中间件，程序拦截每个SQL查询并将其发送到一个或所有服务器。每个服务器独立运行。必须将读写查询发送到所有服务器，以便每个服务器都接收任何更改。但只读查询可以发送到一个服务器，允许读取工作负载在它们之间分配。 如果查询只是未经修改的广播，则random（），CURRENT_TIMESTAMP和sequence等函数可以在不同的服务器上具有不同的值。这是因为每个服务器都独立运行，并且因为SQL查询是广播的（而不是实际修改的行）。如果这是不可接受的，则中间件或应用程序必须从单个服务器查询此类值，然后在写入查询中使用这些值。另一种选择是将此复制选项与传统的主 - 备用设置一起使用，即数据修改查询仅发送到主服务器，并通过主 - 备复制而不是复制中间件传播到备用服务器。还必须注意所有事务在所有服务器上提交或中止，可能使用两阶段提交（PREPARE TRANSACTION和COMMIT PREPARED）。 Pgpool-II和Continuent Tungsten是这种复制的例子。 Asynchronous Multimaster Replication 对于没有定期连接的服务器，如笔记本电脑或远程服务器，在服务器之间保持数据一致是一项挑战。 使用异步多主机复制，每个服务器独立工作，并定期与其他服务器通信以识别冲突的事务。 用户或冲突解决规则可以解决冲突。 Bucardo就是这种复制的一个例子。 Synchronous Multimaster Replication 在同步多主机复制中，每个服务器都可以接受写入请求，并且在每个事务提交之前，已修改的数据从原始服务器传输到每个其他服务器。重写活动可能导致过度锁定，导致性能不佳。实际上，写入性能通常比单个服务器差。读取请求可以发送到任何服务器。某些实现使用共享磁盘来减少通信开销。同步多主机复制最适合大多数读取工作负载，但它的最大优点是任何服务器都可以接受写入请求 - 不需要在主服务器和备用服务器之间分配工作负载，并且因为数据更改从一个服务器发送到另一个服务器，对于像random（）这样的非确定性函数没有问题。 PostgreSQL不提供这种类型的复制，但PostgreSQL两阶段提交（PREPARE TRANSACTION和COMMIT PREPARED）可用于在应用程序代码或中间件中实现此功能。 Commercial Solutions 由于PostgreSQL是开源的并且易于扩展，因此许多公司采用了PostgreSQL并创建了具有独特故障转移，复制和负载平衡功能的商业闭源解决方案。 Data Partitioning 数据分区将表拆分为数据集。 每组只能由一台服务器修改。 例如，数据可以由办公室分区，例如伦敦和巴黎，每个办公室都有一台服务器。 如果需要组合伦敦和巴黎数据的查询，则应用程序可以查询两个服务器，或者主/备复制可用于在每个服务器上保留其他办公室数据的只读副本。 Multiple-Server Parallel Query Execution 上述许多解决方案允许多个服务器处理多个查询，但是没有一个允许单个查询使用多个服务器来更快地完成。 此解决方案允许多个服务器在单个查询上同时工作。 它通常通过在服务器之间拆分数据并让每个服务器执行其部分查询并将结果返回到中央服务器来完成，然后将它们组合并返回给用户。 Pgpool-II具有此功能。 此外，这可以使用PL /代理工具集来实现。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性哈希算法]]></title>
    <url>%2F%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[白话解析：一致性哈希算法 consistent hashing 场景描述假设，我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为0号、1号、2号，现在，有3万张图片需要缓存，我们希望这些图片被均匀的缓存到这3台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存1万张左右的图片，那么，我们应该怎样做呢？如果我们没有任何规律的将3万张图片平均的缓存在3台服务器上，可以满足我们的要求吗？可以！但是如果这样做，当我们需要访问某个缓存项时，则需要遍历3台缓存服务器，从3万个缓存项中找到我们需要访问的缓存，遍历的过程效率太低，时间太长，当我们找到需要访问的缓存项时，时长可能是不能被接受的，也就失去了缓存的意义，缓存的目的就是提高速度，改善用户体验，减轻后端服务器压力，如果每次访问一个缓存项都需要遍历所有缓存服务器的所有缓存项，想想就觉得很累，那么，我们该怎么办呢？原始的做法是对缓存项的键进行哈希，将hash后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上，这样说可能不太容易理解，我们举例说明，仍然以刚才描述的场景为例，假设我们使用图片名称作为访问图片的key，假设图片名称是不重复的，那么，我们可以使用如下公式，计算出图片应该存放在哪台服务器上。 hash（图片名称）% N 因为图片的名称是不重复的，所以，当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有3台服务器，使用哈希后的结果对3求余，那么余数一定是0、1或者2，没错，正好与我们之前的服务器编号相同，如果求余的结果为0， 我们就把当前图片名称对应的图片缓存在0号服务器上，如果余数为1，就把当前图片名对应的图片缓存在1号服务器上，如果余数为2，同理，那么，当我们访问任意一个图片的时候，只要再次对图片名称进行上述运算，即可得出对应的图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将3万张图片随机的分布到3台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，这样就能满足我们的需求了，我们暂时称上述算法为HASH算法或者取模算法，取模算法的过程可以用下图表示。 但是，使用上述HASH算法进行缓存时，会出现一些缺陷，试想一下，如果3台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？没错，很简单，多增加两台缓存服务器不就行了，假设，我们增加了一台缓存服务器，那么缓存服务器的数量就由3台变成了4台，此时，如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，被除数不变的情况下，余数肯定不同，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变，换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据，同理，假设3台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从3台变为2台，如果想要访问一张图片，这张图片的缓存位置必定会发生改变，以前缓存的图片也会失去缓存的作用与意义，由于大量缓存在同一时间失效，造成了缓存的雪崩，此时前端缓存已经无法起到承担部分压力的作用，后端服务器将会承受巨大的压力，整个系统很有可能被压垮，所以，我们应该想办法不让这种情况发生，但是由于上述HASH算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，一致性哈希算法诞生了。 我们来回顾一下使用上述算法会出现的问题。 当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。 当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变，怎样才能尽量减少受影响的缓存呢？ 其实，上面两个问题是一个问题，那么，一致性哈希算法能够解决上述问题吗？我们现在就来了解一下一致性哈希算法。 一致性哈希算法的基本概念其实，一致性哈希算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性哈希算法是对2^32取模，什么意思呢？我们慢慢聊。 首先，我们把二的三十二次方想象成一个圆，就像钟表一样，钟表的圆可以理解成由60个点组成的圆，而此处我们把这个圆想象成由2^32个点组成的圆，示意图如下： 圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1,也就是说0点左侧的第一个点代表2^32-1 我们把这个由2的32次方个点组成的圆环称为hash环。 那么，一致性哈希算法与上图中的圆环有什么关系呢？我们继续聊，仍然以之前描述的场景为例，假设我们有3台缓存服务器，服务器A、服务器B、服务器C，那么，在生产环境中，这三台服务器肯定有自己的IP地址，我们使用它们各自的IP地址进行哈希计算，使用哈希后的结果对2^32取模，可以使用如下公式示意。 hash（服务器A的IP地址） % 2^32 通过上述公式算出的结果一定是一个0到2^32-1之间的一个整数，我们就用算出的这个整数，代表服务器A，既然这个整数肯定处于0到2^32-1之间，那么，上图中的hash环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器A，那么，服务器A就可以映射到这个环上，用下图示意 同理，服务器B与服务器C也可以通过相同的方法映射到上图中的hash环中 hash（服务器B的IP地址） % 2^32 hash（服务器C的IP地址） % 2^32 通过上述方法，可以将服务器B与服务器C映射到上图中的hash环上，示意图如下 假设3台服务器映射到hash环上以后如上图所示（当然，这是理想的情况，我们慢慢聊）。 好了，到目前为止，我们已经把缓存服务器与hash环联系在了一起，我们通过上述方法，把缓存服务器映射到了hash环上，那么使用同样的方法，我们也可以将需要缓存的对象映射到hash环上。 假设，我们需要使用缓存服务器缓存图片，而且我们仍然使用图片的名称作为找到图片的key，那么我们使用如下公式可以将图片映射到上图中的hash环上。 hash（图片名称） % 2^32 映射后的示意图如下，下图中的橘黄色圆形表示图片 好了，现在服务器与图片都被映射到了hash环上，那么上图中的这个图片到底应该被缓存到哪一台服务器上呢？上图中的图片将会被缓存到服务器A上，为什么呢？因为从图片的位置开始，沿顺时针方向遇到的第一个服务器就是A服务器，所以，上图中的图片将会被缓存到服务器A上，如下图所示。 没错，一致性哈希算法就是通过这种方法，判断一个对象应该被缓存到哪台服务器上的，将缓存服务器与被缓存对象都映射到hash环上以后，从被缓存对象的位置出发，沿顺时针方向遇到的第一个服务器，就是当前对象将要缓存于的服务器，由于被缓存对象与服务器hash后的值是固定的，所以，在服务器不变的情况下，一张图片必定会被缓存到固定的服务器上，那么，当下次想要访问这张图片时，只要再次使用相同的算法进行计算，即可算出这个图片被缓存在哪个服务器上，直接去对应的服务器查找对应的图片即可。 刚才的示例只使用了一张图片进行演示，假设有四张图片需要缓存，示意图如下 1号、2号图片将会被缓存到服务器A上，3号图片将会被缓存到服务器B上，4号图片将会被缓存到服务器C上。 一致性哈希算法的优点经过上述描述，我想兄弟你应该已经明白了一致性哈希算法的原理了，但是话说回来，一致性哈希算法能够解决之前出现的问题吗，我们说过，如果简单的对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，那么使用一致性哈希算法，能够避免这个问题吗？我们来模拟一遍，即可得到答案。 假设，服务器B出现了故障，我们现在需要将服务器B移除，那么，我们将上图中的服务器B从hash环上移除即可，移除服务器B以后示意图如下。 在服务器B未移除时，图片3应该被缓存到服务器B中，可是当服务器B移除以后，按照之前描述的一致性哈希算法的规则，图片3应该被缓存到服务器C中，因为从图片3的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器C，也就是说，如果服务器B出现故障被移除时，图片3的缓存位置会发生改变 但是，图片4仍然会被缓存到服务器C中，图片1与图片2仍然会被缓存到服务器A中，这与服务器B移除之前并没有任何区别，这就是一致性哈希算法的优点，如果使用之前的hash算法，服务器数量发生改变时，所有服务器的所有缓存在同一时间失效了，而使用一致性哈希算法时，服务器的数量如果发生改变，并不是所有缓存都会失效，而是只有部分缓存会失效，前端的缓存仍然能分担整个系统的压力，而不至于所有压力都在同一时间集中到后端服务器上。 这就是一致性哈希算法所体现出的优点。 hash环的偏斜在介绍一致性哈希的概念时，我们理想化的将3台服务器均匀的映射到了hash环上，如下图所示 但是，理想很丰满，现实很骨感，我们想象的与实际情况往往不一样。在实际的映射中，服务器可能会被映射成如下模样。 聪明如你一定想到了，如果服务器被映射成上图中的模样，那么被缓存的对象很有可能大部分集中缓存在某一台服务器上，如下图所示。 上图中，1号、2号、3号、4号、6号图片均被缓存在了服务器A上，只有5号图片被缓存在了服务器B上，服务器C上甚至没有缓存任何图片，如果出现上图中的情况，A、B、C三台服务器并没有被合理的平均的充分利用，缓存分布的极度不均匀，而且，如果此时服务器A出现故障，那么失效缓存的数量也将达到最大值，在极端情况下，仍然有可能引起系统的崩溃，上图中的情况则被称之为hash环的偏斜，那么，我们应该怎样防止hash环的偏斜呢？一致性hash算法中使用”虚拟节点”解决了这个问题，我们继续聊。 虚拟节点话接上文，由于我们只有3台服务器，当我们把服务器映射到hash环上的时候，很有可能出现hash环偏斜的情况，当hash环偏斜以后，缓存往往会极度不均衡的分布在各服务器上，聪明如你一定已经想到了，如果想要均衡的将缓存分布到3台服务器上，最好能让这3台服务器尽量多的、均匀的出现在hash环上，但是，真实的服务器资源只有3台，我们怎样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来，这些由实际节点虚拟复制而来的节点被称为”虚拟节点”。加入虚拟节点以后的hash环如下。 “虚拟节点”是”实际节点”（实际的物理服务器）在hash环上的复制品,一个实际节点可以对应多个虚拟节点。 从上图可以看出，A、B、C三台服务器分别虚拟出了一个虚拟节点，当然，如果你需要，也可以虚拟出更多的虚拟节点。引入虚拟节点的概念后，缓存的分布就均衡多了，上图中，1号、3号图片被缓存在服务器A中，5号、4号图片被缓存在服务器B中，6号、2号图片被缓存在服务器C中，如果你还不放心，可以虚拟出更多的虚拟节点，以便减小hash环偏斜所带来的影响，虚拟节点越多，hash环上的节点就越多，缓存被均匀分布的概率就越大。]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础知识]]></title>
    <url>%2FJava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[零散的面试常考基础知识 String 和 StringBuffer、StringBuilder 的区别是什么？可变性简单的来说：String 类中使用 final 关键字字符数组保存字符串， private final char value[]，所以 String 对象是不可变的。而StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串 char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 StringBuilder 与 StringBuffer 的构造方法都是调用父类构造方法也就是 AbstractStringBuilder 实现的，大家可以自行查阅源码。 AbstractStringBuilder.java 123456789abstract class AbstractStringBuilder implements Appendable, CharSequence &#123; char[] value; int count; AbstractStringBuilder() &#123; &#125; AbstractStringBuilder(int capacity) &#123; value = new char[capacity]; &#125;&#125; 线程安全性String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的，如果一个StringBuffer对象在字符串缓冲区被多个线程使用时，StringBuffer中很多方法可以带有synchronized关键字。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。 StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StirngBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据 = String 单线程操作字符串缓冲区下操作大量数据 = StringBuilder 多线程操作字符串缓冲区下操作大量数据 = StringBuffer 关于 final 关键字的一些总结final关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为fianl Object类的常见方法总结123456789101112131415161718192021public final native Class&lt;?&gt; getClass()//native方法，用于返回当前运行时对象的Class对象，使用了final关键字修饰，故不允许子类重写。public native int hashCode() //native方法，用于返回对象的哈希码，主要使用在哈希表中，比如JDK中的HashMap。public boolean equals(Object obj)//用于比较2个对象的内存地址是否相等，String类对该方法进行了重写用户比较字符串的值是否相等。protected native Object clone() throws CloneNotSupportedException//naitive方法，用于创建并返回当前对象的一份拷贝。一般情况下，对于任何对象 x，表达式 x.clone() != x 为true，x.clone().getClass() == x.getClass() 为true。Object本身没有实现Cloneable接口，所以不重写clone方法并且进行调用的话会发CloneNotSupportedException异常。public String toString()//返回类的名字@实例的哈希码的16进制的字符串。建议Object所有的子类都重写这个方法。public final native void notify()//native方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。public final native void notifyAll()//native方法，并且不能重写。跟notify一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。public final native void wait(long timeout) throws InterruptedException//native方法，并且不能重写。暂停线程的执行。注意：sleep方法没有释放锁，而wait方法释放了锁 。timeout是等待时间。public final void wait(long timeout, int nanos) throws InterruptedException//多了nanos参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上nanos毫秒。public final void wait() throws InterruptedException//跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念protected void finalize() throws Throwable &#123; &#125;//实例被垃圾回收器回收的时候触发的操作 Java的异常体系 Error（错误）是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（VirtualMachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 Exception（异常）unchecked Exception（RuntimeException）RuntimeException 异常由Java虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以0时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。 对未检查的异常(unchecked exception )的几种处理方式： 捕获 继续抛出 不处理 checked Exception（非RuntimeException）对于不是你犯的错，我们统称为非RuntimeException，也叫checked Exception。对检查的异常的几种处理方式： 继续抛出，消极的方法，一直可以抛到java虚拟机来处理 用try…catch捕获 对于检查的异常必须处理，或者必须捕获或者必须抛出 异常处理完成以后，Exception对象会在下一个垃圾回收过程中被回收掉。 注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。 Throwable类常用方法public string getMessage():返回异常发生时的详细信息public string toString():返回异常发生时的简要描述public string getLocalizedMessage():返回异常对象的本地化信息。使用Throwable的子类覆盖这个方法，可以声称本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与getMessage（）返回的结果相同 public void printStackTrace():在控制台上打印Throwable对象封装的异常信息 异常处理总结 try 块：用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。 catch 块：用于处理try捕获到的异常。 finally 块：无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。 在以下4种特殊情况下，finally块不会被执行： 在finally语句块中发生了异常。 在前面的代码中用了System.exit()退出程序。 程序所在的线程死亡。 关闭CPU 获取用键盘输入常用的的两种方法方法1：通过 Scanner 123Scanner input = new Scanner(System.in);String s = input.nextLine();input.close(); 方法2：通过 BufferedReader 12BufferedReader input = new BufferedReader(new InputStreamReader(System.in));String s = input.readLine(); 接口和抽象类的区别是什么 接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），抽象类可以有非抽象的方法 接口中的实例变量默认是 final 类型的，而抽象类中则不一定 一个类可以实现多个接口，但最多只能实现一个抽象类 一个类实现接口的话要实现接口的所有方法，而抽象类不一定 接口不能用 new 实例化，但可以声明，但是必须引用一个实现该接口的对象 。从设计层面来说，抽象是对类的抽象，是一种模板设计，接口是行为的抽象，是一种行为的规范。 备注:在JDK8中，接口也可以定义静态方法，可以直接用接口名调用。实现类和实现是不可以调用的。如果同时实现两个接口，接口中定义了一样的默认方法，必须重写，不然会报错。 深拷贝与浅拷贝Java深拷贝浅拷贝将一个对象的引用复制给另外一个对象，一共有三种方式。第一种方式是直接赋值，第二种方式是浅拷贝，第三种是深拷贝。 直接赋值：A a1 = a2，复制的是引用，也就是说a1和a2指向的是同一个对象。因此，当a1变化的时候，a2里面的成员变量也会跟着变化。 浅：属性不一样，是独立的，对象（方法）什么的一样，是同一份。clone()主要做了些什么，创建一个新对象，然后将当前对象的非静态字段复制到该新对象，如果字段是值类型的，那么对该字段执行复制；如果该字段是引用类型的话，则复制引用但不复制引用的对象。因此，原始对象及其副本引用同一个对象。 深：都不一样 12345678910//奥义在于深拷贝在super.clone之后还得把属性什么的new一个，然后做和原本的构造函数给属性赋值一样的操作，这样属性就也不一样了@Overrideprotected DeepCloneExample clone() throws CloneNotSupportedException &#123; DeepCloneExample result = (DeepCloneExample) super.clone(); result.arr = new int[arr.length]; for (int i = 0; i &lt; arr.length; i++) &#123; result.arr[i] = arr[i]; &#125; return result;&#125; new Integer(123) 与 Integer.valueOf(123) 的区别在于，new Integer(123) 每次都会新建一个对象，而 Integer.valueOf(123) 可能会使用缓存对象，因此多次使用 Integer.valueOf(123) 会取得同一个对象的引用。 反射、泛型泛型泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。 泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。 12345List arrayList = new ArrayList();arrayList.add("aaaa");List&lt;String&gt; arrayList = new ArrayList&lt;String&gt;();//arrayList.add(100); 在编译阶段，编译器就会报错 泛型只在编译阶段有效。见“反射”，用方法的反射绕过编译 通配符的出现是为了指定泛型中的类型范围。 &lt;?&gt;被称作无限定的通配符。 &lt;? extends T&gt;被称作有上限的通配符。 &lt;? super T&gt;被称作有下限的通配符。 泛型类、泛型接口、泛型方法 泛型擦除： Java 编译器生成的字节码文件不包含有泛型信息，泛型信息将在编译时被擦除，这个过程称为泛型擦除。其主要过程为 将所有泛型参数用其最左边界（最顶级的父类型）类型替换； 移除 all 的类型参数。 反射javap 原生的 看class文件 类是java.lang.class类的实例对象 12345678910111213141516171819202122232425262728//第一种表示方式---&gt;实际在告诉我们任何一个类都有一个隐含的静态成员变量class Class c1 = Foo.class; //第二中表达方式 已经知道该类的对象通过getClass方法 Class c2 = foo1.getClass(); /*官网 c1 ,c2 表示了Foo类的类类型(class type) 万事万物皆对象，类也是对象，是Class类的实例对象 * 这个对象我们称为该类的类类型 */ //不管c1 or c2都代表了Foo类的类类型，一个类只可能是Class类的一个实例对象 System.out.println(c1 == c2);相等 //第三种表达方式 Class c3 = null; try &#123; c3 = Class.forName("com.imooc.reflect.Foo"); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println(c2==c3);相等 //我们完全可以通过类的类类型创建该类的对象实例----&gt;通过c1 or c2 or c3创建Foo的实例对象 try &#123; Foo foo = (Foo)c1.newInstance();//需要有无参数的构造方法 foo.print(); &#125; catch (InstantiationException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; new创建对象是静态加载类，编译时刻就需要加载所有可能用到的类 12345678Class c = obj.getClass();/** 成员变量也是对象* java.lang.reflect.Field* Field类封装了关于成员变量的操作* getFields()方法获取的是所有的public的成员变量的信息* getDeclaredFields获取的是该类自己声明的成员变量的信息 *///Field[] fs = c.getFields();Field[] fs = c.getDeclaredFields(); 要获取一个方法就是获取类的信息，获取类的信息首先要获取类的类类型. 获取方法 名称和参数列表来决定 getMethod获取的是public的方法、getDelcaredMethod自己声明的方法 123456"Method m = c.getMethod(""print"", int.class,int.class);//方法的反射操作 //a1.print(10, 20);方法的反射操作是用m对象来进行方法调用 和a1.print调用的效果完全相同//方法如果没有返回值返回null,有返回值返回具体的返回值//Object o = m.invoke(a1,new Object[]&#123;10,20&#125;);Object o = m.invoke(a1, 10,20); Class 和 java.lang.reflect 一起对反射提供了支持，java.lang.reflect 类库主要包含了以下三个类： Field ：可以使用 get() 和 set() 方法读取和修改 Field 对象关联的字段； Method ：可以使用 invoke() 方法调用与 Method 对象关联的方法； Constructor ：可以用 Constructor 创建新的对象。 反射的优点： 可扩展性 ：应用程序可以利用全限定名创建可扩展对象的实例，来使用来自外部的用户自定义类。 类浏览器和可视化开发环境 ：一个类浏览器需要可以枚举类的成员。可视化开发环境（如 IDE）可以从利用反射中可用的类型信息中受益，以帮助程序员编写正确的代码。 调试器和测试工具 ： 调试器需要能够检查一个类里的私有成员。测试工具可以利用反射来自动地调用类里定义的可被发现的 API 定义，以确保一组测试中有较高的代码覆盖率。 反射的缺点： 尽管反射非常强大，但也不能滥用。如果一个功能可以不用反射完成，那么最好就不用。在我们使用反射技术时，下面几条内容应该牢记于心。 性能开销 ：反射涉及了动态类型的解析，所以 JVM 无法对这些代码进行优化。因此，反射操作的效率要比那些非反射操作低得多。我们应该避免在经常被执行的代码或对性能要求很高的程序中使用反射。 安全限制 ：使用反射技术要求程序必须在一个没有安全限制的环境中运行。如果一个程序必须在有安全限制的环境中运行，如 Applet，那么这就是个问题了。 内部暴露 ：由于反射允许代码执行一些在正常情况下不被允许的操作（比如访问私有的属性和方法），所以使用反射可能会导致意料之外的副作用，这可能导致代码功能失调并破坏可移植性。反射代码破坏了抽象性，因此当平台发生改变的时候，代码的行为就有可能也随着变化。 数据库驱动为什么用反射反射我们知道是对一个类的主动使用，会触发类的初始化过程，在jvm定义中，对类加载的前几个步骤在什么情况下执行没有具体规定，但是对初始化过程做了一下规定，凡是主动对一个类的使用，就会触发初始化，既然初始化触发，那么“加载，连接（验证，准备，解析（不一定在这一步）），初始化”肯定都执行了。此外，对一个类的初始化，首先会看他的父类有没有初始化，如果没有，还要先进行父类的初始化。所谓初始化，就是调用其静态代码块，为静态变量赋值。 来看看com.mysql.jdbc.Driver在初始化过程中究竟做了那些事。 1234567static &#123; try &#123; // 放入到一个copyonwritearraylist中 java.sql.DriverManager.registerDriver(new Driver()); &#125; catch (SQLException E) &#123;throw new RuntimeException("Can't register driver!");&#125;&#125; 由此可见，Driver通过静态代码块把自己注册到DriverManger中去了，这也是下一步Connection conn = DriverManager.getConnection(url, username, password);能够获取连接的原因，看看具体代码 123456789101112131415161718192021for(DriverInfo aDriver : registeredDrivers) &#123; // If the caller does not have permission to load the driver then skip it. if(isDriverAllowed(aDriver.driver, callerCL)) &#123; try &#123; println(" trying " + aDriver.driver.getClass().getName()); // 通过具体注册的driver的connect方法获取连接 Connection con = aDriver.driver.connect(url, info); if (con != null) &#123; println("getConnection returning"+aDriver.driver.getClass().getName()); return (con); &#125; &#125; catch (SQLException ex) &#123; if (reason == null) &#123; reason = ex;&#125;&#125; &#125; else &#123; println("skipping: " + aDriver.getClass().getName()); &#125;&#125; 本质上是调用了mysql.Driver的connect方法，通过建立到数据库的socket连接，来完成接下来sql的执行。JDBC只是jdk提出的一种java连接数据库的规范，提供了一些接口和抽象方法，并没有提供到某个具体数据库的实现，由各个数据库厂家来实现JDBC，比如上面提到的mysql.Driver就是一种具体实现。 工厂模式，反射的作用就是，无论你使用哪种数据库（数据库类型）只需要把数据库的驱动名称传过来就能穿件对象，而制定类只能创建你制定的数据库对象。 以上对JDBC连接数据库的具体源码做了一个粗略的分析，实际上可以看出来，只要是对com.mysql.jdbc.Driver的主动使用都会触发那个注册操作，为什么一定要使用反射呢？因为反射是运行时根据全类名动态生成的Class对象，完全可以把这个全类名写在xml或者properties中去，不仅从代码上解耦和，而且需要更换数据库时，不需要进行代码的重新编译。 内省 (Introspector)Introspector 是操作 javaBean 的 API，用来访问某个属性的 getter/setter 方法。对于一个标准的 javaBean 来说，它包括属性、get 方法和 set 方法，这是一个约定俗成的规范。为此 sun 提供了 Introspector 工具包，来使开发者更好或者更灵活的操作 javaBean。 核心类是 Introspector, 它提供了的 getBeanInfo 系类方法，可以拿到一个 JavaBean 的所有信息。 通过 BeanInfo 的 getPropertyDescriptors 方法和 getMethodDescriptors 方法可以拿到 javaBean 的字段信息列表和 getter 和 setter 方法信息列表。 PropertyDescriptors 可以根据字段直接获得该字段的 getter 和 setter 方法。 MethodDescriptors 可以获得方法的元信息，比如方法名，参数个数，参数字段类型等。 123456789101112131415161718192021public class User &#123; private String name; private int age; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 12345678910111213141516171819@Testpublic void test1() throws Exception &#123; // 获取整个Bean的信息 // BeanInfo beanInfo= Introspector.getBeanInfo(user.getClass()); // 在Object类时候停止检索，可以选择在任意一个父类停止 BeanInfo beanInfo = Introspector.getBeanInfo(User.class, Object.class); // 获取所有的属性描述 PropertyDescriptor[] pds = beanInfo.getPropertyDescriptors(); for (PropertyDescriptor propertyDescriptor : pds) &#123; System.out.println(propertyDescriptor.getName()); &#125; for (MethodDescriptor methodDescriptor : beanInfo.getMethodDescriptors()) &#123; System.out.println(methodDescriptor.getName()); // Method method = methodDescriptor.getMethod(); &#125;&#125; 12345678910111213141516171819202122 @Test public void test2 () throws Exception &#123; User user = new User(&quot;jack&quot;, 21); String propertyName = &quot;name&quot;; PropertyDescriptor namePd = new PropertyDescriptor(propertyName, User.class); System.out.println(&quot;名字：&quot; + namePd.getReadMethod().invoke(user)); namePd.getWriteMethod().invoke(user, &quot;tom&quot;); System.out.println(&quot;名字：&quot; + namePd.getReadMethod().invoke(user)); System.out.println(&quot;========================================&quot;); String agePropertyName = &quot;age&quot;; PropertyDescriptor agePd = new PropertyDescriptor(agePropertyName, User.class); System.out.println(&quot;年龄：&quot; + agePd.getReadMethod().invoke(user)); agePd.getWriteMethod().invoke(user, 22); System.out.println(&quot;年龄：&quot; + agePd.getReadMethod().invoke(user)); &#125;名字：jack名字：tom========================================年龄：21年龄：22 Java WebServlet的生命周期与工作原理Servlet运行在Servlet容器中，其生命周期由容器来管理。Servlet的生命周期通过javax.servlet.Servlet接口中的init()、service()和destroy()方法来表示,Servlet的生命周期包含了下面4个阶段： 加载和实例化 初始化 请求处理 服务终止 Web Client 向Servlet容器（Tomcat）发出Http请求 Servlet容器接收Web Client的请求 Servlet容器创建一个HttpRequest对象，将Web Client请求的信息封装到这个对象中。 Servlet容器创建一个HttpResponse对象 Servlet容器调用HttpServlet对象的service方法，把HttpRequest对象与HttpResponse对象作为参数传给HttpServlet 对象。 HttpServlet调用HttpRequest对象的有关方法，获取Http请求信息。 HttpServlet调用HttpResponse对象的有关方法，生成响应数据。 Servlet容器把HttpServlet的响应结果传给Web Client。 Tomcat 解析Tomcat内部结构和请求过程 结构Tomcat是一个JSP/Servlet容器。其作为Servlet容器，有三种工作模式：独立的Servlet容器、进程内的Servlet容器和进程外的Servlet容器。 Tomcat是一个基于组件的服务器，它的构成组件都是可配置的，其中最外层的是Catalina servlet容器，其他组件按照一定的格式要求配置在这个顶层容器中。 Tomcat的各种组件都是在Tomcat安装目录下的/conf/server.xml文件中配置的。 123456789101112&lt;Server&gt; //顶层类元素，可以包括多个Service &lt;Service&gt; //顶层类元素，可包含一个Engine，多个Connecter &lt;Connector&gt; //连接器类元素，代表通信接口 &lt;Engine&gt; //容器类元素，为特定的Service组件处理客户请求，要包含多个Host &lt;Host&gt; //容器类元素，为特定的虚拟主机组件处理客户请求，可包含多个Context &lt;Context&gt; //容器类元素，为特定的Web应用处理所有的客户请求 &lt;/Context&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Connector&gt; &lt;/Service&gt;&lt;/Server&gt; 由上图可看出Tomca的心脏是两个组件：Connecter和Container。一个Container可以选择多个Connecter，多个Connector和一个Container就形成了一个Service。Service可以对外提供服务，而Server服务器控制整个Tomcat的生命周期。 Service 和 Server 管理它下面组件的生命周期。Tomcat 中组件的生命周期是通过 Lifecycle 接口来控制的，组件只要继承这个接口并实现其中的方法就可以统一被拥有它的组件控制了，这样一层一层的直到一个最高级的组件就可以控制 Tomcat 中所有组件的生命周期，这个最高的组件就是 Server，而控制 Server 的是 Startup，也就是您启动和关闭 Tomcat。 Tomca的两大组件：Connecter和Container Connecter组件 一个Connecter将在某个指定的端口上侦听客户请求，接收浏览器的发过来的 tcp 连接请求，创建一个 Request 和 Response 对象分别用于和请求端交换数据，然后会产生一个线程来处理这个请求并把产生的 Request 和 Response 对象传给处理Engine(Container中的一部分)，从Engine出获得响应并返回客户。Tomcat中有两个经典的Connector，一个直接侦听来自Browser的HTTP请求，另外一个来自其他的WebServer请求。Cotote HTTP/1.1 Connector在端口8080处侦听来自客户Browser的HTTP请求，Coyote JK2 Connector在端口8009处侦听其他Web Server的Servlet/JSP请求。Connector 最重要的功能就是接收连接请求然后分配线程让 Container 来处理这个请求，所以这必然是多线程的，多线程的处理是 Connector 设计的核心。 Container组件 Container的体系结构如下： Container是容器的父接口，该容器的设计用的是典型的责任链的设计模式，它由四个自容器组件构成，分别是Engine、Host、Context、Wrapper。这四个组件是负责关系，存在包含关系。通常一个Servlet class对应一个Wrapper，如果有多个Servlet定义多个Wrapper，如果有多个Wrapper就要定义一个更高的Container，如Context。Context 还可以定义在父容器 Host 中，Host 不是必须的，但是要运行 war 程序，就必须要 Host，因为 war 中必有 web.xml 文件，这个文件的解析就需要 Host 了，如果要有多个 Host 就要定义一个 top 容器 Engine 了。而 Engine 没有父容器了，一个 Engine 代表一个完整的 Servlet 引擎。 Engine 容器Engine 容器比较简单，它只定义了一些基本的关联关系 Host 容器Host 是 Engine 的字容器，一个 Host 在 Engine 中代表一个虚拟主机，这个虚拟主机的作用就是运行多个应用，它负责安装和展开这些应用，并且标识这个应用以便能够区分它们。它的子容器通常是 Context，它除了关联子容器外，还有就是保存一个主机应该有的信息。 Context 容器Context 代表 Servlet 的 Context，它具备了 Servlet 运行的基本环境，理论上只要有 Context 就能运行 Servlet 了。简单的 Tomcat 可以没有 Engine 和 Host。Context 最重要的功能就是管理它里面的 Servlet 实例，Servlet 实例在 Context 中是以 Wrapper 出现的，还有一点就是 Context 如何才能找到正确的 Servlet 来执行它呢？ Tomcat5 以前是通过一个 Mapper 类来管理的，Tomcat5 以后这个功能被移到了 request 中，在前面的时序图中就可以发现获取子容器都是通过 request 来分配的。 Wrapper 容器Wrapper 代表一个 Servlet，它负责管理一个 Servlet，包括的 Servlet 的装载、初始化、执行以及资源回收。Wrapper 是最底层的容器，它没有子容器了，所以调用它的 addChild 将会报错。Wrapper 的实现类是 StandardWrapper，StandardWrapper 还实现了拥有一个 Servlet 初始化信息的 ServletConfig，由此看出 StandardWrapper 将直接和 Servlet 的各种信息打交道。 Tomcat 还有其它重要的组件，如安全组件 security、logger 日志组件、session、mbeans、naming 等其它组件。这些组件共同为 Connector 和 Container 提供必要的服务。 Tomcat Server处理一个HTTP请求的过程 1、用户点击网页内容，请求被发送到本机端口8080，被在那里监听的Coyote HTTP/1.1 Connector获得。2、Connector把该请求交给它所在的Service的Engine来处理，并等待Engine的回应。3、Engine获得请求localhost/test/index.jsp，匹配所有的虚拟主机Host。4、Engine匹配到名为localhost的Host（即使匹配不到也把请求交给该Host处理，因为该Host被定义为该Engine的默认主机），名为localhost的Host获得请求/test/index.jsp，匹配它所拥有的所有的Context。Host匹配到路径为/test的Context（如果匹配不到就把该请求交给路径名为“ ”的Context去处理）。5、path=“/test”的Context获得请求/index.jsp，在它的mapping table中寻找出对应的Servlet。Context匹配到URL PATTERN为*.jsp的Servlet,对应于JspServlet类。6、构造HttpServletRequest对象和HttpServletResponse对象，作为参数调用JspServlet的doGet（）或doPost（）.执行业务逻辑、数据存储等程序。7、Context把执行完之后的HttpServletResponse对象返回给Host。8、Host把HttpServletResponse对象返回给Engine。9、Engine把HttpServletResponse对象返回Connector。10、Connector把HttpServletResponse对象返回给客户Browser。 Java 序列化和反序列化补充阅读 序列化是指 把 Java 对象字节序列化的过程，就是说将原本保存在 内存 中的对象，以字节序列的形式，保存到 硬盘 (或数据库等) 中。当需要使用时，再 反序列化 恢复到内存中使用。 如何实现只要对象实现了 Serializable 接口，这个对象就可以通过如下方法进行序列化和反序列化 ( 注意： Serializable 接口仅仅是一个标记接口，里面没有任何方法 )： 序列化： 1234// 创建一个 OutputStream 流并将其封装在一个 ObjectOutputStream 对象内。ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream("worm.out"));// 调用 ObjectOutputStream 对象的 writeObject() 方法，即可将对象 wa 序列化。out.writeObject(wa); 反序列化： 1234// 创建一个 InputStream 流并将其封装在一个 ObjectInputStream 对象内。ObjectInputStream in = new ObjectInputStream(new FileInputStream("worm.out"));// 调用 ObjectInputStream 对象的 readObject() 方法，可获得一个向上转型的 Object 对象引用，然后将获得的 Object 对象向下转型即可。Worm newWa = (Worm) in.readObject(); 注意： 将一个对象从它的序列化状态恢复出来所需要的必要条件：保证 Java JVM 能够找到相关的 .class 文件，否则会抛出 ClassNotFoundException 异常。 被 static 修饰的字段是无法被序列化的，因为它根本就不保存在对象中，而是保存在方法区中，如果想要序列化 static 值，必须自己手动去实现，并手动调用方法，一般会在类中加上 serializeStaticState(ObjectOutputStream os) 和 deserializeStaticState(ObjectInputStream os) 这两个方法用来序列化 static 字段。 1234567891011121314151617181920class Square implements Serializable &#123; private static int color = RED; public static void serializeStaticState(ObjectOutputStream os) throws IOException &#123; os.writeInt(color); // 在这里 &#125; public static void deserializeStaticState(ObjectInputStream os) throws IOException &#123; color = os.readInt(); &#125;&#125;// 在序列化和反序列化 Square 对象时的时候需要手动调用：// 序列化：ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream("square.out"));Square.serializeStaticState(out); // 在 writeObject 前先把 static 变量的值写道 Output 流中out.writeObject(sq);// 反序列化：ObjectInputStream in = new ObjectInputStream(new FileInputStream("square.out"));Square.deserializeStaticState(in); // 在 readObject 前先把 static 变量的值从 Input 流中取出来Square newSq = (Square) in.readObject(); * 另外要注意安全问题，序列化也会将 private 数据保存下来，必要的时候可以把敏感数据用 transient 关键字修饰，防止其被序列化。一旦变量被 transient 修饰，变量将不再是对象持久化的一部分，在对象反序列化后，transient 修饰的变量被设为初始值，即 int 型数据的值为 0，对象型数据为 null。 另一种实现序列化和反序列化的方法：实现 ExternalSerializable 接口。 序列化的控制：Externalizable 接口Externalizable 接口继承自 Serializable 接口，同时增加了两个方法：writeExternal()和readExternal()，这两个方法会在序列化和反序列化还原的过程中被自动调用，我们可以在writeExternal()中将来自对象的重要信息写入，然后在readExternal()中恢复数据。(默认是不写入任何成员对象的) 对比 transient 关键字： Externalizable 接口：选择要进行序列化的字段进行序列化操作。 transient 关键字：选择不要进行序列化的字段取消序列化操作。 对比 Serializable 接口： Externalizable 接口：会调用普通的默认构造器，因此必须有 public 的默认构造器，否则会抛出异常。相当于新 new 了一个对象，然后把writeExternal()中进行序列化的成员变量进行重新赋值。 Serializable 接口：对象完全以它存储的二进制位为基础来构造，不调用构造器。 Externalizable 接口的替代方法Externalizable 接口使用起来较为麻烦，我们可以实现 Serializable 接口，并添加 (是”添加”，既不是”覆盖”也不是”实现”) 如下两个方法： 12private void writeObject(ObjectOutputStream stream) throws IOException &#123; ... &#125;private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException &#123; ... &#125; ObjectOutputStream 和 ObjectInputStream 对象的 writeObject() 和 readObject() 方法会调用我们的对象的 writeObject() 和 readObject() 方法。 既然如此我们为什么不为这两个方法写个接口呢？因为这两个方法是 private 的，而接口中定义的东西都是默认 public 的，所以只能是”添加”这两个方法了。 在我们的 writeObject() 和 readObject() 方法中，可以调用 defaultWriteObject() 和 defaultReadObject() 方法来选择执行默认的 writeObject() 和 readObject() 方法，比较方便好用。 注意： 如果我们打算使用默认机制写入对象的非 transient 部分，那么必须调用 defaultWriteObject() 作为 writeObject() 的第一个操作，调用 defaultReadObject() 作为 readObject() 的第一个操作。 常见的序列化协议 XML： XML是一种常用的序列化和反序列化协议，具有跨机器，跨语言等优点，并且可读性强。 JSON： 这种 Associative array 格式非常符合工程师对对象的理解。 它保持了XML的人眼可读（Human-readable）的优点。 相对于XML而言，序列化后的数据更加简洁。 它具备 Javascript 的先天性支持，所以被广泛应用于 Web browser 的应用常景中，是 Ajax 的事实标准协议。 与 XML 相比，其协议比较简单，解析速度比较快。 松散的 Associative array 使得其具有良好的可扩展性和兼容性。 Java 动态代理代理模式（JDK代理）设计模式 JDK动态代理所用到的代理类在程序调用到代理类对象时才由JVM真正创建，JVM根据传进来的 业务实现类对象 以及 方法名 ，动态地创建了一个代理类的class文件并被字节码引擎执行，然后通过该代理类对象进行方法调用。我们需要做的，只需指定代理类的预处理、调用后操作即可。 1：首先，定义业务逻辑接口1234public interface BookFacade &#123; public void addBook(); &#125; 2：然后，实现业务逻辑接口创建业务实现类 12345678public class BookFacadeImpl implements BookFacade &#123; @Override public void addBook() &#123; System.out.println("增加图书方法。。。"); &#125; &#125; 3：最后，实现 调用管理接口InvocationHandler 创建动态代理类 123456789101112131415161718192021222324252627public class BookFacadeProxy implements InvocationHandler &#123; private Object target;//这其实业务实现类对象，用来调用具体的业务方法 /** * 绑定业务对象并返回一个代理类 */ public Object bind(Object target) &#123; this.target = target; //接收业务实现类对象参数 //通过反射机制，创建一个代理类对象实例并返回。用户进行方法调用时使用 //创建代理对象时，需要传递该业务类的类加载器（用来获取业务实现类的元数据，在包装方法是调用真正的业务方法）、接口、handler实现类 return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), this); &#125; /** * 包装调用方法：进行预处理、调用后处理 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object result=null; System.out.println("预处理操作——————"); //调用真正的业务方法 result=method.invoke(target, args); System.out.println("调用后处理——————"); return result; &#125; &#125; 4：在使用时，首先创建一个业务实现类对象和一个代理类对象，然后定义接口引用（这里使用向上转型）并用代理对象.bind(业务实现类对象)的返回值进行赋值。最后通过接口引用对象调用业务方法即可。（接口引用真正指向的是一个绑定了业务类的代理类对象，所以通过接口方法名调用的是被代理的方法们） 1234567public static void main(String[] args) &#123; BookFacadeImpl bookFacadeImpl=new BookFacadeImpl(); BookFacadeProxy proxy = new BookFacadeProxy(); BookFacade bookfacade = (BookFacade) proxy.bind(bookFacadeImpl); bookfacade.addBook(); &#125; JDK动态代理的代理对象在创建时，需要使用业务实现类所实现的接口作为参数（因为在后面代理方法时需要根据接口内的方法名进行调用）。如果业务实现类是没有实现接口而是直接定义业务方法的话，就无法使用JDK动态代理了。并且，如果业务实现类中新增了接口中没有的方法，这些方法是无法被代理的（因为无法被调用）。 CGLIB代理CGLIB（Code Generator Library）是一个强大的、高性能的代码生成库。其被广泛应用于AOP框架（Spring、dynaop）中，用以提供方法拦截操作。 cglib是针对类来实现代理的，原理是对指定的业务类生成一个子类，并覆盖其中业务方法实现代理。因为采用的是继承，所以不能对final修饰的类进行代理。 1：首先定义业务类，无需实现接口（当然，实现接口也可以，不影响的） 1234567public class BookFacadeImpl1 &#123; public void addBook() &#123; System.out.println("新增图书..."); &#125; &#125; 2：实现 MethodInterceptor方法代理接口，创建代理类 12345678910111213141516171819202122public class BookFacadeCglib implements MethodInterceptor &#123; private Object target;//业务类对象，供代理方法中进行真正的业务方法调用 //相当于JDK动态代理中的绑定 public Object getInstance(Object target) &#123; this.target = target; //给业务对象赋值 Enhancer enhancer = new Enhancer(); //创建加强器，用来创建动态代理类 enhancer.setSuperclass(this.target.getClass()); //为加强器指定要代理的业务类（即：为下面生成的代理类指定父类） //设置回调：对于代理类上所有方法的调用，都会调用CallBack，而Callback则需要实现intercept()方法进行拦 enhancer.setCallback(this); // 创建动态代理类对象并返回 return enhancer.create(); &#125; // 实现回调方法 public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println("预处理——————"); proxy.invokeSuper(obj, args); //调用业务类（父类中）的方法 System.out.println("调用后操作——————"); return null; &#125; 3：创建业务类和代理类对象，然后通过 代理类对象.getInstance(业务类对象) 返回一个动态代理类对象（它是业务类的子类，可以用业务类引用指向它）。最后通过动态代理类对象进行方法调用。 1234567public static void main(String[] args) &#123; BookFacadeImpl1 bookFacade=new BookFacadeImpl1()； BookFacadeCglib cglib=new BookFacadeCglib(); BookFacadeImpl1 bookCglib=(BookFacadeImpl1)cglib.getInstance(bookFacade); bookCglib.addBook(); &#125; 静态代理是通过在代码中显式定义一个业务实现类一个代理，在代理类中对同名的业务方法进行包装，用户通过代理类调用被包装过的业务方法； JDK动态代理是通过接口中的方法名，在动态生成的代理类中调用业务实现类的同名方法； CGlib动态代理是通过继承业务类，生成的动态代理类是业务类的子类，通过重写业务方法进行代理； Java注解深入理解JAVA注解 要深入学习注解，我们就必须能定义自己的注解，并使用注解，在定义自己的注解之前，我们就必须要了解Java为我们提供的元注解和相关定义注解的语法。 元注解（meta-annotation）：元注解的作用就是负责注解其他注解。Java5.0定义了4个标准的meta-annotation类型，它们被用来提供对其它 annotation类型作说明。Java5.0定义的元注解： 1.@Target, 2.@Retention, 3.@Documented, 4.@Inherited 这些类型和它们所支持的类在java.lang.annotation包中可以找到。下面我们看一下每个元注解的作用和相应分参数的使用说明。 @Target： @Target说明了Annotation所修饰的对象范围：Annotation可被用于 packages、types（类、接口、枚举、Annotation类型）、类型成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数）。在Annotation类型的声明中使用了target可更加明晰其修饰的目标。 作用：用于描述注解的使用范围（即：被描述的注解可以用在什么地方） 取值(ElementType)有： 1.CONSTRUCTOR:用于描述构造器 2.FIELD:用于描述域 3.LOCAL_VARIABLE:用于描述局部变量 4.METHOD:用于描述方法 5.PACKAGE:用于描述包 6.PARAMETER:用于描述参数 7.TYPE:用于描述类、接口(包括注解类型) 或enum声明 使用实例： 123456789101112@Target(ElementType.TYPE)public @interface Table &#123; /** * 数据表名称注解，默认值为类名称 * @return */ public String tableName() default "className";&#125;@Target(ElementType.FIELD)public @interface NoDBColumn &#123;&#125; 注解Table 可以用于注解类、接口(包括注解类型) 或enum声明,而注解NoDBColumn仅可用于注解类的成员变量。 @Retention： @Retention定义了该Annotation被保留的时间长短：某些Annotation仅出现在源代码中，而被编译器丢弃；而另一些却被编译在class文件中；编译在class文件中的Annotation可能会被虚拟机忽略，而另一些在class被装载时将被读取（请注意并不影响class的执行，因为Annotation与class在使用上是被分离的）。使用这个meta-Annotation可以对 Annotation的“生命周期”限制。 作用：表示需要在什么级别保存该注释信息，用于描述注解的生命周期（即：被描述的注解在什么范围内有效） 取值（RetentionPoicy）有： 1.SOURCE:在源文件中有效（即源文件保留） 2.CLASS:在class文件中有效（即class保留） 3.RUNTIME:在运行时有效（即运行时保留） Retention meta-annotation类型有唯一的value作为成员，它的取值来自java.lang.annotation.RetentionPolicy的枚举类型值。具体实例如下： 123456789@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface Column &#123; public String name() default "fieldName"; public String setFuncName() default "setField"; public String getFuncName() default "getField"; public boolean defaultDBValue() default false;&#125; Column注解的的RetentionPolicy的属性值是RUTIME,这样注解处理器可以通过反射，获取到该注解的属性值，从而去做一些运行时的逻辑处理 @Documented: @Documented用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。Documented是一个标记注解，没有成员。 12345678910@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Column &#123; public String name() default "fieldName"; public String setFuncName() default "setField"; public String getFuncName() default "getField"; public boolean defaultDBValue() default false;&#125; @Inherited： @Inherited 元注解是一个标记注解，@Inherited阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类。 注意：@Inherited annotation类型是被标注过的class的子类所继承。类并不从它所实现的接口继承annotation，方法并不从它所重载的方法继承annotation。 当@Inherited annotation类型标注的annotation的Retention是RetentionPolicy.RUNTIME，则反射API增强了这种继承性。如果我们使用java.lang.reflect去查询一个@Inherited annotation类型的annotation时，反射代码检查将展开工作：检查class和其父类，直到发现指定的annotation类型被发现，或者到达类继承结构的顶层。 实例代码： 1234567@Inheritedpublic @interface Greeting &#123; public enum FontColor&#123; BULE,RED,GREEN&#125;; String name(); FontColor fontColor() default FontColor.GREEN;&#125; 自定义注解： 使用@interface自定义注解时，自动继承了java.lang.annotation.Annotation接口，由编译程序自动完成其他细节。在定义注解时，不能继承其他的注解或接口。@interface用来声明一个注解，其中的每一个方法实际上是声明了一个配置参数。方法的名称就是参数的名称，返回值类型就是参数的类型（返回值类型只能是基本类型、Class、String、enum）。可以通过default来声明参数的默认值。 定义注解格式： public @interface 注解名 {定义体} 注解参数的可支持数据类型： 1.所有基本数据类型（int,float,boolean,byte,double,char,long,short) 2.String类型 3.Class类型 4.enum类型 5.Annotation类型 6.以上所有类型的数组 Annotation类型里面的参数该怎么设定: 第一,只能用public或默认(default)这两个访问权修饰.例如,String value();这里把方法设为defaul默认类型； 第二,参数成员只能用基本类型byte,short,char,int,long,float,double,boolean八种基本数据类型和 String,Enum,Class,annotations等数据类型,以及这一些类型的数组.例如,String value();这里的参数成员就为String; 第三,如果只有一个参数成员,最好把参数名称设为”value”,后加小括号.例:下面的例子FruitName注解就只有一个参数成员。 简单的自定义注解和使用注解实例： 1234567891011121314151617181920212223242526272829303132333435363738package annotation;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/** * 水果名称注解 */@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface FruitName &#123; String value() default "";&#125;/** * 水果颜色注解 * */@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface FruitColor &#123; /** * 颜色枚举 */ public enum Color&#123; BULE,RED,GREEN&#125;; /** * 颜色属性 * @return */ Color fruitColor() default Color.GREEN;&#125; 1234567891011121314151617181920212223242526272829package annotation;import annotation.FruitColor.Color;public class Apple &#123; @FruitName("Apple") private String appleName; @FruitColor(fruitColor=Color.RED) private String appleColor; public void setAppleColor(String appleColor) &#123; this.appleColor = appleColor; &#125; public String getAppleColor() &#123; return appleColor; &#125; public void setAppleName(String appleName) &#123; this.appleName = appleName; &#125; public String getAppleName() &#123; return appleName; &#125; public void displayName() &#123; System.out.println("水果的名字是：苹果"); &#125;&#125; 注解元素的默认值： 注解元素必须有确定的值，要么在定义注解的默认值中指定，要么在使用注解时指定，非基本类型的注解元素的值不可为null。因此, 使用空字符串或0作为默认值是一种常用的做法。这个约束使得处理器很难表现一个元素的存在或缺失的状态，因为每个注解的声明中，所有元素都存在，并且都具有相应的值，为了绕开这个约束，我们只能定义一些特殊的值，例如空字符串或者负数，一次表示某个元素不存在，在定义注解时，这已经成为一个习惯用法。例如： 123456789101112131415161718192021222324252627282930package annotation;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/** * 水果供应者注解 */@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface FruitProvider &#123; /** * 供应商编号 * @return */ public int id() default -1; /** * 供应商名称 * @return */ public String name() default ""; /** * 供应商地址 * @return */ public String address() default "";&#125; 定义了注解，并在需要的时候给相关类，类属性加上注解信息，如果没有响应的注解信息处理流程，注解可以说是没有实用价值。如何让注解真真的发挥作用，主要就在于注解处理方法，下一步我们将学习注解信息的获取和处理！ 注解的使用 第一步：新建一个annotation，名字为：MyAnnotation.java。 1234567891011121314package com.dragon.test.annotation;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface MyAnnotation&#123; String hello () default "hello"; String world();&#125; 第二步：建立一个MyTest.java 来使用上面的annotation。 12345678package com.dragon.test.annotation;public class MyTest&#123; @MyAnnotation(hello = "Hello,Beijing",world = "Hello,world") public void output() &#123; System.out.println("method output is running "); &#125;&#125; 第三步：用反射机制来调用注解中的内容 1234567891011121314151617181920212223242526272829303132333435package com.dragon.test.annotation;import java.lang.annotation.Annotation;import java.lang.reflect.Method;public class MyReflection&#123; public static void main(String[] args) throws Exception &#123; // 获得要调用的类 Class&lt;MyTest&gt; myTestClass = MyTest.class; // 获得要调用的方法，output是要调用的方法名字，new Class[]&#123;&#125;为所需要的参数。空则不是这种 Method method = myTestClass.getMethod("output", new Class[]&#123;&#125;); // 是否有类型为MyAnnotation的注解 if (method.isAnnotationPresent(MyAnnotation.class)) &#123; // 获得注解 MyAnnotation annotation = method.getAnnotation(MyAnnotation.class); // 调用注解的内容 System.out.println(annotation.hello()); System.out.println(annotation.world()); &#125; System.out.println("----------------------------------"); // 获得所有注解。必须是runtime类型的 Annotation[] annotations = method.getAnnotations(); for (Annotation annotation : annotations) &#123; // 遍历所有注解的名字 System.out.println(annotation.annotationType().getName()); &#125; &#125;&#125;Hello,BeijingHello,world----------------------------------com.dragon.test.annotation.MyAnnotation 通过接口引用对象对于参数类型，要优先使用接口而不是类。通俗地讲，应该优先使用接口而不是类来引用对象。如果有合适的接口类型存在，那么对于参数、返回值、变量和域来说，就应该使用接口类型来声明。只有当你利用构造函数创建某个对象的时候，才真正引用这个对象的类。 考虑Vector的情形，它是List接口的一个实现，在声明变量的时候应该养成这样的习惯： 12//Good - use interface as typeList&lt;？&gt; list= new Vector&lt;？&gt;(); 而不是像这样的声明： 12//Bad - use class as typeVector&lt;?&gt; list= new Vector&lt;?&gt;(); 优点： 假如一个类实现了多个接口,那么用接口类型来定义它的引用变量的话,一眼就可以明白,这里是需要这个类的哪些方法。 程序更加灵活。当你决定更换实现时，只需要改变构造器中类的名称。其他使用list地方的代码根本不需要改动。第一个声明可以被改变为：1List&lt;?&gt; list= new ArrayList&lt;?&gt;(); 注意：list只能使用ArrayList已经实现了的List接口中的方法，ArrayList中那些自己的、没有在List接口中定义的方法是不可以被访问到的。List.add其实是List接口的方法，但是调用ArrayList方法如clone（）方法是调用不到的。 适合于用类来引用对象的情形： 如果没有合适的接口存在，可以用类来引用对象。例如，考虑值类（String、BigInteger）很少用多个实现编写，他们通常是final的，并且很少有对应的接口。使用这种值类作为参数、变量、域或者返回值类型就比较合适。 对象属于一个框架，而框架的基本类型是类，不是接口。（对象属于基于类的框架）例如java.util.TimerTask抽象类。应该用相关的基类（往往是抽象类）来引用对象，而不是它的实现类。 类实现了接口，但是它提供了接口中不存在的额外方法。例如LinkedHashMap，程序依赖于这些额外的方法，这种类就应该只被用来引用它的实例。 以上这些例子并不全面，而只是代表了一些“适合于用类来引用对象”的情形 总结：给定的对象是否具有适当的接口应该是很明显的。如果是，用接口引用对象就会使程序更加灵活；如果不是，则使用类层次结构中提供了必要功能的最基础的类。 作者：真爱也枉然链接：https://www.jianshu.com/p/a5fa6300177e来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式---基于Java详解(全)]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[创建型模式： 对象实例化的模式，创建型模式用于解耦对象的实例化过程。 单例模式：某个类只能有一个实例，提供一个全局的访问点。 简单工厂模式：一个工厂类根据传入的参量决定创建出那一种产品类的实例。 工厂方法模式：定义一个创建对象的接口，让子类决定实例化那个类。 抽象工厂模式：创建相关或依赖对象的家族，而无需明确指定具体类。 建造者模式：封装一个复杂对象的构建过程，并可以按步骤构造。 原型模式：通过复制现有的实例来创建新的实例。 结构型模式： 把类或对象结合在一起形成一个更大的结构。 适配器模式：将一个类的方法接口转换成客户希望的另外一个接口。 外观模式：对外提供一个统一的方法，来访问子系统中的一群接口。 组合模式：将对象组合成树形结构以表示“”部分-整体“”的层次结构。 装饰模式：动态的给对象添加新的功能。 代理模式：为其他对象提供一个代理以便控制这个对象的访问。 享元模式：通过共享技术来有效的支持大量细粒度的对象。 桥接模式：将抽象部分和它的实现部分分离，使它们都可以独立的变化。 行为型模式： 类和对象如何交互，及划分责任和算法。 模板模式：定义一个算法结构，而将一些步骤延迟到子类实现。 迭代器模式：一种遍历访问聚合对象中各个元素的方法，不暴露该对象的内部结构。 策略模式：定义一系列算法，把他们封装起来，并且使它们可以相互替换。 状态模式：允许一个对象在其对象内部状态改变时改变它的行为。 观察者模式：对象间的一对多的依赖关系。 解释器模式：给定一个语言，定义它的文法的一种表示，并定义一个解释器。 备忘录模式：在不破坏封装的前提下，保持对象的内部状态。 中介者模式：用一个中介对象来封装一系列的对象交互。 命令模式：将命令请求封装为一个对象，使得可以用不同的请求来进行参数化。 访问者模式：在不改变数据结构的前提下，增加作用于一组对象元素的新功能。 责任链模式：将请求的发送者和接收者解耦，使的多个对象都有处理这个请求的机会。 设计模式 根据目的分类： 创建型，：单例模式 结构型，：适配器模式（一个接口与其他接口兼容）、桥接（将对象的抽象和其实现分离）、组合（如何构造一个类层次式结构，部分-整体）、代理（通过提 供与对象相同的接口来控制对这个对象的访问）、享元（运用共享技术有效支持大量细粒度对象，解决使用大量对象造成很大存储开销的情况，内蕴，外蕴）、外观（如何用单个对象表示整个子系统）、装饰模式（描述如何动态为对象添加职责） 行为型 命令模式（将请求封装在对象中作为参数传递）、策略模式（将算法封装在对象中，以方便地指定一个对象所使用的算法，灵活地添加对同一问题的不同处理方案。用于多个类只区别于表现行为不同） 备忘录模式（保存和跟踪对象的状态，originator,memento） 状态模式（替换ifelse ，context，state, concrete state) 生成器模式（将一个复杂对象的构建与其表示分离，用于构建复杂对象，对象不同表示） 抽象工厂（为gui定义不同平台的并行类层次结构） 关联的多重度是指一个类的实例能与另一个类的多少个实例相关联。 候选类的选择运用良性依赖原则，不会在实际中造成危害的依赖关系。 候选类的删除运用接口隔离原则（ISP）。不应该强迫客户依赖于他们不用的方法，接口属于客户，不属于它所在的类层次结构。 原型模式：用原型实体指定创建对象的种类，并且通过复制这些原型创建新的对象。 创建者模式单例模式单例模式是一种对象创建型模式，使用单例模式，可以保证为一个类只生成唯一的实例对象。也就是说，在整个程序空间中，该类只存在一个实例对象。GoF对单例模式的定义是：保证一个类、只有一个实例存在，同时提供能对该实例加以访问的全局访问方法。 应用场景： 在多个线程之间，比如servlet环境，共享同一个资源或者操作同一个对象 在整个程序空间使用全局变量，共享资源 大规模系统中，为了性能的考虑，需要节省对象的创建时间等等。 优点 于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级的对象而言，是很重要的. 不需要频繁创建对象，GC压力也减轻了，而在GC中会有STW(stop the world)，从这一方面也节约了GC的时间 缺点 简单的单例模式设计开发都比较简单，但是复杂的单例模式需要考虑线程安全等并发问题，引入了部分复杂度。 设计单例模式的时候一般需要考虑几种因素:线程安全；延迟加载 ；代码安全:如防止序列化攻击，防止反射攻击(防止反射进行私有方法调用) ；性能因素 饿汉式 static final field123456789public class Singleton&#123; private static final Singleton instance = new Singleton(); // 类加载时就初始化 private Singleton()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125;&#125; Singleton类被加载的时候就会被初始化，java虚拟机规范虽然没有强制性约束在什么时候开始类加载过程，但是对于类的初始化，虚拟机规范则严格规定了有且只有四种情况必须立即对类进行初始化，遇到new、getStatic、putStatic或invokeStatic这4条字节码指令时，如果类没有进行过初始化，则需要先触发其初始化。 生成这4条指令最常见的java代码场景是： 使用new关键字实例化对象 读取一个类的静态字段（被final修饰、已在编译期把结果放在常量池的静态字段除外） 设置一个类的静态字段（被final修饰、已在编译期把结果放在常量池的静态字段除外） 调用一个类的静态方法 这个知识点详见《Java虚拟机的类加载机制》 懒汉式（线程不安全）12345678910111213public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) //只有为空才new，很懒，饿汉一上来就new不管怎样。 &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 懒汉式（线程安全）12345678public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance;&#125; 双重检验锁 double check lock12345678910111213141516171819public class Singleton &#123; private volatile static Singleton instance; // 声明成 volatile private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 延迟初始化。synchronized同步块里面能够保证只创建一个对象。但是通过在synchronized的外面增加一层判断，就可以在对象一经创建以后，不再进入synchronized同步块。这种方案不仅减小了锁的粒度，保证了线程安全，性能方面也得到了大幅提升。volatile在这里是用来防止指令重排序的。 volatile知识点详见《Java多线程与并发》 静态内部类 static nested class123456789101112public class Singleton &#123; private static class InnerClass//这个对象的初始化锁，看哪个线程能拿到 &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return InnerClass.INSTANCE; &#125;&#125; 基于类初始化的延迟加载解决方案。（既保证了线程的安全，有能够延迟加载，也就是在第一次使用的时候加载）一个线程初始化时，另一个线程会被锁到Class对象的初始化那里，无法进入到 分配对象的内存空间 设置instance指向内存空间 初始化对象这些步骤里。这种写法仍然使用 JVM 本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒汉式的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。 枚举 Enum123456789101112131415161718192021222324252627282930313233public enum Singleton &#123; INSTANCE; private String objName; public String getObjName() &#123; return objName; &#125; public void setObjName(String objName) &#123; this.objName = objName; &#125; public static void main(String[] args) &#123; // 单例测试 Singleton firstSingleton = Singleton.INSTANCE; firstSingleton.setObjName("firstName"); System.out.println(firstSingleton.getObjName()); Singleton secondSingleton = Singleton.INSTANCE; secondSingleton.setObjName("secondName"); System.out.println(firstSingleton.getObjName()); System.out.println(secondSingleton.getObjName()); // 反射获取实例测试 try &#123; Singleton[] enumConstants = Singleton.class.getEnumConstants(); for (Singleton enumConstant : enumConstants) &#123; System.out.println(enumConstant.getObjName()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; enum没有无参构造器，所以无法用反射方法破坏单例。 深度分析Java的枚举类型—-枚举的线程安全性及序列化问题 123456789101112131415161718192021222324252627282930313233343536373839404142public enum t &#123; SPRING,SUMMER,AUTUMN,WINTER;&#125;反编译public final class T extends Enum&#123; private T(String s, int i) &#123; super(s, i); &#125; public static T[] values() &#123; T at[]; int i; T at1[]; System.arraycopy(at = ENUM$VALUES, 0, at1 = new T[i = at.length], 0, i); return at1; &#125; public static T valueOf(String s) &#123; return (T)Enum.valueOf(demo/T, s); &#125; public static final T SPRING; public static final T SUMMER; public static final T AUTUMN; public static final T WINTER; private static final T ENUM$VALUES[]; static &#123; SPRING = new T("SPRING", 0); SUMMER = new T("SUMMER", 1); AUTUMN = new T("AUTUMN", 2); WINTER = new T("WINTER", 3); ENUM$VALUES = (new T[] &#123; SPRING, SUMMER, AUTUMN, WINTER &#125;); &#125;&#125; 序列化破坏单例模式原理解析及解决方案反序列化的时候是用反射创建的，所以不一样 解决方法，在类里加个方法， readResolve这个名字是ObjectStream规定好的 123private Object readResolve()&#123; return hungrySingleton;&#125; 反射攻击解决方案及原理分析通过反射来修改获取、修改构造函数，然后获取方法 12345Class objectClass = HungrySingleton.class;Constructor constructor = objectClass.getDeclaredConstructor(HungrySingleton.class);constructor.setAccessible(true);HungrySingleton instance = HungrySingleton.getInstance();HungrySingleton newInstance = (HungrySingleton) constructor.newInstance();//调用无参构造方法，对象实例化 123456789101112131415在类加载的时候就初始化好对象的，两种private HungrySingleton()&#123; if(hungrySingleton != null)&#123; throw new RuntimeException("单例构造器禁止反射调用"); &#125;&#125;private StaticInnerClassSingleton()&#123; if(InnerClass.staticInnerClassSingleton != null)&#123; throw new RuntimeException("单例构造器禁止反射调用"); &#125;&#125; 对于懒汉，这种后初始化对象的，如果反射先进来，就会有两个对象，在构造器里加判断也没有；反射后进来这种方法还有用。 容器单例12345678910111213141516171819public class ContainerSingleton &#123; private ContainerSingleton()&#123;&#125; private static Map&lt;String,Object&gt; singletonMap = new HashMap&lt;String,Object&gt;(); public static void putInstance(String key,Object instance) &#123; if(StringUtils.isNotBlank(key) &amp;&amp; instance != null) &#123; if(!singletonMap.containsKey(key)) &#123; singletonMap.put(key,instance); &#125; &#125; &#125; public static Object getInstance(String key) &#123; return singletonMap.get(key); &#125;&#125; 这种方式实现的单例是线程不安全的。如果需要线程安全的可以使用HashTable但是HashTable每次存取都会加上同步锁，性能损耗比较严重。或者使用ConcurrentHashMap。 ThreadLocal线程单例这个单例严格意义上讲并不完全算是单例，它只能算在单个线程中的单例，也就是在同一个线程中的它是单例的。 工厂方法模式简单工厂 简单工厂并不是一个设计模式，而是一种编程习惯。 logger、calendar等类是这个模式 作为抽象工厂模式的孪生兄弟，工厂方法模式定义了一个创建对象的接口，但由子类决定要实例化的类是哪一个，也就是说工厂方法模式让实例化推迟到子类。 工厂方法模式非常符合“开闭原则”，当需要增加一个新的产品时，我们只需要增加一个具体的产品类和与之对应的具体工厂即可，无须修改原有系统。同时在工厂方法模式中用户只需要知道生产产品的具体工厂即可，无须关系产品的创建过程，甚至连具体的产品类名称都不需要知道。虽然他很好的符合了“开闭原则”，但是由于每新增一个新产品时就需要增加两个类，这样势必会导致系统的复杂度增加。 Collection的Iterator方法是工厂方法模式。 抽象工厂模式 所谓抽象工厂模式就是提供一个接口，用于创建相关或者依赖对象的家族，而不需要明确指定具体类。他允许客户端使用抽象的接口来创建一组相关的产品，而不需要关系实际产出的具体产品是什么。这样一来，客户就可以从具体的产品中被解耦。 它的优点是隔离了具体类的生成，使得客户端不需要知道什么被创建了，而缺点就在于新增新的行为会比较麻烦，因为当添加一个新的产品对象时，需要更加需要更改接口及其下所有子类。 建造者模式 对于建造者模式而已，它主要是将一个复杂对象的构建与表示分离，使得同样的构建过程可以创建不同的表示。适用于那些产品对象的内部结构比较复杂。 建造者模式将复杂产品的构建过程封装分解在不同的方法中，使得创建过程非常清晰，能够让我们更加精确的控制复杂产品对象的创建过程，同时它隔离了复杂产品对象的创建和使用，使得相同的创建过程能够创建不同的产品。但是如果某个产品的内部结构过于复杂，将会导致整个系统变得非常庞大，不利于控制，同时若几个产品之间存在较大的差异，则不适用建造者模式，毕竟这个世界上存在相同点大的两个产品并不是很多，所以它的使用范围有限。 原型模式 结构型模式适配器模式 类适配器（继承实现） 通过多重继承目标接口和被适配者类方式来实现适配。 多重继承，其中继承的目标接口部分达到适配目的，而继承被适配者类的部分达到通过调用被适配者类里的方法来实现目标接口的功能。但是java是不支持多重继承的，但是可以继承类然后继承接口，算是间接的实现了多重继承吧。 对象适配器（委让实现） 对象适配器和类适配器使用了不同的方法实现适配，对象适配器使用组合，类适配器使用继承. 在我们的应用程序中我们可能需要将两个不同接口的类来进行通信，在不修改这两个的前提下我们可能会需要某个中间件来完成这个衔接的过程。这个中间件就是适配器。所谓适配器模式就是将一个类的接口，转换成客户期望的另一个接口。它可以让原本两个不兼容的接口能够无缝完成对接。 作为中间件的适配器将目标类和适配者解耦，增加了类的透明性和可复用性。 这个举一个项目的例子：用火鸡冒充鸭子。就是外在是鸭子，其实内在是火鸡，就是用火鸡冒充鸭子。首先通过类对象适配器的方式来实现 123456789101112131415161718public interface Turkey &#123; public void gobble(); public void fly();&#125;public class WildTurkey implements Turkey &#123; @Override public void gobble() &#123; System.out.println(" Go Go"); &#125; @Override public void fly() &#123; System.out.println("I am flying a short distance"); &#125;&#125; 123456789101112131415161718192021对象适配器public class TuckeyAdapter implements Duck&#123; private Turkey turkey; public TuckeyAdapter(Turkey turkey) &#123; this.turkey = turkey; &#125; @Override public void quack() &#123; turkey.gobble(); &#125; @Override public void fly() &#123; turkey.fly(); turkey.fly(); &#125;&#125; 12345678910111213141516类适配器public class TurkeyAdapter2 extends WildTurkey implements Duck &#123; @Override public void quack() &#123; super.gobble(); &#125; @Override public void fly() &#123; super.fly(); super.fly(); &#125;&#125; 对象适配器模式其实是通过使用对象组合，以修改的接口去包装被适配者，所以他不仅可以适配某个类，而且可以适配该类的任何子类；而类适配器需要多重的继承。 其实就是改了下外在，但是内在不变，有点像装饰者模式哈，因为就像给火鸡做了一层外包装。但是装饰者模式和适配器模式还是不一样的，一方面装饰者模式是可以继承和嵌套超类的，但是这里的适配器模式是只能继承某一接口，并且适配器模式是接口功能的改变，不能扩展，但是装饰者模式是为了实现某些功能的扩展，举个例子吧，比如usb转换器这个东西，我们要给他添加什么电容保护什么功能的时候用装饰者模式，然后我们要将其转换成type C 的接口就要用适配器模式，也就是让他适配type C的功能模式。 Java I/O 库大量使用了适配器模式，如 ByteArrayInputStream 是一个适配器类，它继承了 InputStream 的接口，并且封装了一个 byte 数组。换言之，它将一个 byte 数组的接口适配成 InputStream 流处理器的接口。 在 OutputStream 类型中，所有的原始流处理器都是适配器类。ByteArrayOutputStream 继承了 OutputStream 类型，同时持有一个对 byte 数组的引用。它一个 byte 数组的接口适配成 OutputString 类型的接口，因此也是一个对象形式的适配器模式的应用。 FileOutputStream 继承了 OutputStream 类型，同时持有一个对 FileDiscriptor 对象的引用。这是一个将 FileDiscriptor 接口适配成 OutputStream 接口形式的对象型适配器模式。 Reader 类型的原始流处理器都是适配器模式的应用。StringReader 是一个适配器类，StringReader 类继承了 Reader类型，持有一个对 String 对象的引用。它将 String 的接口适配成 Reader 类型的接口。 外观模式 门面角色：外观模式的核心。它被客户角色调用，它熟悉子系统的功能。内部根据客户角色的需求预定了几种功能的组合。 子系统角色:实现了子系统的功能。它对客户角色和Facade时未知的。它内部可以有系统内的相互交互，也可以由供外界调用的接口。 客户角色:通过调用Facede来完成要实现的功能。 组合模式 装饰模式装饰模式介绍装饰模式又名包装(Wrapper)模式。装饰模式以对客户端透明的方式扩展对象的功能，是继承关系的一个替代方案。就增加功能来说，Decorator模式比生成子类更为灵活。 抽象构件(Component)角色：给出一个抽象接口，以规范准备接收附加责任的对象。 具体构件(ConcreteComponent)角色：定义一个将要接收附加责任的类。 装饰(Decorator)角色：持有一个构件(Component)对象的实例，并定义一个与抽象构件接口一致的接口。 具体装饰(ConcreteDecorator)角色：负责给构件对象“贴上”附加的责任。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// 抽象接口，用来规范将要被附加一些操作的对象interface People&#123; public void wear();&#125;// 具体的对象，该对象将被附加一些额外的操作class Jane implements People&#123; public void wear() &#123; System.out.println("今天该穿什么呢?"); &#125;&#125;// 装饰者类，持有一个将要被装饰的接口对象的实例class Decorator implements People&#123; private People people; public Decorator(People people) &#123; this.people = people; &#125; public void wear() &#123; people.wear(); &#125;&#125;// 具体的装饰者类，负责给增加附加的操作：穿衬衫class DecoratorShirt extends Decorator&#123; public DecoratorShirt(People people) &#123; super(people); &#125; public void wear() &#123; super.wear(); System.out.println("穿个衬衫"); &#125;&#125;// 具体的装饰者类，负责给增加附加的操作：穿西服class DecoratorSuit extends Decorator&#123; public DecoratorSuit(People people) &#123; super(people); &#125; public void wear() &#123; super.wear(); System.out.println("穿个西服"); &#125;&#125;可以方便的组合顺序public class DecoratorTest &#123; public static void main(String[] args) &#123; People p1 = new DecoratorSuit(new DecoratorShirt(new Jane())); p1.wear(); System.out.println("--------------"); People p2 = new DecoratorTShirt(new DecoratorPants(new Jane())); p2.wear(); System.out.println("--------------"); &#125;&#125;顺序 从里到外今天该穿什么呢? 穿个衬衫 穿个西服 ————– 今天该穿什么呢? 穿裤子 穿个T-Shirt 装饰者模式与Java I/O 由上图可知在Java中应用程序通过输入流（InputStream）的Read方法从源地址处读取字节，然后通过输出流（OutputStream）的Write方法将流写入到目的地址。流的来源主要有三种下面的图可以看出Java中的装饰者类和被装饰者类以及它们之间的关系，这里只列出了InputStream中的关系。 代理模式为另一个对象提供一个替身或占位符以控制对这个对象的访问。 Proxy 中有一个 RealSubject 对象，我们拿不到 RealSubject 对象，只能拿到 Proxy 对象； Proxy 和 RealSubject 都实现了 Subject 接口，它们有相同的方法； 我们通过 Proxy 对象调用 RealSubject 对象的方法，不过在调用前，Proxy 会先检查一下这个调用合不合理，不合理它就不调用 RealSubject 对象的方法。 静态代理电影是电影公司委托给影院进行播放的，但是影院可以在播放电影的时候，产生一些自己的经济收益，比如卖爆米花、可乐等，然后在影片开始结束时播放一些广告。 123456789101112131415161718192021222324252627282930313233343536373839public interface Movie &#123; void play();&#125;public class RealMovie implements Movie &#123; @Override public void play() &#123; // TODO Auto-generated method stub System.out.println("您正在观看电影 《肖申克的救赎》"); &#125;&#125; 代理 public class Cinema implements Movie &#123; RealMovie movie; public Cinema(RealMovie movie) &#123; super(); this.movie = movie; &#125; @Override public void play() &#123; guanggao(true); movie.play(); guanggao(false); &#125; public void guanggao(boolean isStart) &#123; if ( isStart ) &#123; System.out.println("爆米花！"); &#125; else &#123; System.out.println("可乐！"); &#125; &#125;&#125; 代理模式可以在不修改被代理对象的基础上，通过扩展代理类，进行一些功能的附加与增强。值得注意的是，代理类和被代理类应该共同实现一个接口，或者是共同继承某个类。上面介绍的是静态代理的内容，为什么叫做静态呢？因为它的类型是事先预定好的，比如上面代码中的 Cinema 这个类。下面要介绍的内容就是动态代理。 动态代理上一节代码中 Cinema 类是代理，我们需要手动编写代码让 Cinema 实现 Movie 接口，而在动态代理中，我们可以让程序在运行的时候自动在内存中创建一个实现 Movie 接口的代理，而不需要去定义 Cinema 这个类。这就是它被称为动态的原因。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public interface Subject &#123; public void doSomething();&#125;public class RealSubject implements Subject &#123; public void doSomething() &#123; System.out.println("call doSomething()"); &#125;&#125;代理代理类及其实例是程序自动生成的，因此我们不需要手动去创建代理类public class ProxyHandler implements InvocationHandler&#123; private Object proxied; // 被代理对象 public ProxyHandler(Object proxied) &#123; this.proxied = proxied; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; // 在转调具体目标对象之前，可以执行一些功能处理 System.out.println("前置增强处理： yoyoyo..."); // 转调具体目标对象的方法(三要素：实例对象 + 实例方法 + 实例方法的参数) Object obj = method.invoke(proxied, args); // 在转调具体目标对象之后，可以执行一些功能处理 System.out.println("后置增强处理：hahaha..."); return obj; &#125;&#125;public class Test &#123; public static void main(String args[]) &#123; // 真实对象real Subject real = new RealSubject(); // 生成real的代理对象 Subject proxySubject = (Subject) Proxy.newProxyInstance( Subject.class.getClassLoader(), new Class[] &#123; Subject.class &#125;,new ProxyHandler(real)); proxySubject.doSomething(); System.out.println("代理对象的类型 ： " + proxySubject.getClass().getName()); System.out.println("代理对象所在类的父类型 ： " + proxySubject.getClass().getGenericSuperclass()); &#125;&#125;--------------------- 前置增强处理： yoyoyo...call doSomething()后置增强处理：hahaha...代理对象的类型 ： com.sun.proxy.$Proxy0代理对象所在类的父类型 ： class java.lang.reflect.Proxy 这在 AOP 面向切面编程领域经常见。 在软件业，AOP为Aspect Oriented Programming的缩写，意为：面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。AOP是OOP的延续，是软件开发中的一个热点，也是Spring框架中的一个重要内容，是函数式编程的一种衍生范型。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。 Spring IOC 容器创建Bean(目标类对象)； Bean创建完成后，Bean后处理器(BeanPostProcessor)根据具体的切面逻辑及Bean本身使用Java动态代理技术生成代理对象； 应用程序使用上述生成的代理对象替代原对象来完成业务逻辑，从而达到增强处理的目的。 主要功能日志记录，性能统计，安全控制，事务处理，异常处理等等。 总结 代理分为静态代理和动态代理两种。 静态代理，代理类需要自己编写代码写成。 动态代理，代理类通过 Proxy.newInstance() 方法生成。 不管是静态代理还是动态代理，代理与被代理者都要实现两样接口，它们的实质是面向接口编程。 静态代理和动态代理的区别是在于要不要开发者自己定义 Proxy 类。 动态代理通过 Proxy 动态生成 proxy class，但是它也指定了一个 InvocationHandler 的实现类。 代理模式本质上的目的是为了增强现有代码的功能。 享元模式 桥接模式 行为型模式模板模式 迭代器模式 策略模式 Strategy: 定义所有支持的算法的公共接口抽象类. ConcreteStrategy: 封装了具体的算法或行为，继承于Strategy Context: 用一个ConcreteStrategy来配置，维护一个对Strategy对象的引用。 Strategy模式有下面的一些优点： 相关算法系列 Strategy类层次为Context定义了一系列的可供重用的算法或行为。 继承有助于析取出这些算法中的公共功能。 提供了可以替换继承关系的办法： 继承提供了另一种支持多种算法或行为的方法。你可以直接生成一个Context类的子类，从而给它以不同的行为。但这会将行为硬行编制到 Context中，而将算法的实现与Context的实现混合起来,从而使Context难以理解、难以维护和难以扩展，而且还不能动态地改变算法。最后你得到一堆相关的类 , 它们之间的唯一差别是它们所使用的算法或行为。 将算法封装在独立的Strategy类中使得你可以独立于其Context改变它，使它易于切换、易于理解、易于扩展。 消除了一些if else条件语句 ：Strategy模式提供了用条件语句选择所需的行为以外的另一种选择。当不同的行为堆砌在一个类中时 ,很难避免使用条件语句来选择合适的行为。将行为封装在一个个独立的Strategy类中消除了这些条件语句。含有许多条件语句的代码通常意味着需要使用Strategy模式。 实现的选择 Strategy模式可以提供相同行为的不同实现。客户可以根据不同时间 /空间权衡取舍要求从不同策略中进行选择。 Strategy模式缺点: 客户端必须知道所有的策略类，并自行决定使用哪一个策略类: 本模式有一个潜在的缺点，就是一个客户要选择一个合适的Strategy就必须知道这些Strategy到底有何不同。此时可能不得不向客户暴露具体的实现问题。因此仅当这些不同行为变体与客户相关的行为时 , 才需要使用Strategy模式。 Strategy和Context之间的通信开销 ：无论各个ConcreteStrategy实现的算法是简单还是复杂, 它们都共享Strategy定义的接口。因此很可能某些 ConcreteStrategy不会都用到所有通过这个接口传递给它们的信息；简单的 ConcreteStrategy可能不使用其中的任何信息！这就意味着有时Context会创建和初始化一些永远不会用到的参数。如果存在这样问题 , 那么将需要在Strategy和Context之间更进行紧密的耦合。 策略模式将造成产生很多策略类：可以通过使用享元模式在一定程度上减少对象的数量。 增加了对象的数目 Strategy增加了一个应用中的对象的数目。有时你可以将 Strategy实现为可供各Context共享的无状态的对象来减少这一开销。任何其余的状态都由 Context维护。Context在每一次对Strategy对象的请求中都将这个状态传递过去。共享的 Strategy不应在各次调用之间维护状态。 从项目“模拟鸭子游戏开始”：首先定义一个超类DUCK： 1234567891011121314151617181920212223public abstract class Duck &#123; public Duck() &#123;&#125; public void Quack() &#123; System.out.println("~~gaga~~"); &#125; public abstract void display(); public void swim() &#123; System.out.println("~~im swim~~"); &#125;&#125;public class GreenHeadDuck extends Duck&#123; @Override public void display() &#123; System.out.println("**GreenHead**"); &#125;&#125; 现在我们有一个新的需求; (1)添加会飞的鸭子 我们想的就是在DUCK这个类里面添加一个方法 12345public abstract class Duck &#123; ... public void Fly() &#123; System.out.println("~~im fly~~"); &#125; 然后这样的话就导致了所有的鸭子都是会飞的，那要是有些鸭子不会飞呢，然后我们想的是在子类里面重写这个方法，例如 123456public class GreenHeadDuck extends Duck &#123; ... @Override public void Fly() &#123; System.out.println("~~no fly~~"); &#125; 这样的话所有的不会飞的鸭子全部要改动，这样的话工作量很大，上面的设计思想就是我们经常会想到的方法，也就是用面向对象的方式去设计。 继承的问题：对类的局部改动，尤其超类的局部改动，会影响其他部分。影响会有溢出效应 而且超类挖的一个坑，每个子类都要来填，增加工作量，复杂度O(N^2)。不是好的设计方式 需要新的设计方式，应对项目的扩展性，降低复杂度：1）分析项目变化与不变部分，提取变化部分，抽象成接口+实现；2）鸭子哪些功能是会根据新需求变化的？叫声、飞行… 所以设计两个接口一个是飞行，一个是叫声 12345678910111213141516171819202122232425public interface FlyBehavior &#123; void fly();&#125;public interface QuackBehavior &#123; void quack();&#125;然后不同的叫声各种实现这个叫声接口例如public class GaGaQuackBehavior implements QuackBehavior&#123; @Override public void quack() &#123; System.out.println("__GaGa__"); &#125; &#125;public class GeGeQuackBehavior implements QuackBehavior&#123; @Override public void quack() &#123; System.out.println("__GeGe__"); &#125;&#125; 重新设计DUCK这个类12345678910111213141516171819202122232425262728293031323334353637383940public abstract class Duck &#123; FlyBehavior mFlyBehavior; QuackBehavior mQuackBehavior; public Duck() &#123;&#125; public void Fly() &#123; mFlyBehavior.fly(); &#125; public void Quack() &#123; mQuackBehavior.quack(); &#125; public abstract void display(); public void SetQuackBehavoir(QuackBehavior qb) &#123; mQuackBehavior = qb; &#125; public void SetFlyBehavoir(FlyBehavior fb) &#123; mFlyBehavior = fb; &#125; public void swim() &#123; System.out.println(&quot;~~im swim~~&quot;); &#125;&#125;重写DUCK的子类GreenHeadDuck ：public class GreenHeadDuck extends Duck &#123; public GreenHeadDuck() &#123; mFlyBehavior = new GoodFlyBehavior(); //实例化的时候必须要传入的参数，因为在具体的父类中会用到，并且在用的时候用的是父类中的方法，而且你不必去计较底层是怎么实现的 mQuackBehavior = new GaGaQuackBehavior(); &#125; @Override public void display() &#123;...&#125;&#125; 状态模式 观察者模式实例Observer模式是行为模式之一，它的作用是当一个对象的状态发生变化时，能够自动通知其他关联对象，自动刷新对象状态。这样的话就不会错过该对象感兴趣的事情。对象甚至可以在运行时决定是否需要继续被通知。 Observer模式提供给关联对象一种同步通信的手段，使某个对象与依赖它的其他对象之间保持状态同步。 Subject（被观察者）被观察的对象。当需要被观察的状态发生变化时，需要通知队列中所有观察者对象。Subject需要维持（添加，删除，通知）一个观察者对象的队列列表。 ConcreteSubject被观察者的具体实现。包含一些基本的属性状态及其他操作。 Observer（观察者）接口或抽象类。当Subject的状态发生变化时，Observer对象将通过一个callback函数得到通知。 ConcreteObserver观察者的具体实现。得到通知后将完成一些具体的业务逻辑处理。 首先想到的是设计两个类，一个是天气的数据，一个就是公告板也就是对天气的数据进行操作的类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class CurrentConditions &#123; private float mTemperature; private float mPressure; private float mHumidity; public void update(float mTemperature,float mPressure,float mHumidity) &#123; this.mTemperature=mTemperature; this.mPressure=mPressure; this.mHumidity=mHumidity; display(); &#125; public void display() &#123; System.out.println("***Today mTemperature: "+mTemperature+"***"); System.out.println("***Today mPressure: "+mPressure+"***"); System.out.println("***Today mHumidity: "+mHumidity+"***"); &#125;&#125;public class WeatherData &#123; private float mTemperature; private float mPressure; private float mHumidity; private CurrentConditions currentConditions; public WeatherData( CurrentConditions currentConditions)&#123; this.currentConditions = currentConditions; &#125; public float getmTemperature() &#123; return mTemperature; &#125; public float getmPressure() &#123; return mPressure; &#125; public float getmHumidity() &#123; return mHumidity; &#125; public void dataChange()&#123; currentConditions.update( getmTemperature(),getmPressure(),getmHumidity()); &#125; public void setData(float mTemperature,float mPressure,float mHumidity) &#123; this.mTemperature=mTemperature; this.mPressure=mPressure; this.mHumidity=mHumidity; dataChange(); &#125;&#125; 这样设计带来的问题是什么呢？ 1）其他第三方公司接入气象站获取数据的问题2）无法在运行时动态的添加第三方 也就是说我们想要再添加新的公告板的时候，同时需要更改WeatherData类，这样的话工作量变大，不符合设计要求。 根据上面观察者模式，我们重新设计我们的方案，也就是留出subject和observer两个接口，由WeatherDada和CurrentConditions分别继承实现，这样的话我再来新的公告板的话只需要扩展observer这个接口就可以了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public interface Observer &#123; public void update(float mTemperatrue,float mPressure,float mHumidity);&#125;public interface Subject &#123; public void registerObserver(Observer o); public void removeObserver(Observer o); public void notifyObservers();&#125;public class WeatherDataSt implements Subject &#123; private float mTemperature; private float mPressure; private float mHumidityp; ArrayList&lt;Observer&gt; observers; public float getmTemperature() &#123; return mTemperature; &#125; public float getmPressure() &#123; return mPressure; &#125; public float getmHumidityp() &#123; return mHumidityp; &#125; public WeatherDataSt() &#123; observers = new ArrayList&lt;&gt;(); &#125; @Override public void registerObserver(Observer o) &#123; observers.add(o); &#125; @Override public void removeObserver(Observer o) &#123; if (observers.contains(o)) &#123; observers.remove(o); &#125; &#125; @Override public void notifyObservers() &#123; for (int i = 0; i &lt; observers.size(); i++) &#123; observers.get(i).update(getmTemperature(),getmPressure(),getmHumidityp()); &#125; &#125; public void dataChange() &#123; notifyObservers(); &#125; public void setData(float mTemperature,float mPressure,float mHumidity) &#123; this.mTemperature = mTemperature; this.mPressure = mPressure; this.mHumidityp = mHumidity; dataChange(); &#125;&#125;public class ConcurrentConditions implements Observer&#123; private float mTemperature; private float mPressure; private float mHumidityp; @Override public void update(float mTemperature, float mPressure, float mHumidity) &#123; this.mTemperature =mTemperature; this.mPressure = mPressure; this.mHumidityp = mHumidity; display(); &#125; public void display() &#123; System.out.println("***Today mTemperatrue:" + mTemperature + "***"); System.out.println("***Today mPressure:" + mPressure + "***"); System.out.println("***Today mHumidity:" + mHumidityp + "***"); &#125;&#125; 这个时候我想要添加一个气象预报的公告板，只需要继承Observer接口然后在WeatherData中注册就可以了 1234567891011121314151617181920212223242526272829303132333435public class ForcastConditions implements Observer&#123; private float mTemperature; private float mPressure; private float mHumidityp; @Override public void update(float mTemperature, float mPressure, float mHumidity) &#123; this.mTemperature = mTemperature; this.mPressure = mPressure; this.mHumidityp = mHumidity; displsy(); &#125; public void displsy() &#123; System.out.println("**明天温度:"+(mTemperature+Math.random())+"**"); System.out.println("**明天气压:"+(mPressure+10*Math.random())+"**"); System.out.println("**明天湿度:"+(mHumidityp+Math.random())+"**"); &#125;&#125;public class InternetWeatherOb &#123; public static void main(String[] args) &#123; ConcurrentConditions concurrentConditions = new ConcurrentConditions(); WeatherDataSt weatherDataSt = new WeatherDataSt(); weatherDataSt.registerObserver(concurrentConditions); weatherDataSt.setData(10,200,20); ForcastConditions forcastConditions = new ForcastConditions(); weatherDataSt.registerObserver(forcastConditions); weatherDataSt.setData(20,300,30); &#125;&#125; 通过上面的例子我们可以看出通过观察者模式，我们可以轻松的扩展程序，扩展后只需要让观察者通知被观察者发生的改变即可。 其实观察者模式有两种的实现方式，就是推和拉的模式，推的模式其实就是有主题去通知观察者那些状态发生了改变，拉的方式就是有观察者自己去获取自己需要的状态的改变，对于不需要的不需要获取。 java内置的观察者模式Java内置的观察者： Observable Observer Observer模式的典型应用: 侦听事件驱动程序设计中的外部事件 侦听/监视某个对象的状态变化 发布者/订阅者(publisher/subscriber)模型中，当一个外部事件（新的产品，消息的出现等等）被触发时，通知邮件列表中的订阅者 解释器模式 备忘录模式 中介者模式 命令模式 访问者模式 责任链模式 参考： 图说设计模式 JAVA设计模式总结之23种设计模式 如何正确地写出单例模式]]></content>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启发式算法]]></title>
    <url>%2F%E5%90%AF%E5%8F%91%E5%BC%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[遗传算法遗传算法思想 借鉴生物进化论，遗传算法将要解决的问题模拟成一个生物进化的过程，通过复制、交叉、突变等操作产生下一代的解，并逐步淘汰掉适应度函数值低的解，增加适应度函数值高的解。这样进化N代后就很有可能会进化出适应度函数值很高的个体。 举个例子，使用遗传算法解决“0-1背包问题”的思路：0-1背包的解可以编码为一串0-1字符串（0：不取，1：取） ；首先，随机产生M个0-1字符串，然后评价这些0-1字符串作为0-1背包问题的解的优劣；然后，随机选择一些字符串通过交叉、突变等操作产生下一代的M个字符串，而且较优的解被选中的概率要比较高。这样经过G代的进化后就可能会产生出0-1背包问题的一个“近似最优解”。 编码：需要将问题的解编码成字符串的形式才能使用遗传算法。最简单的一种编码方式是二进制编码，即将问题的解编码成二进制位数组的形式。例如，问题的解是整数，那么可以将其编码成二进制位数组的形式。将0-1字符串作为0-1背包问题的解就属于二进制编码。 遗传算法有3个最基本的操作：选择，交叉，变异。 选择：选择一些染色体来产生下一代。一种常用的选择策略是 “比例选择”，也就是个体被选中的概率与其适应度函数值成正比。假设群体的个体总数是M，那么那么一个体Xi被选中的概率为f(Xi)/( f(X1) + f(X2) + …….. + f(Xn) )。比例选择实现算法就是所谓的“轮盘赌算法”( Roulette Wheel Selection ) ，轮盘赌算法的一个简单的实现如下： 12345678910111213141516171819轮盘赌算法/** 按设定的概率，随机选中一个个体* P[i]表示第i个个体被选中的概率*/int RWS()&#123; m =0; r =Random(0,1); //r为0至1的随机数 for(i=1;i&lt;=N; i++) &#123; /* 产生的随机数在m~m+P[i]间则认为选中了i * 因此i被选中的概率是P[i] */ m = m + P[i]; if(r&lt;=m) return i; &#125;&#125; 交叉(Crossover)：2条染色体交换部分基因，来构造下一代的2条新的染色体。例如： 交叉前： 00000|==011100000000==|1000011100|000001111110|00101 交叉后： 00000|000001111110|1000011100|==011100000000==|00101 染色体交叉是以一定的概率发生的，这个概率记为Pc 。 变异(Mutation)：在繁殖过程，新产生的染色体中的基因会以一定的概率出错，称为变异。变异发生的概率记为Pm 。例如： 变异前：000001110000==0==00010000 变异后：000001110000==1==00010000 适应度函数 ( Fitness Function )：用于评价某个染色体的适应度，用f(x)表示。有时需要区分染色体的适应度函数与问题的目标函数。例如：0-1背包问题的目标函数是所取得物品价值，但将物品价值作为染色体的适应度函数可能并不一定适合。适应度函数与目标函数是正相关的，可对目标函数作一些变形来得到适应度函数。 基本遗传算法的伪代码12345678910111213141516171819202122232425262728基本遗传算法伪代码/** Pc：交叉发生的概率* Pm：变异发生的概率* M：种群规模* G：终止进化的代数* Tf：进化产生的任何一个个体的适应度函数超过Tf，则可以终止进化过程*/初始化Pm，Pc，M，G，Tf等参数。随机产生第一代种群Popdo&#123; 计算种群Pop中每一个体的适应度F(i)。 初始化空种群newPop do &#123; 根据适应度以比例选择算法从种群Pop中选出2个个体 if ( random ( 0 , 1 ) &lt; Pc ) &#123; 对2个个体按交叉概率Pc执行交叉操作 &#125; if ( random ( 0 , 1 ) &lt; Pm ) &#123; 对2个个体按变异概率Pm执行变异操作 &#125; 将2个新个体加入种群newPop中 &#125; until ( M个子代被创建 ) 用newPop取代Pop&#125;until ( 任何染色体得分超过Tf， 或繁殖代数超过G ) 基本遗传算法优化 下面的方法可优化遗传算法的性能。 精英主义(Elitist Strategy)选择：是基本遗传算法的一种优化。为了防止进化过程中产生的最优解被交叉和变异所破坏，可以将每一代中的最优解原封不动的复制到下一代中。 插入操作：可在3个基本操作的基础上增加一个插入操作。插入操作将染色体中的某个随机的片段移位到另一个随机的位置。 爬山算法 ( Hill Climbing )介绍模拟退火前，先介绍爬山算法。爬山算法是一种简单的贪心搜索算法，该算法每次从当前解的临近解空间中选择一个最优解作为当前解，直到达到一个局部最优解。 爬山算法实现很简单，其主要缺点是会陷入局部最优解，而不一定能搜索到全局最优解。如图1所示：假设C点为当前解，爬山算法搜索到A点这个局部最优解就会停止搜索，因为在A点无论向那个方向小幅度移动都不能得到更优的解。 模拟退火(SA,Simulated Annealing)爬山法是完完全全的贪心法，每次都鼠目寸光的选择一个当前最优解，因此只能搜索到局部的最优值。模拟退火其实也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法以一定的概率来接受一个比当前解要差的解，因此有可能会跳出这个局部的最优解，达到全局的最优解。以图1为例，模拟退火算法在搜索到局部最优解A后，会以一定的概率接受到E的移动。也许经过几次这样的不是局部最优的移动后会到达D点，于是就跳出了局部最大值A。 模拟退火算法描述： 若J( Y(i+1) )&gt;= J( Y(i) ) (即移动后得到更优解)，则总是接受该移动 若J( Y(i+1) )&lt; J( Y(i) ) (即移动后的解比当前解要差)，则以一定的概率接受移动，而且这个概率随着时间推移逐渐降低（逐渐降低才能趋向稳定） 这里的“一定的概率”的计算参考了金属冶炼的退火过程，这也是模拟退火算法名称的由来。 根据热力学的原理，在温度为T时，出现能量差为dE的降温的概率为P(dE)，表示为：P(dE) = exp( dE/(kT) ) 其中k是一个常数，exp表示自然指数，且dE&lt;0。这条公式说白了就是：温度越高，出现一次能量差为dE的降温的概率就越大；温度越低，则出现降温的概率就越小。又由于dE总是小于0（否则就不叫退火了），因此dE/kT &lt; 0 ，所以P(dE)的函数取值范围是(0,1) 。 随着温度T的降低，P(dE)会逐渐降低。 我们将一次向较差解的移动看做一次温度跳变过程，我们以概率P(dE)来接受这样的移动。 关于爬山算法与模拟退火，有一个有趣的比喻： 爬山算法：兔子朝着比现在高的地方跳去。它找到了不远处的最高山峰。但是这座山不一定是珠穆朗玛峰。这就是爬山算法，它不能保证局部最优值就是全局最优值。 模拟退火：兔子喝醉了。它随机地跳了很长时间。这期间，它可能走向高处，也可能踏入平地。但是，它渐渐清醒了并朝最高方向跳去。这就是模拟退火。 下面给出模拟退火的伪代码表示。 模拟退火算法伪代码12345678910111213141516171819202122232425262728代码/** J(y)：在状态y时的评价函数值* Y(i)：表示当前状态* Y(i+1)：表示新的状态* r： 用于控制降温的快慢* T： 系统的温度，系统初始应该要处于一个高温的状态* T_min ：温度的下限，若温度T达到T_min，则停止搜索*/while( T &gt; T_min )&#123; dE = J( Y(i+1) ) - J( Y(i) ) ; if ( dE &gt;=0 ) //表达移动后得到更优解，则总是接受移动Y(i+1) = Y(i) ; //接受从Y(i)到Y(i+1)的移动 else &#123;// 函数exp( dE/T )的取值范围是(0,1) ，dE/T越大，则exp( dE/T )也if ( exp( dE/T ) &gt; random( 0 , 1 ) )Y(i+1) = Y(i) ; //接受从Y(i)到Y(i+1)的移动 &#125; T = r * T ; //降温退火 ，0&lt;r&lt;1 。r越大，降温越慢；r越小，降温越快 /* * 若r过大，则搜索到全局最优解的可能会较高，但搜索的过程也就较长。若r过小，则搜索的过程会很快，但最终可能会达到一个局部最优值 */ i ++ ;&#125; 使用模拟退火算法解决旅行商问题 旅行商问题 ( TSP , Traveling Salesman Problem ) ：有N个城市，要求从其中某个问题出发，唯一遍历所有城市，再回到出发的城市，求最短的路线。 旅行商问题属于所谓的NP完全问题，精确的解决TSP只能通过穷举所有的路径组合，其时间复杂度是O(N!) 。 使用模拟退火算法可以比较快的求出TSP的一条近似最优路径。（使用遗传算法也是可以的，我将在下一篇文章中介绍）模拟退火解决TSP的思路： 产生一条新的遍历路径P(i+1)，计算路径P(i+1)的长度L( P(i+1) ) 若L(P(i+1)) &lt; L(P(i))，则接受P(i+1)为新的路径，否则以模拟退火的那个概率接受P(i+1) ，然后降温 重复步骤1，2直到满足退出条件 产生新的遍历路径的方法有很多，下面列举其中3种： 随机选择2个节点，交换路径中的这2个节点的顺序。 随机选择2个节点，将路径中这2个节点间的节点顺序逆转。 随机选择3个节点m，n，k，然后将节点m与n间的节点移位到节点k后面。 算法评价模拟退火算法是一种随机算法，并不一定能找到全局的最优解，可以比较快的找到问题的近似最优解。 如果参数设置得当，模拟退火算法搜索效率比穷举法要高。 原文]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程(4)]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B4%2F</url>
    <content type="text"><![CDATA[COMPLETED RESTORING OFF-LINE CONSISTENT DATABASE 完成将片段副本还原到一致的全局检查点后，我们现在将根据还原的数据开始重建有序索引。 在重建有序索引之后，我们准备将START_RECCONF发送到起始DBDIH。 START_RECCONF通过DBLQH代理发送，因此在所有DBLQH实例完成此阶段并使用START_RECCONF响应之前，它不会传递到DBDIH。 此时，在DBLQH实例中，我们已恢复节点中所有数据的一致但旧的变体。 仍然没有有序索引，仍然有很多工作要让节点再次与其他节点同步。 对于群集重新启动，可能是节点已完全准备就绪，但是可能某些节点仍需要与已恢复更新的全局检查点的节点同步。 然后，起始节点的DBDIH将启动接管过程，因为起始节点具有一致的片段副本。 我们将通过为我们将复制的每个片段副本发送PREPARE_COPY_FRAG_REQ来为复制阶段准备起始节点的DBLQH。 这是一个可以并行化的顺序过程。 接管片段副本的过程非常复杂。 它首先将PREPARE_COPY_FRAGREQ / CONF发送到起始DBLQH，然后我们将UPDATE_TOREQ / CONF发送到主DBDIH，以确保我们在接管开始之前锁定片段信息。 在接收到该片段锁定的确认之后，起始节点向所有节点发送UPDATE_FRAG_STATEREQ / CONF以将新节点包括在片段上的所有操作中。 完成此操作后，我们再次向主节点发送UPDATE_TOREQ / CONF以通知新状态并解锁片段信息上的锁定。 然后我们准备执行片段的实际复制。 这是通过将COPY_FRAGREQ / CONF发送到将复制数据的节点来完成的。 完成此复制后，我们将COPY_ACTIVEREQ / CONF发送到起始节点以激活片段副本。 接下来，我们再次向主服务器发送UPDATE_TOREQ / CONF，通知我们即将安装接管新片段副本的提交。 接下来，我们通过向所有节点发送UPDATE_FRAG_STATEREQ / CONF来提交新的片段副本，通知它们片段副本的复制完成。 最后，我们使用UPDATE_TOREQ / CONF向主节点发送另一个更新。 现在我们终于完成了这个片段的复制。 这个方案的想法是第一个UPDATE_FRAG_STATEREQ确保我们是片段上所有事务的一部分。 在执行COPY_FRAGREQ以逐行地将起始节点的片段副本与活动节点的片段副本同步之后，我们确信两个片段副本完全同步，我们可以执行新的UPDATE_FRAG_STATEREQ以确保所有节点都知道我们 完成了同步。 COMPLETED RESTORING ON-LINE NOT RECOVERABLE DATABASE此时，我们通过在线一次添加一个片段来恢复数据库的在线变体。 数据库仍然无法恢复，因为我们尚未启用日志记录，并且起始节点中没有数据的本地检查点。 下一步是启用所有片段的日志记录，完成此步骤后，我们将END_TOREQ发送到主DBDIH。 此时，我们将等到本地检查点完成，其中涉及此节点。 最后，当本地检查点完成后，我们将END_TOCONF发送到起始节点，然后我们将发送START_COPYCONF，这将完成重启的这个阶段。 COMPLETED RESTORING ON-LINE RECOVERABLE DATABASE此时我们已经设法恢复所有数据，并且我们已经将它带到了在线状态，现在我们还在启用日志记录时执行了本地检查点，因此现在起始节点中的数据也是可恢复的。 所以这意味着数据库现在再次完全联机。 完成NDB_STTOR阶段5之后，此处等待点中已同步的所有节点将再次启动，NDBCNTR将继续运行NDB_STTOR的阶段6。 在此阶段DBLQH，DBDICT和DBTC设置一些状态变量，指示现在启动已完成（它尚未完全完成，但这些模块运行所需的所有服务都已完成.DBDIH还启动全局检查点协议以进行集群启动/重启 它已成为主节点。 现在，在群集启动/重启的情况下，所有节点还有一个等待点。 STTOR阶段5的最后一步是SUMA，它读取已配置的节点，获取节点组成员，如果有节点重新启动，它会要求另一个节点重新创建它的订阅。 STTOR Phase 6我们现在进入STTOR阶段6.在此阶段，NDBCNTR获取节点的节点组，DBUTIL获取systable id，准备一组操作供以后使用，并连接到TC以使其能够代表其他模块运行关键操作 稍后的。 STTOR Phase 7接下来我们进入STTOR阶段7.DBDICT现在启动索引统计循环，该循环将在节点存在时运行。 QMGR将启动仲裁处理，以处理我们面临网络分区风险的情况。 BACKUP将更新磁盘检查点速度（重启期间有一个配置变量用于速度，一个用于正常操作，这里我们安装正常运行速度）。 如果初始启动BACKUP也将通过DBUTIL创建备份序列。 如果SUMA在主节点中运行并且它是初始启动，它将创建一个序列。 SUMA还将始终计算它负责处理的桶。 最后，DBTUX将开始监控有序索引。 STTOR Phase 8然后我们转到STTOR阶段8.这里首先要运行NDB_STTOR的第7阶段，其中DBDICT启用外键。 如果我们正在进行集群启动/重启，下一个NDBCNTR也将等待所有节点到达此处。 下一个CMVMI将状态设置为STARTED，QMGR将启用与所有API节点的通信。 STTOR Phase 101在此阶段之后，唯一剩下的阶段是STTOR阶段101，其中SUMA接管它负责异步复制处理的桶的责任。 目前为止主要的潜在消费者：内存分配中的所有步骤（READ_CONFIG_REQ的所有步骤）。 CMVMI STTOR第1阶段可以锁定内存。 运行节点包含协议的QMGR阶段1。 NDBCNTR STTOR阶段2等待CNTR_START_REQ，DBLQH REDO日志初始化为STTOR阶段2中发生的初始启动类型。鉴于每次只有一个节点可以处于此阶段，这可能会被另一个节点的本地检查点等待停顿 开始。 所以这等待可能相当长。 DBLQH建立与DBACC和DBTUP的连接，这是NDB_STTOR阶段2.NDB_STTOR阶段2中的DBDIH也可以等待元数据被锁定，它可以等待对START_PERMREQ的响应。 对于初始启动，等待DBLQH完成NDB_STTOR阶段3，在此阶段初始化REDO日志的设置。 完成NDB_STTOR阶段3后，在STTOR阶段4中用于集群启动/重启的NDBCNTR必须等待所有节点到达此点，然后它必须等待NDB_STARTREQ完成。 对于节点重启，我们在等待对START_MEREQ信号和START_COPYREQ的响应时有延迟，这实际上是重启的大部分实际工作完成的地方。 重新订阅订阅的SUMA STTOR第5阶段是另一个潜在的时间消费者。 所有等待点都是潜在的时间消费者。 这些主要位于NDBCNTR（等待点5.2,5,1和6）。 Historical anecdotes: 1）NDB内核运行时环境最初是为AX虚拟机设计的。在AX中，开始使用模块MISSRA来驱动各种启动阶段的STTOR / STTORRY信号。 MISSRA后来被并入NDBCNTR，现在是NDBCNTR的子模块。 STTOR和STTORRY的名称在早期的AX系统命名信号方式中有一些基础，但现在已经被遗忘了。 ST至少可以通过启动/重启来完成任务。 2）引入NDB_STTOR的原因是我们设想了一个系统，其中NDB内核只是运行时环境中的一个子系统。因此，我们为NDB子系统引入了单独的启动阶段。随着时间的推移，对这种子系统启动阶段的需求不再存在，但软件已经为此设计，因此它以这种方式保存。 3）数据库启动的分布式部分的责任也是分开的。 QMGR负责发现节点何时上下。 NDBCNTR维护用于故障处理和节点配置的其他更改的协议。最后，DBDIH负责数据库部分的分布式启动。它与DBLQH交互很多，DBLQH负责按照DBDIH的指示启动一个节点数据库部分。 Local checkpoint processing in MySQL Cluster此注释试图描述MySQL Cluster中发生的检查点处理。 它还阐明了潜在的瓶颈所在。 此注释主要用作MySQL Cluster开源代码的内部文档。 MySQL Cluster中本地检查点的原因是为了确保我们在磁盘上有数据副本，可用于运行REDO日志以在崩溃后恢复MySQL Cluster中的数据。 我们首先在MySQL Cluster中引入不同的重启变体。第一个变体是正常节点重启，这意味着节点已经短时间丢失，但现在又重新上线。我们首先安装所有表的检查点版本（包括执行REDO日志的正确部分）。下一步是使用仍在线的副本使检查点版本保持最新。副本始终按节点组进行组织，节点组的最常见大小是两个节点。因此，当节点启动时，它使用同一节点组中的另一个节点来使在线版本的表重新联机。在正常的节点重启中，我们首先恢复了所有表的稍微旧版本，然后再使用其他节点进行同步。这意味着我们只需要发送自节点重启之前节点失败以来已更新的最新版本的行。我们还有初始节点重启的情况，其中所有数据都必须从另一个节点恢复，因为起始节点中的检查点太旧而无法重用，或者当一个全新的节点启动时它根本不存在。 重新启动的第三个变体是所谓的系统重启，这意味着整个群集在群集崩溃后或在群集受控停止后启动。 在此重新启动类型中，我们首先在运行REDO日志之前在所有节点上恢复检查点，以使系统处于一致且最新的状态。 如果任何节点还原到较旧的全局检查点而不是重新启动的节点，则必须使用节点重新启动中使用的相同代码将这些节点置于联机状态。 系统重启将恢复所谓的全局检查点。 一组事务被组合在一起成为一个全局检查点，当这个全局检查点完成后，属于它的事务是安全的并且将在集群崩溃后继续存在。 我们在第二级运行全局检查点，本地检查点将整个数据集写入磁盘，并且是一个耗时至少几分钟的较长过程。 在可以将起始节点声明为完全恢复之前，它必须参与本地检查点。 崩溃节点错过了恢复群集所需的一组REDO日志记录，因此节点未完全恢复，直到它可用于恢复系统重启时拥有的所有数据。 因此，当执行滚动节点重新启动时，群集中的所有节点都重新启动（例如，升级MySQL群集中的软件），一次重启一组节点是有意义的，因为我们只能在一个节点重新启动一组节点。 时间。 这是了解本地检查站需求的一个先决条件。 我们现在转到如何处理本地检查点的描述。 本地检查点是一个分布式进程。 它由名为DBDIH（简称DIH，DIstribution Handler）的软件模块控制。 DIH包含有关每个片段（与分区的同义词）的各种副本放置在何处以及这些副本上的各种数据的所有信息。 DIH将分发信息存储在每个表的一个文件中。 这个文件实际上是两个文件，这是为了确保我们可以仔细编写文件。 我们首先写文件0，当这个完成后，我们写文件1，这样我们就可以在编写表描述时轻松处理任何崩溃。 当本地检查点完成后，DIH立即启动该过程以启动下一个检查点。 自从我们启动新的本地检查点之前启动本地检查点以来，必须至少完成一个全局检查点。 下一个本地检查点的第一步是检查我们是否已准备好运行它。 这是通过将消息TCGETOPSIZEREQ发送到集群中的所有TC来执行的。 这将通过检查TC中收到的所有写入事务的信息来报告生成的REDO日志信息量。 该消息将由主DIH发送。 主服务器的角色被分配给最旧的幸存数据节点，这使得当目前充当主数据节点的数据节点死亡时，可以轻松选择新主服务器。 所有节点都同意进入群集的节点的顺序，因此节点的年龄在群集中的所有节点中都是一致的。 当所有消息都将REDO日志写入大小返回到主DIH时，我们将它与配置变量TimeBetweenLocalCheckpoints进行比较（此变量以大小的对数设置，因此例如25表示我们等待2 ^ 25个单词的REDO日志已创建于 该集群是128 MByte的REDO日志信息）。当生成足够数量的REDO日志时，我们启动下一个本地检查点，第一步是清除所有TC计数器，这是通过将TC_CLOPSIZEREQ发送到集群中的所有TC来完成的。 下一步是计算保持GCI（这是需要在REDO日志中保留的最早的全局检查点ID）。 这个数字非常重要，因为我们可以向前移动REDO日志的尾部。 如果我们用完REDO日志空间，我们将无法运行任何写入事务，直到我们启动下一个本地检查点，从而向前移动REDO日志尾部。 我们通过检查每个片段需要恢复的GCI来计算这个数字。 我们目前保留两个旧的本地检查点仍然有效，因此我们不会将GCI移回以使每个片段的两个最旧的本地检查点无效。 完成此计算后可恢复的GCI是循环遍历所有片段时发现的最小GCI。 接下来，我们在集群中所有节点的Sysfile中写下此编号和新的本地检查点ID以及其他一些信息。 在系统重新启动时开始恢复群集时，我们首先看到此Sysfile，因此在此文件中使此类信息正确非常重要。 完成此操作后，我们将计算将参与本地检查点的节点（当前执行重启的早期部分的节点不是本地检查点的一部分，显然也不是死节点）。 我们将有关起始本地检查点的信息发送给系统中的所有其他DIH。我们必须始终保持所有其他DIH的最新状态，以确保在主DIH崩溃或在本地检查点过程中停止时也很容易继续本地检查点。每个DIH记录参与本地检查点的节点集。他们还在每个副本记录上设置一个标志，指示本地检查点正在进行中，在每个片段记录上我们还设置了作为此本地检查点一部分的副本数。 现在我们已经完成了本地检查点的准备工作，现在是时候开始实际检查点写入实际数据了。主DIH通过为应检查点的每个片段副本发送LCP_FRAG_ORD来控制此过程。 DIH目前每个节点有2个这样的LCP_FRAG_ORD未完成，排队的2个片段副本。每个LDM线程可以一次处理一个片段副本的写入，并且可以对排队的下一个片段副本有一个请求。扩展此数字非常简单，以便可以并行写入更多的片段副本，并且可以对更多片段副本进行排队。 当片段副本的本地检查点完成时，LCP_FRAG_REP被发送到所有DIH。 当DIH发现表的所有片段副本都已完成本地检查点时，则应该将表描述写入文件系统。 这将记录所有片段副本的有趣的本地检查点信息。 有两件事可以导致这种情况等待。 首先编写和读取整个表描述只能一次发生一次，这主要发生在正在处理本地检查点时正在进行某些节点故障处理的情况。 可以阻止写表描述的第二件事是，目前最多可以并行写入4个表描述。 这可能很容易成为瓶颈，因为每次写入文件可能需要大约50毫秒。 所以这意味着我们目前每秒只能写出大约80个这样的表。 在具有许多表和少量数据的系统中，这可能成为瓶颈。 然而，它应该不是一个困难的瓶颈。 当主DIH已将所有请求发送到检查点所有片段副本时，它将向所有节点发送一个特殊的LCP_FRAG_ORD，指示不再发送任何片段副本。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字典树简介(Trie)]]></title>
    <url>%2Ftrie%2F</url>
    <content type="text"><![CDATA[在 NLP 中一般会用其存储大量的字典字符以用于文本的快速分词；除此之外，典型应用场景还包括大批量文本的：词频统计、字符串查询和模糊匹配（比如关键词的模糊匹配）、字符串排序等任务；由于 Trie 大幅降低了无谓的字符串比较，因此在执行上述任务时，其效率非常的高。 Trie 树的简介Trie 树中文名叫字典树、前缀树（个人比较喜欢这个名字，看完下文就会明白）等等。这些名字暗示其与字符的处理有关，事实也确实如此，它主要用途就是将字符串（当然也可以不限于字符串）整合成树形。我们先来看一下由“清华”、“清华大学”、“清新”、“中华”、“华人”五个中文词构成的 Trie 树形（为了便于叙述，下文提到该实例，以“例树”简称）： 这个树里面每一个方块代表一个节点，其中 ”Root” 表示根节点，不代表任何字符；紫色代表分支节点；绿色代表叶子节点。除根节点外每一个节点都只包含一个字符。从根节点到叶子节点，路径上经过的字符连接起来，构成一个词。而叶子节点内的数字代表该词在字典树中所处的链路（字典中有多少个词就有多少条链路），具有共同前缀的链路称为串。除此之外，还需特别强调 Trie 树的以下几个特点： 具有相同前缀的词必须位于同一个串内；例如“清华”、“清新”两个词都有“清”这个前缀，那么在 Trie 树上只需构建一个“清”节点，“华”和“新”节点共用一个父节点即可，如此两个词便只需三个节点便可存储，这在一定程度上减少了字典的存储空间。 Trie 树中的词只可共用前缀，不可共用词的其他部分；例如“中华”、“华人”这两个词虽然前一个词的后缀是后一个词的前缀，但在树形上必须是独立的两条链路，而不可以通过首尾交接构建这两个词，这也说明 Trie 树仅能依靠公共前缀压缩字典的存储空间，并不能共享词中的所有相同的字符；当然，这一点也有“例外”，对于复合词，可能会出现两词首尾交接的假象，比如“清华大学”这个词在上例 Trie 树中看起来似乎是由“清华”、“大学”两词首尾交接而成，但是叶子节点的标识已经明确说明 Trie 树里面只有”清华“和”清华大学“两个词，它们之间共用了前缀，而非由“清华”和”大学“两词首尾交接所得，因此上例 Trie 树中若需要“大学”这个词则必须从根节点开始重新构建该词。 Trie 树中任何一个完整的词，都必须是从根节点开始至叶子节点结束，这意味着对一个词进行检索也必须从根节点开始，至叶子节点才算结束。 搜索 Trie 树的时间复杂度在 Trie 树中搜索一个字符串，会从根节点出发，沿着某条链路向下逐字比对字符串的每个字符，直到抵达底部的叶子节点才能确认字符串为该词，这种检索方式具有以下两个优点： 公共前缀的词都位于同一个串内，查词范围因此被大幅缩小（比如首字不同的字符串，都会被排除）。 Trie 树实质是一个有限状态自动机（(Definite Automata, DFA），这就意味着从 Trie 树的一个节点（状态）转移到另一个节点（状态）的行为完全由状态转移函数控制，而状态转移函数本质上是一种映射，这意味着：逐字搜索 Trie 树时，从一个字符到下一个字符比对是不需要遍历该节点的所有子节点的。对于确定性有限自动机感兴趣的同学，可以看看以下引用： 1234567891011确定的有限自动机 M 是一个五元组：M = (Σ, Q, δ, q0, F)其中，Σ 是输入符号的有穷集合；Q 是状态的有限集合；δ 是 Q 与 Σ 的直积 Q × Σ 到Q (下一个状态) 的映射。它支配着有限状态控制的行为，有时也称为状态&gt;转移函数。q0 ∈ Q 是初始状态；F 是终止状态集合，F ⊆ Q；可以把DFA想象成一个单放机，插入一盘磁带，随着磁带的转动，DFA读取一个符号，依靠状态转移函数&gt;改变自己的状态，同时磁带转到下一个字符。 这两个优点相结合可以最大限度地减少无谓的字符比较，使得搜索的时间复杂度理论上仅与检索词的长度有关：O(m)，其中 m 为检索词的长度。 Trie 树的缺点综上可知， Trie 树主要是利用词的公共前缀缩小查词范围、通过状态间的映射关系避免了字符的遍历，从而达到高效检索的目的。这一思想有赖于字符在词中的前后位置能够得到表达，因此其设计哲学是典型的“以信息换时间”，当然，这种优势同样是需要付出代价的： 由于结构需要记录更多的信息，因此 Trie 树的实现稍显复杂。好在这点在大多数情况下并非不可接受。 Trie 型词典不仅需要记录词，还需要记录字符之间、词之间的相关信息，因此字典构建时必须对每个词和字逐一进行处理，而这无疑会减慢词典的构建速度。对于强调实时更新的词典而言，这点可能是致命的，尤其是采用双数组实现的 Trie 树，更新词典很大概率会造成词典的全部重构，词典构建过程中还需处理各种冲突，因此重构的时间非常长，这导致其大多用于离线；不过也有一些 Trie 可以实现实时更新，但也需付出一定的代价，因此这个缺点一定程度上影响了 Trie 树的应用范围。 公共前缀虽然可以减少一定的存储空间，但 Trie 树相比普通字典还需表达词、字之间的各种关系，其实现也更加复杂，因此实际空间消耗相对更大（大多少，得根据具体实现而定）。尤其是早期的“Array Trie”，属于典型的以空间换时间的实现，（其实 Trie 本身的实现思想是是以信息换时间，而非以空间换时间，这就给 Trie 树的改进提供了可能），然而 Trie 树现今已经得到了很好的改进，总体来说，对于类似词典这样的应用，Trie 是一个优秀的数据结构。 Trie 树的几种实现Array Trie 树很多文章里将这种实现称为“标准 Trie 树”，但其实它只是 Trie 众多实现中的一种而已，由于这种实现结构简单，检索效率很好，作为讲解示例很不错，因此特地改称其为“经典 Trie 树”，这里引用一下别人家的示例图： 如上图，abc、d、da、dda 四个字符串构成的 Trie 树，如果是字符串会在节点的尾部进行标记。没有后续字符的 branch 分支指向NULL。这种实现的特点是：每个节点都由指针数组存储，每个节点的所有子节点都位于一个数组之中，每个数组都是完全一样的。对于英文而言，每个数组有27个指针，其中一个作为词的终结符，另外 26 个依次代表字母表中的一个字母，对应指针指向下一个状态，若没有后续字符则指向NULL。由于数组取词的复杂度为O(1)，因此这种实现的 Trie 树效率非常的高，比如要在一个节点中写入字符“c”,则直接在相应数组的第三个位置标入状态即可，而要确定字母“b”是否在现有节点的子节点之中，检查子节点所在数组第二个元素是否为空即可，这种实现巧妙的利用了等长数组中元素位置和值的一一对应关系，完美的实现了了寻址、存值、取值的统一。但其缺点也很明显，它强制要求链路每一层都要有一个数组，每个数组都必须等长，这在实际应用中会造成大多数的数组指针空置（从上图就可以看出），事实上，对于真实的词典而言，公共前缀相对于节点数量而言还是太少，这导致绝大多数节点下并没有太多子节点。而对于中文这样具有大量单字的语言，若采取这样的实现，空置指针的数量简直不可想象。因此，经典 Trie 树是一种典型的以“空间换时间”的实现方式。一般只是拿来用于课程设计和新手练习，很少实际应用。 List Trie 树由于数组的长度是不可变，因此经典 Trie 树存在着明显的空间浪费。但是如果将每一层都换成可变数组（不同语言对这种数据结构称呼不同，比如在 Python 中为List，C# 称为 LinkedList）来存储节点（见下图），每层可以根据节点的数量动态调整数组的长度，就可以避免大量的空间浪费。 但是可变长数组的取词复杂度是O(d),其中 d 为数组的长度，这意味着状态转移函数无法通过映射转移到下一节点，必须先遍历数组，找到节点后再做转移，因此Trie 树实际时间复杂度变为O(m*n)(其中n为每层数组中节点的数量)。这显然降低了查询效率,因此还算不上完善。 Hash Trie 树可变数组取词速度太慢，于是就有人想起用一组键值对（Java中可用HashMap类型，Python 中为 dict 类型，C#为Dictionary类型）代替可变数组：其中每个节点包含一组 Key-Value，每个 Key 对应该节点下的一个子节点字符，value 则指向相应的后一个状态。这种方式可以有效的减少空间浪费，同时由于键值对本质上就是一个哈希实现，因此理论上其查词效率也很高（理想状态下取词复杂度为O(1)）。 但是哈希有的缺点，这种实现的 Trie 树也会有： 为了尽可能的避免键值冲突，哈希表需要额外的空间避开碰撞，因此仍有一部分的空间会被浪费； 哈希表很难做到完美，尤其是数据体量增大之后，其查词复杂度常常难以维持在O(1)，同时，对哈希值的计算也需要额外的时间，因此实际查询效率要比经典实现低，其具体复杂度由相应的哈希实现来定。 与数组和可变数组实现相比，这种实现做到了空间和时间上的一种平衡，这个结果并不意外，因为哈希表本身就是平衡数组（查寻迅速、增删悲剧）和可变数组（增删迅速，查询悲剧）相应优点和缺点的一种数据结构。总体而言，Hash Trie 结构简单，性能堪用，而且由于哈希实现可以为每个节点分配唯一的id,因此可以做到节点的实时动态添加（这点是非常大的优势）因此对于中小规模的词典或者对词典的实时更新有需求的应用，该实现非常适合。 Double-array Trie 树双数组 Trie 树是目前 Trie 树各种实现中性能和存储空间均达到很好效果的实现。但其完整的实现比较复杂，对于新手而言入手相对较难，因此本节将花费较多的篇幅对其解读。 Base Array 的作用双数组 Trie 树和经典 Trie 树一样，也是用数组实现 Trie 树。只不过它是将所有节点的状态都记录到一个数组之中（Base Array），以此避免数组的大量空置。以行文开头的示例为例，每个字符在 Base Array 中的状态可以是这样子的： 为了能使单个数组承载更多的信息，Base Array 仅仅会通过数组的位置记录下字符的状态（节点），比如用数组中的位置 2 指代“清”节点、 位置 7 指代 “中”节点；而数组中真正存储的值其实是一个整数，这个整数我们称之为“转移基数”，比如位置2的转移基数为 base[2]=3位置7的转移基数为base[7]=2，因此在不考虑叶子节点的情况下， Base Array 是这样子的： 转移基数是为了在一维数组中实现 Trie 树中字符的链路关系而设计的，举例而言，如果我们知道一个词中某个字符节点的转移基数，那么就可以据此推断出该词下一个节点在 Base Array 中的位置：比如知道 “清华”首字的转移基数为base[2]=3，那么“华”在数组中的位置就为base[2]+code(&quot;华&quot;)，这里的code(&quot;华&quot;)为字符表中“华”的编码，假设例树的字符编码表为： 清-1，华-2，大-3，学-4，新-5，中-6，人-7 那么“华”的位置应该在Base Array 中的的第 5 位（base[2]+code(&quot;华&quot;)=3+2=5）： 而所有词的首字，则是通过根节点的转移基数推算而来。因此，对于字典中已有的词，只要我们每次从根节点出发，根据词典中各个字符的编码值，结合每个节点的转移基数，通过简单的加法，就可以在Base Array 中实现词的链路关系。以下是“清华”、“清华大学”、“清新”、“中华”、“华人”五个词在 Base Array 中的链路关系： Base Array 的构造可见 Base Array 不仅能够表达词典中每个字符的状态，而且还能实现高效的状态转移。那么，Base Array 又是如何构造的呢？ 事实上，同样一组词和字符编码，以不同的顺序将字符写入 Trie 树中，获得的 Base Array 也是不同的，以“清华”、“清华大学”、“清新”、“中华”、“华人”五个词，以及字符编码：[清-1，华-2，大-3，学-4，新-5，中-6，人-7] 为例，在不考虑叶子节点的情况下，两种处理方式获得的 base array 为： 首先依次处理“清华”、“清华大学”、“清新”、“中华”、“华人”五个词的首字，然后依次处理所有词的第二个字…直到依次处理完所有词的最后一个字，得到的 Base Array 为： 依次处理“清华”、“清华大学”、“清新”、“中华”、“华人”五个词中的每个字，得到的 Base Array 为： 可以发现，不同的字符处理顺序，得到的 Base Array 存在极大的差别：两者各状态的转移基数不仅完全不同，而且 Base Array 的长度也有差别。然而，两者获得的方法却是一致的，下面以第一种字符处理顺序讲解一下无叶子节点的 Base Array 构建： 首先人为赋予根节点的转移基数为1（可自定义，详见下文），然后依次将五个词中的首字”清”、“中”、“华”写入数组之中，写入的位置由base[1]+code(字符)确定，每个位置的转移基数（base[i]）等于上一个状态的转移基数（此例也即base[1]），这个过程未遇到冲突，最终结果见下图： 然后依次处理每个词的第二个字，首先需要处理的是“清华”这个词的“华”字，程序先从根节点出发，通过base[1]+code(“清”)=2找到“清”节点，然后以此计算“华”节点应写入的位置，通过计算base[2]+code(“华”)=3寻找到位置 3,却发现位置3已有值，于是后挪一位，在位置4写入“华”节点，由于“华”节点未能写入由前驱节点“清”预测的位置，因此为了保证通过“清”能够找到“华”，需要重新计算“清”节点的转移基数，计算公式为4-code(“华”)=2,获得新的转移基数后，改写“清”节点的转移基数为2，然后将“华”节点的转移基数与“清”节点保持一致，最终结果为： 重复上面的步骤，最终获得整个 Base Array： 通过以上步骤，可以发现 base array 的构造重点在于状态冲突的处理，对于双数组 Trie 而言，词典构造过程中的冲突是不可避免的，冲突的产生来源于多词共字的情况，比如“中华”、“清华”、“华人”三个词中都有“华”，虽然词在 Trie 树中可以共用前缀，但是对于后缀同字或者后缀与前缀同字的情况却只能重新构造新的节点，这势必会导致冲突。一旦产生冲突，那么父节点的转移基数必须改变，以保证基于前驱节点获得的位置能够容纳下所有子节点（也即保证 base[i]+code(n1)、base[i]+code(n2)、base[i]+code(n3)….都为空，其中n1、n2、n3...为父节点的所有子节点字符，base[i]为父节点新的转移基数，i为父节在数组中的位置）这意味着其他已经构造好的子节点必须一并重构。 因此，双数组 Trie 树的构建时间比较长，有新词加入，运气不好的话，还可能能导致全树的重构：比如要给词典添加一个新词，新词的首字之前未曾写入过，现在写入时若出现冲突，就需要改写根节点的转移基数，那么之前构建好的词都需要重构（因为所有词的链路都是从根节点开始）。上例中，第二种字符写入顺序就遇到了这个问题，导致在词典构造过程中，根节点转移基数被改写了两次，全树也就被重构了三次： 可见不同的节点构建顺序，对 Base Aarry 的构建速度、空间利用率都有影响。建议实际应用中应首先构建所有词的首字，然后逐一构建各个节点的子节点，这样一旦产生冲突，可以将冲突的处理局限在单个父节点和子节点之间，而不至于导致大范围的节点重构。 叶子节点的处理上面关于 Base Array 的叙述，只涉及到了根节点、分支节点的处理，事实上，Base Array 同样也需要负责叶子节点的表达，但是由于叶子节点的处理，具体的实现各不一致，因此特地单列一节予以论述。 一般词的最后一个字都不需要再做状态转移，因此有人建议将词的最后一个节点的转移基数统一改为某个负数（比如统一设置为-2），以表示叶子节点，按照这种处理，对于示例而言，base array 是这样的： 但细心的童鞋可能会发现，“清华” 和 “清华大学” 这两个词中，只有“清华大学”有叶子节点，既是公共前缀又是单个词的“清华”实际上无法用这种方法表示出叶子节点。 也有人建议为词典中所有的词增加一个特殊词尾（比如将“清华”这个词改写为“清华\0”），再将这些词构建为树，特殊字符词尾节点的转移基数统一设置设为-2，以此作为每个词的叶子节点[4]。这种方法的好处是不用对现有逻辑做任何改动，坏处是增加了总节点的数量，相应的会增加词典构建的时长和空间的消耗。 最后，个人给出一个新的处理方式：直接将现有 base array 中词尾节点的转移基数取负，而数组中的其他信息不用改变。 以树例为例，处理叶子节点前，Base Array 是这样子的： 处理叶子节点之后，Base Array 会是这样子的： 每个位置的转移基数绝对值与之前是完全相同的，只是叶子节点的转移基数变成了负数，这样做的好处是：不仅标明了所有的叶子节点，而且程序只需对状态转移公式稍加改变，便可对包括“清华”、“清华大学”这种情况在内的所有状态转移做一致的处理，这样做的代价就是需要将状态转移函数base[s]+code(字符)改为|base[s]|+code(字符)，意味着每次转移需要多做一次取绝对值运算，不过好在这种处理对性能的影响微乎其微。 对此，其他童鞋若有更好的想法， 欢迎在底部留言！ Check Array 的构造“双数组 Trie 树”，必定是两个数组，因此单靠 Base Array 是玩不起来的….上面介绍的 Base Array 虽然解决了节点存储和状态转移两个核心问题，但是单独的 Base Array 仍然有个问题无法解决： Base Array 仅仅记录了字符的状态，而非字符本身，虽然在 Base Array，字典中已有的任意一个词，其链路都是确定的、唯一的，因此并不存在歧义；但是对于一个新的字符串（不管是检索字符串还是准备为字典新增的词），Base Array 是不能确定该词是否位于词典之中的。对于这点，我们举个例子就知道了： 如果我们要在例树中确认外部的一个字符串“清中”是否是一个词，按照 Trie 树的查找规则，首先要查找“清”这个字，我们从根节点出发，获得|base[1]|+code(“清”)=3，然后转移到“清”节点，确认清在数组中存在，我们继续查找“中”，通过|base[3]|+code(“中”)=9获得位置9，字符串此时查询完毕，根据位置9的转移基数base[9]=-2确定该词在此终结，从而认为字符串“清中”是一个词。而这显然是错误的！事实上我们知道 “清中”这个词在 base array 中压根不存在，但是此时的 base array 却不能为此提供更多的信息。 为了解决这些问题，双数组 Trie 树专门设计了一个 check 数组： check array 与 base array 等长，它的作用是标识出 base array 中每个状态的前一个状态，以检验状态转移的正确性。 因此， 例树的 check array 应为： 如图，check array 元素与 base array 一一对应，每个 check array 元素标明了base array 中相应节点的父节点位置，比如“清”节点对应的check[2]=0，说明“清”节点的父节点在 base array 的0 位（也即根节点）。对于上例，程序在找到位置9之后，会检验 check[9]==2，以检验该节点是否与“清”节点处于同一链路，由于check[9]!=2，那么就可以判定字符串“清中”并不在词典之中。 综上，check array 巧妙的利用了父子节点间双向关系的唯一性（公式化的表达就是base[s]+c=t &amp; check[t]=s是唯一的，其中 s为父节点位置，t为子节点位置），避免了 base array 之中单向的状态转移关系所造成的歧义（公式化的表达就是base[s]+c=t）。 Trie 树的压缩双数组 Trie 树虽然大幅改善了经典 Trie 树的空间浪费，但是由于冲突发生时，程序总是向后寻找空地址，导致数组不可避免的出现空置，因此空间上还是会有些浪费。另外， 随着节点的增加，冲突的产生几率也会越来越大，字典构建的时间因此越来越长，为了改善这些问题，有人想到对双数组 Trie 进行尾缀压缩，具体做法是：将非公共前缀的词尾合并为一个节点（tail 节点），以此大幅减少节点总数，从而改善树的构建速度；同时将合并的词尾单独存储在另一个数组之中（Tail array）， 并通过 tail 节点的 base 值指向该数组的相应位置，以 {baby#, bachelor#, badge#, jar# }四词为例，其实现示意图如下： 对于这种改进的效果，看一下别人家的测试就知道了： 速度减少了base， check的状态数，以及冲突的概率，提高了插入的速度。在本地做了一个简单测试，随机插入长度1-100的随机串10w条，no tail的算法需120秒，而tail的算法只需19秒。查询速度没有太大差别。 内存状态数的减少的开销大于存储tail的开销，节省了内存。对于10w条线上URL，匹配12456条前缀，内存消耗9M，而no tail的大约16M 删除能很方便的实现删除，只需将tail删除即可。 对于本文的例树，若采用tail 改进，其最终效果是这一子的： 总结 Trie 树是一种以信息换时间的数据结构，其查询的复杂度为O(m) Trie 的单数组实现能够达到最佳的性能，但是其空间利用率极低，是典型的以空间换时间的实现 Trie 树的哈希实现可以很好的平衡性能需求和空间开销，同时能够实现词典的实时更新 Trie 树的双数组实现基本可以达到单数组实现的性能，同时能够大幅降低空间开销；但是其难以做到词典的实时更新 对双数组 Trie 进行 tail 改进可以明显改善词典的构建速度，同时进一步减少空间开销 原文]]></content>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程(3) START OF DATABASE RECOVERY]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B3%2F</url>
    <content type="text"><![CDATA[LCP：本地检查点，在NDB中，这意味着主内存中的所有数据都写入磁盘，我们还将更改的磁盘页写入磁盘，以确保磁盘上某个点之前的所有更改都可用。 执行REDO日志：这意味着我们一次读取REDO日志一条REDO日志记录，并在需要时执行REDO日志记录中的操作。 Apply the REDO log：执行REDO日志的同义词。 Prepare REDO log record: 这是一个REDO日志记录，包含有关数据库更改的信息（插入/删除/更新/写入）。 COMMIT REDO log：这是一个REDO日志记录，指定要实际执行Prepare REDO日志记录。 COMMIT REDO日志记录包含Prepare REDO日志记录的后向引用。 ABORT REDO log record：与COMMIT REDO日志记录类似，但此处事务已中止，因此无需应用REDO日志记录。 Database:在此上下文中表示当节点重新启动时驻留在集群或节点中的所有数据。 Off-line Database:意味着我们节点中的数据库不在线，因此不能用于读取。 这是恢复LCP后但在应用REDO日志之前的数据库状态。 Off-line Consistent database:这是一个数据库状态，它与最新的更改不是最新的，但它表示先前存在的数据库中的旧状态。 恢复LCP并执行REDO日志后，即可实现此状态。 On-line Database:这是一个最新的数据库状态，任何可用于读取数据的节点都有其数据库在线（实际上片段是逐个联机的）。 On-line Recoverable Database:这是一个也可以恢复的在线数据库。 在节点重启中，我们首先到达状态在线数据库，但是我们需要运行LCP，然后数据库也可以恢复到其当前状态。 可恢复的数据库也是持久的，这意味着当我们达到此状态时，我们将ACID中的D添加到数据库中。 Node:有API节点，数据节点和管理服务器节点。 数据节点是ndbd / ndbmtd进程，它运行所有数据库逻辑并包含数据库数据。 管理服务器节点是运行包含群集配置的ndb_mgmd并且还执行许多管理服务的进程。 API节点是应用程序进程的一部分，在mysqld中。 每个应用程序进程可以有多个API节点。 每个API节点通过套接字（或其他通信介质）连接到每个数据节点和管理服务器节点。 当一个人引用本文中的节点时，我们主要暗示的是我们在谈论一个数据节点。 Node Group:一组数据节点，它们都包含相同的数据。 节点组中的节点数等于我们在集群中使用的副本数。 Fragment:表的一部分，完全存储在一个节点组中。 Partition:Synonym of fragment. Fragment replica:这是一个节点中的一个片段。 一个片段最多可以有4个副本（因此节点组中最多可以有4个节点）。 Distribution information: 这是有关表的分区（片段的同义词）以及它们驻留在哪些节点上的信息，以及有关在每个片段副本上执行的LCP的信息。 Metadata:这是有关表，索引，触发器，外键，哈希映射，文件，日志文件组，表空间的信息。 字典信息：元数据的同义词。 LDM:代表本地数据管理器，这些是执行处理一个数据节点内处理的数据的代码的块。 它包含处理元组存储的块，哈希索引，T树索引，页面缓冲区管理器，表空间管理器，写入LCP的块和恢复LCP的块，磁盘数据的日志管理器。 作为START_COPYREQ的一部分，真正的数据库恢复过程是什么。 这里执行大多数重要的数据库恢复算法以使数据库再次联机。 仍需要早期阶段来恢复元数据和设置通信，设置内存并将起始节点作为数据节点集群中的完整公民。 START_COPYREQ遍历所有分发信息，并将START_FRAGREQ发送到拥有的DBLQH模块实例，以便在节点上恢复每个片段副本。 DBLQH将立即启动以恢复这些片段副本，它将对片段副本进行排队并一次恢复一个。 这发生在两个阶段，首先需要恢复本地检查点的所有片段副本开始这样做。 在发送了所有要恢复的片段副本之后，我们已经从存储在磁盘上的本地检查点恢复了所有片段（或者有时通过从活动节点获取整个片段），然后是运行磁盘数据UNDO日志的时候了。 最后，在运行此UNDO日志之后，我们已准备好通过应用REDO日志将片段副本恢复到最新的磁盘持久状态。 DBDIH将所有片段副本的所有必需信息发送到DBLQH，然后它将START_RECREQ发送到DBLQH以指示现在已发送所有片段副本信息。 START_RECREQ通过DBLQH代理模块发送，并且该部分被并行化，以便所有LDM实例并行执行以下部分。 如果我们正在进行初始节点重启，我们不需要恢复任何本地检查点，因为初始节点重启意味着我们在没有文件系统的情况下启动。 所以这意味着我们必须从节点组中的其他节点恢复所有数据。 在这种情况下，当我们收到START_FRAGREQ时，我们立即开始在DBLQH中应用片段副本的复制。 在这种情况下，我们不需要运行任何撤消或重做日志，因为没有本地检查点来恢复片段。 完成此操作后，DBDIH报告已通过将START_RECREQ发送到DBLQH发送了所有要启动的片段副本，我们将向TSMAN发送START_RECREQ，之后我们将完成数据的恢复。 我们将指定要作为REDO日志执行的一部分进行恢复的所有片段副本。 这是通过信号EXEC_FRAGREQ完成的。 当所有这些信号都被发送后，我们发送EXEC_SRREQ表示我们已经准备好在DBLQH中执行下一个REDO日志执行阶段。 当发送所有这些信号时，我们已经完成了所谓的DBLQH的第2阶段，DBLQH中的阶段1是在NDB_STTOR阶段3中开始准备REDO日志以进行读取。 因此，当这两个阶段都完成时，我们就可以开始在DBLQH中开始所谓的阶段3。 这些DBLQH阶段与启动阶段无关，这些阶段是DBLQH模块中启动的内部阶段。 DBLQH中的第3阶段是读取REDO日志并将其应用于从本地检查点恢复的片段副本。 这是创建在特定全局检查点上同步的数据库状态所必需的。 因此，我们首先为所有片段安装本地检查点，接下来我们应用REDO日志将片段副本与某个全局检查点同步。 在执行REDO日志之前，我们需要通过检查我们将恢复到所需全局检查点的所有片段副本的限制来计算要在REDO日志中应用的起始GCI和最后一个GCI。 DBDIH已存储有关片段副本的每个本地检查点的信息，这些信息是从REDO日志运行所需的全局检查点范围，以使其进入某个全局检查点的状态。 此信息已在START_FRAGREQ信号中发送。 DBLQH会将每个片段副本的所有这些限制合并到全局范围的全局检查点，以便为此LDM实例运行。 因此每个片段副本都有自己的GCP id范围来执行，这意味着所有这些起始范围的最小值和所有结束范围的最大值是我们需要在REDO日志中执行以引入集群的全局GCP ID范围 再次在线。 下一步是使用start和stop全局检查点id计算每个日志部分的REDO日志中的开始和停止兆字节。 计算这个所需的所有信息已经在内存中，所以这是一个纯粹的计算。 当我们执行REDO日志时，实际上我们只在正确的全局检查点范围内应用COMMIT记录。 COMMIT记录和实际更改记录位于REDO日志中的不同位置，因此对于每兆字节的REDO日志，我们记录REDO日志中我们必须返回多长时间才能找到更改记录。 在运行REDO日志时，我们维护一个相当大的REDO日志缓存，以避免在事务运行很长时间的情况下我们必须执行磁盘读取。 这意味着长时间运行和大型事务可能会对重新启动时间产生负面影响。 在所有日志部分完成此计算后，我们现在准备开始执行REDO日志。 在执行REDO日志完成后，我们还会在REDO日志中写入一些内容，以表明我们之前使用的任何信息都不会在以后使用。 我们现在需要等待所有其他日志部分也完成其REDO日志部分的执行。 REDO日志执行的设计使我们可以在多个阶段执行REDO日志，这适用于我们可以从多个活动节点重建节点的情况。 目前绝不应使用此代码。 因此，下一步是检查REDO日志部件的新头部和尾部。 这是通过使用启动和停止全局检查点来计算此数字的相同代码完成的。 代码的这一阶段还通过确保正确的REDO日志文件打开来准备REDO日志部分以编写新的REDO日志记录。 它还涉及一些相当棘手的代码，以确保正确处理已脏的页面。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程 (2) READ_CONFIG_REQ]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B2%2F</url>
    <content type="text"><![CDATA[READ_CONFIG_REQ对所有软件模块或多或少都相同。 它分配软件模块所需的内存并初始化内存（创建各种空闲列表等）。 它还读取模块感兴趣的各种配置参数（这些参数通常会影响我们分配的内存大小）。原文 它从CMVMI开始，分配大部分全局内存池，接下来我们有NDBFS为磁盘数据创建必要的文件目录，它还创建一次可以由一个文件使用的绑定IO线程（初始线程数可配置） 通过InitalNoOpenFiles），然后它创建了一些磁盘数据文件（用于处理磁盘数据的所有文件）使用的空闲线程（它们可通过IOThreadPool配置的数量），每个这样的线程可用于打开/读取/写入/关闭 磁盘数据文件。 最后，NDBFS还会创建从文件系统线程返回到其他线程的通信通道。 所有其他模块遵循相同的标准，它们根据硬编码定义或通过配置变量计算多个大小，它们为这些变量分配内存，最后它们初始化那些分配的内存结构。 STTOR Phase 0执行的第一个STTOR阶段是STTOR阶段0.在此阶段执行任何操作的唯一模块是NDBCNTR，如果启动是初始启动，则清除文件系统，CMVMI创建文件系统目录。 STTOR Phase 1下一阶段执行的是STTOR阶段1，在此阶段，大多数模块初始化一些更多数据，必要时设置对相邻模块的引用。 此外，DBDIH创建了一些特殊的互斥锁，确保一次只能在代码的某些部分中涉及一个进程。 NDBCNTR在阶段2开始初始化与运行NDB_STTOR相关的一些数据。如果配置为这样，CMVMI将锁定内存，此后它会安装正常的监视程序超时，因为现在执行所有大内存分配。 CMVMI还启动定期内存报告。 QMGR是此阶段中最活跃的模块。 它初始化一些数据，它从DBDIH获得重启类型（初始启动或正常启动），它打开与集群中所有节点的通信，它开始检查包含节点处理的节点故障。 最后，它运行协议以将新节点包括在心跳协议中。 这可能需要一段时间，因为节点包含过程一次只能引入一个节点，并且协议包含一些延迟。然后，BACKUP模块启动磁盘速度检查循环，该循环将在节点启动并运行时运行。 STTOR Phase 2下一步是执行STTOR阶段2.在STTOR阶段2中执行任何操作的唯一模块是NDBCNTR，它要求DIH执行重启类型，它从配置中读取节点，它初始化控制多长时间的部分超时变量 在我们执行部分启动之前等待。 NDBCNTR将信号CNTR_START_REQ发送到当前主节点中的NDBCNTR，该信号使主节点能够在必要时由于其他起始节点或某些其他条件而延迟该节点的启动。 对于集群启动/重启，它还使主节点有机会确保在启动节点之前等待足够的节点启动。 主设备一次只接受一个接收CNTR_START_CONF的节点，下一个节点只能在前一个起始节点完成复制元数据并释放元数据锁并锁定DIH信息后接收CNTR_START_CONF，这在STTOR阶段5中发生。 因此，在滚动重启中，第一个节点将获得CNTR_START_CONF，然后在DICT锁定上被阻塞，等待LCP完成，这是很常见的。 并行启动的其他节点将在CNTR_START_CONF上等待，因为一次只有一个节点可以通过它。 收到CNTR_START_CONF后，NDBCNTR继续运行NDB_STTOR阶段1.此处DBLQH初始化节点记录，它启动报告服务。 它还初始化有关REDO日志的数据，这还包括在所有类型的初始启动时初始化磁盘上的REDO日志（可能非常耗时）。 DBDICT初始化模式文件（包含已在集群中创建的表和其他元数据对象）。 DBTUP初始化一个默认值片段，DBTC和DBDIH初始化一些数据变量。 完成NDBCNTR中的NDB_STTOR阶段后，STTOR阶段2中没有更多工作。 STTOR Phase 3下一步是运行STTOR阶段3.大多数需要集群中节点列表的模块在此阶段读取此信息。 DBDIH在此阶段读取节点，DBDICT设置重启类型。 下一个NDBCNTR接收此阶段并启动NDB_STTOR阶段2.在此阶段，DBLQH设置从其操作记录到DBACC和DBTUP中的操作记录的连接。 这是针对所有DBLQH模块实例并行完成的。 DBDIH现在通过锁定元数据来准备节点重启过程。 这意味着我们将等待任何正在进行的元数据操作完成，并且当它完成时，我们将锁定元数据，这样在我们完成复制元数据信息的阶段之前，不能进行元数据更改。 锁定的原因是所有元数据和分发信息都是完全复制的。 因此，在将数据从主节点复制到起始节点时，我们需要锁定此信息。 虽然我们保留了此锁，但我们无法通过元数据事务更改元数据。 在稍后复制元数据之前，我们还需要确保没有运行本地检查点，因为这也会更新分发信息。 锁定后，我们需要请求从主节点启动节点的权限。 启动节点的许可请求由发送START_PERMREQ到主节点的起始节点处理。 如果另一个节点已在处理节点重启，则可能会收到否定答复，如果需要初始启动，则可能会失败。 如果另一个节点已经启动，我们将等待3秒钟再试一次。 这在DBDIH中作为NDB_STTOR阶段2的一部分执行。 在完成NDB_STTOR阶段2之后，STTOR阶段3继续由CMVMI模块激活对扫描和密钥操作使用的发送打包数据的检查。 接下来，BACKUP模块读取配置的节点。 接下来，SUMA模块设置对页面池的引用，以便它可以重用此全局内存池中的页面，然后DBTUX设置重新启动类型。 最后，PGMAN启动一个stats循环和一个清理循环，只要节点启动并运行，它就会运行。 如果我们的节点仍然涉及主节点中正在进行的某些进程，我们可能会崩溃节点。 这是相当正常的，只会触发一次崩溃，然后是天使进程正常的新启动。 权限请求由主服务器向所有节点发送信息来处理。 对于初始启动，权限请求可能非常耗时，因为我们必须使所有节点上元数据中所有表的所有本地检查点无效。 目前没有此失效过程的并行化，因此它将一次使一个表无效。 STTOR Phase 4完成STTOR阶段3后，我们进入STTOR阶段4.此阶段由DBLQH在BACKUP模块中获取备份记录开始，该备份记录将用于本地检查点处理。 下一个NDBCNTR启动NDB_STTOR阶段3.这也在DBLQH中开始，我们在其中读取已配置的节点。 然后我们开始阅读REDO日志以进行设置（我们将在后台设置它，它将通过稍后描述的集群重启/节点重启的另一部分进行同步），对于所有类型的初始启动，我们将等到 REDO日志的初始化已经完成，直到报告此阶段完成为止。 下一个DBDICT将读取已配置的节点，然后DBTC将读取已配置的节点并启动事务计数器报告。 接下来在NDB_STTOR阶段3中，DBDIH初始化重启数据以进行初始启动。 在完成STTOR第4阶段的工作之前，NDBCNTR将设置一个等待点，使所有起始节点在继续之前达到此点。 这仅适用于群集启动/重新启动，因此不适用于节点重新启动。 主节点控制此等待点，并在集群重启的所有节点都达到此点时将信号NDB_STARTREQ发送到DBDIH。 稍后将详细介绍此信号。 STTOR阶段4中发生的最后一件事是DBSPJ读取配置的节点。 STTOR Phase 5我们现在进入STTOR阶段5.这里做的第一件事是运行NDB_STTOR阶段4.只有DBDIH在这里做了一些工作，它只在节点重启时做了一些事情。 在这种情况下，它要求当前主节点通过向其发送START_MEREQ信号来启动它。 START_MEREQ的工作原理是从主DBDIH节点复制分发信息，然后从主DBDICT复制元数据信息。 它一次复制一个分发信息表，这使得该过程有点慢，因为它包括将表写入起始节点中的磁盘。 跟踪此事件的唯一方法是在起始节点中的DBDIH中将每个表的表分布信息写入。 我们可以跟踪在起始节点DBDICT中接收的DICTSTARTREQ的接收。 当复制DBDIH和DBDICT信息时，我们需要阻止全局检查点，以便从现在开始将新节点包含在元数据和分发信息的所有更改中。 这是通过将INCL_NODEREQ发送到所有节点来执行的。 在此之后，我们可以释放由DBDIH在STTOR阶段2中设置的元数据锁。 完成NDB_STTOR阶段4后，NDBCNTR以下列方式再次同步启动： 如果初始集群启动和主节点然后创建系统表如果集群启动/重新启动，则等待所有节点到达此点。 等待集群启动/重启中的节点后，在主节点中运行NDB_STTOR阶段5（仅发送到DBDIH）。 如果节点重新启动，则运行NDB_STTOR阶段5（仅发送到DBDIH）。 DBDIH中的NDB_STTOR阶段5正在等待本地检查点的完成（如果它是主设备）并且我们正在运行集群启动/重启。 对于节点重启，我们将信号START_COPYREQ发送到起始节点，要求将数据复制到我们的节点。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程 (1) Restart Phases in MySQL Cluster]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B1%2F</url>
    <content type="text"><![CDATA[在MySQL Cluster中，重新启动是分阶段处理的，节点的重启由一组阶段驱动。此外，节点重启也与已启动的节点以及与我们的节点并行启动的其他节点同步。此注释将描述所使用的各个阶段。 原文启动节点的第一步是创建数据节点运行时环境。数据节点进程通常使用angel进程运行，此angel进程确保数据节点万一失败时能够自动重新启动。因此，再次运行数据节点的唯一原因是在OS崩溃之后或运营商关闭之后或作为软件升级的一部分。 当启动数据节点时，数据节点需要一个节点id，这是通过设置参数–ndb-nodeid在启动datanode时，或者在检索配置时由管理服务器分配。对于数据节点的所有重新启动，angel进程将确保所分配的节点id将相同。 在分配数据节点进程之后，启动进程保持为angel进程并且新进程成为实际的数据节点进程。实际的数据节点进程首先从managementserver检索配置。 在这个阶段我们已经阅读了选项，我们已经分配了一个节点ID，我们从管理服务器加载了配置。我们将在数据节点日志中打印一些关于我们的线程配置和其他一些重要信息。为了确保我们找到正确的文件并在正确的位置创建文件，我们设置了数据节点进程的数据文件夹。 接下来我们必须启动看门狗线程，因为我们现在开始执行操作，我们希望确保我们不要由于某些软件错误而卡住了。 接下来我们将分配全局内存池的内存，这是分配最多内存的地方，我们仍然有相当多的内存分配，作为NDB内核中各种软件模块初始化的部分，但是我们正在逐步使用全局内存池。 分配内存可能是一个相当耗时的过程，其中操作系统可以为每个分配的GByte内存提供长达一秒的时间（自然是OS依赖的并且会随着时间的推移而变化）。 实际上在这里消耗时间实际上是我们还触摸每个页面以确保分配的内存也被映射到真实物理内存以避免在我们运行该过程时页面未命中。为了加快这个过程，我们已经触及了内存多线程。实际上，大多数内存的分配是可配置的，配置变量LateAlloc可用于延迟大多数内存分配到重启的早期阶段。 分配全局内存池后，我们初始化运行时环境使用的所有数据。 这可确保我们准备好在数据节点进程启动后立即在线程之间发送和接收数据。 在这一点上，我们只启动了看门狗进程并且线程在创建进程的过程中启动（如果我们运行ndbmtd，这个线程稍后将被转换为第一个接收线程，如果我们正在运行，则该线程将被转换为唯一的执行线程NDBD）。 下一步是加载所有软件模块并初始化它们，以确保在消息开始到达执行时正确设置它们。 在我们启动运行时环境之前，我们还需要激活发送和接收服务。 这涉及创建一个套接字客户端线程，该线程试图连接到集群中其他节点的套接字服务器部分，并创建一个线程来监听用于我们作为套接字服务器通信的那些数据节点的套接字服务器。 默认行为是nodeid最低的节点是通信设置中的套接字服务器。 这可以在数据节点配置中更改。 在我们继续并启动数据节点环境之前，我们将运行时环境的启动信号放在其正确的作业缓冲区中。 实际上，为了启动系统，需要在作业缓冲区中放置两个相等的信号。 第一个启动信号开始与其他节点的通信，并设置状态以等待下一个信号实际启动系统。 第二个将开始运行启动阶段。 最后，我们启动运行时环境的所有线程。 这些当前可以包括主线程，代表线程，多个tc线程，多个发送线程，多个接收线程和多个ldm线程。 鉴于已预先分配了所有线程的通信缓冲区，我们可以在这些线程启动时立即开始发送信号。 接收线程一旦到达其线程启动代码中的那一点就会开始处理其接收到的信号。 有两个相同的启动信号，第一个启动定期发送的重复信号，以跟踪数据节点中的时间。 只有第二个开始执行各种启动阶段。 数据节点的启动在一组阶段中处理。 第一阶段是将信号READ_CONFIG_REQ发送到内核中的所有软件模块，然后将STTOR类似地发送到256个阶段的所有软件模块，编号从0到255.这些模块的编号从0到255，我们不使用全部 这些阶段，但代码是灵活的，以便任何这些阶段可以现在使用或在将来的某个时间使用。 此外，我们还有6个模块，这些模块涉及另外一组启动阶段。 在这些阶段发送的信号称为NDB_STTOR。 最初的想法是将此消息视为NDB子系统的本地启动。 这些信号由NDBCNTR发送和处理，并作为NDBCNTR中STTOR处理的一部分发送。 这意味着它成为启动阶段的连续部分。 在开始阶段之前，我们确保任何管理节点都可以连接到我们的节点，并且所有其他节点都已断开连接，并且它们只能向QMGR模块发送消息。 管理服务器接收关于数据节点中的各种事件的报告，并且QMGR模块负责将数据节点包括在集群中。 在我们被包含在集群中之前，我们无法以任何方式与其他节点通信。 开始总是从主线程开始，其中每个软件模块至少由所有多线程模块包含的代理模块表示。 代理模块使用一条消息和一条回复，可以轻松地向一组相同类型的模块发送和接收消息。 READ_CONFIG_REQ信号始终以相同的顺序发送。 它首先发送到CMVMI，这是接收启动顺序的块，它执行许多功能，软件模块可以从这些功能影响运行时环境。 它通常会分配进程的大部分内存并触及所有内存。 它是主线程的一部分。 接收READ_CONFIG_REQ的下一个模块是NDBFS，这是控制文件系统线程的模块，该模块位于主线程中。 下一个模块是DBINFO，该模块支持ndbinfo数据库，用于以表格格式获取有关数据节点内部的信息，该模块位于主线程中。 接下来是DBTUP，这是存储实际数据的模块。 下一个DBACC，存储主键和唯一键哈希索引的模块以及我们控制行锁的位置。 这两个块都包含在ldm线程中。 接下来是DBTC，即管理事务协调的模块，该模块是tc线程的一部分。 接下来是DBLQH，该模块通过键操作和扫描控制对数据的操作，并且还处理REDO日志。 这是ldm线程的主要模块。 接下来是DBTUX，它操作有序索引重用页面，用于在DBTUP中存储行，也是ldm线程的一部分。 接下来是DBDICT，这是一个字典模块，用于存储和处理有关表和列，表空间，日志文件等的所有元数据信息。 DICT是主线程的一部分。 接下来是DBDIH，用于存储和处理有关所有表，表分区和每个分区的所有副本的分发信息的模块。 它控制本地检查点进程，全局检查点进程并控制重新启动处理的主要部分。 DIH模块是主线程的一部分。 接下来是控制重启阶段的NDBCNTR，它是主线程的一部分。 接下来是QMGR，它负责处理心跳协议以及包含和排除集群中的节点。 它是主线程的一部分。 接下来是TRIX，它执行与有序索引和其他基于触发器的服务相关的一些服务。 它是tc线程的一部分。 接下来是BACKUP，它用于备份和本地检查点，是ldm线程的一部分。 接下来是DBUTIL，它提供了许多服务，例如代表模块中的代码执行密钥操作。 它是主线程的一部分。 接下来是负责复制事件的SUMA模块，这是由rep线程处理的模块。 接下来是TSMAN，然后是LGMAN，然后是PGMAN，它们都是磁盘数据处理的一部分，负责处理表空间，UNDO日志记录和页面管理。 它们都是ldm线程的一部分。 RESTORE是一个用于在启动时恢复本地检查点的模块。 该模块也是ldm线程的一部分。 最后，我们有DBSPJ模块来处理向下推送到数据节点的连接查询，它作为tc线程的一部分执行。 DBTUP，DBACC，DBLQH，DBTUX，BACKUP，TSMAN，LGMAN，PGMAN，RESTORE都是紧密集成的模块，它们在每个节点中本地处理数据和索引。 这组模块形成一个LDM实例，每个节点可以有多个LDM实例，这些实例可以分布在一组线程上。 每个LDM实例都拥有自己的数据分区。 我们还有两个不属于重启处理的模块，这是TRPMAN模块，它执行许多与传输相关的功能（与其他节点通信）。 它在接收线程中执行。 最后，我们有THRMAN在每个线程中执行并执行一些线程管理功能。 所有模块都接收READ_CONFIG_REQ，所有模块也接收STTOR用于阶段0和阶段1.在阶段1中，他们报告他们希望获得更多信息的起始阶段。 在READ_CONFIG_REQ期间，线程可以在模块中执行很长时间，因为我们可以分配和触摸大尺寸的存储器。 这意味着我们的监视程序线程在此阶段有一个特殊的超时，以确保我们不会因为长时间初始化内存而导致进程崩溃。 在正常操作中，每个信号应仅执行少量微秒。 通过将消息STTOR发送到所有模块来同步启动阶段，逻辑上每个模块从0到255获得每个启动阶段的该信号。然而，响应消息STTORRY包含模块真正感兴趣的启动阶段列表。 处理起始相位信号的NDBCNTR模块可以优化掉不需要的任何信号。 模块接收STTOR消息的顺序对于所有阶段都是相同的： 1) NDBFS2) DBTC3) DBDIH4) DBLQH5) DBACC6) DBTUP7) DBDICT8) NDBCNTR9) CMVMI10) QMGR11) TRIX12) BACKUP13) DBUTIL14) SUMA15) DBTUX16) TSMAN17) LGMAN18) PGMAN19) RESTORE20) DBINFO21) DBSPJ 此外，还有一个由NDBCNTR控制的特殊启动阶段处理，因此当NDBCNTR收到自己的STTOR消息时，它会启动涉及模块的本地启动阶段处理，DBLQH，DBDICT，DBTUP，DBACC，DBTC和DBDIH。 对于阶段2到8，会发生这种情况。在这些启动阶段发送的消息是NDB_STTOR和NDB_STTORRY，它们的处理方式与STTOR和STTORRY类似。 模块还以相同的顺序接收所有阶段的启动阶段，此顺序为： 1) DBLQH2) DBDICT3) DBTUP4) DBACC5) DBTC6) DBDIH 对于那些多线程的模块，STTOR和NDB_STTOR消息始终由在主线程中执行的代理模块接收。 然后，代理模块将STTOR和NDB_STTOR消息发送到模块的每个单独实例（实例数通常与线程数相同，但有时可能不同）。 它并行执行，因此所有实例并行执行STTOR。 因此，有效地，模块的每个实例将在逻辑上首先接收READ_CONFIG_REQ，然后为每个启动阶段接收一组STTOR消息，并且一些模块也将按特定顺序接收NDB_STTOR。 所有这些消息都按特定顺序发送并按顺序发送。 因此，这意味着我们能够通过在正确的启动阶段执行操作来控制何时完成任务。 接下来，我们将逐步描述节点重启（或节点作为集群启动/重启的一部分启动）中发生的情况。 启动目前是一个顺序过程，除非声明它并行发生。 以下描述因此描述了当前实际发生的顺序。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库中的undo日志、redo日志、检查点]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84undo%E6%97%A5%E5%BF%97%E3%80%81redo%E6%97%A5%E5%BF%97%E3%80%81%E6%A3%80%E6%9F%A5%E7%82%B9%2F</url>
    <content type="text"><![CDATA[数据库存放数据的文件，本文称其为data file。数据库的内容在内存里是有缓存的，这里命名为db buffer。某次操作，我们取了数据库某表格中的数据，这个数据会在内存中缓存一些时间。对这个数据的修改在开始时候也只是修改在内存中的内容。当db buffer已满或者遇到其他的情况，这些数据会写入data file。 undo，redo日志在内存里也是有缓存的，这里将其叫做log buffer。磁盘上的日志文件称为log file。log file一般是追加内容，可以认为是顺序写，顺序写的磁盘IO开销要小于随机写。 Undo日志记录某数据被修改前的值，可以用来在事务失败时进行rollback；Redo日志记录某数据块被修改后的值，可以用来恢复未写入data file的已成功事务更新的数据。 例如某一事务的事务序号为T1，其对数据X进行修改，设X的原值是5，修改后的值为15，那么Undo日志为&lt;T1, X, 5&gt;，Redo日志为&lt;T1, X, 15&gt;。 当用户生成一个数据库事务时，undo log buffer会记录被修改的数据的原始值，redo会记录被修改的数据的更新后的值。 redo日志应首先持久化在磁盘上，然后事务的操作结果才写入db buffer，（此时，内存中的数据和data file对应的数据不同，我们认为内存中的数据是脏数据），db buffer再选择合适的时机将数据持久化到data file中。这种顺序可以保证在需要故障恢复时恢复最后的修改操作。先持久化日志的策略叫做Write Ahead Log，即预写日志。 在很多系统中，undo日志并非存到日志文件中，而是存放在数据库内部的一个特殊段中。本文中就把这些存储行为都泛化为undo日志存储到undo log file中。 对于某事务T，在log file的记录中必须开始于事务开始标记（比如“start T”），结束于事务结束标记（比如“end T”、”commit T”）。在系统恢复时，如果在log file中某个事务没有事务结束标记，那么需要对这个事务进行undo操作，如果有事务结束标记，则redo。 在db buffer中的内容写入磁盘数据库文件之前，应当把log buffer的内容写入磁盘日志文件。 有一个问题，redo log buffer和undo log buffer存储的事务数量是多少，是按照什么规则将日志写入log file？如果存储的事务数量都是1个，也就意味着是将日志立即刷入磁盘，那么数据的一致性很好保证。在执行事T时，突然断电，如果未对磁盘上的redo log file发生追加操作，可以把这个事务T看做未成功。如果redo log file被修改，则认为事务是成功了，重启数据库使用redo log恢复数据到db buffer和 data file即可。 如果存储多个的话，其实也挺好解释的。就是db buffer写入data file之前，先把日志写入log file。这种方式可以减少磁盘IO，增加吞吐量。不过，这种方式适用于一致性要求不高的场合。因为如果出现断电等系统故障，log buffer、db buffer中的完成的事务会丢失。以转账为例，如果用户的转账事务在这种情况下丢失了，这意味着在系统恢复后用户需要重新转账。 检查点checkpointcheckpoint是为了定期将db buffer的内容刷新到data file。当遇到内存不足、db buffer已满等情况时，需要将db buffer中的内容/部分内容（特别是脏数据）转储到data file中。在转储时，会记录checkpoint发生的”时刻“。在故障回复时候，只需要redo/undo最近的一次checkpoint之后的操作。 幂等性问题在日志文件中的操作记录应该具有幂等性。幂等性，就是说同一个操作执行多次和执行一次，结果是一样的。例如，5*1 = 5*1*1*1，所以对5的乘1操作具有幂等性。日志文件在故障恢复中，可能会回放多次（比如第一次回放到一半时系统断电了，不得不再重新回放），如果操作记录不满足幂等性，会造成数据错误。 InnoDB Redo Flush及脏页刷新机制深入分析我们知道InnoDB采用Write Ahead Log策略来防止宕机数据丢失，即事务提交时，先写重做日志，再修改内存数据页，这样就产生了脏页。既然有重做日志保证数据持久性，查询时也可以直接从缓冲池页中取数据，那为什么还要刷新脏页到磁盘呢？如果重做日志可以无限增大，同时缓冲池足够大，能够缓存所有数据，那么是不需要将缓冲池中的脏页刷新到磁盘。但是，通常会有以下几个问题： 服务器内存有限，缓冲池不够用，无法缓存全部数据 重做日志无限增大成本要求太高 宕机时如果重做全部日志恢复时间过长事实上，当数据库宕机时，数据库不需要重做所有的日志，只需要执行上次刷入点之后的日志。这个点就叫做Checkpoint，它解决了以上的问题： 缩短数据库恢复时间 缓冲池不够用时，将脏页刷新到磁盘 重做日志不可用时，刷新脏页 InnoDB引擎通过LSN(Log Sequence Number)来标记版本，LSN是日志空间中每条日志的结束点，用字节偏移量来表示。每个page有LSN，redo log也有LSN，Checkpoint也有LSN。可以通过命令show engine innodb status来观察：]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机性能监控及调优]]></title>
    <url>%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%8F%8A%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[常用虚拟机性能监控工具JDK 命令行工具其中的重中之重是 jstat 命令！而它最常用的参数就是 -gcutil，使用格式如下：1jstat -gcutil [pid] [intervel] [count] 输出如下： S0：堆上 Survivor space 0 区已使用空间的百分比 S1：堆上 Survivor space 1 区已使用空间的百分比 E：堆上 Eden 区已使用空间的百分比 O：堆上 Old space 区已使用空间的百分比 P：堆上 Perm space 区已使用空间的百分比 YGC：从程序启动到采样时发生的 Minor GC 次数 YGCT：从程序启动到采样时 Minor GC 所用的时间 FGC：从程序启动到采样时发生的 Full GC 次数 FGCT：从程序启动到采样时 Full GC 所用的时间 GCT：从程序启动到采样时 GC 的总时间ps 命令 (Linux)对于 jps 命令，其实没必要使用，一般使用 Linux 里的 ps 就够了，ps 为我们提供了当前进程状态的一次性的查看，它所提供的查看结果并不动态连续的，如果想对进程时间监控，应该用 top 工具。 Linux 上进程的 5 种状态 运行 [R, Runnable]：正在运行或者在运行队列中等待； 中断 [S, Sleep]：休眠中, 受阻, 在等待某个条件的形成或接受到信号； 不可中断 [D]：收到信号不唤醒和不可运行, 进程必须等待直到有中断发生； 僵死 [Z, zombie]：进程已终止, 但进程描述符存在, 直到父进程调用 wait4() 系统调用后释放； 停止 [T, Traced or stop]：进程收到 SIGSTOP, SIGSTP, SIGTIN, SIGTOU 信号后停止运行运行。 1234567891011ps -A # 列出所有进程信息（非详细信息）ps aux # 列出所有进程的信息ps aux | grep zshps -ef # 显示所有进程信息，连同命令行ps -ef | grep zsh ps -u root # 显示指定用户信息ps -l # 列出这次登录bash相关信息ps axjf # 同时列出进程树状信息 JVM 常见参数设置内存设置参数 -Xms：初始堆大小，JVM 启动的时候，给定堆空间大小。 -Xmx：最大堆大小，如果初始堆空间不足的时候，最大可以扩展到多少。 -Xmn：设置年轻代大小。整个堆大小 = 年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为 64M，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun 官方推荐配置为整个堆的 3/8。 -Xss： 设置每个线程的 Java 栈大小。JDK 5 后每个线程 Java 栈大小为 1M。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000 左右。 -XX:NewRatio=n：设置年轻代和年老代的比值。如为 3，表示年轻代与年老代比值为 1:3。 -XX:MaxTenuringThreshold：设置垃圾最大年龄。如果设置为 0 的话，则年轻代对象不经过 Survivor 区，直接进入年老代。对于年老代比较多的应用（即 Minor GC 过后有大量对象存活的应用），可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在 Survivor 区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概率。 设置经验 开发过程的测试应用，要求物理内存大于 4G 1234-Xmx3550m-Xms3550m -Xmn2g-Xss128k 高并发本地测试使用，大对象相对较多（如 IO 流） 1234567-Xmx3550m-Xms3550m-Xss128k-XX:NewRatio=4-XX:SurvivorRatio=4-XX:MaxPermSize=160m-XX:MaxTenuringThreshold=0 环境： 16G 物理内存，高并发服务，重量级对象中等（线程池，连接池等），常用对象比例为 40%（即运行过程中产生的对象 40% 是生命周期较长的） 1234567-Xmx10G-Xms10G-Xss1M-XX:NewRatio=3-XX:SurvivorRatio=4 -XX:MaxPermSize=2048m-XX:MaxTenuringThreshold=5 收集器设置参数 收集器设置 -XX:+UseSerialGC：设置串行收集器，年轻带收集器。 -XX:+UseParallelGC：设置并行收集器。 -XX:+UseParNewGC：设置年轻代为并行收集。可与 CMS 收集同时使用。JDK 5.0 以上，JVM 会根据系统配置自行设置，所以无需再设置此值。 -XX:+UseParallelOldGC：设置并行年老代收集器，JDK6.0 支持对年老代并行收集。 -XX:+UseConcMarkSweepGC：设置年老代并发收集器，测试中配置这个以后，-XX:NewRatio 的配置失效，原因不明。所以，此时年轻代大小最好用 -Xmn 设置。 -XX:+UseG1GC：设置 G1 收集器。 并行收集器参数设置 -XX:ParallelGCThreads=n：设置并行收集器收集时最大线程数使用的 CPU 数。并行收集线程数。 -XX:MaxGCPauseMillis=n：设置并行收集最大暂停时间，单位毫秒。 -XX:GCTimeRatio=n：设置垃圾回收时间占程序运行时间的百分比。 -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的 Survivor 区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。 -XX:CMSFullGCsBeforeCompaction=n：由于 CMS 不对内存空间进行压缩、整理，所以运行一段时间以后会产生”碎片”，使得运行效率降低。此值设置运行多少次 GC 以后对内存空间进行压缩、整理。 -XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片。虚拟机调优案例分析高性能硬件上的程序部署策略补充：64 位虚拟机在 Java EE 方面，企业级应用经常需要使用超过 4GB 的内存，此时，32 位虚拟机将无法满足需求，可是 64 位虚拟机虽然可以设置更大的内存，却存在以下缺点： 内存问题： 由于指针膨胀和各种数据类型对齐补白的原因，运行于 64 位系统上的 Java 应用程序需要消耗更多的内存，通常要比 32 位系统额外增加 10% ~ 30% 的内存消耗。 性能问题： 64 位虚拟机的运行速度在各个测试项中几乎全面落后于 32 位虚拟机，两者大概有 15% 左右的性能差距。 服务系统经常出现卡顿（Full GC 时间太长）首先 jstat -gcutil 观察 GC 的耗时，jstat -gccapacity 检查内存用量（也可以加上 -verbose:gc 参数获取 GC 的详细日志），发现卡顿是由于 Full GC 时间太长导致的，然后 jinfo -v pid，查看虚拟机参数设置，发现 -XX:NewRatio=9，这就是原因： 新生代太小，对象提前进入老年代，触发 Full GC 老年代较大，一次 Full GC 时间较长 可以调小 NewRatio 的值，尽肯能让比较少的对象进入老年代。 除了 Java 堆和永久代之外，会占用较多内存的区域 区域 大小调整 / 说明 内存不足时抛出的异常 直接内存 -XX:MaxDirectMemorySize OutOfMemoryError: Direct buffer memory 线程堆栈 -Xss StackOverflowError 或 OutOfMemoryError: unable to create new native thread Socket 缓存区 每个 Socket 连接都有 Receive(37KB) 和 Send(25KB) 两个缓存区 IOException: Too many open files JNI 代码 如果代码中使用 JNI 调用本地库，那本地库使用的内存也不在堆中 虚拟机和 GC 虚拟机、GC 代码执行要消耗一定内存 从 GC 调优角度解决新生代存活大量对象问题（Minor GC 时间太长） 将 Survivor 空间去除，让新生代中存活的对象在第一次 Minor GC 后立刻进入老年代，等到 Full GC 时再清理。 参数调整方法： -XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0 -XX:AlwaysTenure]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络基础知识]]></title>
    <url>%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[一次完整的 HTTP 请求所经历的步骤即在浏览器中输入 URL 地址 &gt;&gt; 显示主页的过程。总体来说分为以下几个过程： DNS 解析 TCP 连接 发送 HTTP 请求 服务器处理请求并返回 HTTP 报文 浏览器解析渲染页面 连接结束 简单的来说，就是先经过下图的过程，将客户端的请求传到服务器，再经过下图过程的逆过程，将客户端请求的数据返回给客户端，然后客户端浏览器对返回的数据进行渲染，最终得到了我们看到的页面。 DNS 解析DNS 解析的过程就是寻找哪台机器上有你需要资源的过程。也就是说，把你输入的 URL 地址转换为 IP 地址。 TCP 连接客户端 (浏览器) 通过 TCP 传输协议建立到服务器的 TCP 连接，详见后面 TCP 是如何传输数据，以及三次握手和四次挥手等。 发送 HTTP 请求发送 HTTP 请求的过程就是构建 HTTP 请求报文并通过 TCP 协议中发送到服务器指定端口 (HTTP 协议 80/8080，HTTPS 协议443)。HTTP 请求报文是由三部分组成： 请求行 、 请求报头 、 请求正文 。 服务器处理请求并返回 HTTP 报文后端从固定的端口接收到 TCP 报文（这一部分对应于编程语言中的 socket），它会对 TCP 连接进行处理，对 HTTP 协议进行解析，并按照报文格式进一步封装成 HTTP Request 对象，供上层使用。这一部分工作一般是由 Web 服务器去进行，我使用过的 Web 服务器有 Tomcat，Jetty。 HTTP 响应报文也是由三部分组成： 状态码 、 响应报头 、 响应报文 。 状态码： 1xx：指示信息–表示请求已接收，继续处理。 2xx：成功–表示请求已被成功接收、理解、接受。 3xx：重定向–要完成请求必须进行更进一步的操作。 4xx：客户端错误–请求有语法错误或请求无法实现。 5xx：服务器端错误–服务器未能实现合法的请求。 浏览器解析渲染页面即浏览器收到 HTML、CSS、JS 文件后，把页面呈现到屏幕上的过程。 DNS 解析DNS 解析的过程就是寻找哪台机器上有你需要资源的过程。也就是说，把你输入的 URL 地址转换为 IP 地址。 DNS 域名解析过程如下图所示，简单来说就是先查自己的本地域名服务器，如果自己就有缓存，直接从缓存里面读就可以，如果缓存里没有，本地域名服务器会发出递归连环问去查找。 DNS 负载均衡 DNS 可以根据每台机器的负载量，该机器离用户地理位置的距离等等，返回一个合适的机器的 IP 给用户，这个过程就是 DNS 负载均衡，又叫做 DNS 重定向。大家耳熟能详的 CDN (Content Delivery Network) 就是利用 DNS 的重定向技术实现的，DNS 服务器会返回一个跟用户最接近的服务器的 IP 地址给用户，CDN 节点的服务器负责响应用户的请求。 CDN（Content Distribute Network）CDN，内容分发网络。最简单的 CDN 网络由一个 DNS 服务器和几台缓存服务器组成： 当用户点击网站页面上的内容 URL，经过本地 DNS 系统解析，DNS 系统会最终将域名的解析权交给 CNAME 指向的 CDN 专用 DNS 服务器。 CDN 的 DNS 服务器将 CDN 的全局负载均衡设备 IP 地址返回用户。 用户向 CDN 的全局负载均衡设备发起内容 URL 访问请求。 CDN 全局负载均衡设备根据用户 IP 地址，以及用户请求的内容 URL，选择一台用户所属区域的区域负载均衡设备，告诉用户这台服务器的 IP 地址，让用户向这台设备发起请求。选择的依据包括： 根据用户 IP 地址，判断哪一台服务器距用户最近； 根据用户所请求的 URL 中携带的内容名称，判断哪一台服务器上有用户所需内容； 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力； 用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端。 如果这台缓存服务器上并没有用户想要的内容，而区域均衡设备依然将它分配给了用户，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。 TCP 是如何传输数据的？TCP (Transmission Control Protocol, TCP)，是一种面向连接、确保数据在端到端间可靠传输的协议。在传输前需要先建立一条可靠的传输链路，然后让数据在这条链路上流动，完成传输。简单来说就是，TCP 在想尽各种办法保证数据传输的可靠性，为了可靠性 TCP 会这样进行数据传输： 三次握手建立连接； 对发出的每一个字节进行编号确认，校验每一个数据包的有效性，在出现超时进行重传； 通过流量控制（通过滑动窗口协议实现）和拥塞控制（慢启动和拥塞避免、快重传和快恢复）等机制，避免网络状况恶化而影响数据传输； 四次挥手断开连接。 TCP 报头结构TCP 头部长度的前 20 字节是固定的，后面部分长度不定，但最多 40 字节 ，因此 TCP 头部一般在 20 ~ 60 字节之间。它的结构图如下： 它的每一字段的说明如下： 0 ~ 32 比特：源端口和目的端口 ，各占 16 比特（2 字节）。 32 ~ 64 比特：序列号 seq ，当前 TCP 数据报数据部分的第一个字节的序号（4 字节）。 64 ~ 96 比特：确认序号 ack ，表示当前主机作为接收端时，下一个希望接收的序列号是多少，确认号 = 当前主机已经正确接收的最后一个字节的序列号 + 1 96 ~ 112 比特：数据报报头长度，保留字段，标识符。 标识符：用于表示 TCP 报文的性质，只能是 0 或 1。TCP 的常用标识符： URG=1：紧急指针有效性标志，表示本数据报的数据部分包含紧急信息，紧急数据一定位于当前数据包数据部分的最前面，后面的紧急指针则标明了紧急数据的尾部。 ACK=1：在连接建立后传送的所有报文段都必须把 ACK 置 1，也就是说 ACK=1 后确认号字段才有效。 PSH=1：接收方应尽快将报文段提交至应用层，不会等到缓冲区满后再提交，一些交互式应用需要这样的功能，降低命令的响应时间。 RST=1：当该值为 1 时，表示当前 TCP 连接出现严重问题，必须要释放重连。 SYN=1：用在建立连接时 SYN=1, ACK=0：当前报文段是一个连接请求报文。 SYN=1, ACK=1：表示当前报文段是一个同意建立连接的应答报文。 FIN=1：表示此报文段是一个释放连接的请求报文。 112 ~ 128 比特：接收窗口大小 ，该字段用于实现 TCP 的流量控制。 它表示当前接收方的接收窗口的剩余大小，发送方收到该值后会将发送窗口调整成该值的大小（收到一个数据报就调整一次）。发送窗口的大小又决定了发送速率，所以接收方通过设置该值就可以控制发送放的发送速率。 128 ~ 144 比特：校验和 ，用于接收端检验整个数据包在传输过程中是否出错。 144 ~ 160 比特：紧急指针 ， 用来标明紧急数据的尾部，和 URG 标识符一起使用。 TCP 三次握手、四次挥手 TCP 的三次握手：为了建立可靠的通信信道，即双方确认自己与对方的发送和接收都是正常的。 TCP 的四次挥手：确保双方都在没有想说的内容之后，释放 TCP 连接。 三次握手三次握手的流程 第一次握手：A 向 B 发送建立连接请求（A 对 B 说：“我们在一起吧！”） 第二次握手：B 收到 A 的建立连接请求后，发给 A 一个同意建立连接的应答报文（B 对 A 说：“好的，同意和你在一起啦”） 第三次握手：A 向 B 发送个报文，表示我已经收到你的应答了（A 对 B 说：“亲爱的，你同意真是太好了，我们可以互相砸数据了”） 为什么 TCP 连接需要三次握手，两次不可以吗？首先，我们要知道，这三次握手是为了让双方确认自己与对方的发送和接收都是正常的。但是，只成功完成两次握手的时候，B 不知道自己的发送能力是否正常，也不知道 A 的收报能力是否正常，所以它们需要第三次握手。 同时，如果没有第三次连接，很有可能导致 B 建立一个脏连接。 脏连接建立的过程： A 发送的第一个建立连接的请求，这个连接好久好久都没有达到 B 那里； 所以，A 又重新发送了一个新的建立连接请求给 B，这个请求成功了，A 和 B 愉快的交换完了数据并且断开了连接； 此时，A 第一个发的建立连接的请求终于穿越 n 个路由器到达了 B，B 以为这时 A 发来的新的建立连接的请求，愉快的返回了同意建立连接的请求； 如果只需要两次握手，此时 B 会单方面的建立起与 A 的连接，而 A 根本就不在 SYN_SENT 状态，它会把 B 的应答请求直接丢掉，不会建立连接。此时，B单方面创建的这个连接就是脏连接。 四次挥手四次挥手的流程 A：我们分手吧。 B：好的，我收到了你的分手请求，再等一会，我把你剩我这的东西打包给你。（此时 A 已经不能再给 B 发东西了） B：好了，东西发完了，分吧。（此时 B 也不能再给 A 发东西了） A：好的，知道你东西都发完了，我在等 2MSL，然后就消失了。 TIME_WAIT 存在的必要性如果 A 发送完最后一个 ACK=1 后，立即进入 CLOSED 状态，可能会导致 B 无法进入 CLOSED 状态。 原因：假设 A 最后的 ACK 在网络传输中丢失了，B 会认为 A 根本没收到自己发的 FIN=1, ACK=1 报文，会导致 B 超时重发 FIN=1, ACK=1 报文。A 第二次收到 FIN=1, ACK=1 报文后，会再发一次 ACK，并重新开始 TIME_WAIT 的计时。如果 A 发完最后一个 ACK 后立即关闭，B 可能会永远接收不到最后一个 ACK，也就无法进入 CLOSED 状态。 在高并发上，可以将 TIME_WAIT 调到小于 30s 为宜。 netstat -n | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39;123456789101112具体的解决方式vim /etc/sysctl.confnet.ipv4.tcp_syncookies = 1// 表示开启SYN cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭net.ipv4.tcp_tw_reuse = 1//表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；net.ipv4.tcp_tw_recycle = 1//表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭net.ipv4.tcp_fin_timeout = 30//修改系統默认的 TIMEOUT 时间/sbin/sysctl -p //保存后生效 目前看来最好的办法是让每个TIME_WAIT早点过期。 注：seq 表示序列号，ack 表示确认号，2MSL 是报文在网络中生存的最长时间。 TCP 流量控制、拥塞控制流量控制产生原因： 如果发送方数据发送的过快，接收方可能来不及接收，这会造成数据的丢失。 解决方法： 通过滑动窗口实现，接收端告诉发送发自己的接收窗口有多大，发送端会调整自己的发送窗口不超过接收端的接收窗口大小。 流量控制引发的死锁： 当发送者收到了一个窗口为 0 的应答后，发送者会停止发送，等待接收者的下一个应答。但是如果这个窗口不为 0 的应答在传输过程丢失，发送者一直等待下去，而接收者以为发送者已经收到该应答，等待接收新数据，这样双方就相互等待，从而产生死锁。 拥塞控制慢启动和拥塞避免首先，发送方维持一个叫做 拥塞窗口 cwnd（congestion window） 的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于 min{拥塞窗口, 接收窗口}。 慢启动就是： 不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。具体过程如下： 慢启动的时候，拥塞窗口每次是呈 2 的指数次方增长的，因为开始的时候需要比较快速的将拥塞窗口的大小增长到一个合适值。如果我们一直使用慢启动的方法确认拥塞窗口 cwnd 的大小，cwnd 会飞速增大，而且增长的粒度会越来越粗，一不小心就增的过大了，就会导致网络的拥塞。 为了避免这种情况，我们设定了一个慢开始门限 ssthresh，令 cwnd 大于一定值之后就采用拥塞避免算法，拥塞避免算法和慢启动算法的区别在于：拥塞避免算法每次只将 cwnd 增加 1，也就是呈加法增长的。ssthresh 的用法如下： cwnd &lt; ssthresh 时，使用慢开始算法 cwnd &gt; ssthresh 时，改用拥塞避免算法 cwnd = ssthresh时，慢开始与拥塞避免算法任意 拥塞避免算法会让拥塞窗口缓慢增长，即每经过一个往返时间 RTT 就把发送方的拥塞窗口 cwnd 加 1，而不是加倍。这样拥塞窗口按线性规律缓慢增长。 无论是在 慢启动阶段 还是在 拥塞避免阶段 ，只要发送方判断 网络出现拥塞 （其根据就是没有收到确认，虽然没有收到确认可能是其他原因的分组丢失，但是因为无法判定，所以都当做拥塞来处理）， 就把慢开始门限设置为出现拥塞时的发送窗口大小的一半（乘法减小算法）。然后把拥塞窗口设置为 1，执行慢开始算法。 通过使用慢启动与拥塞避免算法，拥塞窗口的大小变化大致如下图所示： 注：这里只是为了讨论方便而将拥塞窗口大小的单位改为数据报的个数，实际上应当是字节。 快重传和快恢复快重传 要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不等到自己发送数据时捎带确认。 快重传算法规定：发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传时间计时器到期。如下图： 快恢复 要求当发送发连续接收到三个确认时，就执行乘法减小算法，把慢启动开始门限（ssthresh）减半，但是接下来并不执行慢开始算法。而是将 cwnd 设置为 ssthresh 的大小，然后执行拥塞避免算法。 通过使用快重传与快恢复算法，拥塞窗口的大小变化大致如下图所示： TCP 滑动窗口停止等待协议（ARQ 协议，滑动窗口协议的简易版）原理： A 向 B 每发送一个分组，都要停止发送，等待 B 的确认应答；A 只有收到了 B 的确认应答后才能发送下一个分组。 A 发送的分组丢失或出错 丢失：发送者 A 拥有超时计时器。每发送一个分组便会启动超时计时器，等待 B 的应答。若超时仍未收到应答，A 就会重发刚才的分组。 出错：若 B 收到分组，但通过检查和字段发现分组在运输途中出现差错，它会直接丢弃该分组，并且不会有任何其他动作。A 超时后便会重新发送该分组，直到 B 正确接收为止。 B 发送的确认应答丢失或迟到 丢失：A 迟迟收不到 B 的确认应答，会进行超时重传，B 收到重复的分组后会立即补发一个确认应答。 迟到：A 会根据分组号得知该分组已被接收，会直接丢弃该应答。 滑动窗口协议（连续 ARQ 协议）ARQ 协议的缺点： 每次只发送一个分组，在该分组的应答到来前只能等待。为了解决这个问题，我们改成一次发送一堆，也就是我们有个窗口，在发送端没有收到确认应答时，可以继续发送窗口中的分组，而不是干等着。 累计确认： 接收端不用为每一个分组发送一个应答了，改为为一组分组发送一个确认应答。这个应答会通过 TCP 头中的 ack（确认号）来告诉发送端它下一个希望接收的分组号是多少。 发送窗口： 发送端收到接收端发来的一个确认应答后，会根据确认应答的 TCP 头中的各种信息移动 P1、P2、P3 三个指针： 根据 ack 的值移动 P1 指针，确认哪些分组被成功接收了； 然后根据窗口大小移动 P3 = P1 + 窗口大小； 然后 P2 从 P1 开始向 P3 移动，向接收端发送分组数据。 接收窗口： 接收者收到的字节会存入接收窗口，接收者会对已经正确接收的有序字节进行累计确认，发送完确认应答后，接收窗口就可以向前移动指定字节。 如果某些字节并未按序收到，接收者只会确认最后一个有序的字节，从而乱序的字节就会被重新发送。 注意： 同一时刻发送窗口的大小并不一定和接收窗口一样大（因为时延和拥塞窗口）。 TCP 标准并未规定未按序到达的字节的处理方式。但 TCP 一般都会缓存这些字节，等缺少的字节到达后再交给应用层处理（应用层可以对它进行排序）。这比直接丢弃乱序的字节要节约带宽。 TCP 与 UDP 的区别从特点上看： TCP 是面向连接的，UDP 是无连接的，即 TCP 在传输数据前要先通过三次握手建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制建立连接，数据传输结束后要释放连接。 UDP 在传输数据前不需要建立连接，远地主机在接收到报文之后也无需确认。 所以 TCP 是可靠传输，UDP 是不可靠传输。 TCP 因为有连接，所以数据以字节流的形式传输，UDP 则以数据报文段形式传输，而且 TCP 只能是一对一的，而 UDP 可以各种通信。 从性能上看： TCP 传输效率慢，需要资源多，但可靠。UDP 传输效率快，需要资源少，但不可靠。 应用场景： TCP 应用在要求传输数据可靠的情况下，如文件传输、邮件传输等。UDP 应用在要求通信速度但对可靠性要求比较低的场景，如 QQ 语音、视频等。 首部字节： TCP 首部有 20 ~ 60 个字节，UDP 首部由 8 个字节 4 个字段组成。 怎么用 UDP 实现 TCP？ 在传输层 UDP 是不可靠的，所以需要在应用层自己实现一些保证可靠传输的机制，简单来说，就是使用 UDP 来构建可靠的面向连接的数据传输，就是在应用层实现类似于 TCP 的超时重传（定时器），拥塞控制（滑动窗口），有序接收（添加包序号），应答确认（ack 和 seq）等。目前已经有了实现 UDP 可靠运输的机制 —— UDT：主要用于高速广域网海量数据传输，是应用层协议。 TCP粘包、拆包TCP粘包通俗来讲，就是发送方发送的多个数据包，到接收方后粘连在一起，导致数据包不能完整的体现发送的数据。 TCP粘包原因分析导致TCP粘包的原因，可能是发送方的原因，也有可能是接受方的原因。 发送方由于TCP需要尽可能高效和可靠，所以TCP协议默认采用Nagle算法，以合并相连的小数据包，再一次性发送，以达到提升网络传输效率的目的。但是接收方并不知晓发送方合并数据包，而且数据包的合并在TCP协议中是没有分界线的，所以这就会导致接收方不能还原其本来的数据包。 接收方TCP是基于“流”的。网络传输数据的速度可能会快过接收方处理数据的速度，这时候就会导致，接收方在读取缓冲区时，缓冲区存在多个数据包。在TCP协议中接收方是一次读取缓冲区中的所有内容，所以不能反映原本的数据信息。 解决TCP粘包分析了产生TCP粘包的原因之后，针对发生的原因，针对性的采取解决方法。 禁用Nagle算法因为TCP协议采用Nagle算法，导致粘包。所以可以禁用Nagle算法。 1234567const char chOpt = 1;int nErr = setsockopt(m_socket, IPPROTO_TCP, TCP_NODELAY, &amp;chOpt, sizeof(char)); if(nErr == -1)&#123; TRACE( "setsockopt() error\n", WSAGetLastError()); return ;&#125; 这种方法虽然能一定程度上解决TCP粘包，但是并不能完全解决问题。因为接收方也是可能造成粘包的原因，这种方法只是发送方有效。而且禁用Nagle算法，一定程度上使TCP传输效率降低了。所以，这并不是一种理想的方法。 PUSH标志PUSH是TCP报头中的一个标志位，发送方在发送数据的时候可以设置这个标志位。该标志通知接收方将接收到的数据全部提交给接收进程。这里所说的数据包括与此PUSH包一起传输的数据以及之前就为该进程传输过来的数据。当Server端收到这些数据后，它需要立刻将这些数据提交给应用层进程，而不再等待是否还有额外的数据到达。设置PUSH标志也不能完全解决TCP粘包，只是降低了接收方粘包的可能性。实际上现在的TCP协议栈基本上都可以自行处理这个问题，而不是交给应用层处理。所以设置PUSH标志，也不是一种理想的方法。 自定协议自定协议，将数据包分为了封包和解包两个过程。在发送方发送数据时，对发送的数据进行封包操作。在接收方接收到数据时对接收的数据包需要进行解包操作。自定协议时，封包就是为发送的数据增加包头，包头包含数据的大小的信息，数据就跟随在包头之后。当然包头也可以有其他的信息，比如一些做校验的信息。这里主要讨论TCP粘包的问题，所以不考虑其他的。 粘包、拆包发生原因发生TCP粘包或拆包有很多原因，现列出常见的几点，可能不全面，欢迎补充，1、要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包。2、待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包。3、要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包。4、接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。等等。 粘包、拆包解决办法通过以上分析，我们清楚了粘包或拆包发生的原因，那么如何解决这个问题呢？解决问题的关键在于如何给每个数据包添加边界信息，常用的方法有如下几个：1、发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，这样接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度了。2、发送端将每个数据包封装为固定长度（不够的可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。3、可以在数据包之间设置边界，如添加特殊符号，这样，接收端通过这个边界就可以将不同的数据包拆分开。 HTTP 长连接、短连接短连接在 HTTP/1.0 中默认使用 短连接：客户端和服务器每进行一次 HTTP 操作，就建立一次连接，任务结束就中断连接。 当客户端浏览器访问的某个 HTML 或其他类型的 Web 页中包含有其他的 Web 资源（如：JavaScript 文件、图像文件、CSS 文件等）时，浏览器就会重新建立一个 HTTP 会话。 应用： WEB 网站的 http 服务一般都用短链接，因为并发量大，但每个用户无需频繁操作。 长连接而从 HTTP/1.1 起，默认使用长连接。使用长连接的HTTP协议，会在响应头加入这行代码： 1Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。 Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接，有关 TCP 长连接、短连接的介绍请看下一小节。 应用： 适用于操作频繁的点对点通讯，而且连接数不能太多。 TCP 长连接、短连接短链接短连接过程： client 向 server 发起连接请求，server 接到请求，然后双方建立连接。client 向 server 发送消息，server 回应 client，然后一次请求就完成了。这时候双方任意都可以发起 close 操作，不过一般都是 client 先发起 close 操作。也就是说，短连接一般只会在 client 和 server 间进行一次请求操作。 优点： 管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。 缺点： 如果客户请求频繁，将在 TCP 的建立和关闭操作上浪费时间和带宽。 长连接长连接过程： client 向 server 发起连接，server 接受 client 连接，双方建立连接，client 与 server完成一次请求后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 TCP 的保活功能主要为服务器应用提供。如果客户端已经消失而连接未断开，则会使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，此时服务器将永远等待客户端的数据。保活功能就是试图在服务端器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何动作，服务器就向客户发送一个探测报文段，根据客户端主机响应探测 4 个客户端状态： 客户主机依然正常运行，且服务器可达。此时客户的TCP响应正常，服务器将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。上述情况下客户端都不能响应 TCP。服务端将无法收到客户端对探测的响应。服务器总共发送 10 个这样的探测，每个间隔 75 秒。若服务器没有收到任何一个响应，它就认为客户端已经关闭并终止连接。 客户端崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户机正常运行，但是服务器不可达。这种情况与第二种状态类似。 优点： 对于请求比较频繁客户来说，可以节省在 TCP 的建立和关闭操作上浪费时间和带宽。 缺点： 存活探测周期太长，而且 client 端一般不会主动关闭它与服务器之间的连接，如果 client 与 server 之间的连接一直不关闭的话，随着客户端连接越来越多，server 早晚有扛不住的时候 ，这时候 server 端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，以避免一些恶意连接导致 server 端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。 HTTP、HTTPS 区别HTTPS 的全称为：HTTP over SSL，简单理解就是在之前的 HTTP 传输上增加了 SSL 协议加密。 1234567|------| | HTTP ||------| &lt;-- HTTPS 在 HTTP 和 TCP 间加了一层 SSL or TLS| TCP ||------|| IP ||------| HTTP 通信存在的问题 容易被监听： http 通信都是明文，数据在客户端与服务器通信过程中，任何一点都可能被劫持，如果明文保存的密码被截取了是很危险的。 被伪装： http 通信时无法保证双方是合法的。比如你请求 www.taobao.com， 你无法知道返回的数据就是来自淘宝，还是中间人伪装的淘宝。 被篡改： 中间人将发给你的信息篡改了你也不知道。 因为 http 不安全，所以 https 出现了！ 区别 HTTPS 需要到 CA 申请证书，HTTP 不需要 HTTPS 密文传输，HTTP 明文传输 连接方式不同，HTTPS 默认使用 443 端口，HTTP 使用 80 端口 HTTPS = HTTP + 加密 + 认证 + 完整性保护，比 HTTP 安全 HTTP 1.1 与 HTTP 1.0 的区别 1.0 需要设置 keep-alive 参数来告知服务器建立长连接，1.1 默认建立长连接。 1.1 支持只发 header 不带 body，如果服务器认为客户端有权利访问，返回 100，否则返回 401，客户端可以接到 100 后再把 body 发过去，接到 401 就不发了，这样比较节省带宽。 1.1 有 host 域，1.0 没有。 host 域用于处理一个IP地址对应多个域名的情况，假设我的虚拟机服务器 IP 是 111.111.111.111，我们可以把 www.qiniu.com，www.taobao.com 和 www.jd.com 这些网站都架设那台虚拟机上面，但是这样会有一个问题，我们每次访问这些域名其实都是解析到服务器 IP 111.111.111.111，那么如何来区分每次根据域名显示出不同的网站的内容呢？就是通过 Host 域的设置，可以在 Tomcat 的 conf 目录下的 server.xml 进行配置。 1.1 会进行带宽优化。1.0 存在浪费带宽的现象，而且不支持断点续传，1.1 在请求头中引入了 range 域，允许只请求某个资源的某个部分，即返回状态码 206。 如果客户端不断的发送请求连接会怎样服务器端会为每个请求创建一个链接，然后向 client 端发送创建连接时的回复，然后进行等待客户端发送第三次握手数据包，这样会白白浪费资源。DDos 攻击就是基于这一点达到的。 DDos 攻击简单的说就是不停的向服务器发送建立连接请求，但不发送第三次握手的数据包。 客户端向服务器端发送连接请求数据包 服务器向客户端回复连接请求数据包，然后服务器等待客户端发送tcp/ip链接的第三步数据包 如果客户端不向服务器端发送最后一个数据包，服务器须等待 30s 到 2min 才能将此连接关闭。当大量的请求只进行到第二步，而不进行第三步，服务器将有大量的资源在等待第三个数据包，造成DDos攻击。 DDos 预防 DDoS清洗：对用户请求数据进行实时监控，及时发现异常流量，封掉异常流量的 IP，使用的命令：iptables。 用 iptables 屏蔽单个 IP 的命令：iptables -I INPUT -s ***.***.***.*** -j DROP 用 iptables 屏蔽整个 IP 段命令：iptables -I INPUT -s 121.0.0.0/8 -j DROP 用 iptables 解禁 IP 命令：iptables -D INPUT -s ***.***.***.*** -j DROP 相当于在 /etc/iptables.conf 配置文件中写入：-A INPUT -s ***.***.***.*** -j DROP CDN 加速：在现实中，CDN 服务将网站访问流量分配到了各个节点中，这样一方面隐藏网站的真实 IP，另一方面即使遭遇 DDoS 攻击，也可以将流量分散到各个节点中，防止源站崩溃。 GET 和 POST 区别 Http 报文层面：GET 将请求信息放在 URL 中，POST 方法在报文中，所以 POST 方法更安全，毕竟数据在地址栏上不可见。 数据库层面：GET 符合幂等性，POST 不符合。 缓存层面：GET 可以被缓存、被存储（书签），而 POST 不行。 转发(forward)和重定向(redirect) 转发是服务器行为，重定向是客户端行为。 转发(Forword) 通过RequestDispatcher对象的forward(HttpServletRequest request,HttpServletResponse response) 方法实现的。 RequestDispatcher 可以通过 HttpServletRequest 的 getRequestDispatcher() 方法获得。例如下面的代码就是跳转到 login_success.jsp 页面。request.getRequestDispatcher(&quot;login_success.jsp&quot;).forward(request, response) 重定向(Redirect) 是利用服务器返回的状态码来实现的。客户端浏览器请求服务器的时候，服务器会返回一个状态码。服务器通过HttpServletRequestResponse的setStatus(int status)方法设置状态码。如果服务器返回301或者 302，则浏览器会到新的网址重新请求该资源。 从地址栏显示来说: forward是服务器请求资源,服务器直接访问目标地址的URL,把那个URL的响应内容读取过来,然后把这些内容再发给浏览器.浏览器根本不知道服务器发送的内容从哪里来的,所以它的地址栏还是原来的地址.redirect是服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址.所以地址栏显示的是新的URL. 从数据共享来说: forward:转发页面和转发到的页面可以共享request里面的数据. redirect:不能共享数据. 从运用地方来说: forward:一般用于用户登陆的时候,根据角色转发到相应的模块. redirect:一般用于用户注销登陆时返回主页面和跳转到其它的网站等 从效率来说: forward:高. redirect:低. HTTP 返回码中 301 和 302 的区别： 301，302 都是 HTTP 的状态码，都代表着某个 URL 发生了转移，不同之处在于： 301 代表永久性转移 (Permanently Moved)。 302 代表暂时性转移(Temporarily Moved )。搜索引擎会抓取新的内容而保留旧的网址，因为服务器返回 302 代码，搜索引擎认为新的网址只是暂时的。 使用场景： 域名到期不想续费； 在搜索引擎的搜索结果中出现了不带 www 的域名，而带 www 的域名却没有收录，这个时候可以用 301 重定向来告诉搜索引擎我们目标的域名是哪一个； 空间服务器不稳定，换空间的时候。 注意：尽量使用 301 跳转！原因：网址劫持！ 从网址 A 做一个 302 重定向到网址 B 时，主机服务器的隐含意思是网址 A 随时有可能改主意，重新显示本身的内容或转向其他的地方。大部分的搜索引擎在大部分情况下，当收到 302 重定向时，一般只要去抓取目标网址就可以了，也就是说网址 B。如果搜索引擎在遇到 302 转向时，百分之百的都抓取目标网址 B 的话，就不用担心网址 URL 劫持了。问题就在于，有的时候搜索引擎，尤其是 Google，并不能总是抓取目标网址。比如说，有的时候 A 网址很短，但是它做了一个 302 重定向到 B 网址，而 B 网址是一个很长的乱七八糟的 URL 网址，甚至还有可能包含一些问号之类的参数。很自然的，A 网址更加用户友好，而 B 网址既难看，又不用户友好。这时 Google 很有可能会仍然显示网址A。 这就造成了网址 URL 劫持的可能性。也就是说，一个不道德的人在他自己的网址 A 做一个 302 重定向到你的网址 B，出于某种原因， Google 搜索结果所显示的仍然是网址 A，但是所用的网页内容却是你的网址 B 上的内容，这种情况就叫做网址 URL 劫持。你辛辛苦苦所写的内容就这样被别人偷走了。 302 重定向很容易被搜索引擎误认为是利用多个域名指向同一网站，那么你的网站就会被封掉，罪名是 “利用重复的内容来干扰 Google 搜索结果的网站排名”。 URL 和 URI“URI可以分为URL,URN或同时具备locators 和names特性的一个东西。URN作用就好像一个人的名字，URL就像一个人的地址。换句话说：URN确定了东西的身份，URL提供了找到它的方式。” 通过这些描述我们可以得到一些结论： NAT网络地址转换常用于私有地址与公有地址的转换，以解决 IP 地址匮乏的问题。 NAT 的基本工作原理是：当私有网主机和公共网主机通信的 IP 包经过 NAT 网关时，将 IP 包中的源 IP 或目的 IP 在私有 IP 和 NAT 的公共 IP 之间进行转换。 路由器 路由器分组转发流程 从数据报的首部提取目的主机的 IP 地址 D，得到目的网络地址 N。 若 N 就是与此路由器直接相连的某个网络地址，则进行直接交付； 若路由表中有目的地址为 D 的特定主机路由，则把数据报传送给表中所指明的下一跳路由器； 若路由表中有到达网络 N 的路由，则把数据报传送给路由表中所指明的下一跳路由器； 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器； 报告转发分组出错。 路由选择协议路由选择协议都是自适应的，能随着网络通信量和拓扑结构的变化而自适应地进行调整。 互联网可以划分为许多较小的自治系统 AS，一个 AS 可以使用一种和别的 AS 不同的路由选择协议。 可以把路由选择协议划分为两大类： 自治系统内部的路由选择：RIP 和 OSPF 自治系统间的路由选择：BGP 1. 内部网关协议 RIPRIP 是一种基于距离向量的路由选择协议。距离是指跳数，直接相连的路由器跳数为 1。跳数最多为 15，超过 15 表示不可达。 RIP 按固定的时间间隔仅和相邻路由器交换自己的路由表，经过若干次交换之后，所有路由器最终会知道到达本自治系统中任何一个网络的最短距离和下一跳路由器地址。 距离向量算法： 对地址为 X 的相邻路由器发来的 RIP 报文，先修改报文中的所有项目，把下一跳字段中的地址改为 X，并把所有的距离字段加 1； 对修改后的 RIP 报文中的每一个项目，进行以下步骤： 若原来的路由表中没有目的网络 N，则把该项目添加到路由表中； 否则：若下一跳路由器地址是 X，则把收到的项目替换原来路由表中的项目；否则：若收到的项目中的距离 d 小于路由表中的距离，则进行更新（例如原始路由表项为 Net2, 5, P，新表项为 Net2, 4, X，则更新）；否则什么也不做。 若 3 分钟还没有收到相邻路由器的更新路由表，则把该相邻路由器标为不可达，即把距离置为 16。 RIP 协议实现简单，开销小。但是 RIP 能使用的最大距离为 15，限制了网络的规模。并且当网络出现故障时，要经过比较长的时间才能将此消息传送到所有路由器。 2. 内部网关协议 OSPF开放最短路径优先 OSPF，是为了克服 RIP 的缺点而开发出来的。 开放表示 OSPF 不受某一家厂商控制，而是公开发表的；最短路径优先表示使用了 Dijkstra 提出的最短路径算法 SPF。 OSPF 具有以下特点： 向本自治系统中的所有路由器发送信息，这种方法是洪泛法。 发送的信息就是与相邻路由器的链路状态，链路状态包括与哪些路由器相连以及链路的度量，度量用费用、距离、时延、带宽等来表示。 只有当链路状态发生变化时，路由器才会发送信息。 所有路由器都具有全网的拓扑结构图，并且是一致的。相比于 RIP，OSPF 的更新过程收敛的很快。 3. 外部网关协议 BGPBGP（Border Gateway Protocol，边界网关协议） AS 之间的路由选择很困难，主要是由于： 互联网规模很大； 各个 AS 内部使用不同的路由选择协议，无法准确定义路径的度量； AS 之间的路由选择必须考虑有关的策略，比如有些 AS 不愿意让其它 AS 经过。 BGP 只能寻找一条比较好的路由，而不是最佳路由。 每个 AS 都必须配置 BGP 发言人，通过在两个相邻 BGP 发言人之间建立 TCP 连接来交换路由信息。 扫描二维码之后发生的事情当老王访问 https://wx.qq.com/，他在网页中得到一个二维码。在这个过程中，老王的浏览器和微信服务器之间建立了一个阻塞的长链接，同时服务器还专门为老王新开了一个有独立UID为SWJNSjNJblBJQT09的Session，并把这个UID嵌入到二维码中返回。注意，这个UID只是一个随机不重复数，并不是Session ID。老王看到二维码后，使用手机微信中的扫一扫功能读取了二维码中的UID。手机微信把具有老王身份信息的token A和这个SWJNSjNJblBJQT09的UID打包并做一些特殊的处理后，发给微信服务器进行确认。 当微信服务器通过确认时，它已经同时获得了用户老王和UID为SWJNSjNJblBJQT09的网页端的信息，并知道老王将要通过UID为 SWJNSjNJblBJQT09的Session来访问微信服务。因为老王是一个真实的用户，所以微信服务器就给UID为SWJNSjNJblBJQT09的Session分配一个token B，让老王能够在一定限定条件下访问微信服务。由于网页在加载时候已经把大部分相关资源都预先加载进来了，这个长链接得到确认信息后，网页端只需要加载用户列表，所以响应速度很快，也不需要手动刷新。]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机的执行机制]]></title>
    <url>%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E6%89%A7%E8%A1%8C%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[类文件结构Class 文件是一组以 8 位字节为基础单位的二进制流，各个数据项目严格按照顺序紧凑地排列在 Class 文件中，中间没有任何分隔符。Java 虚拟机规范规定 Class 文件采用一种类似 C 语言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：无符号数和表，我们之后也主要对这两种类型的数据类型进行解析。 无符号数： 无符号数属于基本数据类型，以 u1、u2、u4、u8 分别代表 1 个字节、2 个字节、4 个字节和 8 个字节的无符号数，可以用它来描述数字、索引引用、数量值或 utf-8 编码的字符串值。 表： 表是由多个无符号数或其他表为数据项构成的复合数据类型，名称上都以 _info 结尾。 魔数与版本号Class 文件的头 8 个字节是魔数和版本号，其中头 4 个字节是魔数，也就是 0xCAFEBABE，它可以用来确定这个文件是否为一个能被虚拟机接受的 Class 文件（这通过扩展名来识别文件类型要安全，毕竟扩展名是可以随便修改的）。 后 4 个字节则是当前 Class 文件的版本号，其中第 5、6 个字节是次版本号，第 7、8 个字节是主版本号。 常量池 从第 9 个字节开始，就是常量池的入口，常量池是 Class 文件中： 与其他项目关联最多的的数据类型； 占用 Class 文件空间最大的数据项目； Class 文件中第一个出现的表类型数据项目。 常量池的开始的两个字节，也就是第 9、10 个字节，放置一个 u2 类型的数据，标识常量池中常量的数量 cpc (constant_pool_count)，这个计数值有一个十分特殊的地方，就是它是从 1 开始而不是从 0 开始的，也就是说如果 cpc = 22，那么代表常量池中有 21 项常量，索引值为 1 ~ 21，第 0 项常量被空出来，为了满足后面某些指向常量池的索引值的数据在特定情况下需要表达“不引用任何一个常量池项目”时，将让这个索引值指向 0 即可。 常量池中记录的是代码出现过的所有 token（类名，成员变量名等，也是我们接下来要修改的地方）以及符号引用（方法引用，成员变量引用等），主要包括以下两大类常量： 字面量： 接近于 Java 语言层面的常量概念，包括 文本字符串 声明为 final 的常量值 符号引用： 以一组符号来描述所引用的目标，包括 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 常量池中的每一项常量都通过一个表来存储。目前一共有 14 种常量，不过麻烦的地方就在于，这 14 种常量类型每一种都有自己的结构，我们在这里只详细介绍两种：CONSTANT_Class_info 和 CONSTANT_Utf8_info。 CONSTANT_Class_info 的存储结构为： 12... [ tag=7 ] [ name_index ] ...... [ 1位 ] [ 2位 ] ... 其中，tag 是标志位，用来区分常量类型的，tag = 7 就表示接下来的这个表是一个 CONSTANT_Class_info，name_index 是一个索引值，指向常量池中的一个 CONSTANT_Utf8_info 类型的常量所在的索引值，CONSTANT_Utf8_info 类型常量一般被用来描述类的全限定名、方法名和字段名。它的存储结构如下： 12... [ tag=1 ] [ 当前常量的长度 len ] [ 常量的符号引用的字符串值 ] ...... [ 1位 ] [ 2位 ] [ len位 ] ... 访问标志 常量池之后的两个字节代表访问标志，即这个class是类还是接口，是否为public等的信息。不同的含义有不同的标志值（没有用到的标志位一律为0。），具体信息如下： 类索引、父类索引、接口索引集合 类索引占两个字节，分别指向常量池中的CONSTANT_Class_info类型的常量，这个类型的常量结构见常量池中的图表，其中包含一个指向全限定名常量项的索引。 因为java只允许单继承，所以只有一个父类，具体内容同上-类索引。 接口索引开始两个字节用来表示接口的数量，之后的每两个字节表示一个接口索引，用法同类索引与父类索引。 字段表集合字段用于描述接口或者类中声明的变量，包括类级变量以及实例变量，但不包括局部变量。 字段域的开始两个字节表示字段数量，之后为紧密排列的字段结构体数据，其结构如下： 其中的字段和方法的描述符，对于字段来说用来描述字段的数据类型；而对于方法来说，描述的就是方法的参数列表（包括数量、类型以及顺序）和返回值，这个描述顺序也是固定的，必须是参数列表在前，返回值在后，参数列表必须放在一组小括号内。同时为了节省空间，各种数据类型都使用规定的一个字母来表示，具体如下： 对象使用L加上对象的全限定名来表示，而数组则是在每一个维度前添加一个&quot;[&quot;来描述。属性表在之后进行介绍。 方法表集合class文件中对方法的描述与以前对字段的描述几乎采用了完全一致的方式，唯一的区别就是访问类型不完全一致。 属性表集合java7中预定义了21项属性，具体内容限于篇幅不再列出。对于每个属性的结构，没有特别严格的要求，并且可以自定义属性信息，jvm运行时会忽略不认识的属性。符合规范的属性表基本结构如下： 其中前两个字节为指向常量池中的CONSTANT_Utf8_info类型的属性名称，之后4个字节表示属性值所占用的位数，最后就是具体属性了。 其中有一个比较重要的名称为「Code」的属性为方法的代码，即字节码指令。Code属性表结构如下： 虚拟机的类加载机制类加载的时机JVM 会在程序第一次主动引用类的时候，加载该类，被动引用时并不会引发类加载的操作。也就是说，JVM 并不是在一开始就把一个程序就所有的类都加载到内存中，而是到不得不用的时候才把它加载进来，而且只加载一次。那么什么是主动引用，什么是被动引用呢？ 主动引用 遇到 new、getstatic、putstatic、invokestatic 字节码指令，例如： 使用 new 实例化对象； 读取或设置一个类的 static 字段（被 final 修饰的除外）； 调用类的静态方法。 对类进行反射调用； 初始化一个类时，其父类还没初始化（需先初始化父类）； 这点类与接口具有不同的表现，接口初始化时，不要求其父接口完成初始化，只有真正使用父接口时才初始化，如引用父接口中定义的常量。 虚拟机启动，先初始化包含 main() 函数的主类； JDK 1.7 动态语言支持：一个 java.lang.invoke.MethodHandle 的解析结果为 REF_getStatic、REF_putStatic、REF_invokeStatic。 被动引用 通过子类引用父类静态字段，不会导致子类初始化； Array[] arr = new Array[10]; 不会触发 Array 类初始化； static final VAR 在编译阶段会存入调用类的常量池，通过 ClassName.VAR 引用不会触发 ClassName 初始化。 也就是说，只有发生主动引用所列出的 5 种情况，一个类才会被加载到内存中，也就是说类的加载是 lazy-load 的，不到必要时刻是不会提前加载的，毕竟如果将程序运行中永远用不到的类加载进内存，会占用方法区中的内存，浪费系统资源。 类的显式加载和隐式加载 显示加载： 调用 ClassLoader#loadClass(className) 或 Class.forName(className)。 两种显示加载 .class 文件的区别： Class.forName(className) 加载 class 的同时会初始化静态域，ClassLoader#loadClass(className) 不会初始化静态域； Class.forName 借助当前调用者的 class 的 ClassLoader 完成 class 的加载。 隐式加载： new 类对象； 使用类的静态域； 创建子类对象； 使用子类的静态域； 其他的隐式加载，在 JVM 启动时： BootStrapLoader 会加载一些 JVM 自身运行所需的 Class； ExtClassLoader 会加载指定目录下一些特殊的 Class； AppClassLoader 会加载 classpath 路径下的 Class，以及 main 函数所在的类的 Class 文件。 类加载的过程类的生命周期123加载 --&gt; 验证 --&gt; 准备 --&gt; 解析 --&gt; 初始化 --&gt; 使用 --&gt; 卸载 |&lt;------- 连接 -------&gt;||&lt;------------- 类加载 ----------------&gt;| 类的生命周期一共有 7 个阶段，其中前五个阶段较为重要，统称为类加载，第 2 ~ 4 阶段统称为连接，加载和连接中的三个过程开始的顺序是固定的，但是执行过程中是可以交叉执行的。接下来，我们将对类加载的 5 个阶段进行一一讲解。 加载加载的 3 个阶段 通过类的全限定名获取二进制字节流（将 .class 文件读进内存）； 将字节流的静态存储结构转化为运行时的数据结构； 在内存中生成该类的 Class 对象； HotSpot 虚拟机把这个对象放在方法区，非 Java 堆。 分类 非数组类 系统提供的引导类加载器 用户自定义的类加载器 数组类 不通过类加载器，由 Java 虚拟机直接创建 创建动作由 newarray 指令触发，new 实际上触发了 [L全类名 对象的初始化 规则 数组元素是引用类型 加载：递归加载其组件 可见性：与引用类型一致 数组元素是非引用类型 加载：与引导类加载器关联 可见性：public 验证 目的： 确保 .class 文件中的字节流信息符合虚拟机的要求。 4 个验证过程： 文件格式验证：是否符合 Class 文件格式规范，验证文件开头 4 个字节是不是 “魔数” 0xCAFEBABE 元数据验证：保证字节码描述信息符号 Java 规范（语义分析） 字节码验证：程序语义、逻辑是否正确（通过数据流、控制流分析） 符号引用验证：对类自身以外的信息（常量池中的符号引用）进行匹配性校验 这个操作虽然重要，但不是必要的，可以通过 -Xverify:none 关掉。 准备 描述： 为 static 变量在方法区分配内存。 static 变量准备后的初始值： public static int value = 123; 准备后为 0，value 的赋值指令 putstatic 会被放在 &lt;client&gt;() 方法中，&lt;client&gt;()方法会在初始化时执行，也就是说，value 变量只有在初始化后才等于 123。 public static final int value = 123; 准备后为 123，因为被 static final 赋值之后 value 就不能再修改了，所以在这里进行了赋值之后，之后不可能再出现赋值操作，所以可以直接在准备阶段就把 value 的值初始化好。 解析 描述： 将常量池中的 “符号引用” 替换为 “直接引用”。 在此之前，常量池中的引用是不一定存在的，解析过之后，可以保证常量池中的引用在内存中一定存在。 什么是 “符号引用” 和 “直接引用” ？ 符号引用：以一组符号描述所引用的对象（如对象的全类名），引用的目标不一定存在于内存中。 直接引用：直接指向被引用目标在内存中的位置的指针等，也就是说，引用的目标一定存在于内存中。 初始化 描述： 执行类构造器 &lt;client&gt;() 方法的过程。 &lt;client&gt;() 方法 包含的内容： 所有 static 的赋值操作； static 块中的语句； &lt;client&gt;() 方法中的语句顺序： 基本按照语句在源文件中出现的顺序排列； 静态语句块只能访问定义在它前面的变量，定义在它后面的变量，可以赋值，但不能访问。 与 &lt;init&gt;() 的不同： 不需要显示调用父类的 &lt;client&gt;() 方法； 虚拟机保证在子类的 &lt;client&gt;() 方法执行前，父类的 &lt;client&gt;() 方法一定执行完毕。 也就是说，父类的 static 块和 static 字段的赋值操作是要先于子类的。 接口与类的不同： 执行子接口的 &lt;client&gt;() 方法前不需要先执行父接口的 &lt;client&gt;() 方法（除非用到了父接口中定义的 public static final 变量）； 执行过程中加锁： 同一时刻只能有一个线程在执行 &lt;client&gt;() 方法，因为虚拟机要保证在同一个类加载器下，一个类只被加载一次。 非必要性： 一个类如果没有任何 static 的内容就不需要执行 &lt;client&gt;() 方法。 注：初始化时，才真正开始执行类中定义的 Java 代码。 类加载器如何判断两个类 “相等” “相等” 的要求 同一个 .class 文件 被同一个虚拟机加载 被同一个类加载器加载 判断 “相等” 的方法 instanceof 关键字 Class 对象中的方法： equals() isInstance() isAssignableFrom() 类加载器的分类 启动类加载器（Bootstrap）：Bootstrp加载器是用C++语言写的，它是在Java虚拟机启动后初始化的，它主要负责加载%JAVA_HOME%/jre/lib,-Xbootclasspath参数指定的路径以及%JAVA_HOME%/jre/classes中的类。 扩展类加载器（Extension）：&lt;JAVA_HOME&gt;/lib/ext、java.ext.dirs系统变量指定的路径 应用程序类加载器（Application）： -classpath 参数 双亲委派模型 工作过程 当前类加载器收到类加载的请求后，先不自己尝试加载类，而是先将请求委派给父类加载器 因此，所有的类加载请求，都会先被传送到启动类加载器 只有当父类加载器加载失败时，当前类加载器才会尝试自己去自己负责的区域加载 实现 检查该类是否已经被加载 将类加载请求委派给父类 如果父类加载器为 null，默认使用启动类加载器 parent.loadClass(name, false) 当父类加载器加载失败时 catch ClassNotFoundException 但不做任何处理 调用自己的 findClass() 去加载 我们在实现自己的类加载器时只需要 extends ClassLoader，然后重写 findClass() 方法而不是 loadClass() 方法，这样就不用重写 loadClass() 中的双亲委派机制了 优点 Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。 其次是考虑到安全因素，java核心api中定义类型不会被随意替换，假设通过网络传递一个名为java.lang.Integer的类，通过双亲委托模式传递到启动类加载器，而启动类加载器在核心Java API发现这个名字的类，发现该类已被加载，并不会重新加载网络传递的过来的java.lang.Integer，而直接返回已加载过的Integer.class，这样便可以防止核心API库被随意篡改。 破坏双亲委派为什么需要破坏双亲委派？因为在某些情况下父类加载器需要委托子类加载器去加载class文件。受到加载范围的限制，父类加载器无法加载到需要的文件，以Driver接口为例，由于Driver接口定义在jdk当中的，而其实现由各个数据库的服务商来提供，比如mysql的就写了MySQL Connector，那么问题就来了，DriverManager（也由jdk提供）要加载各个实现了Driver接口的实现类，然后进行管理，但是DriverManager由启动类加载器加载，只能记载JAVA_HOME的lib下文件，而其实现是由服务商提供的，由系统类加载器加载，这个时候就需要启动类加载器来委托子类来加载Driver实现，从而破坏了双亲委派，这里仅仅是举了破坏双亲委派的其中一个情况。 破坏双亲委派的实现我们结合Driver来看一下在spi（Service Provider Inteface）中如何实现破坏双亲委派。 先从DriverManager开始看，平时我们通过DriverManager来获取数据库的Connection： 12String url = &quot;jdbc:mysql://localhost:3306/testdb&quot;;Connection conn = java.sql.DriverManager.getConnection(url, &quot;root&quot;, &quot;root&quot;); 参考 虚拟机字节码执行引擎运行时栈帧结构 局部变量表 存放方法参数和方法内部定义的局部变量； Java 程序编译为 class 文件时，就确定了每个方法需要分配的局部变量表的最大容量。 最小单位：Slot； 一个 Slot 中可以存放：boolean，byte，char，short，int，float，reference，returnAddress (少见)； 虚拟机可通过局部变量表中的 reference 做到： 查找 Java 堆中的实例对象的起始地址； 查找方法区中的 Class 对象。 局部变量表的空间分配 Slot 的复用定义： 如果当前位置已经超过某个变量的作用域时，例如出了定义这个变量的代码块，这个变量对应的 Slot 就可以给其他变量使用了。但同时也说明，只要其他变量没有使用这部分 Slot 区域，这个变量就还保存在那里，这会对 GC 操作产生影响。 对 GC 操作的影响： 123456public static void main(String[] args) &#123; &#123; byte[] placeholder = new byte[64 * 1024 * 1024]; &#125; System.gc();&#125; -verbose:gc 输出： 12[GC (System.gc()) 68813K-&gt;66304K(123904K), 0.0034797 secs][Full GC (System.gc()) 66304K-&gt;66204K(123904K), 0.0086225 secs] // 没有被回收 进行如下修改： 1234567public static void main(String[] args) &#123; &#123; byte[] placeholder = new byte[64 * 1024 * 1024]; &#125; int a = 1; // 新加一个赋值操作 System.gc();&#125; -verbose:gc 输出： 12[GC (System.gc()) 68813K-&gt;66320K(123904K), 0.0017394 secs][Full GC (System.gc()) 66320K-&gt;668K(123904K), 0.0084337 secs] // 被回收了 第二次修改后，placeholder 能被回收的原因？ placeholder 能否被回收的关键：局部变量表中的 Slot 是否还存在关于 placeholder 的引用； 出了 placeholder 所在的代码块后，还没有进行其他操作，所以 placeholder 所在的 Slot 还没有被其他变量复用，也就是说，局部变量表的 Slot 中依然存在着 placeholder 的引用； 第二次修改后，int a 占用了原来 placeholder 所在的 Slot，所以可以被 GC 掉了。 操作数栈 元素可以是任意 Java 类型，32 位数据占 1 个栈容量，64 位数据占 2 个栈容量； Java 虚拟机的解释执行称为：基于栈的执行引擎，其中 “栈” 指的就是操作数栈； 动态连接 指向运行时常量池中该栈帧所属方法的引用； 为了支持方法调用过程中的动态连接，什么是动态连接会在下一篇文章进行讲解，先知道有这么个东西就行。 方法返回地址 两种退出方法的方式： 遇到 return； 遇到异常。 退出方法时可能执行的操作： 恢复上层方法的局部变量表和操作数栈； 把返回值压入调用者栈帧的操作数栈； 调整 PC 计数器指向方法调用后面的指令。 方法调用Java 的方法的执行分为两个部分： 方法调用：确定被调用的方法是哪一个； 基于栈的解释执行：真正的执行方法的字节码。 在本节中我们将对方法调用进行详细的讲解，我们知道，一切方法的调用在 Class 文件中存储的都是常量池中的符号引用，而不是方法实际运行时的入口地址（直接引用），直到类加载的时候，甚至是实际运行的时候才回去会去确定要被运行的方法的直接引用，而确定要被运行的方法的直接引用的过程就叫做方法调用。 方法调用字节码指令Java 虚拟机提供了 5 个职责不同的方法调用字节码指令： invokestatic：调用静态方法； invokespecial：调用构造器方法、私有方法、父类方法； invokevirtual：调用所有虚方法，除了静态方法、构造器方法、私有方法、父类方法、final 方法的其他方法叫虚方法； invokeinterface：调用接口方法，会在运行时确定一个该接口的实现对象； invokedynamic：在运行时动态解析出调用点限定符引用的方法，再执行该方法。 除了 invokedynamic，其他 4 种方法的第一个参数都是被调用的方法的符号引用，是在编译时确定的，所以它们缺乏动态类型语言支持，因为动态类型语言只有在运行期才能确定接收者的类型，即变量的类型检查的主体过程在运行期，而非编译期。 final 方法虽然是通过 invokevirtual 调用的，但是其无法被覆盖，没有其他版本，无需对接收者进行多态选择，或者说多态选择的结果是唯一的，所以属于非虚方法。 解析调用解析调用，正如其名，就是 在类加载的解析阶段，就确定了方法的调用版本 。我们知道类加载的解析阶段会将一部分符号引用转化为直接引用，这一过程就叫做解析调用。因为是在程序真正运行前就确定了要调用哪一个方法，所以 解析调用能成立的前提就是：方法在程序真正运行前就有一个明确的调用版本了，并且这个调用版本不会在运行期发生改变。 符合这两个要求的只有以下两类方法： 通过 invokestatic 调用的方法：静态方法； 通过 invokespecial 调用的方法：私有方法、构造器方法、父类方法； 这两类方法根本不可能通过继承或者别的方式重写出来其他版本，也就是说，在运行前就可以确定调用版本了，十分适合在类加载阶段就解析好。它们会在类加载的解析阶被解析为直接引用，即确定调用版本。 分派调用在介绍分派调用前，我们先来介绍一下 Java 所具备的面向对象的 3 个基本特征：封装，继承，多态。 其中多态最基本的体现就是重载和重写了，重载和重写的一个重要特征就是方法名相同，其他各种不同： 重载：发生在同一个类中，入参必须不同，返回类型、访问修饰符、抛出的异常都可以不同； 重写：发生在子父类中，入参和返回类型必须相同，访问修饰符大于等于被重写的方法，不能抛出新的异常。 相同的方法名实际上给虚拟机的调用带来了困惑，因为虚拟机需要判断，它到底应该调用哪个方法，而这个过程会在分派调用中体现出来。其中： 方法重载 —— 静态分派 方法重写 —— 动态分派 静态分派（方法重载）在介绍静态分派前，我们先来介绍一下什么是变量的静态类型和实际类型。 变量的静态类型和实际类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class StaticDispatch &#123; static abstract class Human &#123; &#125; static class Man extends Human &#123; &#125; static class Woman extends Human &#123; &#125; public void sayHello(Human guy) &#123; System.out.println("Hello guy!"); &#125; public void sayHello(Man man) &#123; System.out.println("Hello man!"); &#125; public void sayHello(Woman woman) &#123; System.out.println("Hello woman!"); &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); StaticDispatch sr = new StaticDispatch(); sr.sayHello(man); sr.sayHello(woman); /* 输出： Hello guy! Hello guy! 因为是根据变量的静态类型，也就是左面的类型：Human 来判断调用哪个方法， 所以调用的都是 public void sayHello(Human guy) */ &#125;&#125;/* 简单讲解 */// 使用Human man = new Man();// 实际类型发生变化Human man = new Man();man = new Woman();// 静态类型发生变化sr.sayHello((Man) man); // 输出：Hello man!sr.sayHello((Woman) man); // 输出：Hello woman! 其中 Human 称为变量的静态类型，Man 称为变量的实际类型。 在重载时，编译器是通过方法参数的静态类型，而不是实际类型，来判断应该调用哪个方法的。 通俗的讲，静态分派就是通过方法的参数（类型 &amp; 个数 &amp; 顺序）这种静态的东西来判断到底调用哪个方法的过程。 重载方法匹配优先级，例如一个字符 ‘a’ 作为入参 基本类型 char int long float double Character Serializable（Character 实现的接口） 同时出现两个优先级相同的接口，如 Serializable 和 Comparable，会提示类型模糊，拒绝编译。 Object char…（变长参数优先级最低） 动态分派（方法重写）动态分派就是在运行时，根据实际类型确定方法执行版本的分派过程。 12345678910111213141516171819202122232425262728293031public class DynamicDispatch &#123; static abstract class Human &#123; protected abstract void sayHello(); &#125; static class Man extends Human &#123; protected void sayHello() &#123; System.out.println("Hello man"); &#125; &#125; static class Woman extends Human &#123; protected void sayHello() &#123; System.out.println("Hello woman"); &#125; &#125; public static void main(String[] args) &#123; Human man = new Man(); Human woman = new Woman(); man.sayHello(); woman.sayHello(); man = woman; man.sayHello(); /* 输出 Hello man Hello woman Hello woman */ &#125;&#125; 字节码分析： 123456789101112131415161718192021222324public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=3, args_size=1 0: new #2 // class com/jvm/ch8/DynamicDispatch$Man 3: dup 4: invokespecial #3 // Method com/jvm/ch8/DynamicDispatch$Man.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: new #4 // class com/jvm/ch8/DynamicDispatch$Woman 11: dup 12: invokespecial #5 // Method com/jvm/ch8/DynamicDispatch$Woman.&quot;&lt;init&gt;&quot;:()V 15: astore_2 16: aload_1 // 把刚创建的对象的引用压到操作数栈顶， // 供之后执行sayHello时确定是执行哪个对象的sayHello 17: invokevirtual #6 // 方法调用 20: aload_2 // 把刚创建的对象的引用压到操作数栈顶， // 供之后执行sayHello时确定是执行哪个对象的sayHello 21: invokevirtual #6 // 方法调用 24: aload_2 25: astore_1 26: aload_1 27: invokevirtual #6 // Method com/jvm/ch8/DynamicDispatch$Human.sayHello:()V 30: return 通过字节码分析可以看出，invokevirtual 指令的运行过程大致为： 去操作数栈顶取出将要执行的方法的所有者，记作 C； 查找此方法： 在 C 中查找此方法； 在 C 的各个父类中查找； 查找过程： 查找与常量的描述符和简单名称都相同的方法； 进行访问权限验证，不通过抛出：IllegalAccessError 异常； 通过访问权限验证则返回直接引用； 没找到则抛出：AbstractMethodError 异常，即该方法没被实现。 动态分派在虚拟机种执行的非常频繁，而且方法查找的过程要在类的方法元数据中搜索合适的目标，从性能上考虑，不太可能进行如此频繁的搜索，需要进行性能上的优化。 常用优化手段： 在类的方法区中建立一个虚方法表。 虚方法表中存放着各个方法的实际入口地址，如果某个方法没有被子类方法重写，那子类方法表中该方法的入口地址 = 父类方法表中该方法的入口地址； 使用这个方法表索引代替在元数据中查找； 该方法表会在类加载的连接阶段初始化好。 通俗的讲，动态分派就是通过方法的接收者这种动态的东西来判断到底调用哪个方法的过程。 总结一下：静态分派看左面，动态分派看右面。 单分派与多分派除了静态分派和动态分派这种分派分类方式，还有一种根据宗量分类的方式，可以将方法分派分为单分派和多分派。 宗量：方法的接收者 &amp; 方法的参数。 Java 语言的静态分派属于多分派，根据 方法接收者的静态类型 和 方法参数类型 两个宗量进行选择。 Java 语言的动态分派属于单分派，只根据 方法接收者的实际类型 一个宗量进行选择。 动态类型语言支持什么是动态类型语言？ 就是类型检查的主体过程在运行期，而非编译期的编程语言。 动/静态类型语言各自的优点？ 动态类型语言：灵活性高，开发效率高。 静态类型语言：编译器提供了严谨的类型检查，类型相关的问题能在编码的时候就发现。 Java虚拟机层面提供的动态类型支持： invokedynamic 指令 java.lang.invoke 包 java.lang.invoke 包目的： 在之前的依靠符号引用确定调用的目标方法的方式之外，提供了 MethodHandle 这种动态确定目标方法的调用机制。 MethodHandle 的使用 获得方法的参数描述，第一个参数是方法返回值的类型，之后的参数是方法的入参： 1MethodType mt = MethodType.methodType(void.class, String.class); 获取一个普通方法的调用： 12345678/** * 需要的参数： * 1. 被调用方法所属类的类对象 * 2. 方法名 * 3. MethodType 对象 mt * 4. 调用该方法的对象 */MethodHandle.lookup().findVirtual(receiver.getClass(), "方法名", mt).bindTo(receiver); 获取一个父类方法的调用： 12345678/** * 需要的参数： * 1. 被调用方法所属类的类对象 * 2. 方法名 * 3. MethodType 对象 mt * 4. 调用这个方法的类的类对象 */MethodHandle.lookup().findSpecial(GrandFather.class, "方法名", mt, getClass()); 通过 MethodHandle mh 执行方法： 1234567/* invoke() 和 invokeExact() 的区别：- invokeExact() 要求更严格，要求严格的类型匹配，方法的返回值类型也在考虑范围之内- invoke() 允许更加松散的调用方式*/mh.invoke("Hello world");mh.invokeExact("Hello world"); 使用示例： 12345678910111213141516171819202122232425262728293031323334353637public class MethodHandleTest &#123; static class ClassA &#123; public void println(String s) &#123; System.out.println(s); &#125; &#125; public static void main(String[] args) throws Throwable &#123; /* obj的静态类型是Object，是没有println方法的，所以尽管obj的实际类型都包含println方法， 它还是不能调用println方法 */ Object obj = System.currentTimeMillis() % 2 == 0 ? System.out : new ClassA(); /* invoke()和invokeExact()的区别： - invokeExact()要求更严格，要求严格的类型匹配，方法的返回值类型也在考虑范围之内 - invoke()允许更加松散的调用方式 */ getPrintlnMH(obj).invoke("Hello world"); getPrintlnMH(obj).invokeExact("Hello world"); &#125; private static MethodHandle getPrintlnMH(Object receiver) throws NoSuchMethodException, IllegalAccessException &#123; /* MethodType代表方法类型，第一个参数是方法返回值的类型，之后的参数是方法的入参 */ MethodType mt = MethodType.methodType(void.class, String.class); /* lookup()方法来自于MethodHandles.lookup， 这句的作用是在指定类中查找符合给定的方法名称、方法类型，并且符合调用权限的方法句柄 */ /* 因为这里调用的是一个虚方法，按照Java语言的规则，方法第一个参数是隐式的，代表该方法的接收者， 也即是this指向的对象，这个参数以前是放在参数列表中进行传递，现在提供了bindTo()方法来完成这件事情 */ return MethodHandles.lookup().findVirtual(receiver.getClass(), "println", mt).bindTo(receiver); &#125;&#125; MethodHandles.lookup 中 3 个方法对应的字节码指令： findStatic()：对应 invokestatic findVirtual()：对应 invokevirtual &amp; invokeinterface findSpecial()：对应 invokespecial MethodHandle 和 Reflection 的区别 本质区别：它们都在模拟方法调用，但是 Reflection 模拟的是 Java 代码层次的调用； MethodHandle 模拟的是字节码层次的调用。 包含信息的区别： Reflection 的 Method 对象包含的信息多，包括：方法签名、方法描述符、方法的各种属性的Java端表达方式、方法执行权限等； MethodHandle 对象包含的信息比较少，既包含与执行该方法相关的信息。 invokedynamic 指令Lambda 表达式就是通过 invokedynamic 指令实现的。 JAVA类装载方式，有两种:1.隐式装载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中。 2.显式装载， 通过class.forname()等方法，显式加载需要的类类加载的动态性体现:一个应用程序总是由n多个类组成，Java程序启动时，并不是一次把所有的类全部加载后再运行，它总是先把保证程序运行的基础类一次性加载到jvm中，其它类等到jvm用到的时候再加载，这样的好处是节省了内存的开销，因为java最早就是为嵌入式系统而设计的，内存宝贵，这是一种可以理解的机制，而用到时再加载这也是java动态性的一种体现. 基于栈的字节码解释执行引擎这个栈，就是栈帧中的操作数栈。 解释执行先通过 javac 将代码编译成字节码，虚拟机再通过加载字节码文件，解释执行字节码文件生成机器码，解释执行的流程如下： 1词法分析 -&gt; 语法分析 -&gt; 形成抽象语法树 -&gt; 遍历语法树生成线性字节码指令流 指令集分类基于栈的指令集 优点： 可移植：寄存器由硬件直接提供，程序如果直接依赖这些硬件寄存器，会不可避免的受到硬件的约束； 代码更紧凑：字节码中每个字节对应一条指令，多地址指令集中还需要存放参数； 编译器实现更简单：不需要考虑空间分配问题，所需的空间都在栈上操作。 缺点： 执行速度稍慢 完成相同的功能，需要更多的指令，因为出入栈本身就产生相当多的指令； 频繁的栈访问导致频繁的内存访问，对于处理器而言，内存是执行速度的瓶颈。 示例： 两数相加 1234iconst_1 // 把常量1入栈iconst_1iadd // 把栈顶两元素出栈相加，结果入栈istore_0 // 把栈顶值存入第0个Slot中 基于寄存器的指令集示例： 两数相加 12mov eax, 1add eax, 1 执行过程分析1234567891011121314151617181920212223242526272829303132333435public class Architecture &#123; /* calc函数的字节码分析： public int calc(); descriptor: ()I flags: ACC_PUBLIC Code: stack=2, locals=4, args_size=1 // stack=2，说明需要深度为2的操作数栈 // locals=4，说明需要4个Slot的局部变量表 0: bipush 100 // 将单字节的整型常数值push到操作数栈 2: istore_1 // 将操作数栈顶的整型值出栈并存放到第一个局部变量Slot中 3: sipush 200 6: istore_2 7: sipush 300 10: istore_3 11: iload_1 // 将局部变量表第一个Slot中的整型值复制到操作数栈顶 12: iload_2 13: iadd // 将操作数栈中头两个元素出栈并相加，将结果重新入栈 14: iload_3 15: imul // 将操作数栈中头两个元素出栈并相乘，将结果重新入栈 16: ireturn // 返回指令，结束方法执行，将操作数栈顶的整型值返回给此方法的调用者 */ public int calc() &#123; int a = 100; int b = 200; int c = 300; return (a + b) * c; &#125; public static void main(String[] args) &#123; Architecture architecture = new Architecture(); architecture.calc(); &#125;&#125;]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的 I/O机制]]></title>
    <url>%2FJava%E7%9A%84IO%2F</url>
    <content type="text"><![CDATA[UNIX网络编程对I/O模型的分类Linux 的内核将所有外部设备都看做一个文件来操作（一切皆文件），对一个文件的读写操作会调用内核提供的系统命令，返回一个file descriptor（fd，文件描述符）。而对一个socket的读写也会有响应的描述符，称为socket fd（socket文件描述符），描述符就是一个数字，指向内核中的一个结构体（文件路径，数据区等一些属性）。 根据UNIX网络编程对I/O模型的分类，UNIX提供了5种I/O模型。 进程是无法直接操作I/O设备的，其必须通过系统调用请求内核来协助完成I/O动作，而内核会为每个I/O设备维护一个buffer。 整个请求过程为： 用户进程发起请求，内核接受到请求后，从I/O设备中获取数据到buffer中，再将buffer中的数据copy到用户进程的地址空间，该用户进程获取到数据后再响应客户端。 阻塞I/O模型最常用的I/O模型，默认情况下，所有文件操作都是阻塞的。 比如I/O模型下的套接字接口：在进程空间中调用recvfrom，其系统调用直到数据包到达（比如，还没有收到一个完整的UDP包）且被复制到应用进程的缓冲区中或者发生错误时才返回，而在用户进程这边，整个进程会被阻塞。当内核一直等到数据准备好了，它就会将数据从内核中拷贝到用户内存，然后内核返回结果，用户进程才解除block的状态，重新运行起来。 进程在调用recvfrom开始到它返回的整段时间内都是被阻塞的，所以叫阻塞I/O模型。所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 非阻塞I/O模型当用户进程调用recvfrom时，系统不会阻塞用户进程，而是立刻返回一个ewouldblock错误，从用户进程角度讲 ，并不需要等待，而是马上就得到了一个结果。用户进程判断标志是ewouldblock时，就知道数据还没准备好，于是它就可以去做其他的事了，于是它可以再次发送recvfrom，一旦内核中的数据准备好了。并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 当一个应用程序在一个循环里对一个非阻塞调用recvfrom，我们称为轮询。应用程序不断轮询内核，看看是否已经准备好了某些操作。这通常是浪费CPU时间，但这种模式偶尔会遇到。 I/O复用模型单个process就可以同时处理多个网络连接的IO。它的基本原理就是select/epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 Linux提供select/poll，进程通过将一个或多个fd（文件描述符）传递给select或poll系统调用，阻塞在select操作上，这样，select/poll可以帮我们侦测多个fd是否处于就绪状态。 select/poll是顺序扫描fd是否就绪，而且支持的fd数量有限，因此它的使用受到了一些制约。 Linux还提供一个epoll系统调用，epoll使用基于事件驱动方式代替顺序扫描，因此性能更高。当有fd就绪时，立即回调函数rollback。 I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，pselect，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。 IO多路复用之select、poll、epoll详解 当用户进程调用了select，那么整个进程会被block，而同时，内核会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从内核拷贝到用户进程。 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（多说一句。所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 信号驱动I/O模型首先开启套接口信号驱动I/O功能，并通过系统调用sigaction执行一个信号处理函数（此系统调用立即返回，进程继续工作，非阻塞）。当数据准备就绪时，就为改进程生成一个SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主循环函数处理树立。 异步I/O告知内核启动某个操作，并让内核在整个操作完成后（包括数据的复制）通知进程。 信号驱动I/O模型通知的是何时可以开始一个I/O操作，异步I/O模型有内核通知I/O操作何时已经完成。 I/O多路复用技术I/O编程中，需要处理多个客户端接入请求时，可以利用多线程或者I/O多路复用技术进行处理。 正如前面的简介，I/O多路复用技术通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。 与传统的多线程模型相比，I/O多路复用的最大优势就是系统开销小，系统不需要创建新的额外线程，也不需要维护这些线程的运行，降低了系统的维护工作量，节省了系统资源。 主要的应用场景： 服务器需要同时处理多个处于监听状态或多个连接状态的套接字。 服务器需要同时处理多种网络协议的套接字。 支持I/O多路复用的系统调用主要有select、pselect、poll、epoll。 而当前推荐使用的是epoll，优势如下： 支持一个进程打开的socket fd不受限制。 I/O效率不会随着fd数目的增加而线性下将。 使用mmap加速内核与用户空间的消息传递。 epoll拥有更加简单的API。 Java I/O 概览Java 的 I/O 大概可以分成以下几类： 磁盘操作：File 字节操作：InputStream 和 OutputStream 字符操作：Reader 和 Writer 对象操作：Serializable：序列化就是将一个对象转换成字节序列，方便存储和传输。 网络操作：Socket 新的输入/输出：NIO Java I/O 使用了装饰者模式来实现 Java磁盘操作File 类可以用于表示文件和目录的信息，但是它不表示文件的内容。 递归地列出一个目录下所有文件： 123456789101112public static void listAllFiles(File dir) &#123; if (dir == null || !dir.exists()) &#123; return; &#125; if (dir.isFile()) &#123; System.out.println(dir.getName()); return; &#125; for (File file : dir.listFiles()) &#123; listAllFiles(file); &#125;&#125; 从 Java7 开始，可以使用 Paths 和 Files 代替 File。 Java字节操作实现文件复制1234567891011121314151617public static void copyFile(String src, String dist) throws IOException &#123; FileInputStream in = new FileInputStream(src); FileOutputStream out = new FileOutputStream(dist); byte[] buffer = new byte[20 * 1024]; int cnt; // read() 最多读取 buffer.length 个字节 // 返回的是实际读取的个数 // 返回 -1 的时候表示读到 eof，即文件尾 while ((cnt = in.read(buffer, 0, buffer.length)) != -1) &#123; out.write(buffer, 0, cnt); &#125; in.close(); out.close();&#125; 装饰者模式Java I/O 使用了装饰者模式来实现。以 InputStream 为例， InputStream 是抽象组件； FileInputStream 是 InputStream 的子类，属于具体组件，提供了字节流的输入操作； FilterInputStream 属于抽象装饰者，装饰者用于装饰组件，为组件提供额外的功能。例如 BufferedInputStream 为 FileInputStream 提供缓存的功能。 实例化一个具有缓存功能的字节流对象时，只需要在 FileInputStream 对象上再套一层 BufferedInputStream 对象即可。 12FileInputStream fileInputStream = new FileInputStream(filePath);BufferedInputStream bufferedInputStream = new BufferedInputStream(fileInputStream); DataInputStream 装饰者提供了对更多数据类型进行输入的操作，比如 int、double 等基本类型。 Java字符操作编码与解码编码就是把字符转换为字节，而解码是把字节重新组合成字符。 如果编码和解码过程使用不同的编码方式那么就出现了乱码。 GBK 编码中，中文字符占 2 个字节，英文字符占 1 个字节； UTF-8 编码中，中文字符占 3 个字节，英文字符占 1 个字节； UTF-16be 编码中，中文字符和英文字符都占 2 个字节。 UTF-16be 中的 be 指的是 Big Endian，也就是大端。相应地也有 UTF-16le，le 指的是 Little Endian，也就是小端。 Java 的内存编码使用双字节编码 UTF-16be，这不是指 Java 只支持这一种编码方式，而是说 char 这种类型使用 UTF-16be 进行编码。char 类型占 16 位，也就是两个字节，Java 使用这种双字节编码是为了让一个中文或者一个英文都能使用一个 char 来存储。 String 的编码方式String 可以看成一个字符序列，可以指定一个编码方式将它编码为字节序列，也可以指定一个编码方式将一个字节序列解码为 String。 1234String str1 = "中文";byte[] bytes = str1.getBytes("UTF-8");String str2 = new String(bytes, "UTF-8");System.out.println(str2); 在调用无参数 getBytes() 方法时，默认的编码方式不是 UTF-16be。双字节编码的好处是可以使用一个 char 存储中文和英文，而将 String 转为 bytes[] 字节数组就不再需要这个好处，因此也就不再需要双字节编码。getBytes() 的默认编码方式与平台有关，一般为 UTF-8。 1byte[] bytes = str1.getBytes(); Reader 与 Writer不管是磁盘还是网络传输，最小的存储单元都是字节，而不是字符。但是在程序中操作的通常是字符形式的数据，因此需要提供对字符进行操作的方法。 InputStreamReader 实现从字节流解码成字符流； OutputStreamWriter 实现字符流编码成为字节流。 实现逐行输出文本文件的内容123456789101112131415public static void readFileContent(String filePath) throws IOException &#123; FileReader fileReader = new FileReader(filePath); BufferedReader bufferedReader = new BufferedReader(fileReader); String line; while ((line = bufferedReader.readLine()) != null) &#123; System.out.println(line); &#125; // 装饰者模式使得 BufferedReader 组合了一个 Reader 对象 // 在调用 BufferedReader 的 close() 方法时会去调用 Reader 的 close() 方法 // 因此只要一个 close() 调用即可 bufferedReader.close();&#125; Java对象操作序列化序列化就是将一个对象转换成字节序列，方便存储和传输。 序列化：ObjectOutputStream.writeObject() 反序列化：ObjectInputStream.readObject() 不会对静态变量进行序列化，因为序列化只是保存对象的状态，静态变量属于类的状态。 Serializable序列化的类需要实现 Serializable 接口，它只是一个标准，没有任何方法需要实现，但是如果不去实现它的话而进行序列化，会抛出异常。 123456789101112131415161718192021222324252627282930public static void main(String[] args) throws IOException, ClassNotFoundException &#123; A a1 = new A(123, "abc"); String objectFile = "file/a1"; ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream(objectFile)); objectOutputStream.writeObject(a1); objectOutputStream.close(); ObjectInputStream objectInputStream = new ObjectInputStream(new FileInputStream(objectFile)); A a2 = (A) objectInputStream.readObject(); objectInputStream.close(); System.out.println(a2);&#125;private static class A implements Serializable &#123; private int x; private String y; A(int x, String y) &#123; this.x = x; this.y = y; &#125; @Override public String toString() &#123; return "x = " + x + " " + "y = " + y; &#125;&#125; transienttransient 关键字可以使一些属性不会被序列化。 ArrayList 中存储数据的数组 elementData 是用 transient 修饰的，因为这个数组是动态扩展的，并不是所有的空间都被使用，因此就不需要所有的内容都被序列化。通过重写序列化和反序列化方法，使得可以只序列化数组中有内容的那部分数据。 1private transient Object[] elementData; 想序列化ArrayList的话，把elementdata的元素一个个读出来，一个个序列化。 补充阅读 Java网络操作Java 中的网络支持： InetAddress：用于表示网络上的硬件资源，即 IP 地址； URL：统一资源定位符； Sockets：使用 TCP 协议实现网络通信； Datagram：使用 UDP 协议实现网络通信。 InetAddress没有公有的构造函数，只能通过静态方法来创建实例。 12InetAddress.getByName(String host);InetAddress.getByAddress(byte[] address); URL可以直接从 URL 中读取字节流数据。 1234567891011121314151617181920public static void main(String[] args) throws IOException &#123; URL url = new URL("http://www.baidu.com"); /* 字节流 */ InputStream is = url.openStream(); /* 字符流 */ InputStreamReader isr = new InputStreamReader(is, "utf-8"); /* 提供缓存功能 */ BufferedReader br = new BufferedReader(isr); String line; while ((line = br.readLine()) != null) &#123; System.out.println(line); &#125; br.close();&#125; Sockets ServerSocket：服务器端类 Socket：客户端类 服务器和客户端通过 InputStream 和 OutputStream 进行输入输出。 Datagram DatagramSocket：通信类 DatagramPacket：数据包类 零拷贝 zero-copy概述考虑这样一种常用的情形：你需要将静态内容（类似图片、文件）展示给用户。那么这个情形就意味着你需要先将静态内容从磁盘中拷贝出来放到一个内存buf中，然后将这个buf通过socket传输给用户，进而用户或者静态内容的展示。这看起来再正常不过了，但是实际上这是很低效的流程，我们把上面的这种情形抽象成下面的过程： 1read(file, tmp_buf, len);write(socket, tmp_buf, len); 首先调用read将静态内容，这里假设为文件A，读取到tmp_buf, 然后调用write将tmp_buf写入到socket中，如图： 在这个过程中文件A的经历了4次copy的过程： 首先，调用read时，文件A拷贝到了kernel模式； 之后，CPU控制将kernel模式数据copy到user模式下； 调用write时，先将user模式下的内容copy到kernel模式下的socket的buffer中； 最后将kernel模式下的socket buffer的数据copy到网卡设备中传送； 从上面的过程可以看出，数据白白从kernel模式到user模式走了一圈，浪费了2次copy(第一次，从kernel模式拷贝到user模式；第二次从user模式再拷贝回kernel模式，即上面4次过程的第2和3步骤。)。而且上面的过程中kernel和user模式的上下文的切换也是4次。 幸运的是，你可以用一种叫做Zero-Copy的技术来去掉这些无谓的copy。应用程序用Zero-Copy来请求kernel直接把disk的data传输给socket，而不是通过应用程序传输。Zero-Copy大大提高了应用程序的性能，并且减少了kernel和user模式上下文的切换。 详述Zero-Copy技术省去了将操作系统的read buffer拷贝到程序的buffer，以及从程序buffer拷贝到socket buffer的步骤，直接将read buffer拷贝到socket buffer. Java NIO中的FileChannal.transferTo()方法就是这样的实现，这个实现是依赖于操作系统底层的sendFile()实现的。 1public void transferTo(long position, long count, WritableByteChannel target); 他底层的调用时系统调用sendFile()方法： 1#include &lt;sys/socket.h&gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 下图展示了在transferTo()之后的数据流向： 下图展示了在使用transferTo()之后的上下文切换： 使用了Zero-Copy技术之后，整个过程如下： transferTo()方法使得文件A的内容直接拷贝到一个read buffer（kernel buffer）中； 然后数据(kernel buffer)拷贝到socket buffer中。 最后将socket buffer中的数据拷贝到网卡设备（protocol engine）中传输；这显然是一个伟大的进步：这里把上下文的切换次数从4次减少到2次，同时也把数据copy的次数从4次降低到了3次。但是这是Zero-Copy么，答案是否定的。 进阶Linux 2.1内核开始引入了sendfile函数（上一节有提到）,用于将文件通过socket传送。 1sendfile(socket, file, len); 该函数通过一次系统调用完成了文件的传送，减少了原来read/write方式的模式切换。此外更是减少了数据的copy, sendfile的详细过程如图： 通过sendfile传送文件只需要一次系统调用，当调用sendfile时： 首先（通过DMA）将数据从磁盘读取到kernel buffer中； 然后将kernel buffer拷贝到socket buffer中； 最后将socket buffer中的数据copy到网卡设备（protocol engine）中发送； 这个过程就是第二节（详述）中的那个步骤。 sendfile与read/write模式相比，少了一次copy。但是从上述过程中也可以发现从kernel buffer中将数据copy到socket buffer是没有必要的。 Linux2.4 内核对sendfile做了改进，如图： 改进后的处理过程如下： 将文件拷贝到kernel buffer中； 向socket buffer中追加当前要发生的数据在kernel buffer中的位置和偏移量； 根据socket buffer中的位置和偏移量直接将kernel buffer的数据copy到网卡设备（protocol engine）中； 经过上述过程，数据只经过了2次copy就从磁盘传送出去了。这个才是真正的Zero-Copy(这里的零拷贝是针对kernel来讲的，数据在kernel模式下是Zero-Copy)。 正是Linux2.4的内核做了改进，Java中的TransferTo()实现了Zero-Copy,如下图： Zero-Copy技术的使用场景有很多，比如Kafka, 又或者是Netty等，可以大大提升程序的性能。 Java NIONIO，就是 New IO，是非阻塞 IO。支持面向缓冲区，基于通道的 IO 操作。 传统的 IO 是面向流的，并且是单向的。如果要进行双工的通信，要建立两个流，一个输入流，一个输出流。 NIO 是面向缓冲区的，是双向的，通信双方建立一个通道，然后把数据存放在缓冲区中，然后缓冲区在通道中流动进行数据的传输。打个比方，通道 —— 铁路，缓冲区 —— 火车，数据 —— 人，一堆人想要从 A 地到达 B 地，首先需要把人装上火车，然后火车在铁路上从 A 地跑到 B 地，实现将人从 A 地运到 B 地；同样的道理，一堆数据如果想要从 A 地运到 B 地，需要先把数据装入缓冲区，然后将缓冲区从 A 地跑到 B 地，从而实现将数据从 A 地运到 B 地。 缓冲区 Buffer首先，有这些个 Buffer 种类： ByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer 简而言之就是，8 个基本类型，除了 reference 都有相应类型的缓冲区在。 缓冲区中的核心属性12345// Invariants: mark &lt;= position &lt;= limit &lt;= capacityprivate int mark = -1; // 可以记录当前 position 的位置，然后调用 reset()，可以把 position 恢复到这个位置private int position = 0; // 缓冲区中正在操作的数据的位置private int limit; // 缓冲区中可以操作数据的大小，limit 后的数据是不能进行读写的private int capacity; // 缓冲区中最大存储数据的容量，一旦声明不能改变 Note： 抽象类虽然自身不可以实例化，但是其子类覆盖了所有的抽象方法后，是可以实例化的，所以抽象类的构造函数，适用于给其子类对象进行初始化的。 所以对于 ByteBuffer.allocate() 方法，实际上是新建了一个 ByteBuffer 抽象类的子类 HeapByteBuffer 对象，HeapByteBuffer 类实现了 ByteBuffer 的所有抽象方法，所以我们可以通过调用 ByteBuffer 抽象类的构造函数来初始化 HeapByteBuffer 对象。 直接缓冲区与非直接缓冲区 非直接缓冲区 直接缓冲区 分配方法 allocate() allocateDirect() 特点 将缓冲区建立在 JVM 内存中 将缓冲区建立在物理内存 (直接内存) 中，可以提高效率 可以通过 isDirect() 来判断当前的缓冲区是不是直接缓冲区。 为什么使用直接缓冲区可以提高效率呢？ 正常情况下，如果你想将一些数据写到物理磁盘上，你需要先将数据从 JVM 内存中 copy 到内核地址空间，因为内核才真正具有控制计算机硬件资源的功能，用户态运行的上层应用程序只能通过系统调用来让内核态的资源来帮助将数据写入硬盘。 当要将一个超大的文件写到硬盘上时，这个 copy 的操作就显得很费时了。 而使用直接内存，我们可以操作物理磁盘在内存中的内存映射文件，来直接将物理硬盘上的数据加载进内存，或者将内存中的写入硬盘中，而这个映射，其实是一个物理地址到逻辑地址的转换（或者逆过程）。 常用方法代码示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public class BufferDemo &#123; private ByteBuffer buffer = ByteBuffer.allocate(1024); private ByteBuffer directBuffer = ByteBuffer.allocateDirect(1024); /** * 打印 buffer 的 4 大核心属性 */ @Test public void test0() &#123; System.out.println(buffer.position()); System.out.println(buffer.limit()); System.out.println(buffer.capacity()); System.out.println("---------------------"); &#125; /** * Test put, get, flip, rewind, */ @Test public void test1() &#123; String str = "abcde"; /* put：向缓冲区中写入数据 */ buffer.put(str.getBytes()); System.out.println(buffer.position()); // 5 System.out.println(buffer.limit()); // 1024 System.out.println(buffer.capacity()); // 1024 System.out.println("---------------------"); /* flip：将缓冲区从写状态切换到读状态 */ buffer.flip(); System.out.println(buffer.position()); // 0 System.out.println(buffer.limit()); // 5 System.out.println(buffer.capacity()); // 1024 System.out.println("---------------------"); /* get：从缓冲区中读数据出来 */ byte[] tmp = new byte[2]; buffer.get(tmp); System.out.println("get result: " + new String(tmp)); System.out.println(buffer.position()); // 2 System.out.println(buffer.limit()); // 5 System.out.println(buffer.capacity()); // 1024 System.out.println(buffer.remaining()); // 3，看看还有多少元素可读 System.out.println("---------------------"); /* mark：记录当前 position 的位置到 mark 变量 */ /* reset：令 position = mark */ buffer.mark(); buffer.get(tmp); System.out.println("get result: " + new String(tmp)); System.out.println("before reset: position = " + buffer.position()); // 4 buffer.reset(); System.out.println("after reset: position = " + buffer.position()); // 2 System.out.println("---------------------"); /* rewind：令 position = 0，就是个倒带的操作 */ buffer.rewind(); System.out.println(buffer.position()); // 0 System.out.println(buffer.limit()); // 5 System.out.println(buffer.capacity()); // 1024 System.out.println(buffer.remaining()); // 5 System.out.println("---------------------"); /* compact：从读状态切换回写状态，可以接着上回写的地方继续往下写 */ buffer.compact(); System.out.println(buffer.position()); // 5 System.out.println(buffer.limit()); // 1024 System.out.println(buffer.capacity()); // 1024 System.out.println(buffer.remaining()); // 1019 System.out.println("---------------------"); /* clear，实际上并没有将数据真的清除，只有当新的数据把旧的数据覆盖了，旧的数据才真的被清除 */ buffer.clear(); System.out.println(buffer.position()); // 0 System.out.println(buffer.limit()); // 1024 System.out.println(buffer.capacity()); // 1024 System.out.println(buffer.remaining()); // 1024 System.out.println("---------------------"); /* isDirect：判断缓冲区是不是直接内存缓冲区 */ System.out.println(buffer.isDirect()); // false System.out.println(directBuffer.isDirect()); // true System.out.println("---------------------"); &#125;&#125; 通道 Channel用于连接两个节点。在 NIO 中负责缓冲区中数据的传输，它本身是不能存储数据的。 主要实现类 java.nio.channels.Channel 接口 本地传输： FileChannel 网络传输： SocketChannel ServerSocketChannel DatagramChannel 获取通道Java 中以下类拥有 getChannel() 方法，可以通过这个方法获取对应的通道。 本地 IO FileInputStream / FileOutputStream RandomAccessFile 网络 IO Socket ServerSocket DatagramSocket 除此之外，JDK 1.7 中的 AIO 还提供了以下两种获取通道的方式： 为各个通道提供了静态方法 open() Files.newByteChannel() 通道之间的数据传输 transferFrom() transferTo() 分散读取与聚集写入就是以前是用一个缓冲区协助通道传输数据，Scatter 和 Gather 就是用一堆缓冲区（缓冲区数组）协助通道传输数据。 分散读取 Scattering Reads：按照缓冲区的顺序，将从 Channel 中读取的数据依次将 Buffer 填满。 聚集写入 Gathering Writes：按照缓冲区的顺序，写入position 和 limit 之间的数据到 Channel 。 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118public class ChannelDemo &#123; /** * 利用通道完成文件的复制（非直接缓冲区） */ @Test public void copyFile1() &#123; FileInputStream in = null; FileOutputStream out = null; FileChannel inChannel = null; FileChannel outChannel = null; try &#123; in = new FileInputStream("1.png"); out = new FileOutputStream("2.png"); inChannel = in.getChannel(); outChannel = out.getChannel(); ByteBuffer buffer = ByteBuffer.allocate(1024); while (inChannel.read(buffer) != -1) &#123; buffer.flip(); outChannel.write(buffer); buffer.clear(); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (in != null) &#123; in.close(); &#125; if (out != null) &#123; out.close(); &#125; if (inChannel != null) &#123; inChannel.close(); &#125; if (outChannel != null) &#123; out.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 使用直接缓冲区完成文件的复制(内存映射文件)，通过 Channel 的 map 方法获取的缓冲区就是直接缓冲区 */ @Test public void copyFile2() &#123; FileChannel inChannel = null; FileChannel outChannel = null; try &#123; inChannel = FileChannel.open(Paths.get("1.png"), StandardOpenOption.READ); outChannel = FileChannel.open(Paths.get("2.png"), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE); MappedByteBuffer inBuffer = inChannel.map( FileChannel.MapMode.READ_ONLY, 0, inChannel.size()); MappedByteBuffer outBuffer = outChannel.map( FileChannel.MapMode.READ_WRITE, 0, inChannel.size()); byte[] dst = new byte[inBuffer.limit()]; inBuffer.get(dst); outBuffer.put(dst); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (inChannel != null) &#123; try &#123; inChannel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (outChannel != null) &#123; try &#123; outChannel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 通道之间的数据 transfer (直接缓冲区) */ @Test public void copyFile3() &#123; FileChannel inChannel = null; FileChannel outChannel = null; try &#123; inChannel = FileChannel.open(Paths.get("1.png"), StandardOpenOption.READ); outChannel = FileChannel.open(Paths.get("2.png"), StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE); // 以下两行效果相等 inChannel.transferTo(0, inChannel.size(), outChannel); outChannel.transferFrom(inChannel, 0, inChannel.size()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (inChannel != null) &#123; try &#123; inChannel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (outChannel != null) &#123; try &#123; outChannel.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 阻塞与非阻塞NIO 完成网络通信的三个核心 通道（Channel）：负责连接。 SocketChannel ServerSocketChannel DatagramChannel Pipe.SinkChannel Pipe.SourceChannel 缓冲区（Buffer）：负责数据存取。 选择器（Selector）：是 SelectableChannel 的多路复用器，用于监控 SelectableChannel 的 IO 状况。 DatagramChannel管道 PipeJava NIO 的 Pipe 可以在两个线程之间建立单项的数据连接，管道有一个 Pipe.SinkChannel 和 一个 Pipe.SourceChannel，数据会被写到 Pipe.SinkChannel，然后从 Pipe.SourceChannel 中读取。 代码示例1234567891011121314151617181920212223public class NioPipeDemo &#123; @Test public void test() throws IOException &#123; Pipe pipe = Pipe.open(); Pipe.SinkChannel sinkChannel = pipe.sink(); String str = "我是管道发来的数据"; ByteBuffer byteBuffer = ByteBuffer.allocate(1024); byteBuffer.put(str.getBytes()); byteBuffer.flip(); while (byteBuffer.hasRemaining()) &#123; sinkChannel.write(byteBuffer); &#125; Pipe.SourceChannel sourceChannel = pipe.source(); byteBuffer.clear(); int len = sourceChannel.read(byteBuffer); System.out.println(new String(byteBuffer.array(), 0, len)); sinkChannel.close(); sourceChannel.close(); &#125;&#125;]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java容器]]></title>
    <url>%2FJava%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Collection List Arraylist： Object数组 Vector： Object数组 LinkedList： 双向链表(JDK1.6之前为循环链表，JDK1.7取消了循环) Set HashSet（无序，唯一）: 基于 HashMap 实现的，底层采用 HashMap 来保存元素 LinkedHashSet： LinkedHashSet 继承与 HashSet，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的LinkedHashMap 其内部是基于 Hashmap 实现一样，不过还是有一点点区别的。 TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树。) Map HashMap： JDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间 LinkedHashMap: LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。 HashTable: 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap: 红黑树（自平衡的排序二叉树） ArrayList、LinkedList 、VectorArrayList 与 LinkedList 异同 相同点： 都实现了 List 接口。 都是线程不安全的。 不同点： 底层实现： ArrayList 底层使用的是 Object 数组；LinkedList 底层使用的是双向链表数据结构，维护一个 head 指针和一个 tail 指针。（JDK 1.6 之前为循环链表。为啥要改：因为在链表头 / 尾进行插入 / 删除操作时，循环链表需要处理两头的指针，而非循环链表只需要处理一边，更高效，同时在两头（链头 / 链尾）操作是最普遍的。） 基本操作的时间复杂度：ArrayList 可以高效的访问元素 O(1)，但是不能高效的插入和删除元素 O(n)；LinkedList 可以高效的插入和删除元素 O(1)，但是不能高效访问的元素 O(n)。 内存空间占用：ArrayList 的空间浪费主要体现在在 list 列表的结尾会预留一定的容量空间；而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间。 LinkedList包含两个重要的成员：header 和 size。header是双向链表的表头，它是双向链表节点所对应的类Entry的实例。Entry中包含成员变量： previous, next, element。其中，previous是该节点的上一个节点，next是该节点的下一个节点，element是该节点所包含的值。size是双向链表中节点的个数。 补充：RandomAccess 接口public interface RandomAccess {}RandomAccess 接口里啥都没有，和 Serializable 接口一样，是个标识接口。它标识实现这个接口的类具有随机访问功能。 这个标识有啥用？在 Collections.binarySearch() 方法里有用： 123456public static &lt;T&gt; int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) &#123; if (list instanceof RandomAccess || list.size()&lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key); &#125; 在 binarySearch() 方法中，它要判断传入的 list 是否 RamdomAccess 的实例，如果是，调用 indexedBinarySearch() 方法，如果不是，那么调用 iteratorBinarySearch() 方法indexedBinarySearch() 方法和 iteratorBinarySearch() 方法的区别在于：需要使用 indexedBinarySearch() 方法的集合，是直接通过索引 i 取变量的，而需要使用 iteratorBinarySearch() 方法的集合要取到这个集合的迭代器用来取元素： 123456ListIterator&lt;? extends Comparable&lt;? super T&gt;&gt; i = list.listIterator();while (low &lt;= high) &#123; int mid = (low + high) &gt;&gt;&gt; 1; Comparable&lt;? super T&gt; midVal = get(i, mid); // 取元素 ... &#125; ArrayList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的。 实现了RandomAccess接口的list，优先选择普通for循环 ，其次foreach 未实现RandomAccess接口的list， 优先选择iterator遍历（foreach遍历底层也是通过iterator实现的），大 size的数据，千万不要使用普通for循环 ArrayList 与 Vector 区别Vector 类是线程安全的，给所有会出现线程安全问题的方法都加上 synchronized 修饰，所以很慢。 CopyOnWriteArrayList 实现了List接口 内部持有一个ReentrantLock lock = new ReentrantLock(); 底层是用volatile transient声明的数组 array 读写分离，写时复制出一个新的数组，完成插入、修改或者移除操作后将新数组赋值给array Vector是增删改查方法都加了synchronized，保证同步，但是每个方法执行的时候都要去获得锁，性能就会大大下降，而CopyOnWriteArrayList 只是在增删改上加锁，但是读不加锁，在读方面的性能就好于Vector，CopyOnWriteArrayList支持读多写少的并发情况。 HashMap、HashTable、TreeMap Hash 算法 加法 Hash：把输入元素一个一个的加起来构成最后的结果。 位运算 Hash：这类型 Hash 函做通过利用各种位运算（常见的是移位和异或）来充分的混合输入元素。 乘法 Hash：这种类型的 Hash 函数利用了乘法的不相关性（乘法的这种性质，最有名的莫过于平方取关尾的随机数生成算法，虽然这种算法效果并不好]；jdk5.0 里面的 String 类的 hashCode() 方法也使用乘法Hash；32 位 FNV 算法 除法 Hash：除法和乘法一样，同样具有表面上看起来的不相关性。不过，因为除法太慢，这种方式几乎找不到真正的应用 查表 Hash：查表 Hash 最有名的例子莫过于 CRC 系列算法。虽然 CRC 系列算法本身并不是查表，但是，查表是它的一种最快的实现方式。查表 Hash 中有名的子有：Universal Hashing 和 Zobrist Hashing。他们的表格都是随机生成的。 混合 Hash：混合 Hash 算法利用了以上各种方式。各种常见的 Hash 算法，比如 MD5、Tiger 都属于这个范围。它们一般很少在面向查找的 Hash 函做里面使用 基本区别它们都是最常见的 Map 实现，是以键值对的形式存储数据的容器类型。 HashTable： 线程安全，不支持 null 作为键或值，它的线程安全是通过在所有存在线程安全问题的方法上加 synchronized 实现的，所以性能很差，很少使用。 HashMap： 不是线程安全的，但是支持 null 作为键或值，是绝大部分利用键值对存取场景的首选，put 和 get 基本可以达到常数级别的时间复杂度。 TreeMap： 基于红黑树的一种提供顺序访问的 Map，它的 get，put，remove 等操作是 O(log(n)) 级别的时间复杂度的（因为要保证顺序），具体的排序规则可以由 Comparator 指定：public TreeMap(Comparator&lt;? super K&gt; comparator)。 HashMap 和 HashTable 的区别总结 线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过 synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： HashMap 要比 HashTable 效率高。 对 null key 和 null value 的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同： 创建时不指定容量初始值：HashTable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。 创建时给定了容量初始值：Hashtable 会直接使用你给定的大小，而 HashMap 会使用 tableSizeFor 方法将其扩充为 2 的幂次方大小。 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以减少搜索时间。HashTable 没有这样的机制。 HashMap 的长度为什么要是 2 的幂次方 因为 hashCode 是 -2147483648 到 2147483647 的，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个40亿长度的数组，内存是放不下的。在决定一个元素在哈希表中的真正位置时是要进行 hashCode % n 的运算的（n 是存元素的哈希数组的长度），得到的余数才能用来要存放的位置也就是对应的数组下标，如果 n 是 2 的幂次方的话，这个操作是可以用位运算来解决的：(n - 1) &amp; hash，快. 在对 Map 的顺序没有要求的情况下，HashMap 基本是最好的选择，不过 HashMap 的性能十分依赖于 hashCode 的有效性，所以必须满足： equals 判断相等的对象的 hashCode 一定相等 重写了 hashCode 必须重写 equals hashCode()、equals()、==问题Java hashCode() 和 equals()的若干问题解答 equals() ：定义在JDK的Object.java中。通过判断两个对象的地址是否相等(即，是否是同一个对象)来区分它们是否相等。 若某个类没有覆盖equals()方法，当它的通过equals()比较两个对象时，实际上是比较两个对象是不是同一个对象。这时，等价于通过“==”去比较这两个对象。 我们可以覆盖类的equals()方法，来让equals()通过其它方式比较两个对象是否相等。通常的做法是：若两个对象的内容相等，则equals()方法返回true；否则，返回fasle。 == : 作用是判断两个对象的地址是不是相等。即，判断两个对象是不试同一个对象。 hashCode() ：作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。“将该对象的内部地址转换成一个整数返回”。散列表的本质是通过数组实现的。当我们要获取散列表中的某个“值”时，实际上是要获取数组中的某个位置的元素。而数组的位置，就是通过“键”来获取的；更进一步说，数组的位置，是通过“键”对应的散列码计算得到的。 若重写了equals(Object obj)方法，则有必要重写hashCode()方法。（不然虽然equals认为相等，但是HashSet在添加p1和p2的时候，认为它们不相等） 若两个对象equals(Object obj)返回true，则hashCode（）有必要也返回相同的int数。 若两个对象equals(Object obj)返回false，则hashCode（）不一定返回不同的int数。 （不会在HashSet, Hashtable, HashMap等等这些本质是散列表的数据结构中用到该类，在这种情况下，该类的“hashCode() 和 equals() ”没有半毛钱关系的！） 若两个对象hashCode（）返回相同int数，则equals（Object obj）不一定返回true。（在散列表中，hashCode()相等，即两个键值对的哈希值相等。然而哈希值相等，并不一定能得出键值对相等。补充说一句：“两个不同的键值对，哈希值相等”，这就是哈希冲突。） 若两个对象hashCode（）返回不同int数，则equals（Object obj）一定返回false。 同一对象在执行期间若已经存储在集合中，则不能修改影响hashCode值的相关信息，否则会导致内存泄露问题。 一般一个类的对象如果会存储在HashTable，HashSet,HashMap等散列存储结构中，那么重写equals后最好也重写hashCode，否则会导致存储数据的不唯一性（存储了两个equals相等的数据）。而如果确定不会存储在这些散列结构中，则可以不重写hashCode。 我们注意到，除了 TreeMap，LinkedHashMap 也可以保证某种顺序，它们的 区别 如下： LinkedHashMap：提供的遍历顺序符合插入顺序，是通过为 HashEntry 维护一个双向链表实现的。 TreeMap：顺序由键的顺序决定，依赖于 Comparator。 HashMap 多线程操作导致死循环问题在多线程下，进行 put 操作会导致 HashMap 死循环，原因在于 HashMap 的扩容 resize()方法。由于扩容是新建一个数组，复制原数据到数组。由于数组下标挂有链表，所以需要复制链表，但是多线程操作有可能导致环形链表。jdk1.8已经解决了死循环的问题。 HashMap 源码详细分析(JDK1.8)HashMap 的内部结构如下： Java8以后，数组+链表+红黑树。。O(n)-&gt;O(logn) 解决哈希冲突的常用方法： 开放地址法：出现冲突时，以当前哈希值为基础，产生另一个哈希值。开放定址法为减少冲突，要求装填因子α较小，故当结点规模较大时会浪费很多空间。而拉链法中可取α≥1，且结点较大时，拉链法中增加的指针域可忽略不计，因此节省空间； 再哈希法：同时构造多个不同的哈希函数，发生冲突就换一个哈希方法。 链地址法：将哈希地址相同的元素放在一个链表中，然后把这个链表的表头放在哈希表的对应位置。拉链法处理冲突简单，且无堆积现象，即非同义词决不会发生冲突，因此平均查找长度较短；由于拉链法中各链表上的结点空间是动态申请的，故它更适合于造表前无法确定表长的情况； 拉链法的缺点：指针需要额外的空间，故当结点规模较小时，开放定址法较为节省空间，而若将节省的指针空间用来扩大散列表的规模，可使装填因子变小，这又减少了开放定址法中的冲突，从而提高平均查找速度。 建立公共溢出区：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。 在用拉链法构造的散列表中，删除结点的操作易于实现。只要简单地删去链表上相应的结点即可。而对开放地址法构造的散列表，删除结点不能简单地将被删结 点的空间置为空，否则将截断在它之后填人散列表的同义词结点的查找路径。这是因为各种开放地址法中，空地址单元（即开放地址）都是查找失败的条件。因此在用开放地址法处理冲突的散列表上执行删除操作，只能在被删结点上做删除标记，而不能真正删除结点。 构造方法构造方法分析HashMap 的构造方法不多，只有四个。HashMap 构造方法做的事情比较简单，一般都是初始化一些重要变量，比如 loadFactor 和 threshold。而底层的数据结构则是延迟到插入键值对时再进行初始化。HashMap 相关构造方法如下： 123456789101112131415161718192021222324252627282930/** 构造方法 1 */public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;/** 构造方法 2 */public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;/** 构造方法 3 */public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125;/** 构造方法 4 */public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125; 上面4个构造方法中，大家平时用的最多的应该是第一个了。第一个构造方法很简单，仅将 loadFactor 变量设为默认值。构造方法2调用了构造方法3，而构造方法3仍然只是设置了一些变量。构造方法4则是将另一个 Map 中的映射拷贝一份到自己的存储结构中来，这个方法不是很常用。 上面就是对构造方法简单的介绍，构造方法本身并没什么太多东西，所以就不说了。接下来说说构造方法所初始化的几个的变量。 初始容量、负载因子、阈值我们在一般情况下，都会使用无参构造方法创建 HashMap。但当我们对时间和空间复杂度有要求的时候，使用默认值有时可能达不到我们的要求，这个时候我们就需要手动调参。在 HashMap 构造方法中，可供我们调整的参数有两个，一个是初始容量 initialCapacity，另一个负载因子 loadFactor。通过这两个设定这两个参数，可以进一步影响阈值大小。但初始阈值 threshold 仅由 initialCapacity 经过移位操作计算得出。他们的作用分别如下： 名称 用途 initialCapacity HashMap 初始容量 loadFactor 负载因子 threshold 当前 HashMap 所能容纳键值对数量的最大值，超过这个值，则需扩容 相关代码如下： 12345678910/** The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;/** The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f;final float loadFactor;/** The next size value at which to resize (capacity * load factor). */int threshold; 如果大家去看源码，会发现 HashMap 中没有定义 initialCapacity 这个变量。这个也并不难理解，从参数名上可看出，这个变量表示一个初始容量，只是构造方法中用一次，没必要定义一个变量保存。但如果大家仔细看上面 HashMap 的构造方法，会发现存储键值对的数据结构并不是在构造方法里初始化的。这就有个疑问了，既然叫初始容量，但最终并没有用与初始化数据结构，那传这个参数还有什么用呢？这个问题我先不解释，给大家留个悬念，后面会说明。 默认情况下，HashMap 初始容量是16，负载因子为 0.75。这里并没有默认阈值，原因是阈值可由容量乘上负载因子计算而来（注释中有说明），即threshold = capacity * loadFactor。但当你仔细看构造方法3时，会发现阈值并不是由上面公式计算而来，而是通过一个方法算出来的。这是不是可以说明 threshold 变量的注释有误呢？还是仅这里进行了特殊处理，其他地方遵循计算公式呢？关于这个疑问，这里也先不说明，后面在分析扩容方法时，再来解释这个问题。接下来，我们来看看初始化 threshold 的方法长什么样的的，源码如下： 123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 上面的代码长的有点不太好看，反正我第一次看的时候不明白它想干啥。不过后来在纸上画画，知道了它的用途。总结起来就一句话：找到大于或等于 cap 的最小2的幂。至于为啥要这样，后面再解释。我们先来看看 tableSizeFor 方法的图解： 上面是 tableSizeFor 方法的计算过程图，这里cap = 536,870,913 = 2&lt;sup&gt;29&lt;/sup&gt; + 1，多次计算后，算出n + 1 = 1,073,741,824 = 2&lt;sup&gt;30&lt;/sup&gt;。通过图解应该可以比较容易理解这个方法的用途，这里就不多说了。 说完了初始阈值的计算过程，再来说说负载因子（loadFactor）。对于 HashMap 来说，负载因子是一个很重要的参数，该参数反应了 HashMap 桶数组的使用情况（假设键值对节点均匀分布在桶数组中）。通过调节负载因子，可使 HashMap 时间和空间复杂度上有不同的表现。当我们调低负载因子时，HashMap 所能容纳的键值对数量变少。扩容时，重新将键值对存储新的桶数组里，键的键之间产生的碰撞会下降，链表长度变短。此时，HashMap 的增删改查等操作的效率将会变高，这里是典型的拿空间换时间。相反，如果增加负载因子（负载因子可以大于1），HashMap 所能容纳的键值对数量变多，空间利用率高，但碰撞率也高。这意味着链表长度变长，效率也随之降低，这种情况是拿时间换空间。至于负载因子怎么调节，这个看使用场景了。一般情况下，我们用默认值就可以了。 查找HashMap 的查找操作比较简单，查找步骤与原理篇介绍一致，即先定位键值对所在的桶的位置，然后再对链表或红黑树进行查找。通过这两步即可完成查找，该操作相关代码如下： 123456789101112131415161718192021222324252627282930public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // 1. 定位键值对所在桶的位置 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; // 2. 如果 first 是 TreeNode 类型，则调用黑红树查找方法 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 2. 对链表进行查找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 查找的核心逻辑是封装在 getNode 方法中的，getNode 方法源码我已经写了一些注释，应该不难看懂。我们先来看看查找过程的第一步 - 确定桶位置，其实现代码如下： 12// index = (n - 1) &amp; hashfirst = tab[(n - 1) &amp; hash] 这里通过(n - 1)&amp; hash即可算出桶的在桶数组中的位置，可能有的朋友不太明白这里为什么这么做，这里简单解释一下。HashMap 中桶数组的大小 length 总是2的幂，此时，(n - 1) &amp; hash 等价于对 length 取余。但取余的计算效率没有位运算高，所以(n - 1) &amp; hash也是一个小的优化。举个例子说明一下吧，假设 hash = 185，n = 16。计算过程示意图如下： 上面的计算并不复杂，这里就不多说了。 在上面源码中，除了查找相关逻辑，还有一个计算 hash 的方法。这个方法源码如下： 1234567/** * 计算键的 hash 值 */static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 看这个方法的逻辑好像是通过位运算重新计算 hash，那么这里为什么要这样做呢？为什么不直接用键的 hashCode 方法产生的 hash 呢？ 这样做有两个好处，我来简单解释一下。我们再看一下上面求余的计算图，图中的 hash 是由键的 hashCode 产生。计算余数时，由于 n 比较小，hash 只有低4位参与了计算，高位的计算可以认为是无效的。这样导致了计算结果只与低位信息有关，高位数据没发挥作用。为了处理这个缺陷，我们可以上图中的 hash 高4位数据与低4位数据进行异或运算，即 hash ^ (hash &gt;&gt;&gt; 4)。通过这种方式，让高位数据与低位数据进行异或，以此加大低位信息的随机性，变相的让高位数据参与到计算中。此时的计算过程如下： 在 Java 中，hashCode 方法产生的 hash 是 int 类型，32 位宽。前16位为高位，后16位为低位，所以要右移16位。 上面所说的是重新计算 hash 的一个好处，除此之外，重新计算 hash 的另一个好处是可以增加 hash 的复杂度。当我们覆写 hashCode 方法时，可能会写出分布性不佳的 hashCode 方法，进而导致 hash 的冲突率比较高。通过移位和异或运算，可以让 hash 变得更复杂，进而影响 hash 的分布性。这也就是为什么 HashMap 不直接使用键对象原始 hash 的原因了。 遍历和查找查找一样，遍历操作也是大家使用频率比较高的一个操作。对于 遍历 HashMap，我们一般都会用下面的方式： 123for(Object key : map.keySet()) &#123; // do something&#125; 或 123for(HashMap.Entry entry : map.entrySet()) &#123; // do something&#125; 从上面代码片段中可以看出，大家一般都是对 HashMap 的 key 集合或 Entry 集合进行遍历。上面代码片段中用 foreach 遍历 keySet 方法产生的集合，在编译时会转换成用迭代器遍历，等价于： 123456Set keys = map.keySet();Iterator ite = keys.iterator();while (ite.hasNext()) &#123; Object key = ite.next(); // do something&#125; 大家在遍历 HashMap 的过程中会发现，多次对 HashMap 进行遍历时，遍历结果顺序都是一致的。但这个顺序和插入的顺序一般都是不一致的。产生上述行为的原因是怎样的呢？大家想一下原因。我先把遍历相关的代码贴出来，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; if (ks == null) &#123; ks = new KeySet(); keySet = ks; &#125; return ks;&#125;/** * 键集合 */final class KeySet extends AbstractSet&lt;K&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; HashMap.this.clear(); &#125; public final Iterator&lt;K&gt; iterator() &#123; return new KeyIterator(); &#125; public final boolean contains(Object o) &#123; return containsKey(o); &#125; public final boolean remove(Object key) &#123; return removeNode(hash(key), key, null, false, true) != null; &#125; // 省略部分代码&#125;/** * 键迭代器 */final class KeyIterator extends HashIterator implements Iterator&lt;K&gt; &#123; public final K next() &#123; return nextNode().key; &#125;&#125;abstract class HashIterator &#123; Node&lt;K,V&gt; next; // next entry to return Node&lt;K,V&gt; current; // current entry int expectedModCount; // for fast-fail int index; // current slot HashIterator() &#123; expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; index = 0; if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry // 寻找第一个包含链表节点引用的桶 do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; final Node&lt;K,V&gt; nextNode() &#123; Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123; // 寻找下一个包含链表节点引用的桶 do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; return e; &#125; //省略部分代码&#125; 如上面的源码，遍历所有的键时，首先要获取键集合KeySet对象，然后再通过 KeySet 的迭代器KeyIterator进行遍历。KeyIterator 类继承自HashIterator类，核心逻辑也封装在 HashIterator 类中。HashIterator 的逻辑并不复杂，在初始化时，HashIterator 先从桶数组中找到包含链表节点引用的桶。然后对这个桶指向的链表进行遍历。遍历完成后，再继续寻找下一个包含链表节点引用的桶，找到继续遍历。找不到，则结束遍历。 HashIterator 在初始化时，会先遍历桶数组，找到包含链表节点引用的桶，对应图中就是3号桶。随后由 nextNode 方法遍历该桶所指向的链表。遍历完3号桶后，nextNode 方法继续寻找下一个不为空的桶，对应图中的7号桶。之后流程和上面类似，直至遍历完最后一个桶。以上就是 HashIterator 的核心逻辑的流程，对应下图： 遍历上图的最终结果是 19 -&gt; 3 -&gt; 35 -&gt; 7 -&gt; 11 -&gt; 43 -&gt; 59，为了验证正确性，简单写点测试代码跑一下看看。测试代码如下： 12345678910111213141516171819202122/** * 应在 JDK 1.8 下测试，其他环境下不保证结果和上面一致 */public class HashMapTest &#123; @Test public void testTraversal() &#123; HashMap&lt;Integer, String&gt; map = new HashMap(16); map.put(7, &quot;&quot;); map.put(11, &quot;&quot;); map.put(43, &quot;&quot;); map.put(59, &quot;&quot;); map.put(19, &quot;&quot;); map.put(3, &quot;&quot;); map.put(35, &quot;&quot;); System.out.println(&quot;遍历结果：&quot;); for (Integer key : map.keySet()) &#123; System.out.print(key + &quot; -&gt; &quot;); &#125; &#125;&#125; 遍历结果如下： 插入插入逻辑分析通过前两节的分析，大家对 HashMap 低层的数据结构应该了然于心了。即使我不说，大家也应该能知道 HashMap 的插入流程是什么样的了。首先肯定是先定位要插入的键值对属于哪个桶，定位到桶后，再判断桶是否为空。如果为空，则将键值对存入即可。如果不为空，则需将键值对接在链表最后一个位置，或者更新键值对。这就是 HashMap 的插入流程，是不是觉得很简单。当然，大家先别高兴。这只是一个简化版的插入流程，真正的插入流程要复杂不少。首先 HashMap 是变长集合，所以需要考虑扩容的问题。其次，在 JDK 1.8 中，HashMap 引入了红黑树优化过长链表，这里还要考虑多长的链表需要进行优化，优化过程又是怎样的问题。引入这里两个问题后，大家会发现原本简单的操作，现在略显复杂了。在本节中，我将先分析插入操作的源码，扩容、树化（链表转为红黑树，下同）以及其他和树结构相关的操作，随后将在独立的两小结中进行分析。接下来，先来看一下插入操作的源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 初始化桶数组 table，table 被延迟到插入新数据时再进行初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 如果桶中不包含键值对节点引用，则将新键值对节点的引用存入桶中即可 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 如果键的值以及节点 hash 等于链表中的第一个键值对节点时，则将 e 指向该键值对 if (p.hash == hash &amp;&amp;((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果桶中的引用类型为 TreeNode，则调用红黑树的插入方法 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 对链表进行遍历，并统计链表长度 for (int binCount = 0; ; ++binCount) &#123; // 链表中不包含要插入的键值对节点时，则将该节点接在链表的最后 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 如果链表长度大于或等于树化阈值，则进行树化操作// -1 for 1st if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); break; &#125; // 条件为 true，表示当前链表包含要插入的键值对，终止遍历 if (e.hash == hash &amp;&amp;((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 判断要插入的键值对是否存在 HashMap 中 if (e != null) &#123; // existing mapping for key V oldValue = e.value; // onlyIfAbsent 表示是否仅在 oldValue 为 null 的情况下更新键值对的值 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 键值对数量超过阈值时，则进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 插入操作的入口方法是 put(K,V)，但核心逻辑在V putVal(int, K, V, boolean, boolean) 方法中。putVal 方法主要做了这么几件事情： 当桶数组 table 为空时，通过扩容的方式初始化 table 查找要插入的键值对是否已经存在，存在的话根据条件判断是否用新值替换旧值 如果不存在，则将键值对链入链表中，并根据链表长度决定是否将链表转为红黑树 判断键值对数量是否大于阈值，大于的话则进行扩容操作 以上就是 HashMap 插入的逻辑，并不是很复杂，这里就不多说了。接下来来分析一下扩容机制。 3.4.2 扩容机制在 Java 中，数组的长度是固定的，这意味着数组只能存储固定量的数据。但在开发的过程中，很多时候我们无法知道该建多大的数组合适。建小了不够用，建大了用不完，造成浪费。如果我们能实现一种变长的数组，并按需分配空间就好了。好在，我们不用自己实现变长数组，Java 集合框架已经实现了变长的数据结构。比如 ArrayList 和 HashMap。对于这类基于数组的变长数据结构，扩容是一个非常重要的操作。下面就来聊聊 HashMap 的扩容机制。 在详细分析之前，先来说一下扩容相关的背景知识： 在 HashMap 中，桶数组的长度均是2的幂，阈值大小为桶数组长度与负载因子的乘积。当 HashMap 中的键值对数量超过阈值时，进行扩容。 HashMap 的扩容机制与其他变长集合的套路不太一样，HashMap 按当前桶数组长度的2倍进行扩容，阈值也变为原来的2倍（如果计算过程中，阈值溢出归零，则按阈值公式重新计算）。扩容之后，要重新计算键值对的位置，并把它们移动到合适的位置上去。以上就是 HashMap 的扩容大致过程，接下来我们来看看具体的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; // 如果 table 不为空，表明已经初始化过了 if (oldCap &gt; 0) &#123; // 当 table 容量超过容量最大值，则不再扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 按旧容量和阈值的2倍计算新容量和阈值的大小 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold /* * 初始化时，将 threshold 的值赋值给 newCap， * HashMap 使用 threshold 变量暂时保存 initialCapacity 参数的值 */ newCap = oldThr; else &#123; // zero initial threshold signifies using defaults /* * 调用无参构造方法时，桶数组容量为默认容量， * 阈值为默认容量与默认负载因子乘积 */ newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // newThr 为 0 时，按阈值计算公式进行计算 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; // 创建新的桶数组，桶数组的初始化也是在这里完成的 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 如果旧的桶数组不为空，则遍历桶数组，并将键值对映射到新的桶数组中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 重新映射时，需要对红黑树进行拆分 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; // 遍历链表，并将链表节点按原顺序进行分组 do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 将分组后的链表映射到新桶中 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 上面的源码有点长，希望大家耐心看懂它的逻辑。上面的源码总共做了3件事，分别是： 计算新桶数组的容量 newCap 和新阈值 newThr 根据计算出的 newCap 创建新的桶数组，桶数组 table 也是在这里进行初始化的 将键值对节点重新映射到新的桶数组里。如果节点是 TreeNode 类型，则需要拆分红黑树。如果是普通节点，则节点按原顺序进行分组。 上面列的三点中，创建新的桶数组就一行代码，不用说了。接下来，来说说第一点和第三点，先说说 newCap 和 newThr 计算过程。该计算过程对应 resize 源码的第一和第二个条件分支，如下： 123456789101112// 第一个条件分支if ( oldCap &gt; 0) &#123; // 嵌套条件分支 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;...&#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) &#123;...&#125;&#125; else if (oldThr &gt; 0) &#123;...&#125;else &#123;...&#125;// 第二个条件分支if (newThr == 0) &#123;...&#125; 通过这两个条件分支对不同情况进行判断，进而算出不同的容量值和阈值。它们所覆盖的情况如下： 分支一： 条件 覆盖情况 备注 oldCap &gt; 0 桶数组 table 已经被初始化 oldThr &gt; 0 threshold &gt; 0，且桶数组未被初始化 调用 HashMap(int) 和 HashMap(int, float) 构造方法时会产生这种情况，此种情况下 newCap = oldThr，newThr 在第二个条件分支中算出 oldCap == 0 &amp;&amp; oldThr == 0 桶数组未被初始化，且 threshold 为 0 调用 HashMap() 构造方法会产生这种情况。 这里把oldThr &gt; 0情况单独拿出来说一下。在这种情况下，会将 oldThr 赋值给 newCap，等价于newCap = threshold = tableSizeFor(initialCapacity)。我们在初始化时传入的 initialCapacity 参数经过 threshold 中转最终赋值给了 newCap。这也就解答了前面提的一个疑问：initialCapacity 参数没有被保存下来，那么它怎么参与桶数组的初始化过程的呢？ 嵌套分支： 条件 覆盖情况 备注 oldCap &gt;= 230 桶数组容量大于或等于最大桶容量 230 这种情况下不再扩容 newCap &lt; 230 &amp;&amp; oldCap &gt; 16 新桶数组容量小于最大值，且旧桶数组容量大于 16 该种情况下新阈值 newThr = oldThr &lt;&lt; 1，移位可能会导致溢出 这里简单说明一下移位导致的溢出情况，当 loadFactor小数位为 0，整数位可被2整除且大于等于8时，在某次计算中就可能会导致 newThr 溢出归零。见下图： 分支二： 条件 覆盖情况 备注 newThr == 0 第一个条件分支未计算 newThr 或嵌套分支在计算过程中导致 newThr 溢出归零 说完 newCap 和 newThr 的计算过程，接下来再来分析一下键值对节点重新映射的过程。 在 JDK 1.8 中，重新映射节点需要考虑节点类型。对于树形节点，需先拆分红黑树再映射。对于链表类型节点，则需先对链表进行分组，然后再映射。需要的注意的是，分组后，组内节点相对位置保持不变。关于红黑树拆分的逻辑将会放在下一小节说明，先来看看链表是怎样进行分组映射的。 我们都知道往底层数据结构中插入节点时，一般都是先通过模运算计算桶位置，接着把节点放入桶中即可。事实上，我们可以把重新映射看做插入操作。在 JDK 1.7 中，也确实是这样做的。但在 JDK 1.8 中，则对这个过程进行了一定的优化，逻辑上要稍微复杂一些。在详细分析前，我们先来回顾一下 hash 求余的过程： 上图中，桶数组大小 n = 16，hash1 与 hash2 不相等。但因为只有后4位参与求余，所以结果相等。当桶数组扩容后，n 由16变成了32，对上面的 hash 值重新进行映射： 扩容后，参与模运算的位数由4位变为了5位。由于两个 hash 第5位的值是不一样，所以两个 hash 算出的结果也不一样。上面的计算过程并不难理解，继续往下分析。假设我们上图的桶数组进行扩容，扩容后容量 n = 16，重新映射过程如下: 依次遍历链表，并计算节点 hash &amp; oldCap 的值。如下图所示 如果值为0，将 loHead 和 loTail 指向这个节点。如果后面还有节点 hash &amp; oldCap 为0的话，则将节点链入 loHead 指向的链表中，并将 loTail 指向该节点。如果值为非0的话，则让 hiHead 和 hiTail 指向该节点。完成遍历后，可能会得到两条链表，此时就完成了链表分组： 最后再将这两条链接存放到相应的桶中，完成扩容。如下图： 从上图可以发现，重新映射后，两条链表中的节点顺序并未发生变化，还是保持了扩容前的顺序。以上就是 JDK 1.8 中 HashMap 扩容的代码讲解。另外再补充一下，JDK 1.8 版本下 HashMap 扩容效率要高于之前版本。如果大家看过 JDK 1.7 的源码会发现，JDK 1.7 为了防止因 hash 碰撞引发的拒绝服务攻击，在计算 hash 过程中引入随机种子。以增强 hash 的随机性，使得键值对均匀分布在桶数组中。在扩容过程中，相关方法会根据容量判断是否需要生成新的随机种子，并重新计算所有节点的 hash。而在 JDK 1.8 中，则通过引入红黑树替代了该种方式。从而避免了多次计算 hash 的操作，提高了扩容效率。 本小节的内容讲就先讲到这，接下来，来讲讲链表与红黑树相互转换的过程。 链表树化、红黑树链化与拆分JDK 1.8 对 HashMap 实现进行了改进。最大的改进莫过于在引入了红黑树处理频繁的碰撞，代码复杂度也随之上升。比如，以前只需实现一套针对链表操作的方法即可。而引入红黑树后，需要另外实现红黑树相关的操作。红黑树是一种自平衡的二叉查找树，本身就比较复杂。本篇文章中并不打算对红黑树展开介绍，本文仅会介绍链表树化需要注意的地方。红黑树详细的介绍参考 - 红黑树详细分析。 在展开说明之前，先把树化的相关代码贴出来，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051static final int TREEIFY_THRESHOLD = 8;/** * 当桶数组容量小于该值时，优先进行扩容，而不是树化 */static final int MIN_TREEIFY_CAPACITY = 64;static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125;&#125;/** * 将普通节点链表转换成树形节点链表 */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; // 桶数组容量小于 MIN_TREEIFY_CAPACITY，优先进行扩容而不是树化 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; // hd 为头节点（head），tl 为尾节点（tail） TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; // 将普通节点替换成树形节点 TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); // 将普通链表转成由树形节点链表 if ((tab[index] = hd) != null) // 将树形链表转换成红黑树 hd.treeify(tab); &#125;&#125;TreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; return new TreeNode&lt;&gt;(p.hash, p.key, p.value, next);&#125; 在扩容过程中，树化要满足两个条件： 链表长度大于等于 TREEIFY_THRESHOLD 桶数组容量大于等于 MIN_TREEIFY_CAPACITY 第一个条件比较好理解，这里就不说了。这里来说说加入第二个条件的原因，个人觉得原因如下： 当桶数组容量比较小时，键值对节点 hash 的碰撞率可能会比较高，进而导致链表长度较长。这个时候应该优先扩容，而不是立马树化。毕竟高碰撞率是因为桶数组容量较小引起的，这个是主因。容量小时，优先扩容可以避免一些列的不必要的树化过程。同时，桶容量较小时，扩容会比较频繁，扩容时需要拆分红黑树并重新映射。所以在桶容量比较小的情况下，将长链表转成红黑树是一件吃力不讨好的事。 回到上面的源码中，我们继续看一下 treeifyBin 方法。该方法主要的作用是将普通链表转成为由 TreeNode 型节点组成的链表，并在最后调用 treeify 是将该链表转为红黑树。TreeNode 继承自 Node 类，所以 TreeNode 仍然包含 next 引用，原链表的节点顺序最终通过 next 引用被保存下来。我们假设树化前，链表结构如下： HashMap 在设计之初，并没有考虑到以后会引入红黑树进行优化。所以并没有像 TreeMap 那样，要求键类实现 comparable 接口或提供相应的比较器。但由于树化过程需要比较两个键对象的大小，在键类没有实现 comparable 接口的情况下，怎么比较键与键之间的大小了就成了一个棘手的问题。为了解决这个问题，HashMap 是做了三步处理，确保可以比较出两个键的大小，如下： 比较键与键之间 hash 的大小，如果 hash 相同，继续往下比较 检测键类是否实现了 Comparable 接口，如果实现调用 compareTo 方法进行比较 如果仍未比较出大小，就需要进行仲裁了，仲裁方法为 tieBreakOrder（大家自己看源码吧） tie break 是网球术语，可以理解为加时赛的意思，起这个名字还是挺有意思的。 通过上面三次比较，最终就可以比较出孰大孰小。比较出大小后就可以构造红黑树了，最终构造出的红黑树如下： 橙色的箭头表示 TreeNode 的 next 引用。由于空间有限，prev 引用未画出。可以看出，链表转成红黑树后，原链表的顺序仍然会被引用仍被保留了（红黑树的根节点会被移动到链表的第一位），我们仍然可以按遍历链表的方式去遍历上面的红黑树。这样的结构为后面红黑树的切分以及红黑树转成链表做好了铺垫，我们继续往下分析。 红黑树拆分 扩容后，普通节点需要重新映射，红黑树节点也不例外。按照一般的思路，我们可以先把红黑树转成链表，之后再重新映射链表即可。这种处理方式是大家比较容易想到的，但这样做会损失一定的效率。如上节所说，在将普通链表转成红黑树时，HashMap 通过两个额外的引用 next 和 prev 保留了原链表的节点顺序。这样再对红黑树进行重新映射时，完全可以按照映射链表的方式进行。这样就避免了将红黑树转成链表后再进行映射，无形中提高了效率。 以上就是红黑树拆分的逻辑，下面看一下具体实现吧： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 红黑树转链表阈值static final int UNTREEIFY_THRESHOLD = 6;final void split(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int index, int bit) &#123; TreeNode&lt;K,V&gt; b = this; // Relink into lo and hi lists, preserving order TreeNode&lt;K,V&gt; loHead = null, loTail = null; TreeNode&lt;K,V&gt; hiHead = null, hiTail = null; int lc = 0, hc = 0; /* * 红黑树节点仍然保留了 next 引用，故仍可以按链表方式遍历红黑树。 * 下面的循环是对红黑树节点进行分组，与上面类似 */ for (TreeNode&lt;K,V&gt; e = b, next; e != null; e = next) &#123; next = (TreeNode&lt;K,V&gt;)e.next; e.next = null; if ((e.hash &amp; bit) == 0) &#123; if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; &#125; else &#123; if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; &#125; &#125; if (loHead != null) &#123; // 如果 loHead 不为空，且链表长度小于等于 6，则将红黑树转成链表 if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else &#123; tab[index] = loHead; /* * hiHead == null 时，表明扩容后， * 所有节点仍在原位置，树结构不变，无需重新树化 */ if (hiHead != null) loHead.treeify(tab); &#125; &#125; // 与上面类似 if (hiHead != null) &#123; if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else &#123; tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); &#125; &#125;&#125; 从源码上可以看得出，重新映射红黑树的逻辑和重新映射链表的逻辑基本一致。不同的地方在于，重新映射后，会将红黑树拆分成两条由 TreeNode 组成的链表。如果链表长度小于 UNTREEIFY_THRESHOLD，则将链表转换成普通链表。否则根据条件重新将 TreeNode 链表树化。举个例子说明一下，假设扩容后，重新映射上图的红黑树，映射结果如下： 红黑树链化 前面说过，红黑树中仍然保留了原链表节点顺序。有了这个前提，再将红黑树转成链表就简单多了，仅需将 TreeNode 链表转成 Node 类型的链表即可。相关代码如下： 12345678910111213141516171819final Node&lt;K,V&gt; untreeify(HashMap&lt;K,V&gt; map) &#123; Node&lt;K,V&gt; hd = null, tl = null; // 遍历 TreeNode 链表，并用 Node 替换 for (Node&lt;K,V&gt; q = this; q != null; q = q.next) &#123; // 替换节点类型 Node&lt;K,V&gt; p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; &#125; return hd;&#125;Node&lt;K,V&gt; replacementNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(p.hash, p.key, p.value, next);&#125; 上面的代码并不复杂，不难理解，这里就不多说了。到此扩容相关内容就说完了，不知道大家理解没。 删除如果大家坚持看完了前面的内容，到本节就可以轻松一下。当然，前提是不去看红黑树的删除操作。不过红黑树并非本文讲解重点，本节中也不会介绍红黑树相关内容，所以大家不用担心。 HashMap 的删除操作并不复杂，仅需三个步骤即可完成。第一步是定位桶位置，第二步遍历链表并找到键值相等的节点，第三步删除节点。相关源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value,boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; // 1. 定位桶位置 (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; // 如果键的值与链表第一个节点相等，则将 node 指向该节点 if (p.hash == hash &amp;&amp;((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; // 如果是 TreeNode 类型，调用红黑树的查找逻辑定位待删除节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; // 2. 遍历链表，找到待删除节点 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; // 3. 删除节点，并修复链表或红黑树 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; 删除操作本身并不复杂，有了前面的基础，理解起来也就不难了，这里就不多说了。 被 transient 所修饰 table 变量如果大家细心阅读 HashMap 的源码，会发现桶数组 table 被申明为 transient。transient 表示易变的意思，在 Java 中，被该关键字修饰的变量不会被默认的序列化机制序列化。我们再回到源码中，考虑一个问题：桶数组 table 是 HashMap 底层重要的数据结构，不序列化的话，别人还怎么还原呢？ 这里简单说明一下吧，HashMap 并没有使用默认的序列化机制，而是通过实现readObject/writeObject两个方法自定义了序列化的内容。这样做是有原因的，试问一句，HashMap 中存储的内容是什么？不用说，大家也知道是键值对。所以只要我们把键值对序列化了，我们就可以根据键值对数据重建 HashMap。有的朋友可能会想，序列化 table 不是可以一步到位，后面直接还原不就行了吗？这样一想，倒也是合理。但序列化 talbe 存在着两个问题： table 多数情况下是无法被存满的，序列化未使用的部分，浪费空间 同一个键值对在不同 JVM 下，所处的桶位置可能是不同的，在不同的 JVM 下反序列化 table 可能会发生错误。 以上两个问题中，第一个问题比较好理解，第二个问题解释一下。HashMap 的get/put/remove等方法第一步就是根据 hash 找到键所在的桶位置，但如果键没有覆写 hashCode 方法，计算 hash 时最终调用 Object 中的 hashCode 方法。但 Object 中的 hashCode 方法是 native 型的，不同的 JVM 下，可能会有不同的实现，产生的 hash 可能也是不一样的。也就是说同一个键在不同平台下可能会产生不同的 hash，此时再对在同一个 table 继续操作，就会出现问题。 HashSetHashSet 底层就是基于 HashMap 实现的。add 的元素会被放在 HashMap 的放 key 的地方，HashMap 放 value 的地方放了一个 private static final Object PRESENT = new Object();。 除了 clone() 方法、writeObject() 方法、readObject() 方法是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法实现的。 TreeSetTreeSet 的底层实现是一颗红黑树，那么什么是红黑树呢？ 红黑树是一颗自平衡的二叉查找树，它从根节点到叶子节点的最长路径不会超过最短路径的 2 倍。除此之外，它还具有如下 5 个特点： 节点分为红色或黑色。 根节点一定是黑色的。 每个叶子节点一定是黑色的 null 节点。 每个红色节点的两个子节点都是黑色，即从每个叶子到根的所有路径上不能有两个连续的红色节点（但黑节点的子节点可以还是黑节点，就红节点事多……）。 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 红黑树在插入和删除节点的时候，可能破坏以上 5 条规则，一旦规则被破坏，红黑树主要依靠以下 3 个操作来恢复： 变色 逆时针旋转 顺时针旋转 红黑树的插入与删除详见：教你透彻了解红黑树。 ConcurrentHashMap特点 ConcorrentHashMap 实现了 ConcorrentMap 接口，能在并发环境实现更高的吞吐量，而在单线程环境中只损失很小的性能； 采用分段锁，使得任意数量的读取线程可以并发地访问 Map，一定数量的写入线程可以并发地修改 Map； 不会抛出 ConcorrentModificationException，它返回迭代器具有“弱一致性”，即可以容忍并发修改，但不保证将修改操作反映给容器； size() 的返回结果可能已经过期，只是一个估计值，不过 size() 和 isEmpty() 方法在并发环境中用的也不多； 提供了许多原子的复合操作： V putIfAbsent(K key, V value);：K 没有相应映射才插入 boolean remove(K key, V value);：K 被映射到 V 才移除 boolean replace(K key, V oldValue, V newValue);：K 被映射到 oldValue 时才替换为 newValue ConcurrentHashMap 内部结构： JDK1.7:在构造的时候，Segment 的数量由所谓的 concurrentcyLevel 决定，默认是 16； Segment 是基于 ReentrantLock 的扩展实现的，在 put 的时候，会对修改的区域加锁。 JDK1.8的实现已经摒弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap，虽然在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本 锁分段实现原理不同线程在同一数据的不同部分上不会互相干扰，例如，ConcurrentHashMap 支持 16 个并发的写入器，是用 16 个锁来实现的。它的实现原理如下： 使用了一个包含 16 个锁的数组，每个锁保护所有散列桶的 1/16，其中第 N 个散列桶由第（N % 16）个锁来保护； 这大约能把对于锁的请求减少到原来的 1/16，也是 ConcurrentHashMap 最多能支持 16 个线程同时写入的原因； 对于 ConcurrentHashMap 的 size() 操作，为了避免枚举每个元素，ConcurrentHashMap 为每个分段都维护了一个独立的计数，并通过每个分段的锁来维护这个值，而不是维护一个全局计数； 代码示例： 123456789101112131415161718192021222324252627282930313233343536public class StripedMap &#123; // 同步策略：buckets[n]由locks[n % N_LOCKS]保护 private static final int N_LOCKS = 16; private final Node[] buckets; private final Object[] locks; // N_LOCKS个锁 private static class Node &#123; Node next; Object key; Object value; &#125; public StripedMap(int numBuckets) &#123; buckets = new Node[numBuckets]; locks = new Object[N_LOCKS]; for (int i = 0; i &lt; N_LOCKS; i++) locks[i] = new Object(); &#125; private final int hash(Object key) &#123; return Math.abs(key.hashCode() % buckets.length); &#125; public Object get(Object key) &#123; int hash = hash(key); synchronized (locks[hash % N_LOCKS]) &#123; // 分段加锁 for (Node m = buckets[hash]; m != null; m = m.next) if (m.key.equals(key)) return m.value; &#125; return null; &#125; public void clear() &#123; for (int i = 0; i &lt; buckets.length; i++) &#123; synchronized (locks[i % N_LOCKS]) &#123; // 分段加锁 buckets[i] = null; &#125; &#125; &#125;&#125; 注意 关于 put 操作： 是否需要扩容 在插入元素前判断是否需要扩容， 比 HashMap 的插入元素后判断是否需要扩容要好，因为可以插入元素后，Map 扩容，之后不再有新的元素插入，Map就进行了一次无效的扩容 如何扩容 先创建一个容量是原来的2倍的数组，然后将原数组中的元素进行再散列后插入新数组中 为了高效，ConcurrentHashMap 只对某个 segment 进行扩容 关于 size 操作： 存在问题：如果不进行同步，只是计算所有 Segment 维护区域的 size 总和，那么在计算的过程中，可能有新的元素 put 进来，导致结果不准确，但如果对所有的 Segment 加锁，代价又过高。 解决方法：重试机制，通过获取两次来试图获取 size 的可靠值，如果没有监控到发生变化，即 Segment.modCount 没有变化，就直接返回，否则获取锁进行操作。 JDK 1.8 的改变ConcurrentHashMap 取消了 Segment 分段锁，采用 CAS 和 synchronized 来保证并发安全。数据结构跟 HashMap1.8 的结构类似，数组 + 链表 / 红黑二叉树。 synchronized 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，效率又提升 N 倍。 ConcurrentHashMap和HashTable的区别 底层数据结构： JDK1.7的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟 HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： ConcurrentHashMap（分段锁）：在JDK1.7的时候， 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本； Hashtable(同一把锁) ：使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低 LinkedHashMap简单的来说，LinkedHashMap 就是在 HashMap 的基础上加了一条双向链表用来维护 LinkedHashMap 中元素的插入顺序。 LinkedHashMap extends HashMap 且 LinkedHashMap.Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt;，它们的结构图如下： LinkedHashMap.Entry&lt;K,V&gt; 的结构： LinkedHashMap 的结构： 迭代器 Iterator对容器进行迭代操作时，我们要考虑它是不是会被其他的线程修改，如果是我们自己写代码，可以考虑通过如下方式对容器的迭代操作加锁： 1234synchronized (vector) &#123; for (int i = 0; i &lt; vector.size(); i++) doSomething(vector.get(i));&#125; 不过 Java 自己的同步容器类并没有考虑并发修改的问题，它主要采用了一种 快速失败 (fail-fast) 的方法，即一旦容器被其他线程修改，它就会抛出异常，例如 Vector 类，它的内部实现是这样的： 12345678910synchronized (Vector.this) &#123; // 类名.this：在内部类中，要用到外围类的 this 对象，使用“外围类名.this” checkForComodification(); // 在进行 next 和 remove 操作前，会先检查以下容器是否被修改 ...&#125;/* checkForComodification()方法 */final void checkForComodification() &#123; if (modCount != expectedModCount) // 在 Itr 的成员变量中有一个：int exceptedModCount = modCount; throw new ConcurrentModificationException(); // 如果容器被修改了，modCount 会变&#125; 因此，我们在调用 Vector 的如下方法时，要小心，因为它们会隐式的调用 Vector 的迭代操作。 toString hashCode equals containsAll removeAll retainAll Iterator 的 安全失败 (fail-safe) 是基于对底层集合做拷贝实现的，因此，它不受源集合上修改的影响。 快速失败（fail—fast） 在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的结构进行了修改（增加、删除），则会抛出Concurrent Modification Exception。 原理：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果结构发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。 注意：这里异常的抛出条件是检测到 modCount！=expectedmodCount 这个条件。如果集合发生变化时修改modCount值刚好又设置为了expectedmodCount值，则异常不会抛出。因此，不能依赖于这个异常是否抛出而进行并发操作的编程，这个异常只建议用于检测并发修改的bug。 场景：java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改）。 安全失败（fail—safe） 采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。 由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发Concurrent Modification Exception。 基于拷贝内容的优点是避免了Concurrent Modification Exception，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。 java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改。 Enumeration接口Enumeration 接口的作用与 Iterator 接口类似，但只提供了遍历 Vector 和 Hashtable 类型集合元素的功能，不支持元素的移除操作。 例如：遍历Vector v中的元素： 12for (Enumeration&lt;E&gt; e = v.elements();e.hasMoreElements();)System.out.println(e.nextElement()); Iterator 接口添加了一个可选的移除操作，并使用较短的方法名。新的实现应该优先考虑使用 Iterator 接口而不是 Enumeration 接口。 区别：Enumeration速度是Iterator的2倍，同时占用更少的内存。但是，Iterator远远比Enumeration安全，因为其他线程不能够修改正在被iterator遍历的集合里面的对象。同时，Iterator允许调用者删除底层集合里面的元素，这对Enumeration来说是不可能的。 Iterator 接口的用法： 12345Iterator it = list.iterator();while(it.hasNext())&#123; System.out.println(it.next());&#125; 容器中的设计模式迭代器模式：Collection 实现了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历 Collection 中的元素。 适配器模式：java.util.Arrays#asList() 可以把数组类型转换为 List 类型。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot实现基于Token的web后台认证机制]]></title>
    <url>%2FSpringBoot%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EToken%E7%9A%84web%E5%90%8E%E5%8F%B0%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[几种常用的认证机制HTTP Basic Auth、OAuth、Cookie Auth、Token Auth HTTP Basic AuthHTTP Basic Auth简单点说明就是每次请求API时都提供用户的username和password，简言之，Basic Auth是配合RESTful API 使用的最简单的认证方式，只需提供用户名密码即可，但由于有把用户名密码暴露给第三方客户端的风险，在生产环境下被使用的越来越少。因此，在开发对外开放的RESTful API时，尽量避免采用HTTP Basic Auth OAuth2.0OAuth（开放授权）是一个开放的授权标准，允许用户让第三方应用访问该用户在某一web服务上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和密码提供给第三方应用。 Cookie AuthCookie认证机制就是为一次请求认证在服务端创建一个Session对象，同时在客户端的浏览器端创建了一个Cookie对象；通过客户端带上来Cookie对象来与服务器端的session对象匹配来实现状态管理的。默认的，当我们关闭浏览器的时候，cookie会被删除。但可以通过修改cookie 的expire time使cookie在一定时间内有效； Token AuthToken机制相对于Cookie机制又有什么好处呢？ 支持跨域访问: Cookie是不允许垮域访问的，这一点对Token机制是不存在的，前提是传输的用户认证信息通过HTTP头传输. 无状态(也称：服务端可扩展行):Token机制在服务端不需要存储session信息，因为Token 自身包含了所有登录用户的信息，只需要在客户端的cookie或本地介质存储状态信息. 更适用CDN: 可以通过内容分发网络请求你服务端的所有资料（如：javascript，HTML,图片等），而你的服务端只要提供API即可. 去耦: 不需要绑定到一个特定的身份验证方案。Token可以在任何地方生成，只要在你的API被调用的时候，你可以进行Token生成调用即可. 更适用于移动应用: 当你的客户端是一个原生平台（iOS, Android，Windows 8等）时，Cookie是不被支持的（你需要通过Cookie容器进行处理），这时采用Token认证机制就会简单得多。 CSRF:因为不再依赖于Cookie，所以你就不需要考虑对CSRF（跨站请求伪造）的防范。 性能: 一次网络往返时间（通过数据库查询session信息）总比做一次HMACSHA256计算 的Token验证和解析要费时得多. 不需要为登录页面做特殊处理: 如果你使用Protractor 做功能测试的时候，不再需要为登录页面做特殊处理. 基于标准化:你的API可以采用标准化的 JSON Web Token (JWT). 这个标准已经存在多个后端库（.NET, Ruby, Java,Python, PHP）和多家公司的支持（如：Firebase,Google, Microsoft）. 基于Token的身份验证流程如下 客户端使用用户名跟密码请求登录 服务端收到请求，去验证用户名与密码 验证成功后，服务端会签发一个 Token，再把这个 Token 发送给客户端 客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里或者 Local Storage 里 客户端每次向服务端请求资源的时候需要带着服务端签发的 Token 服务端收到请求，然后去验证客户端请求里面带着的 Token，如果验证成功，就向客户端返回请求的数据 Spring Boot 实现实现见UserService，LoginController，PassportInterceptor12345678public class LoginTicket&#123; private int id; private int userId; private Date expired; private int status;//0有效 private String ticket;&#125; 用户先去请求注册或者是登陆，然后服务器去验证他的用户名和密码 验证成功后userservice会生成一个Token，这里是ticket，客户端收到ticket之后呢会把ticket存在Cookie中，登录成功之后会有一个与当前用户对应的ticket 每次访问服务器资源的时候需要带着这个ticket，然后怎么判断是否有呢？就要用拦截器来实现过滤，用拦截器去判断这个ticket当前的状态是什么样的？有没有过期？身份状态是不是有效的？然后根据这个来判断应该赋予什么样的权限？当验证成功之后就把ticket对应的用户的通过下面一段发送给freemaker的上下文，实现页面的正常的渲染 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.zhaole.interceptor;import com.zhaole.dao.LoginTicketDAO;import com.zhaole.dao.UserDAO;import com.zhaole.model.HostHolder;import com.zhaole.model.LoginTicket;import com.zhaole.model.User;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.Date;/** * 拦截器 * @ 用来判断用户的 *1. 当preHandle方法返回false时，从当前拦截器往回执行所有拦截器的afterCompletion方法，再退出拦截器链。也就是说，请求不继续往下传了，直接沿着来的链往回跑。 2.当preHandle方法全为true时，执行下一个拦截器,直到所有拦截器执行完。再运行被拦截的Controller。然后进入拦截器链，运 行所有拦截器的postHandle方法,完后从最后一个拦截器往回执行所有拦截器的afterCompletion方法. */@Componentpublic class PasswordInterceptor implements HandlerInterceptor&#123; @Autowired private LoginTicketDAO loginTicketDAO; @Autowired private UserDAO userDAO; @Autowired private HostHolder hostHolder; @Override public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception &#123; String ticket = null; if(httpServletRequest.getCookies()!=null) &#123; for(Cookie cookie:httpServletRequest.getCookies()) &#123; if(cookie.getName().equals("ticket")) &#123; ticket = cookie.getValue(); break; &#125; &#125; &#125; if(ticket!=null)//说明cookie不空，且name是ticket &#123; //去数据库里把ticket找出来看看是否有效 LoginTicket loginTicket = loginTicketDAO.selectByTicket(ticket); if(loginTicket==null || loginTicket.getExpired().before(new Date()) || loginTicket.getStatus()!=0) &#123; return true; &#125; //有效的话，查这个ticket对应的用户，把这个用户设置到threadlocal里。 User user = userDAO.selectById(loginTicket.getUserId()); hostHolder.setUser(user); &#125; return true; &#125; @Override public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception &#123; if (modelAndView != null &amp;&amp; hostHolder.getUser() != null) &#123; modelAndView.addObject("user", hostHolder.getUser()); &#125; &#125; @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception &#123; hostHolder.clear(); &#125;&#125; 当用户登出的时候就把ticket的身份状态置位为无效状态即可]]></content>
      <tags>
        <tag>Spring Boot</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你如何迅速秒杀掉：99%的海量数据处理面试题]]></title>
    <url>%2F%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E8%BF%85%E9%80%9F%E7%A7%92%E6%9D%80%E6%8E%89%EF%BC%9A99-%E7%9A%84%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[原文地址 分而治之/hash映射 + hash统计 + 堆/快速/归并排序； 双层桶划分 Bloom filter/Bitmap； Trie树/数据库/倒排索引； 外排序； 分布式处理之Hadoop/Mapreduce。 一、从set/map谈到hashtable/hash_map/hash_set稍后本文第二部分中将多次提到hash_map/hash_set，下面稍稍介绍下这些容器，以作为基础准备。一般来说，STL容器分两种， 序列式容器(vector/list/deque/stack/queue/heap)， 关联式容器。关联式容器又分为set(集合)和map(映射表)两大类，以及这两大类的衍生体multiset(多键集合)和multimap(多键映射表)，这些容器均以RB-tree完成。此外，还有第3类关联式容器，如hashtable(散列表)，以及以hashtable为底层机制完成的hash_set(散列集合)/hash_map(散列映射表)/hash_multiset(散列多键集合)/hash_multimap(散列多键映射表)。也就是说，set/map/multiset/multimap都内含一个RB-tree，而hash_set/hash_map/hash_multiset/hash_multimap都内含一个hashtable。 所谓关联式容器，类似关联式数据库，每笔数据或每个元素都有一个键值(key)和一个实值(value)，即所谓的Key-Value(键-值对)。当元素被插入到关联式容器中时，容器内部结构(RB-tree/hashtable)便依照其键值大小，以某种特定规则将这个元素放置于适当位置。 包括在非关联式数据库中，比如，在MongoDB内，文档(document)是最基本的数据组织形式，每个文档也是以Key-Value（键-值对）的方式组织起来。一个文档可以有多个Key-Value组合，每个Value可以是不同的类型，比如String、Integer、List等等。123&#123; &quot;name&quot; : &quot;July&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 23 &#125; set/map/multiset/multimap set，同map一样，所有元素都会根据元素的键值自动被排序，因为set/map两者的所有各种操作，都只是转而调用RB-tree的操作行为，不过，值得注意的是，两者都不允许两个元素有相同的键值。 不同的是：set的元素不像map那样可以同时拥有实值(value)和键值(key)，set元素的键值就是实值，实值就是键值，而map的所有元素都是pair，同时拥有实值(value)和键值(key)，pair的第一个元素被视为键值，第二个元素被视为实值。 至于multiset/multimap，他们的特性及用法和set/map完全相同，唯一的差别就在于它们允许键值重复，即所有的插入操作基于RB-tree的insert_equal()而非insert_unique()。 hash_set/hash_map/hash_multiset/hash_multimap hash_set/hash_map，两者的一切操作都是基于hashtable之上。不同的是，hash_set同set一样，同时拥有实值和键值，且实质就是键值，键值就是实值，而hash_map同map一样，每一个元素同时拥有一个实值(value)和一个键值(key)，所以其使用方式，和上面的map基本相同。但由于hash_set/hash_map都是基于hashtable之上，所以不具备自动排序功能。为什么?因为hashtable没有自动排序功能。 至于hash_multiset/hash_multimap的特性与上面的multiset/multimap完全相同，唯一的差别就是它们hash_multiset/hash_multimap的底层实现机制是hashtable（而multiset/multimap，上面说了，底层实现机制是RB-tree），所以它们的元素都不会被自动排序，不过也都允许键值重复。 所以，综上，说白了，什么样的结构决定其什么样的性质，因为set/map/multiset/multimap都是基于RB-tree之上，所以有自动排序功能，而hash_set/hash_map/hash_multiset/hash_multimap都是基于hashtable之上，所以不含有自动排序功能，至于加个前缀multi_无非就是允许键值重复而已。如下图所示： 二、处理海量数据问题之六把密匙分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序1、海量日志数据，提取出某日访问百度次数最多的那个IP。既然是海量数据处理，那么可想而知，给我们的数据那就一定是海量的。针对这个数据的海量，我们如何着手呢?对的，无非就是分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序： 分而治之/hash映射：针对数据太大，内存受限，只能是：把大文件化成(取模映射)小文件，即16字方针：大而化小，各个击破，缩小规模，逐个解决hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。 具体而论，则是： “首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如%1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map对那1000个文件中的所有IP进行频率统计，然后依次找出各个文件中频率最大的那个IP）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。” 关于本题，还有几个问题，如下： Hash取模是一种等价映射，不会存在同一个元素分散到不同小文件中的情况，即这里采用的是mod1000算法，那么相同的IP在hash取模后，只可能落在同一个文件中，不可能被分散的。因为如果两个IP相等，那么经过Hash(IP)之后的哈希值是相同的，将此哈希值取模（如模1000），必定仍然相等。 那到底什么是hash映射呢？简单来说，就是为了便于计算机在有限的内存中处理big数据，从而通过一种映射散列的方式让数据均匀分布在对应的内存位置(如大数据通过取余的方式映射成小树存放在内存中，或大文件映射成多个小文件)，而这个映射散列方式便是我们通常所说的hash函数，设计的好的hash函数能让数据均匀分布而减少冲突。尽管数据映射到了另外一些不同的位置，但数据还是原来的数据，只是代替和表示这些原始数据的形式发生了变化而已。 一致性hash算法，见此文第五部分 2、寻找热门查询，300万个查询字符串中统计最热门的10个查询原题：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。 解答：由上面第1题，我们知道，数据大则划为小的，如如一亿个Ip求Top 10，可先%1000将ip分到1000个小文件中去，并保证一种ip只出现在一个文件中，再对每个小文件中的ip进行hashmap计数统计并按数量排序，最后归并或者最小堆依次处理每个小文件的top10以得到最后的结。 但如果数据规模比较小，能一次性装入内存呢?比如这第2题，虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把他们都放进内存中去（300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理），而现在只是需要一个合适的数据结构，在这里，HashTable绝对是我们优先的选择。 所以我们放弃分而治之/hash映射的步骤，直接上hash统计，然后排序。So，针对此类典型的TOP K问题，采取的对策往往是：hashmap + 堆。如下所示： hash_map统计：先对这批海量数据预处理。具体方法是：维护一个Key为Query字串，Value为该Query出现次数的HashTable，即hash_map(Query，Value)，每次读取一个Query，如果该字串不在Table中，那么加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加一即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计； 堆排序：第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。即借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O（N） + N’ * O（logK），（N为1000万，N’为300万）。 别忘了这篇文章中所述的堆排序思路：“维护k个元素的最小堆，即用容量为k的最小堆存储最先遍历到的k个数，并假设它们即是最大的k个数，建堆费时O（k），并调整堆(费时O（logk）)后，有k1&gt;k2&gt;…kmin（kmin设为小顶堆中最小元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，若x&gt;kmin，则更新堆（x入堆，用时logk），否则不更新堆。这样下来，总费时O（klogk+（n-k）logk）=O（n*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。”–第三章续、Top K算法问题的实现。 当然，你也可以采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。 3、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词由上面那两个例题，分而治之 + hash统计 + 堆/快速排序这个套路，我们已经开始有了屡试不爽的感觉。下面，再拿几道再多多验证下。请看此第3题：又是文件很大，又是内存受限，咋办?还能怎么办呢?无非还是： 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。 堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。4、海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素： 堆排序：在每台电脑上求出TOP10，可以采用包含10个元素的堆完成（TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大）。 求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。 但如果同一个元素重复出现在不同的电脑中呢，如下例子所述： 这个时候，你可以有两种方法： 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。 或者，暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP10。5、有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。 方案1：直接上： hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。 堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件（记为）。最后，对这10个文件进行归并排序（内排序与外排序相结合）。根据此方案1，这里有一份实现。 方案2：一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。 方案3：与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。6、 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 分而治之/hash映射：遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为，这里漏写个了a1）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为）。这样处理后，所有可能相同的url都在对应的小文件（）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。 OK，此第一种方法：分而治之/hash映射 + hash统计 + 堆/快速/归并排序，再看最后4道题，如下：7、怎么在海量数据中找出重复次数最多的一个？方案：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。8、上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。方案：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后利用堆取出前N个出现次数最多的数据。9、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 方案1：如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。 方案2：通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，也可以用trie树统计每个词出现的次数，时间复杂度是O(nle)（le表示单词的平准长度），最终同样找出出现最频繁的前10个词（可用堆来实现），时间复杂度是O(nlg10)。10、 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？ 方案1：这题用trie树比较合适，hash_map也行。 方案2：from xjbzju:，1000w的数据规模插入操作完全不现实，以前试过在stl下100w元素插入set中已经慢得不能忍受，觉得基于hash的实现不会比红黑树好太多，使用vector+sort+unique都要可行许多，建议还是先hash成小文件分开处理再综合。11、 一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。 方案1：首先根据用hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中10个最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词。12、 100w个数中找出最大的100个数。 方案1：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。 方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。 方案3：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。多层划分适用范围：第k大，中位数，不重复或重复的数字基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。13、2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。14、5亿个int找它们的中位数。 思路一：这个例子比上面那个更明显。首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用direct addr table进行统计了。 思路二@绿色夹克衫：同样需要做两遍统计，如果数据存在硬盘上，就需要读取2次。方法同基数排序有些像，开一个大小为65536的Int数组，第一遍读取，统计Int32的高16位的情况，也就是0-65535，都算作0,65536 - 131071都算作1。就相当于用该数除以65536。Int32 除以 65536的结果不会超过65536种情况，因此开一个长度为65536的数组计数就可以。每读取一个数，数组中对应的计数+1，考虑有负数的情况，需要将结果加32768后，记录在相应的数组内。第一遍统计之后，遍历数组，逐个累加统计，看中位数处于哪个区间，比如处于区间k，那么0- k-1的区间里数字的数量sum应该&lt;n/2（2.5亿）。而k+1 - 65535的计数和也&lt;n/2，第二遍统计同上面的方法类似，但这次只统计处于区间k的情况，也就是说(x / 65536) + 32768 = k。统计只统计低16位的情况。并且利用刚才统计的sum，比如sum = 2.49亿，那么现在就是要在低16位里面找100万个数(2.5亿-2.49亿)。这次计数之后，再统计一下，看中位数所处的区间，最后将高位和低位组合一下就是结果了。Bloom filter/Bitmap海量数据处理之Bloom Filter详解适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集基本原理及要点：对于原理来说很简单，位数组+k个独立hash函数。将hash函数对应的值的位数组置1，查找时如果发现所有hash函数对应位都是1说明存在，很明显这个过程并不保证查找的结果是100%正确的。同时也不支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是 counting Bloom filter，用一个counter数组代替位数组，就可以支持删除了。 还有一个比较重要的问题，如何根据输入元素个数n，确定位数组m的大小及hash函数个数。当hash函数个数k=(ln2)(m/n)时错误率最小。在错误率不大于E的情况下，m至少要等于nlg(1/E)才能表示任意n个元素的集合。但m还应该更大些，因为还要保证bit数组里至少一半为0，则m应该&gt;=nlg(1/E)*lge 大概就是nlg(1/E)1.44倍(lg表示以2为底的对数)。 举个例子我们假设错误率为0.01，则此时m应大概是n的13倍。这样k大概是8个。注意这里m与n的单位不同，m是bit为单位，而n则是以元素个数为单位(准确的说是不同元素的个数)。通常单个元素的长度都是有很多bit的。所以使用bloom filter内存上通常都是节省的。 扩展：Bloom filter将集合中的元素映射到位数组中，用k（k为哈希函数个数）个映射位是否全1表示元素在不在这个集合中。Counting bloom filter（CBF）将位数组中的每一位扩展为一个counter，从而支持了元素的删除操作。Spectral Bloom Filter（SBF）将其与集合元素的出现次数关联。SBF采用counter中的最小值来近似表示元素的出现频率。 可以看下上文中的第6题： 6、给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？ 根据这个问题我们来计算下内存的占用，4G=2^32大概是40亿*8大概是340亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些urlip是一一对应的，就可以转换成ip，则大大简单了。 同时，上文的第5题：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。” Bitmap13、在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。 方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。 方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。” 15、给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？ 方案1：frome oo，用位图/Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。Trie树/数据库/倒排索引Trie树 适用范围：数据量大，重复多，但是数据种类小可以放入内存基本原理及要点：实现方式，节点孩子的表示方式扩展：压缩实现。 问题实例： 上面的第2题：寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。 上面的第5题：有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？ 上面的第8题：一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词。其解决方法是：用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词。 数据库索引 适用范围：大数据量的增删改查基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。 关于数据库索引及其优化，更多可参见此文关于MySQL索引背后的数据结构及算法原理关于B 树、B+ 树、B* 树及R 树 倒排索引(Inverted index) 适用范围：搜索引擎，关键字查询基本原理及要点：为何叫倒排索引？一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。 以英文为例，下面是要被索引的文本： T0 = “it is what it is” T1 = “what is it” T2 = “it is a banana” 我们就能得到下面的反向文件索引： “a”: {2} “banana”: {2} “is”: {0, 1, 2} “it”: {0, 1, 2} “what”: {0, 1} 检索的条件”what”,”is”和”it”将对应集合的交集。 正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。扩展：问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。 外排序适用范围：大数据的排序，去重基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树问题实例：有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1M做hash明显不够，所以可以用来排序。内存可以当输入缓冲区使用。如何给10^7个数据量的磁盘文件排序 分布式处理之Mapreduce MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。但如果你要我再通俗点介绍，那么，说白了，Mapreduce的原理就是一个归并排序。 适用范围：数据量大，但是数据种类小可以放入内存基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。问题实例： The canonical example application of MapReduce is a process to count the appearances of each different word in a set of documents: 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数的中数(median)？ Hadhoop框架与MapReduce模式中谈海量数据处理MapReduce技术的初步了解与学习 其它模式/方法论，结合操作系统知识至此，六种处理海量数据问题的模式/方法已经阐述完毕。据观察，这方面的面试题无外乎以上一种或其变形，然题目为何取为是：秒杀99%的海量数据处理面试题，而不是100%呢。OK，给读者看最后一道题，如下：非常大的文件，装不进内存。每行一个int类型数据，现在要你随机取100个数。我们发现上述这道题，无论是以上任何一种模式/方法都不好做，那有什么好的别的方法呢？我们可以看看：操作系统内存分页系统设计(说白了，就是映射+建索引)。Windows 2000使用基于分页机制的虚拟内存。每个进程有4GB的虚拟地址空间。基于分页机制，这4GB地址空间的一些部分被映射了物理内存，一些部分映射硬盘上的交换文 件，一些部分什么也没有映射。程序中使用的都是4GB地址空间中的虚拟地址。而访问物理内存，需要使用物理地址。 物理地址 (physical address): 放在寻址总线上的地址。放在寻址总线上，如果是读，电路根据这个地址每位的值就将相应地址的物理内存中的数据放到数据总线中传输。如果是写，电路根据这个 地址每位的值就将相应地址的物理内存中放入数据总线上的内容。物理内存是以字节(8位)为单位编址的。 虚拟地址 (virtual address): 4G虚拟地址空间中的地址，程序中使用的都是虚拟地址。 使用了分页机制之后，4G的地址空间被分成了固定大小的页，每一页或者被映射到物理内存，或者被映射到硬盘上的交换文件中，或者没有映射任何东西。对于一 般程序来说，4G的地址空间，只有一小部分映射了物理内存，大片大片的部分是没有映射任何东西。物理内存也被分页，来映射地址空间。对于32bit的 Win2k，页的大小是4K字节。CPU用来把虚拟地址转换成物理地址的信息存放在叫做页目录和页表的结构里。 物理内存分页，一个物理页的大小为4K字节，第0个物理页从物理地址 0x00000000 处开始。由于页的大小为4KB，就是0x1000字节，所以第1页从物理地址 0x00001000 处开始。第2页从物理地址 0x00002000 处开始。可以看到由于页的大小是4KB，所以只需要32bit的地址中高20bit来寻址物理页。 返回上面我们的题目：非常大的文件，装不进内存。每行一个int类型数据，现在要你随机取100个数。针对此题，我们可以借鉴上述操作系统中内存分页的设计方法，做出如下解决方案： 操作系统中的方法，先生成4G的地址表，在把这个表划分为小的4M的小文件做个索引，二级索引。30位前十位表示第几个4M文件，后20位表示在这个4M文件的第几个，等等，基于key value来设计存储，用key来建索引。 更多海里数据处理面试题]]></content>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[freemarker基础知识总结]]></title>
    <url>%2Ffreemarker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Freemaker FTL指令常用标签及语法注意**：使用freemaker，要求所有标签必须闭合，否则会导致freemaker无法解析。 freemaker注释:&lt;#– 注释内容 –&gt;格式部分,不会输出 基础语法1、字符输出12345$&#123;emp.name?if_exists&#125; // 变量存在，输出该变量，否则不输出$&#123;emp.name!&#125; // 变量存在，输出该变量，否则不输出$&#123;emp.name?default(&quot;xxx&quot;)&#125; // 变量不存在，取默认值xxx $&#123;emp.name!&quot;xxx&quot;&#125; // 变量不存在，取默认值xxx 常用内部函数： 12345$&#123;&quot;123&lt;br&gt;456&quot;?html&#125; // 对字符串进行HTML编码，对html中特殊字符进行转义$&#123;&quot;str&quot;?cap_first&#125; // 使字符串第一个字母大写 $&#123;&quot;Str&quot;?lower_case&#125; // 将字符串转换成小写 $&#123;&quot;Str&quot;?upper_case&#125; // 将字符串转换成大写$&#123;&quot;str&quot;?trim&#125; // 去掉字符串前后的空白字符 字符串的两种拼接方式拼接： 12$&#123;&quot;hello$&#123;emp.name!&#125;&quot;&#125; // 输出hello+变量名$&#123;&quot;hello&quot;+emp.name!&#125; // 使用+号来连接，输出hello+变量名 可以通过如下语法来截取子串: 1234567891011&lt;#assign str = &quot;abcdefghijklmn&quot;/&gt;// 方法1$&#123;str?substring(0,4)&#125; // 输出abcd// 方法2$&#123;str[0]&#125;$&#123;str[4]&#125; // 结果是ae$&#123;str[1..4]&#125; // 结果是bcde// 返回指定字符的索引$&#123;str?index_of(&quot;n&quot;)&#125; 2、日期输出1$&#123;emp.date?string(&apos;yyyy-MM-dd&apos;)&#125; //日期格式 3、数字输出(以数字20为例)12345678910111213141516171819202122232425$&#123;emp.name?string.number&#125; // 输出20$&#123;emp.name?string.currency&#125; // ￥20.00 $&#123;emp.name?string.percent&#125; // 20%$&#123;1.222?int&#125; // 将小数转为int，输出1&lt;#setting number_format=&quot;percent&quot;/&gt; // 设置数字默认输出方式(&apos;percent&apos;,百分比)&lt;#assign answer=42/&gt; // 声明变量 answer 42#&#123;answer&#125; // 输出 4,200%$&#123;answer?string&#125; // 输出 4,200%$&#123;answer?string.number&#125; // 输出 42$&#123;answer?string.currency&#125; // 输出 ￥42.00$&#123;answer?string.percent&#125; // 输出 4,200%#&#123;answer&#125; // 输出 42数字格式化插值可采用#&#123;expr;format&#125;形式来格式化数字,其中format可以是:mX:小数部分最小X位MX:小数部分最大X位如下面的例子:&lt;#assign x=2.582/&gt;&lt;#assign y=4/&gt;#&#123;x; M2&#125; // 输出2.58#&#123;y; M2&#125; // 输出4#&#123;x; m2&#125; // 输出2.58#&#123;y; m2&#125; // 输出4.0#&#123;x; m1M2&#125; // 输出2.58#&#123;x; m1M2&#125; // 输出4.0 4、申明变量123456789101112131415&lt;#assign foo=false/&gt; // 声明变量,插入布尔值进行显示,注意不要用引号$&#123;foo?string(&quot;yes&quot;,&quot;no&quot;)&#125; // 当为true时输出&quot;yes&quot;,否则输出&quot;no&quot;申明变量的几种方式&lt;#assign name=value&gt; &lt;#assign name1=value1 name2=value2 ... nameN=valueN&gt; &lt;#assign same as above... in namespacehash&gt;&lt;#assign name&gt; capture this &lt;/#assign&gt;&lt;#assign name in namespacehash&gt; capture this &lt;/#assign&gt; 5、比较运算符1234567表达式中支持的比较运算符有如下几个:= 或 == ：判断两个值是否相等.!= ：判断两个值是否不等.&gt; 或 gt ：判断左边值是否大于右边值&gt;= 或 gte ：判断左边值是否大于等于右边值&lt; 或 lt ：判断左边值是否小于右边值&lt;= 或 lte ：判断左边值是否小于等于右边值 6、算术运算符12345FreeMarker表达式中完全支持算术运算,FreeMarker支持的算术运算符包括:+, - , * , / , %注意：（1）、运算符两边必须是数字（2）、使用+运算符时,如果一边是数字,一边是字符串,就会自动将数字转换为字符串再连接,如:$&#123;3 + “5”&#125;,结果是:35 7、逻辑运算符逻辑运算符有如下几个:逻辑与:&amp;&amp;逻辑或:||逻辑非:!逻辑运算符只能作用于布尔值,否则将产生错误 8、FreeMarker中的运算符优先级如下(由高到低排列):①、一元运算符:!②、内建函数:?③、乘除法:*, / , %④、加减法:- , +⑤、比较:&gt; , &lt; , &gt;= , &lt;= (lt , lte , gt , gte)⑥、相等:== , = , !=⑦、逻辑与:&amp;&amp;⑧、逻辑或:||⑨、数字范围:..实际上,我们在开发过程中应该使用括号来严格区分,这样的可读性好,出错少 9、if 逻辑判断（注意：elseif 不加空格）1234567891011121314151617181920&lt;#if condition&gt;...&lt;#elseif condition2&gt;...&lt;#elseif condition3&gt;...&lt;#else&gt;...&lt;/#if&gt;if 空值判断// 当 photoList 不为空时&lt;#if photoList??&gt;...&lt;/#if&gt; 值得注意的是,$&#123;..&#125;只能用于文本部分,不能用于表达式,下面的代码是错误的:&lt;#if $&#123;isBig&#125;&gt;Wow!&lt;/#if&gt;&lt;#if &quot;$&#123;isBig&#125;&quot;&gt;Wow!&lt;/#if&gt;// 正确写法&lt;#if isBig&gt;Wow!&lt;/#if&gt; 10、switch (条件可为数字，可为字符串)12345678910111213&lt;#switch value&gt; &lt;#case refValue1&gt; ....&lt;#break&gt; &lt;#case refValue2&gt; ....&lt;#break&gt; &lt;#case refValueN&gt; ....&lt;#break&gt; &lt;#default&gt; .... &lt;/#switch&gt; 11、集合 &amp; 循环12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// 遍历集合:&lt;#list empList! as emp&gt; $&#123;emp.name!&#125;&lt;/#list&gt;// 可以这样遍历集合:&lt;#list 0..(empList!?size-1) as i&gt; $&#123;empList[i].name!&#125;&lt;/#list&gt;// 与jstl循环类似,也可以访问循环的状态。empList?size // 取集合的长度emp_index: // int类型，当前对象的索引值 emp_has_next: // boolean类型，是否存在下一个对象// 使用&lt;#break&gt;跳出循环&lt;#if emp_index = 0&gt;&lt;#break&gt;&lt;/#if&gt;// 集合长度判断 &lt;#if empList?size != 0&gt;&lt;/#if&gt; // 判断=的时候,注意只要一个=符号,而不是==&lt;#assign l=0..100/&gt; // 定义一个int区间的0~100的集合，数字范围也支持反递增,如100..2&lt;#list 0..100 as i&gt; // 等效于java for(int i=0; i &lt;= 100; i++) $&#123;i&#125;&lt;/#list&gt;// 截取子集合：empList[3..5] //返回empList集合的子集合,子集合中的元素是empList集合中的第4-6个元素// 创建集合：&lt;#list [&quot;星期一&quot;, &quot;星期二&quot;, &quot;星期三&quot;, &quot;星期四&quot;, &quot;星期五&quot;, &quot;星期六&quot;, &quot;星期天&quot;] as x&gt;// 集合连接运算,将两个集合连接成一个新的集合&lt;#list [&quot;星期一&quot;,&quot;星期二&quot;,&quot;星期三&quot;] + [&quot;星期四&quot;,&quot;星期五&quot;,&quot;星期六&quot;,&quot;星期天&quot;] as x&gt;// 除此之外,集合元素也可以是表达式,例子如下:[2 + 2, [1, 2, 3, 4], &quot;whatnot&quot;]// seq_contains：判断序列中的元素是否存在&lt;#assign x = [&quot;red&quot;, 16, &quot;blue&quot;, &quot;cyan&quot;]&gt; $&#123;x?seq_contains(&quot;blue&quot;)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // yes$&#123;x?seq_contains(&quot;yellow&quot;)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // no$&#123;x?seq_contains(16)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // yes$&#123;x?seq_contains(&quot;16&quot;)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // no// seq_index_of：第一次出现的索引&lt;#assign x = [&quot;red&quot;, 16, &quot;blue&quot;, &quot;cyan&quot;, &quot;blue&quot;]&gt; $&#123;x?seq_index_of(&quot;blue&quot;)&#125; // 2// sort_by：排序（升序）&lt;#list movies?sort_by(&quot;showtime&quot;) as movie&gt;&lt;/#list&gt;// sort_by：排序（降序）&lt;#list movies?sort_by(&quot;showtime&quot;)?reverse as movie&gt;&lt;/#list&gt;// 具体介绍：// 不排序的情况：&lt;#list movies as moive&gt; &lt;a href=&quot;$&#123;moive.url&#125;&quot;&gt;$&#123;moive.name&#125;&lt;/a&gt;&lt;/#list&gt;//要是排序，则用&lt;#list movies?sort as movie&gt; &lt;a href=&quot;$&#123;movie.url&#125;&quot;&gt;$&#123;movie.name&#125;&lt;/a&gt;&lt;/#list&gt;// 这是按元素的首字母排序。若要按list中对象元素的某一属性排序的话，则用&lt;#list moives?sort_by([&quot;name&quot;]) as movie&gt; &lt;a href=&quot;$&#123;movie.url&#125;&quot;&gt;$&#123;movie.name&#125;&lt;/a&gt;&lt;/#list&gt;//这个是按list中对象元素的[name]属性排序的，是升序，如果需要降序的话，如下所示：&lt;#list movies?sort_by([&quot;name&quot;])?reverse as movie&gt; &lt;a href=&quot;$&#123;movie.url&#125;&quot;&gt;$&#123;movie.name&#125;&lt;/a&gt;&lt;/#list&gt; 12、Map对象123456789// 创建map&lt;#assign scores = &#123;&quot;语文&quot;:86,&quot;数学&quot;:78&#125;&gt;// Map连接运算符&lt;#assign scores = &#123;&quot;语文&quot;:86,&quot;数学&quot;:78&#125; + &#123;&quot;数学&quot;:87,&quot;Java&quot;:93&#125;&gt;// Map元素输出emp.name // 全部使用点语法emp[&quot;name&quot;] // 使用方括号 13、FreeMarker支持如下转义字符:1234567891011121314151617\” ：双引号(u0022)\’ ：单引号(u0027)\ ：反斜杠(u005C)\n ：换行(u000A)\r ：回车(u000D)\t ：Tab(u0009)\b ：退格键(u0008)\f ：Form feed(u000C)\l ：&lt;\g ：&gt;\a ：&amp;\&#123; ：&#123;\xCode ：直接通过4位的16进制数来指定Unicode码,输出该unicode码对应的字符.如果某段文本中包含大量的特殊符号,FreeMarker提供了另一种特殊格式:可以在指定字符串内容的引号前增加r标记,在r标记后的文件将会直接输出.看如下代码:r”$foo”//输出&#123;foo&#125;$&#123;r”C:/foo/bar”&#125; // 输出 C:/foo/bar 14、include指令123456// include指令的作用类似于JSP的包含指令:&lt;#include &quot;/test.ftl&quot; encoding=&quot;UTF-8&quot; parse=true&gt;// 在上面的语法格式中,两个参数的解释如下:encoding=&quot;GBK&quot; // 编码格式parse=true // 是否作为ftl语法解析,默认是true，false就是以文本方式引入,注意:在ftl文件里布尔值都是直接赋值的如parse=true,而不是parse=&quot;true&quot; 15、import指令123// 类似于jsp里的import,它导入文件，然后就可以在当前文件里使用被导入文件里的宏组件&lt;#import &quot;/libs/mylib.ftl&quot; as my&gt;// 上面的代码将导入/lib/common.ftl模板文件中的所有变量,交将这些变量放置在一个名为com的Map对象中，&quot;my&quot;在freemarker里被称作namespace 17、compress 压缩12345678// 用来压缩空白空间和空白的行 &lt;#compress&gt; ... &lt;/#compress&gt;&lt;#t&gt; // 去掉左右空白和回车换行 &lt;#lt&gt;// 去掉左边空白和回车换行 &lt;#rt&gt;// 去掉右边空白和回车换行 &lt;#nt&gt;// 取消上面的效果 18、escape,noescape 对字符串进行HTML编码1234567891011// escape指令导致body区的插值都会被自动加上escape表达式,但不会影响字符串内的插值,只会影响到body内出现的插值,使用escape指令的语法格式如下:&lt;#escape x as x?html&gt; First name: $&#123;firstName&#125; &lt;#noescape&gt;Last name: $&#123;lastName&#125;&lt;/#noescape&gt; Maiden name: $&#123;maidenName&#125; &lt;/#escape&gt;// 相同表达式First name: $&#123;firstName?html&#125; Last name: $&#123;lastName&#125; Maiden name: $&#123;maidenName?html&#125;]]></content>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于前缀树图文详解敏感词过滤]]></title>
    <url>%2F%E5%9F%BA%E4%BA%8E%E5%89%8D%E7%BC%80%E6%A0%91%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3%E6%95%8F%E6%84%9F%E8%AF%8D%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[一般设计网站的时候，会有问题发布或者是内容发布的功能，这些功能的有一个很重要的点在于如何实现敏感词过滤，要不然可能会有不良信息的发布，或者发布的内容中有夹杂可能会有恶意功能的代码片段，敏感词过滤的基本的算法是前缀树算法，前缀树也就是字典树，通过前缀树匹配可以加快敏感词匹配的速度。首先是过滤HTML代码，在Spring中有直接的函数可以使用：1question.setContent(HtmlUtils.htmlEscape(question.getContent())); 实现的功能就是将html的代码进行转义后显示出来，使其失效。举一个具体的例子：如果有一串字符串为xwabfabcff,敏感词为abc、bf、bc，若这个字符串中包含敏感词，则使用***代替敏感词，实现一个算法。 使用三个指针，指针1指向根节点，指针2指向字符串下标起始值，指针3指向字符串当前下标值。指针1为tempnode=rootnode，指针2为begin=0，指针3为position=0，创建stringbuffer sb来保存结果； 遍历x，tempnode未找到子节点x，将x保存到sb中，1begin=begin+1;position=begin，tempnode=rootnode; 遍历w，tempnode未找到子节点w，将w保存到sb中，1begin=begin+1;position=begin，tempnode=rootnode; 同上 遍历a，tempnode找到子节点a，tempnode指向a节点，则position++； 遍历b，tempnode发现a节点下有b这个子节点，所以，tempnode指向b节点，则position++； 遍历f，tempnode发现b节点下没有f这个子节点，所以，代表以begin开头的字符串，不会有敏感字符，因此，将a存入sb中。1position=begin+1;bigin=position;tempnode=rootnode 遍历b,tempnode找到子节点b，tempnode指向b节点，则position++； 遍历f，tempnode发现b节点下有f这个子节点，而且f值敏感词结尾标记，所以，打码。将**写入sb中，同时，begin=position+1;position=begin;tempnode=rootnode; 遍历a，tempnode找到子节点a，tempnode指向a节点，则position++； 遍历b，tempnode发现a节点下有b这个子节点，所以，tempnode指向b节点，则position++； 遍历c，tempnode发现b节点下有c这个子节点，而且c值敏感词结尾标记，所以，打码。将***写入sb中，同时，begin=position+1;position=begin;tempnode=rootnode; 遍历f，tempnode发现根节点下没有f这个节点，因此，将f存入sb中。position=begin+1;bigin=position;tempnode=rootnode； 遍历f，tempnode发现根节点下没有f这个节点，因此，将f存入sb中。position=begin+1;bigin=position;tempnode=rootnode；因此，最后sb中为：xwa**ff;这里每次是将position指向的字符挨个的与tempnode的子节点进行比较，因此，代码中的while条件应该是1while (position &lt; text.length())&#123;&#125; 同时，需要思考：如果字符串为xwabfabcfb，则最后，begin指向b下标，position指向b下标，tempnode发现根节点下有b节点，因此position++;然后就退出循环了。而此时，sb中还只保存了xwa**f,所以，我们在循环的最后，还要将最后一串字符串加进来。1result.append(text.substring(begin)); 完整的代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158package com.springboot.springboot.service;import org.apache.commons.lang3.CharUtils;import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Service; import java.io.BufferedReader;import java.io.InputStream;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Map;@Servicepublic class SensitiveService implements InitializingBean &#123; private static final Logger logger = LoggerFactory.getLogger(SensitiveService.class); /** * 默认敏感词替换符 */ private static final String DEFAULT_REPLACEMENT = "敏感词"; private class TrieNode &#123; /** * true 关键词的终结 ； false 继续 */ private boolean end = false; /** * key下一个字符，value是对应的节点 */ private Map&lt;Character, TrieNode&gt; subNodes = new HashMap&lt;&gt;(); /** * 向指定位置添加节点树 */ void addSubNode(Character key, TrieNode node) &#123; subNodes.put(key, node); &#125; /** * 获取下个节点 */ TrieNode getSubNode(Character key) &#123; return subNodes.get(key); &#125; boolean isKeywordEnd() &#123; return end; &#125; void setKeywordEnd(boolean end) &#123; this.end = end; &#125; public int getSubNodeCount() &#123; return subNodes.size(); &#125; &#125; /** * 根节点 */ private TrieNode rootNode = new TrieNode(); /** * 判断是否是一个符号 */ private boolean isSymbol(char c) &#123; int ic = (int) c; // 0x2E80-0x9FFF 东亚文字范围 return !CharUtils.isAsciiAlphanumeric(c) &amp;&amp; (ic &lt; 0x2E80 || ic &gt; 0x9FFF); &#125; /** * 过滤敏感词 */ public String filter(String text) &#123; if (StringUtils.isBlank(text)) &#123; return text; &#125; String replacement = DEFAULT_REPLACEMENT; StringBuilder result = new StringBuilder(); TrieNode tempNode = rootNode; //指向树的根节点 int begin = 0; // 回滚数，指向字符串的指针，与树进行交互的 int position = 0; // 当前比较的位置，指向字符串 while (position &lt; text.length()) &#123; char c = text.charAt(position); // 空格直接跳过 if (isSymbol(c)) &#123; if (tempNode == rootNode) &#123; result.append(c); ++begin; &#125; ++position; continue; &#125; tempNode = tempNode.getSubNode(c); // 当前位置的匹配结束 if (tempNode == null) &#123; // 以begin开始的字符串不存在敏感词 result.append(text.charAt(begin)); // 跳到下一个字符开始测试 position = begin + 1; begin = position; // 回到树初始节点 tempNode = rootNode; &#125; else if (tempNode.isKeywordEnd()) &#123; // 发现敏感词， 从begin到position的位置用replacement替换掉 result.append(replacement); position = position + 1; begin = position; tempNode = rootNode; &#125; else &#123; ++position; &#125; &#125; //将最后一次的比较结果添加进去 result.append(text.substring(begin)); return result.toString(); &#125; private void addWord(String lineTxt) &#123; TrieNode tempNode = rootNode; // 循环每个字节 for (int i = 0; i &lt; lineTxt.length(); ++i) &#123; Character c = lineTxt.charAt(i); // 过滤空格 if (isSymbol(c)) &#123; continue; &#125; TrieNode node = tempNode.getSubNode(c); if (node == null) &#123; // 没初始化 node = new TrieNode(); tempNode.addSubNode(c, node); &#125; tempNode = node; if (i == lineTxt.length() - 1) &#123; // 关键词结束， 设置结束标志 tempNode.setKeywordEnd(true); &#125; &#125; &#125; @Override public void afterPropertiesSet() throws Exception &#123; rootNode = new TrieNode(); try &#123; InputStream is = Thread.currentThread().getContextClassLoader() .getResourceAsStream("SensitiveWords.txt"); InputStreamReader read = new InputStreamReader(is); BufferedReader bufferedReader = new BufferedReader(read); String lineTxt; while ((lineTxt = bufferedReader.readLine()) != null) &#123; lineTxt = lineTxt.trim(); addWord(lineTxt); &#125; read.close(); &#125; catch (Exception e) &#123; logger.error("读取敏感词文件失败" + e.getMessage()); &#125; &#125; public static void main(String[] argv) &#123; SensitiveService s = new SensitiveService(); s.addWord("色情"); s.addWord("赌博"); System.out.print(s.filter("你好赌博")); &#125;&#125;]]></content>
      <tags>
        <tag>Spring Boot</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring基础知识]]></title>
    <url>%2FSpring%2F</url>
    <content type="text"><![CDATA[Spring IoC、AOP 的理解以及实现的原理Spring IoC Spring IoC 实现原理：反射创建实例。 IoC 容器的加戴过程：XML -&gt; 读取 -&gt; Resource -&gt; 解析 -&gt; BeanDefinition -&gt; 注册 -&gt; BeanFactory IoC(Inversion of Control,控制翻转) 是Spring 中一个非常非常重要的概念，它不是什么技术，而是一种解耦的设计思想。它的主要目的是借助于“第三方”(Spring 中的 IOC 容器) 实现具有依赖关系的对象之间的解耦(IOC容器管理对象，你只管使用即可)，从而降低代码之间的耦合度。IOC 是一个原则，而不是一个模式，以下模式（但不限于）实现了IoC原则。 Spring IOC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 IOC 容器负责创建对象，将对象连接在一起，配置这些对象，并从创建中处理这些对象的整个生命周期，直到它们被完全销毁。 在实际项目中一个 Service 类如果有几百甚至上千个类作为它的底层，我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IOC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。关于Spring IOC 的理解，推荐看这一下知乎的一个回答：https://www.zhihu.com/question/23277575/answer/169698662 ，非常不错。 控制翻转怎么理解呢? 举个例子：”对象a 依赖了对象 b，当对象 a 需要使用 对象 b的时候必须自己去创建。但是当系统引入了 IOC 容器后， 对象a 和对象 b 之前就失去了直接的联系。这个时候，当对象 a 需要使用 对象 b的时候， 我们可以指定 IOC 容器去创建一个对象b注入到对象 a 中”。 对象 a 获得依赖对象 b 的过程,由主动行为变为了被动行为，控制权翻转，这就是控制反转名字的由来。 DI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去。 Spring AOP Spring AOP 实现原理：动态代理 JDK 的动态代理：如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理来生成 AOP 代理类。 CGLib 动态代理：如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类。 动态代理与 CGLib 实现的区别 Spring Boot 和 Spring 的区别 Spring Boot 是基于 Spring 的一套快速开发整合包； 内嵌了如 Tomcat，Jetty 和 Undertow 这样的容器，也就是说可以直接跑起来，用不着再做部署工作了； 无需再像 Spring 那样搞一堆繁琐的 xml 文件的配置； ApplicationContext 和 BeanFactory 的区别 加载 Bean 的时机不同 BeanFactroy 采用的是延迟加载形式来注入Bean 的，即只有在使用到某个 Bean 时（调用getBean()），才对该 Bean 进行加载实例化，这样，我们就不能发现一些存在的 Spring 的配置问题。 ApplicationContext 是在容器启动时，一次性创建了所有的 Bean。这样，在容器启动时，我们就可以发现 Spring 中存在的配置错误。 Spring Bean 的作用域： singleton：在 Spring 的 IoC 容器中只存在一个对象实例，这个实例会被保存到缓存中，并且对该 bean 的所有后续请求和引用都将返回该缓存中的对象实例。 prototype：每次对该 bean 的请求都会创建一个新的实例。 request：每次 http 请求将会有各自的 bean 实例。 session：在一个 http session 中，一个 bean 定义对应一个 bean 实例。 globalSession：在一个全局的 http session 中，一个 bean 定义对应一个 bean 实例。 Spring Bean 生命周期 Spring 对 Bean 进行实例化。 相当于程序中的 new Xxx()。 Spring 将值和 Bean 的引用注入进 Bean 对应的属性中。 如果 Bean 实现了 BeanNameAware 接口，Spring 将 Bean 的 ID 传递给 setBeanName() 方法。 实现 BeanNameAware 接口主要是为了通过 Bean 的引用来获得 Bean 的 ID，不过一般很少用到 Bean 的 ID。 如果 Bean 实现了 BeanFactoryAware 接口，Spring 将调用 setBeanFactory(BeanFactory bf) 方法并把 BeanFactory 容器实例作为参数传入。 实现 BeanFactoryAware 主要目的是为了获取 Spring 容器，如 Bean 通过 Spring 容器发布事件。 如果 Bean 实现了 ApplicationContextAware 接口，Spring 容器将调用 setApplicationContext(ApplicationContext ctx) 方法，把当前应用上下文作为参数传入。 作用与 BeanFactory 类似，都是为了获取 Spring 容器。 不同的是：Spring 容器在调用 setApplicationContext 方法时会把它自己作为参数传入，而调用 setBeanFactory 方法前需要程序员自己指定（注入）setBeanDactory 里的 BeanFactory 参数。 如果 Bean 实现了 BeanPostProcessor 接口，Spring 将调用它们的 postProcessorBeforeInitialization 方法。 作用是在 Bean 实例创建成功后对进行增强处理，如对 Bean 进行修改或增加某个功能。 如果 Bean 实现了 InitializingBean 接口，Spring 将调用它们的 afterPropertiesSet 方法 作用与在配置文件中对 Bean 使用 init-method 声明初始化的作用一样，都是在 Bean 的全部属性设置成功后执行的初始化方法。 如果 Bean 实现了 BeanPostProcessor 接口，Spring 将调用它们的 postProcessorAfterInitialization 方法。 作用与 postProcessorBeforeInitialization 一样，只不过 postProcessorBeforeInitialization 是在 Bean 初始化前执行，这个在 Bean 初始化后执行。 Bean 就准备就绪了，如果这个 Bean 是 singleton 的，就把它保存到容器的缓存中，如果是 prototype 的，就交给调用者。 如果 Bean 实现了 DisposableBean 接口，Spring 将调用它的 destroy 方法 作用与在配置文件中对 Bean 使用 destory-method 属性一样，都是在 Bean 实例销毁前执行的方法。 Spring 事务Spring 事务中的隔离级别 TransactionDefinition.ISOLATION_DEFAULT：使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ 隔离级别 Oracle 默认采用的 READ_COMMITTED 隔离级别。 TransactionDefinition.ISOLATION_READ_UNCOMMITTED：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 TransactionDefinition.ISOLATION_READ_COMMITTED：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 TransactionDefinition.ISOLATION_REPEATABLE_READ：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE：最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 Spring 事务中的事务传播行为支持当前事务的情况 TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于 TransactionDefinition.PROPAGATION_REQUIRED。 事务怎么配置（基于 Aspectj AOP 配置事务）Spring MVCSpring MVC 原理 客户端将请求发送到 DispatchServlet； DispatchServlet 通过调用 HandleMapping，根据 URL 找到对应的处理器，并返回给 DispatchServlet； DispatchServlet 通过 HandleAdapter 调用 Handle； Handle 运行完会返回 ModelAndView 给 DispatchServlet； DispatchServlet 将 ModelAndView 交给 ViewResolver 解析，解析将得到具体的 View； 将 Model 填充进 View 中，将渲染结果返回客户端。 描述从 Tomcat 开始到 Spring MVC 返回到前端显示的整个流程在没有 Spring MVC 前，Tomcat 处理一个 HTTP 请求的具体处理流程是这样的： Web 客户向 Servlet 容器（Tomcat）发出 HTTP 请求； Servlet 容器分析客户的请求信息 Servlet 容器创建一个 HttpRequest 对象和一个 HttpResponse 对象，并将客户请求的信息封装到 HttpRequest 对象中； Servlet 容器调用 HttpServlet 对象的 service 方法，把 HttpRequest 对象与 HttpResponse 对象作为参数传给 HttpServlet对象； HttpServlet 调用 HttpRequest 对象的有关方法，获取 HTTP 请求信息； HttpServlet 调用 HttpResponse 对象的有关方法，生成响应数据； Servlet 容器把 HttpServlet 的响应结果传给 Web 客户； 我们有多少服务，就写多少个 Servlet，不过有了 Spring MVC 后，服务器里就剩一个 DispatchServlet 了，所有的 HTTP 请求都会被映射到这个 Servlet 上，请求进入到 DispatchServlet 中后，就算进入到了框架之中了，由 DispatchServlet 统一的分配 HTTP 请求到各个 Controller 中进行处理，流程详见Spring MVC 原理。 Spring 中的设计模式 简单工厂 工厂方法 单例模式 适配器模式 装饰者模式 代理模式 观察者模式 策略模式 模板方法 工厂设计模式Spring使用工厂模式可以通过 BeanFactory 或 ApplicationContext 创建 bean 对象。 两者对比： BeanFactory ：延迟注入(使用到某个 bean 的时候才会注入),相比于BeanFactory来说会占用更少的内存，程序启动速度更快。 ApplicationContext ：容器启动的时候，不管你用没用到，一次性创建所有 bean 。BeanFactory 仅提供了最基本的依赖注入支持，ApplicationContext 扩展了 BeanFactory ,除了有BeanFactory的功能还有额外更多功能，所以一般开发人员使用ApplicationContext会更多。 ApplicationContext的三个实现类： ClassPathXmlApplication：把上下文文件当成类路径资源。 FileSystemXmlApplication：从文件系统中的 XML 文件载入上下文定义信息。 XmlWebApplicationContext：从Web系统中的XML文件载入上下文定义信息。 单例设计模式在我们的系统中，有一些对象其实我们只需要一个，比如说：线程池、缓存、对话框、注册表、日志对象、充当打印机、显卡等设备驱动程序的对象。事实上，这一类对象只能有一个实例，如果制造出多个实例就可能会导致一些问题的产生，比如：程序的行为异常、资源使用过量、或者不一致性的结果。 使用单例模式的好处: 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销； 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。 Spring 中 bean 的默认作用域就是 singleton(单例)的。 Spring 实现单例的方式： xml: 注解：@Scope(value = &quot;singleton&quot;) Spring 通过 ConcurrentHashMap 实现单例注册表的特殊方式实现单例模式。Spring 实现单例的核心代码如下： 1234567891011121314151617181920212223242526272829303132// 通过 ConcurrentHashMap（线程安全） 实现单例注册表private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;String, Object&gt;(64);public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(beanName, "'beanName' must not be null"); synchronized (this.singletonObjects) &#123; // 检查缓存中是否存在实例 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; //...省略了很多代码 try &#123; singletonObject = singletonFactory.getObject(); &#125; //...省略了很多代码 // 如果实例对象在不存在，我们注册到单例注册表中。 addSingleton(beanName, singletonObject); &#125; return (singletonObject != NULL_OBJECT ? singletonObject : null); &#125; &#125; //将对象添加到单例注册表 protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized (this.singletonObjects) &#123; this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT)); &#125; &#125;&#125; 代理设计模式代理模式在 AOP 中的应用AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 Spring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示： 当然你也可以使用 AspectJ ,Spring AOP 已经集成了AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。 使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。 Spring AOP 和 AspectJ AOP 有什么区别?Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多。 观察者模式观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，这个对象所依赖的对象也会做出反应。Spring 事件驱动模型就是观察者模式很经典的一个应用。Spring 事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题。 Spring 事件驱动模型中的三种角色事件角色ApplicationEvent (org.springframework.context包下)充当事件的角色,这是一个抽象类，它继承了java.util.EventObject并实现了 java.io.Serializable接口。 Spring 中默认存在以下事件，他们都是对 ApplicationContextEvent 的实现(继承自ApplicationContextEvent)： ContextStartedEvent：ApplicationContext 启动后触发的事件; ContextStoppedEvent：ApplicationContext 停止后触发的事件; ContextRefreshedEvent：ApplicationContext 初始化或刷新完成后触发的事件; ContextClosedEvent：ApplicationContext 关闭后触发的事件。 事件监听者角色ApplicationListener 充当了事件监听者角色，它是一个接口，里面只定义了一个 onApplicationEvent（）方法来处理ApplicationEvent。ApplicationListener接口类源码如下，可以看出接口定义看出接口中的事件只要实现了 ApplicationEvent就可以了。所以，在 Spring中我们只要实现 ApplicationListener 接口实现 onApplicationEvent() 方法即可完成监听事件 1234567package org.springframework.context;import java.util.EventListener;@FunctionalInterfacepublic interface ApplicationListener&lt;E extends ApplicationEvent&gt; extends EventListener &#123; void onApplicationEvent(E var1);&#125; 事件发布者角色ApplicationEventPublisher 充当了事件的发布者，它也是一个接口。 1234567@FunctionalInterfacepublic interface ApplicationEventPublisher &#123; default void publishEvent(ApplicationEvent event) &#123; this.publishEvent((Object)event); &#125; void publishEvent(Object var1);&#125; ApplicationEventPublisher 接口的publishEvent（）这个方法在AbstractApplicationContext类中被实现，阅读这个方法的实现，你会发现实际上事件真正是通过ApplicationEventMulticaster来广播出去的。具体内容过多，就不在这里分析了，后面可能会单独写一篇文章提到。 Spring 的事件流程总结 定义一个事件: 实现一个继承自 ApplicationEvent，并且写相应的构造函数； 定义一个事件监听者：实现 ApplicationListener 接口，重写 onApplicationEvent() 方法； 使用事件发布者发布消息: 可以通过 ApplicationEventPublisher 的 publishEvent() 方法发布消息。 1234567891011121314151617181920212223242526272829303132333435363738// 定义一个事件,继承自ApplicationEvent并且写相应的构造函数public class DemoEvent extends ApplicationEvent&#123; private static final long serialVersionUID = 1L; private String message; public DemoEvent(Object source,String message) &#123; super(source); this.message = message; &#125; public String getMessage() &#123; return message; &#125;// 定义一个事件监听者,实现ApplicationListener接口，重写 onApplicationEvent() 方法；@Componentpublic class DemoListener implements ApplicationListener&lt;DemoEvent&gt;&#123; //使用onApplicationEvent接收消息 @Override public void onApplicationEvent(DemoEvent event) &#123; String msg = event.getMessage(); System.out.println("接收到的信息是："+msg); &#125;&#125;// 发布事件，可以通过ApplicationEventPublisher 的 publishEvent() 方法发布消息。@Componentpublic class DemoPublisher &#123; @Autowired ApplicationContext applicationContext; public void publish(String message) &#123; //发布事件 applicationContext.publishEvent(new DemoEvent(this, message)); &#125;&#125; 适配器模式适配器模式(Adapter Pattern) 将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。 spring AOP中的适配器模式我们知道 Spring AOP 的实现是基于代理模式，但是 Spring AOP 的增强或通知(Advice)使用到了适配器模式，与之相关的接口是AdvisorAdapter 。Advice 常用的类型有：BeforeAdvice（目标方法调用前,前置通知）、AfterAdvice（目标方法调用后,后置通知）、AfterReturningAdvice(目标方法执行结束后，return之前)等等。每个类型Advice（通知）都有对应的拦截器:MethodBeforeAdviceInterceptor、AfterReturningAdviceAdapter、AfterReturningAdviceInterceptor。Spring预定义的通知要通过对应的适配器，适配成 MethodInterceptor接口(方法拦截器)类型的对象（如：MethodBeforeAdviceInterceptor 负责适配 MethodBeforeAdvice）。 spring MVC中的适配器模式在Spring MVC中，DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由HandlerAdapter 适配器处理。HandlerAdapter 作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller 作为需要适配的类。 为什么要在 Spring MVC 中使用适配器模式？ Spring MVC 中的 Controller 种类众多，不同类型的 Controller 通过不同的方法来对请求进行处理。如果不利用适配器模式的话，DispatcherServlet 直接获取对应类型的 Controller，需要的自行来判断，像下面这段代码一样： 1234567if(mappedHandler.getHandler() instanceof MultiActionController)&#123; ((MultiActionController)mappedHandler.getHandler()).xxx &#125;else if(mappedHandler.getHandler() instanceof XXX)&#123; ... &#125;else if(...)&#123; ... &#125; 假如我们再增加一个 Controller类型就要在上面代码中再加入一行 判断语句，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则 – 对扩展开放，对修改关闭。 装饰者模式装饰者模式可以动态地给对象添加一些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个Decorator套在原有代码外面。其实在 JDK 中就有很多地方用到了装饰者模式，比如 InputStream家族，InputStream 类下有 FileInputStream (读取文件)、BufferedInputStream (增加缓存,使读取文件速度大大提升)等子类都在不修改InputStream 代码的情况下扩展了它的功能。 Spring 中配置 DataSource 的时候，DataSource 可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下动态切换不同的数据源？这个时候就要用到装饰者模式(这一点我自己还没太理解具体原理)。Spring 中用到的包装器模式在类名上含有 Wrapper或者 Decorator。这些类基本上都是动态地给一个对象添加一些额外的职责 总结Spring 框架中用到了哪些设计模式： 工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 ……javaguide公众号 Spring依赖注入的3种方式常用的注入方式主要有三种：构造方法注入，setter注入，基于注解的注入。 通过构造方法注入bb通过构造方法注入，就相当于给构造方法的参数传值。Bean必须提供带参数的构造函数 set注入的缺点是无法清晰表达哪些属性是必须的，哪些是可选的，构造注入的优势是通过构造强制依赖关系，不可能实例化不 完全的或无法使用的bean。 123456789101112MemberBean定义四个变量，private String name;private Double salary;private Dept dept;private String sex;加上构造方法和toString方法：方便测试Dept：private String dname;private String deptno; 12345678910111213141516171819202122232425262728293031323334353637第一种方法：根据索引赋值，索引都是以0开头的： &lt;bean id=&quot;memberBean&quot; class=&quot;www.csdn.spring01.constructor.MemberBean&quot;&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;刘晓刚&quot; /&gt; &lt;constructor-arg index=&quot;1&quot; value=&quot;3500&quot; /&gt; &lt;constructor-arg index=&quot;2&quot; ref=&quot;dept&quot;/&gt; &lt;constructor-arg index=&quot;3&quot; value=&quot;男&quot; /&gt;第二种方法是根据所属类型传值这种方法基本上不怎么适用，因为一个类里可以有好几个相同基本类型的变量，很容易就混淆值传给哪一个参数了所以做好不要使用这种方法： &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;刘晓刚&quot; &lt;constructor-arg type=&quot;java.lang.Double&quot; value=&quot;3500&quot; /&gt; &lt;constructor-arg type=&quot;www.csdn.spring01.constructor.Dept&quot; ref=&quot;dept&quot;/&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;男&quot; /&gt; 第三种方法：根据参数的名字传值：（推荐用法）在这几种方法里我感觉这种方法是最实用的，他是根据名字来传值的，所以基本上只要名字对了，这个值就可以获取到 &lt;constructor-arg name=&quot;name&quot; value=&quot;刘晓刚&quot; /&gt; &lt;constructor-arg name=&quot;salary&quot; value=&quot;3500&quot; /&gt; &lt;constructor-arg name=&quot;dept&quot; ref=&quot;dept&quot;/&gt; &lt;constructor-arg name=&quot;sex&quot; value=&quot;男&quot; /&gt;第四种方法：直接传值直接给参数赋值，这种方法也是根据顺序排的，所以一旦调换位置的话，就会出现bug，这种方法已经很原始了 &lt;constructor-arg value=&quot;刘晓刚&quot; /&gt; &lt;constructor-arg value=&quot;3500&quot; /&gt; &lt;constructor-arg ref=&quot;dept&quot;/&gt; &lt;constructor-arg value=&quot;男&quot; /&gt; &lt;/bean&gt;&lt;bean id=&quot;dept&quot; class=&quot;www.csdn.spring01.constructor.Dept&quot; &gt; &lt;property name=&quot;dname&quot; value=&quot;北航&quot;/&gt; &lt;property name=&quot;deptno&quot; value=&quot;00001&quot;/&gt;&lt;/bean&gt; 构造函数注入的 Spring IoCIoC 概述IoC (Inversion of Control)，即控制反转，或者称其为依赖注入更好理解一些，不过这个翻译还是比较晦涩难懂的，所以我们举一个吃西红柿炒鸡蛋的例子来说明一下之前我们要吃西红柿炒鸡蛋，是要自己做的，我们得先上下厨房查菜谱，去超市买西红柿（new Tomato），买鸡蛋（new Egg），然后把它俩放搅和到一块，我们才能有西红柿炒鸡蛋吃。就像这样： 可以看到，我们要 5 行代码才能得到 twe 对象（一盘西红柿炒鸡蛋），而且想吃一回，就给重复一次这 5 行代码，真的是相当的麻烦，可是这只是做法相当简单的西红柿炒鸡蛋啊，这要是想吃个圣诞节烤鸡就不要想着自己做了，即使能找到齐全的菜谱，正常人应该也懒得自己做，所以我们选择冲向饭店，也就是 Spring 容器： 现在，我们把 菜谱.xml 给饭店了，只要 Spring 容器初始化好了（饭店开门了），我们就对服务员（ac 对象）说：“给我来一盘西红柿炒鸡蛋”，然后服务员就会给我们端来一盘西红柿炒鸡蛋（ac.getBean(“TomatoWithEgg”)），只要想吃就向服务员要就行，是不是非常的方便呀。 而且这个饭店还是高度定制化的，你想吃啥就把 菜谱.xml 给它，它就会丝毫不差的照着菜谱给你做你想要的菜（构造你想要的对象），如果你想改造一下菜的做法，只消改一下给饭店的 菜谱.xml 就行，然后 Spring 容器就会照着新的 菜谱.xml 给你造对象。这相当于将应用程序的配置和依赖性规范与实际的应用程序代码分开，也就是说，我们现在要是想往西红柿炒鸡蛋里面放土豆块，只消在 菜谱.xml 中的原料表中加上土豆块，然后给 TomatoWithEgg 的属性中加上土豆块就可以了，不用去代码中搜索所有用到 TomatoWithEgg 对象的地方，再给它加上土豆块属性了。 也就是说，可以把 IoC 模式看做工厂模式的升华，IoC 就是一个大工厂，这个大工厂要生成的对象在 XML 中给出定义，然后就可以利用 Java 反射，根据 XML 中给出的类名生成相应的对象。这种做法有比工厂模式高级在哪里呢？以前的工厂模式都是在工厂方法中把对象的生成代码写死，而 Spring 容器则是由 XML 文件来定义类与类之间的关系，将工厂和对象生成两者独立开来。简单来说， Spring IoC 就是我们把类与类之间的依赖关系在 XML 配置文件中写好，就不再自己 new 对象了，之后我们要用哪个对象就向 Spring 要，Spring 会帮我们按照我们定义的依赖关系给我们创建好相应的对象。 技术准备要实现一个简单的 IoC 框架，我们需要使用到如下技术： 解析 XML 文件（dom4j &amp; XPath） 反射和内省（BeanUtils） 我们先在这里简单的写一下我们会用到的地方，至于详细的介绍可以看： 010-XML.md 011-反射与内省.md 首先，我们要导包： 12345678910111213141516171819&lt;!-- 解析 XML 要用到的包 --&gt;&lt;dependency&gt; &lt;groupId&gt;dom4j&lt;/groupId&gt; &lt;artifactId&gt;dom4j&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;jaxen&lt;/groupId&gt; &lt;artifactId&gt;jaxen&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 通过字符串类名创建对应类的对象并且为对象注入属性要用到的包 --&gt;&lt;dependency&gt; &lt;groupId&gt;commons-beanutils&lt;/groupId&gt; &lt;artifactId&gt;commons-beanutils&lt;/artifactId&gt; &lt;version&gt;1.9.3&lt;/version&gt;&lt;/dependency&gt; dom4j 的简单使用1234567891011121314151617// 获取xml配置文件的输入流InputStream xmlConfigFile = ConfigManager.class.getResourceAsStream(path);// 创建xml文件解析器SAXReader reader = new SAXReader();try &#123; // 得到xml文件的Document对象 Document doc = reader.read(xmlConfigFile); // 取出所有&lt;bean&gt;标签 List&lt;Element&gt; beans = doc.selectNodes("//bean"); // 遍历&lt;bean&gt;标签 for (Element bElement : beans) &#123; // 取出name属性值 String bName = bElement.attributeValue("name"); // 取出bean下的&lt;property&gt;标签 List&lt;Element&gt; properties = bElement.elements("property"); &#125;&#125; catch (DocumentException e) &#123;&#125; BeanUtils 的简单使用123User user = new User();BeanUtils.setProperty(user, "username", "admin");BeanUtils.setProperty(user, "password", "admin123"); 实现流程整个实现流程分为以下几步： 读取配置文件，将配置文件信息加载进容器（dom4j）； 根据配置文件初始化容器，创建容器需要创建的 Bean 对象（BeanUtils）； 通过 getBean(String beanName) 方法获取我们想要的 Bean 对象。 解析配置文件我们需要做的一切都是要基于配置文件的，所有先来看一下配置文件的格式： 123456789101112131415161718192021&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans&gt; &lt;bean name="A" class="org.simplespring.model.A"&gt; &lt;property name="strData" value="A的strData"/&gt; &lt;property name="intData" value="123"/&gt; &lt;/bean&gt; &lt;bean name="B" class="org.simplespring.model.B"&gt; &lt;property name="refData" ref="A"/&gt; &lt;/bean&gt; &lt;bean name="C" class="org.simplespring.model.C" scope="prototype"&gt; &lt;property name="strData" value="C的strData"/&gt; &lt;/bean&gt; &lt;bean name="D" class="org.simplespring.model.D"&gt; &lt;constructor-arg index="0" type="java.lang.String" value="D的name属性值"/&gt; &lt;constructor-arg index="1" type="java.lang.String" value="D的value属性值"/&gt; &lt;constructor-arg index="2" type="org.simplespring.model.A" ref="A"/&gt; &lt;/bean&gt;&lt;/beans&gt; 我们配置了 3 个类：A、B、C，其中 A 和 B 是 singleton 的，也就是说是单例的，只在容器初始化时创建一次，每次调用 getBean 返回的都是同一个对象；C 是多例的，即每次调用 getBean 都返回一个新创建的对象。 我们首先需要考虑的事情是，我们解析出来的 xml 文件的内容应该放在哪里呢？我们采用的解决方案是：创建 3 个 Model 类：Bean 、 Property 和 ConstructorArg，专门放解析出来的 &lt;bean&gt; 、 &lt;property&gt; 和 &lt;constructor-arg&gt; 标签。然后我们把解析出来的 Bean 类放到一个 Map&lt;String, Bean&gt; 中，其中 key 是我们配置的 bean 标签的 name，value 就是对应的 Bean 对象。这部分的实现在 org.simplespring.config.parse.ConfigManager.java 中： 方法： public static Map&lt;String, Bean&gt; getConfig(String path) 流程： 创建解析器：SAXReader 加载 XML 文件 定义 xpath 表达式：&quot;//bean&quot;，取出所有 Bean 对 Bean 元素进行遍历，并通过 setBean 方法进行解析 将 &lt;bean&gt; 标签的 name &amp; class &amp; scope 属性封装到 Bean 对象中 调用 setBeanConstructorArg(Element bElement, Bean bean) 方法，解析 &lt;constructor-arg&gt; 标签 调用 setBeanProperties(Element bElement, Bean bean) 方法，解析 &lt;property&gt; 标签 将 Bean 对象封装到 Map res 中（这个 Map 就是我们的返回结果） 返回 Map res &lt;property&gt; 标签的解析较为简单，而 &lt;constructor-arg&gt; 标签的解析却不是那么的容易，因为如果像 &lt;property&gt; 标签一样，在 Bean 中仅仅通过一个 List&lt;Property&gt; 来存储的话，我们之后将很难通过配置文件给出的配置信息来确定到底应该执行哪一个构造函数来创建对象。因此，为了方便后面构造函数匹配的实现，我们在 Bean 中通过以下结构来存储 &lt;constructor-arg&gt; 标签的信息： 12345/** 用来存储通过索引定位的构造函数参数 */private final Map&lt;Integer, ConstructorArg&gt; indexConstructorArgs = new HashMap&lt;&gt;();/** 用来存储不索引定位的构造函数参数 */private final List&lt;ConstructorArg&gt; genericConstructorArgs = new ArrayList&lt;&gt;(); 因此，在解析 &lt;constructor-arg&gt; 标签时，优先解析 index 属性，将解析结果放入 Bean 的 Map&lt;Integer, ConstructorArg&gt; indexConstructorArgs 集合中，如果该标签没有 index 属性，则将解析结果放入 Bean 的 List&lt;ConstructorArg&gt; genericConstructorArgs 集合中。 具体实现详见：ConfigManager.java. 根据配置文件解析结果初始化容器可以成功的解析配置文件之后，我们就可以进行容器的初始化了，我们先构造一个 BeanFactory 接口，容器的实现类都实现于这个接口，这个接口中有一个 getBean(String beanName) 方法，用于从容器中获取我们在配置文件中配置的对象。 123public interface BeanFactory &#123; public Object getBean(String beanName);&#125; 然后，我们写一个这个接口的实现类：org.simplespring.main.impl.ClassPathXmlApplicationContext，来真正的进行容器的初始化。 首先我们要在 ClassPathXmlApplicationContext 的构造函数中调用我们上一小节写的 ConfigManager.getConfig(path) 加载配置文件进来，然后根据配置文件的配置内容，创建相应的对象，并放入一个 Map&lt;String, Object&gt; 中，其中 key 是我们在配置文件中给该对象配置的 name，value 是容器创建的对象，也是我们之后要通过 getBean 从容器中取的东西。 也就是说，ClassPathXmlApplicationContext 的初始化过程中，我们要完成： 读取配置文件中需要初始化的 Bean 信息： ConfigManager.getConfig() 遍历配置对象 Map，初始化所有不是 prototype 的 Bean 通过方法 Object object = createBean(beanInfo); 完成 将初始化好的 Bean 放入 Map&lt;String, Object&gt; beanMap 中 接下来就是实现关键的 createBean 方法了： 方法： private Object createBean(Bean beanInfo) 流程： 判断容器中是否已经存在该实例，如果存在，直接返回即可 获取要创建的 Bean 的 Class，调用 newObject(Bean beanInfo, Class beanClass) 方法创建对象 获取 Bean 需要的属性对象，将其注入到Bean中 value 属性注入：prop.getValue() != null BeanUtils.setProperty(object, property.getName(), property.getValue()) ref 属性注入：prop.getRef() != null 先来判断一下要加载的 ref 类是否已经创建并放入容器中了 不存在：递归调用 createBean 方法 存在：直接从容器中取出并注入 返回创建好的 object 在创建对象时，我们调用了 newObject(Bean beanInfo, Class beanClass) 方法，在没有配置 constructor-arg 时，Spring 容器会选择使用无参构造函数创建对象，及直接调用 beanClass.newInstance() 方法完成对象的创建。但是当配置文件中配置了 constructor-arg 时，容器就需要通过配置文件中 &lt;constructor-arg&gt; 标签的 type 和 index 属性的配置来选择合适的构造函数创建实例了，具体的匹配方法将会在 ConstructorResolver.matchConstructor 方法的实现流程中进行详细说明，我们首先来看一下 newObject 方法的实现，以下是 newObject 方法的详细实现流程： 方法： public Object newObject(Bean beanInfo, Class beanClass) 流程： 如果 beanInfo 中没有 constructor-arg 的配置信息，直接调用 beanClass.newInstance() 创建对象返回 调用 ConstructorResolver.matchConstructor(Bean beanInfo, Class beanClass, BeanFactory beanFactory) 方法搜索匹配的构造函数及其参数列表 如果能搜索到，则通过反射调用构造函数创建对象，如果无法搜索到则抛出异常 createBean 和 newObject 方法的具体实现详见：ClassPathXmlApplicationContext.java. ConstructorResolver.matchConstructor(Bean beanInfo, Class beanClass, BeanFactory beanFactory) 方法的详细实现流程： 方法： public static ArgumentsHolder matchConstructor(Bean beanInfo, Class beanClass, BeanFactory beanFactory) 流程： 从 beanClass 中取出该类所有的构造函数，并对构造函数们按照 public 在前，参数个数多的在前的顺序进行排序，以方便后面的匹配操作 遍历所有的构造函数，并将每个构造函数的参数数组与配置文件中配置的参数进行匹配 首先按照构造函数的入参个数进行匹配 匹配到入参个数相同的构造函数时，根据 beanInfo 中读取的构造函数参数配置信息，生成对应于当前构造函数的参数列表，生成的过程对参数的类型进行匹配，只有每个位置的参数都符合配置文件中配置的构造函数才能作为匹配结果返回，在遍历参数列表过程中： 先获取配置文件中配置在该 index 下的参数 如果当前位置未设置参数，就按照类型搜索参数 如果按照类型也搜索不到就获取 beanInfo.genericConstructorArgs 中第一个没有使用的参数作为该位置的参数 获得当前位置的参数后，对类型匹配判断，如果类型与当前正在匹配的构造函数相应位置的入参相同，则匹配成功，可以继续匹配下一个参数，如果类型匹配不成功，则当前构造函数匹配失败，跳过该构造函数，继续匹配下一个构造函数 ConstructorResolver 类的具体实现详见：ConstructorResolver.java. 通过以上复杂的构造函数匹配流程，我们也可以理解为什么一般建议使用 set/get 方法的方式注入属性了，因为构造函数注入属性的方式除了存在循环依赖的问题，在容器初始化的时候，由于需要匹配合适的构造函数，会增加容器的初始化时间。 getBean 返回所需对象getBean 方法的实现十分简单，流程如下： 方法： public Object getBean(String beanName) 流程： 从 Map configs 中取出该 bean 的配置，判断 scope 属性 scope == “prototype”：调用 createBean 新创建一个对象返回 scope != “prototype”：直接从 beanMap 中取出对象返回 测试我们在 ClassPathXmlApplicationContextTest 进行了简单的测试： 1234567891011121314151617String path = "/applicationContext.xml";BeanFactory factory = new ClassPathXmlApplicationContext(path);A a1 = (A) factory.getBean("A");B b1 = (B) factory.getBean("B");C c1 = (C) factory.getBean("C");A a2 = (A) factory.getBean("A");C c2 = (C) factory.getBean("C");D d1 = (D) factory.getBean("D");System.out.println(a1);System.out.println(b1);System.out.println(c1);System.out.println(a2);System.out.println(c2);System.out.println(d1); 输出结果： 12345678910创建A对象一次创建B对象一次创建C对象一次创建C对象一次A&#123;strData=&apos;A的strData&apos;, intData=123&#125;B&#123;refData=A&#123;strData=&apos;A的strData&apos;, intData=123&#125;&#125;C&#123;strData=&apos;C的strData&apos;&#125;A&#123;strData=&apos;A的strData&apos;, intData=123&#125;C&#123;strData=&apos;C的strData&apos;&#125;D&#123;nameAttribute=&apos;D的name属性值&apos;, valueAttribute=&apos;D的value属性值&apos;, refAttribute=A&#123;strData=&apos;A的strData&apos;, intData=123&#125;, doubleAttribute=0.0&#125; 我们创建了 2 个 A 和 C 对象，一个 B 对象，通过观察输出结果可以发现，A 对象只会被创建一次，而 C 对象被创建了两次（根据构造函数的执行次数判断的，ABC 每个类的构造函数都会 print “创建X对象一次”），并且我们在配置文件中配置的属性也被注入进了相应的对象，A 对象中的 int 型和 String 型数据都被很好的注入了。至此，简单的 Spring IoC 框架实现完毕。 注解注入之前tiny-spring已经实现了通过xml配置类的方式自动装配和依赖注入，现在要给tiny-spring框架加入自动扫描包下的类，再执行自动装配和依赖注入。 流程步骤可以分为： 类加载器获取包路径； 扫描并加载路径下的类集合； 将扫描到的类集合解析成BeanDefinition对象集合； 交给自动装配和依赖注入； 步骤1~2可以归结为获取指定包下面的类集合，然后再解析，最后自动装配和注入。]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基础知识]]></title>
    <url>%2FRedis%2F</url>
    <content type="text"><![CDATA[简单来说 Redis 就是一个数据库，不过与传统数据库不同的是 Redis 的数据是存在内存中的，所以存写速度非常快，因此 Redis 被广泛应用于缓存方向。此外，redis 也经常用来做分布式锁。 为什么要用 Redis高性能 假如用户第一次访问数据库中的某些数据。因为是从硬盘上读取的，所以这个过程会比较慢。我们可以将该用户这次访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发 高并发系统挂掉，多挂在数据库读写处，因为磁盘操作这个慢呀。直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 为什么要用 Redis 而不用 map 做缓存?缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 JVM 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 Redis 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 Redis 服务的高可用，会使整个程序的架构变的较为复杂。 Redis 常见数据结构以及使用场景分析StringString 数据结构是简单的 key-value 类型，value 其实不仅可以是 String，也可以是数字。常规 key-value 缓存应用；常规计数：微博数，粉丝数等。 List双向链表，应用场景：微博的关注列表，粉丝列表，消息列表等。 常用命令1234567891011lpush #在key对应list的头部添加rpush #尾部lpoprpopblpop # 阻塞 pop，bl：blocklrange # LRANGE mylist 0 1，取出 list 0~1 的元素llenlrem #lrem key count value从key对应 list 中删除 count(&gt;0从头到尾,=0所有) 个和 value 相同的元素lindex # 按 index get list 种的元素linsert # LINSERT mylist BEFORE &quot;World&quot; &quot;There&quot;lset # LSET mylist 0 &quot;four&quot;，把第 0 个位置改成 &quot;four&quot; 底层实现链表。 HashHash 是一个 string 类型的 field 和 value 的映射表，又名字典，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以Hash数据结构来存储用户信息，商品信息等等。 比如可以用 hash 类型存放： 1234567key=JavaUser293847value=&#123; "id": 1, "name": "SnailClimb", "age": 22, "location": "Wuhan, Hubei"&#125; 常用命令12345678hset # HSET myhash field1 &quot;Hello&quot;，field1 是 key，&quot;Hello&quot; 是 valuehgethdel # 就 hash 特别，删除叫 del，别人都叫 remhgetAll # 返回所有的 field 和 value，顺序：field1，value1，field2，value2，field3，value3 ...hexistshkeyshvalshsetnx # 字段不存在时才 set，字段存在不 set 底层实现 哈希表，一个字典有两个哈希表（ht[0] &amp; ht[1]），一个是平时用的，一个是 rehash 时用的。 插入一个新的键值对时，会先根据 key 计算出哈希值和索引值，然后把键值对发到对应索引处。 哈希算法：MurmurHash2，该算法即使输入的键是由规律的，也能给出一个很好的随机性，并且速度快。 解决键冲突：链地址法，每个哈希表节点有一个 next 指针，冲突的键会形成一个单链表。 为了让哈希表的负载因子维持在一个合理的范围，当哈希表保存的键值对数量太多或太少时，程序会对哈希表的大小进行相应的扩展或收缩，这个过程叫 rehash。步骤如下： ht[0] 是现在正在用的哈希表，Redis 会根据 ht[0] 中当前包含的键值对个数（ht[0].used）为 ht[1] 分配空间，空间大小取决于： 扩展操作：ht[1].size = 第一个大于 ht[0].used * 2 的 2^n 收缩操作：ht[1].size = 第一个大于 ht[0].used 的 2^n 将 ht[0] 上的键值对 rehash 到 ht[1] 上； 将 ht[1] 设置为 ht[0]，并在 ht[1] 新创建一个空白哈希表用于下一次 rehash。 Set适用于无顺序的集合，点赞点踩，抽奖，已读，共同好友（适合用来去重） 常用命令12345678910sadd # SADD myset one two three，可以一次 add 一坨sinter # 两个集合的交集，SINTER key1 key2，其中 key1 和 key2 是两个 set 名sunion # 两个集合的并集sdiff # 第一个集合 - 交集smemberssismembersrem # 删除元素smove # 把一个set中的元素移动到另一个集合scard # 集合的sizesrandmember # SRANDMEMBER myset n，随机取 n 个，可以用来做抽奖，不写 n 就是随机取一个 底层实现Set 有两种类型，一种是 intset，就是整数集合，另一种是对象集合。 Sorted Set排行榜，优先队列（适合用来排序） 多个节点可以包含相同的 score，不过成员对象必须是唯一的。元素先按照 score 大小进行排序，score 相同时，按照成员对象大小（字典序之类的）进行排序。 常用命令12345678910111213zadd # ZADD myzset 2 &quot;two&quot; 3 &quot;three&quot;zcard # 集合的 sizezcount # 可以计算一个范围内数的 sizezscore # 查询 key 的值zincrby # 做加减法，对不存在的值加分会默认新建该值，并且初始值为 0zrange # ZRANGE myzset 0 -1 WITHSCORES，排行功能，打印排行后的结果zrevrange # 反转 Setzrank # key 的正向排名zrevrank # key 的反向排名# 遍历for (Tuple tuple : jedis.zrangeByScoreWithScores(rankKey, &quot;60&quot;, &quot;100&quot;)) &#123; print(37, tuple.getElement() + &quot;:&quot; + String.valueOf(tuple.getScore()));&#125; 底层实现（跳跃表）传说中的跳跃表。跳跃表（skiplist）是一种随机化的数据，跳跃表以有序的方式在层次化的链表中保存元素， 效率和平衡树媲美 —— 查找、删除、添加等操作都可以在对数期望时间下完成， 并且比起平衡树来说， 跳跃表的实现要简单直观得多。 首先说一下我们的需求，我们要一个有序的列表，因为一个有序的列表搜索起来可以用二分法，快啊！ 所以当我们要插入新元素的时候，就不能直接往表尾一放，我们需要保证把这个节点放进去之后，这个表还是有序的，所以我们的插入操作要分两步来进行： 找：把新节点插哪我们的表还有序 插：把新节点插到我们上一步找到的位置处 这需求一看就是平衡树了，可是红黑树之流实现起来有多复杂，大家也是有目共睹的，所以，Redis 用了一种叫跳跃表的数据结构。我们先来介绍一下 跳跃表（这个小灰讲的超好，我就是在下面挑我关心的重点复述一下）： 首先，本质上来讲，跳跃表还是一个链表的，这样插入和删除就很快啦！不过存链表的查找很慢呀，根本用不了二分法之流，只能从头到尾一个一个的遍历，也就是说，它的查找移动步长只能是 1，那么怎么解决这个问题呢？我们首先想到的方法应该就是想尽一切办法能让查找操作以比较大的步长移动。 那么如何实现大步长移动呢？这就想到了 MySQL 的 InnoDB 的索引的实现方式，我们知道 InnoDB 的一个数据页中的数据和一个链表差不多，不过它有一个叫页目录的东西，这个页目录就是从它所在的页中，以一定的间隔抽出一些节点，作为这个页的目录，这样我们就能大跨度的在单链表中进行查找了。 跳跃表就是基于这个方法实现的大跨度的查找，它从真正的数据中抽出了一些作为一级索引，又从一级索引中抽出一部分作为二级索引，就这样抽抽抽，直到最高层只有两个节点为止（就剩俩了也没有必要继续抽了……）。抽完之后大概是这样的： 那么我们抽索引的时候，抽谁呢？跳跃表是个选择障碍症患者，所以它抛硬币，每插入一个新节点，它都有 50% 的概率被选为索引，所以跳跃表的插入操作的是这样滴，比如我们要在上面的那个表里插个 9： 那么删除呢？删除超简单，比如说我们要删个 5，那么我们从最高层开始找 5： 第一层有 5，删了，然后这层就剩 1，然后干脆把这层都删了…… 第二层有 5，删了 第三层有 5，删了，好了，删光了！ 不过呢，Redis 的实现方式和上面描述的过程还是有区别的，Redis 实现跳跃表用了两种结构体： zskiplistNode：表示跳跃表节点； zskiplist：保存跳跃表的相关信息； 先来看一下 zskiplistNode 的定义： 1234567891011121314151617181920typedef struct zskiplistNode &#123; /* 下面俩货是我们的数据，不多说了 */ robj *obj; // member 对象 double score; // 分值 /* 后退指针，指向当前节点的前一个节点，用于从表尾向表头遍历跳跃表中的所有节点 */ struct zskiplistNode *backward; /* 层数组，实现大幅度跳跃的关键 */ /* 每个节点的层数组长度不一定，是一个 1~32 的随机数，是根据幂次定理随机的，就是说数越大，出现的概率越小 */ /* 每一个节点的同一层组成一个单链表，就是那个前进指针，比如 level[3] 吧，它就指向下一个有 level[3] 的节点 */ struct zskiplistLevel &#123; struct zskiplistNode *forward; // 前进指针 unsigned int span; // 这个层跨越的节点数量 // 不要小看这个东西，这个东西可以拿来计算排位 rank // 在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来， // 得到的结果就是目标节点在跳跃表中的排位 // 要是没有这个东西，鬼知道你在进行大步跨越时，跨越了多少节点啊 &#125; level[];&#125; zskiplistNode; 再来看一下 zskiplist 的定义： 12345typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; // 头节点，尾节点 unsigned long length; // 节点数量 int level; // 目前表内节点的最大层数&#125; zskiplist; 所以 Redis 的表看起来是这样子滴：如果我们想要遍历整个跳跃表，就是把 L1 层的链表从头遍历到尾，过程如下图虚线所示：如果我们想查找一个节点，比如 o2，我们的查找过程如下图虚线所示： Redis 设置过期时间Redis 中有个设置时间过期的功能，即对存储在 Redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 对于这种数据，我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。这样就不需要我们再在应用中手动判断这个 key 是否过期，而是将这个任务交给 Redis 来做。 过期时间设置方式：EXPIRE &lt;key&gt; &lt;ttl&gt;原子性的命令：我们是不管了，那么 Redis 到底是怎么删除这个过期数据的呢？ Redis 删除过期数据的方式： 定期删除 + 惰性删除 。 定期删除： Redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。 为什么要随机呢？你想一想假如 Redis 存了几十万个 key ，如果每隔 100ms 就遍历所有的设置过期时间的 key 的话，CPU怕是要废了…… 惰性删除： 定期删除可能会导致很多过期 key 到了时间并没有被删除掉，所以就有了惰性删除。惰性删除就是：假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，这时候你的系统去查了一下那个 key，Redis 会先检查下数据过期没，如果过期了，就会被 Redis 删除掉。 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？当然会有好多过期的 key 堆积在内存中了，如果大量过期 key 堆积在内存里，是导致 Redis 内存块耗尽的。那么怎么解决这个问题呢？这就要依靠 Redis 的内存淘汰机制了。 Redis 内存淘汰机制即 MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，那么如何保证 Redis 中的数据都是热点数据呢？ Redis 提供了如下 8 种数据淘汰策略： 策略 描述 volatile-lru 从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 allkeys-lru 从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 volatile-lfu 从已设置过期时间的数据集（server.db[i].expires）中挑选使用频率最小的数据淘汰 allkeys-lfu 从数据集（server.db[i].dict）中挑选挑选使用频率最小的数据淘汰 volatile-random 从已设置过期时间的数据集（server.db[i].expires）中挑选任意选择数据淘汰 allkeys-random 从数据集（server.db[i].dict）中挑选任意选择数据淘汰 volatile-ttl 从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 noeviction (Default) 禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错 设置方式：在配置文件 redis.conf 中写入 maxmemory-policy 策略名。 记忆： LRU：Least Recently Used LFU：Least Frequently Used volatile：已设置过期时间的数据集（server.db[i].expires） allkeys：数据集（server.db[i].dict） Redis 持久化机制即怎么保证 Redis 挂掉之后再重启数据可以进行恢复？ Redis 是支持持久化的，而且还支持两种，它们分别是： 快照（snapshotting，RDB） 只追加文件（append-only file，AOF） RDB 持久化Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis 主从结构，主要用来提高 Redis 性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是 Redis 默认采用的持久化方式，在 redis.conf 配置文件中默认有此下配置： 123save 900 1 # 在 900 秒(15分钟)之后，如果至少有 1 个 key 发生变化，Redis 就会自动触发 BGSAVE 命令创建快照save 300 10 # 在 300 秒(5分钟)之后，如果至少有 10 个 key 发生变化，Redis 就会自动触发 BGSAVE 命令创建快照save 60 10000 # 在 60 秒(1分钟)之后，如果至少有 10000 个 key 发生变化，Redis 就会自动触发 BGSAVE 命令创建快照 BGSAVE原理：copy-on-write AOF 持久化与快照持久化相比，AOF 持久化的实时性更好，因此已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF 方式的持久化，需要通过 appendonly 参数开启：appendonly yes开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入硬盘中的 AOF 文件。AOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。 在 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是：123appendfsync always # 每次有数据修改发生时都会写入 AOF 文件，这样会严重降低 Redis 的速度appendfsync everysec # 每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no # 让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis 还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 Redis 事务Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 一个事务从开始到结束会经历以下三个阶段： 事务开始 命令入队 事务执行 流程如下： 123456MULTI # 事务开始SET "name" "xxx" # 放到事务队列的 index 0 处GET "name" # 放到事务队列的 index 1 处SET "author" "Bean" # 放到事务队列的 index 2 处GET "author" # 放到事务队列的 index 3 处EXEC # 从 index 0 到 index 3 执行事务队列中的命令，并将 4 个命令的结果返回客户端 那么 WATCH 命令是干嘛的呢？它是一个乐观锁，可以在 EXEC 命令执行前，监视任意数量的数据库键，在 EXEC 命令执行时，只要有一个被监视的键发生了变化，服务器就会拒绝执行事务，并返回：(nil) 表示事务执行失败。 在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的可靠性和安全性。在 Redis 中，事务总是具有原子性（Atomicity)、一致性（Consistency）和隔离性（Isolation），并且当 Redis 运行在某种特定的持久化模式下时，事务也具有持久性（Durability）。 缓存穿透、缓存雪崩、缓存击穿问题的解决方案缓存穿透问题描述： 缓存穿透是指查询一个一定不存在的数据，由于缓存是命中时被动写的，这个数据不存在，所以缓存肯定没有，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能 DB 就挂掉了，要是有人利用不存在的 key 频繁攻击我们的应用，这就是漏洞。 解决方法的思路： 要能快速的判断出一个 key 在我们的系统到底存不存在，数据不存在，就不去 DB 查了。 解决方法：布隆过滤器。 将所有可能存在的数据哈希到一个足够大的 bitmap 中，要查询一个 key 之前，都先用这个 bitmap 判断一下存不存在，数据不存在就不去 DB 查了，从而避免了对底层存储系统的查询压力。 另外也有一个更为简单粗暴的方法，即如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 缓存雪崩问题描述： 缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到 DB，DB 瞬时压力过重雪崩。 解决方法： 将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如 1-5 分钟的随机时间，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 缓存击穿问题描述： 对于一些设置了过期时间的 key，如果这些 key 可能会在某些时间点被超高并发地访问，是一种非常 “热点” 的数据。但当缓存在某个时间点过期的时候，如果恰好在这个时间点有对这个 key 的大量并发请求过来，这些请求发现缓存过期了，就会选择从后端 DB 加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端 DB 压垮。 有如下 3 种解决方法： 使用互斥锁（mutex key）就是在缓存失效的时候（判断拿出来的值为空），不是立即去 load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如 Redis 的 SETNX）去 set 一个 mutex key，当操作返回成功时，再进行 load db 的操作并回设缓存；否则，就重试整个 get 缓存的方法（此时其他成功的请求可能已经把缓存更新好了，这个请求就可以成功的 get 到了）。 也就是说，我们只放一个请求去 load DB，把其他的请求都拦在了缓存层。 “提前” 使用互斥锁在这个 key 的 value 内部设置 1 个超时值 (timeout1)，timeout1 比实际的 timeout (timeout2) 要小。当从 cache 读取到 timeout1 发现它已经过期时候，马上延长 timeout1 并重新设置到 cache。然后再从数据库加载数据并设置到 cache 中。 也就是说，我们根本不让这个热点数据在缓存中不存在，热点数据快过期了就更新一下它，不让它真正的过期。 永远不过期缓存在 Redis 中的热点数据根本不设置过期时间，把过期时间存在 key 对应的 value 里，如果发现要过期了，通过一个后台的异步线程更新缓存。 如何解决 Redis 的并发竞争 Key 问题所谓 Redis 的并发竞争 key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（但如果不存在 Redis 的并发竞争 key 问题，不要使用分布式锁，会影响性能）。 分布式锁应该是怎么样的： setnx k v 互斥性：可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。 这把锁要是一把可重入锁（避免死锁） 不会发生死锁：有一个客户端在持有锁的过程中崩溃而没有解锁，也能保证其他客户端能够加锁。 获取锁和释放锁的性能要好 基于数据库表基于 Redis 的分布式锁基于 Zookeeper 实现分布式锁基于 Zookeeper 临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在 Zookeeper 上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 如何保证缓存与数据库双写时的数据一致性？你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，就是读的时候肯定不写，写的时候肯定不读，这样就可以保证一定不会出现不一致的情况，但是这会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 最经典的缓存 + 数据库读写的模式：Cache Aside Pattern。 即： 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候， 先更新数据库，然后再删除缓存 。（一个 lazy 计算的思想） 为什么是删除缓存，而不是更新缓存？ 很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值，而可能是多个数据库中的结果计算出来的。比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 数据库频繁更新的数据不一定是被频繁访问的数据，这种情况下，数据库更新一次就修改一次缓存是很不值得的。 Cache Aside Pattern 可能会出现的问题： 1. 先修改数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。 解决思路： 先删除缓存，再修改数据库。如果数据库修改失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中。虽然数据还是旧的，不过至少数据库和缓存是一致的。 2. 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了… 更新数据的时候，根据 数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据 + 更新缓存的操作，根据唯一标识路由之后，也发送同一个 jvm 内部队列中。 一个队列对应一个工作线程，每个工作线程 串行 拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时即使一个读请求过来，读到了空的缓存，它也会先将缓存更新的请求发送到队列中，等待缓存更新完成，而不是拿了旧数据就走，走前还不忘把旧数据放缓存里…… 优化点： 一个队列中，多个更新缓存请求串在一起是没意义的，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值，但不会把这个旧值设置到缓存中去。 用Redis做消息队列列表。rpush推入、lpop取出缺点：没有等待队列里有值就直接消费。弥补：应用层sleep，然后lpop重试 或blpop list seconds 发布订阅：subscribe、publish。但是消息无状态，不保证可达。需要用kafka什么的 参考： 如何保证缓存与数据库双写时的数据一致性？ 分布式锁解决并发的三种实现方式 缓存穿透，缓存击穿，缓存雪崩解决方案分析 如何保证redis中存放的都是热点数据当redis使用的内存超过了设置的最大内存时，会触发redis的key淘汰机制，在redis 3.0中有6种淘汰策略： noeviction: 不删除策略。当达到最大内存限制时, 如果需要使用更多内存,则直接返回错误信息。（redis默认淘汰策略）allkeys-lru: 在所有key中优先删除最近最少使用(less recently used ,LRU) 的 key。allkeys-random: 在所有key中随机删除一部分 key。volatile-lru: 在设置了超时时间（expire ）的key中优先删除最近最少使用(less recently used ,LRU) 的 key。volatile-random: 在设置了超时时间（expire）的key中随机删除一部分 key。volatile-ttl: 在设置了超时时间（expire ）的key中优先删除剩余时间(time to live,TTL) 短的key。场景： 数据库中有1000w的数据，而redis中只有50w数据，如何保证redis中10w数据都是热点数据？ 方案： 限定 Redis 占用的内存，Redis 会根据自身数据淘汰策略，留下热数据到内存。所以，计算一下 50W 数据大约占用的内存，然后设置一下 Redis 内存限制即可，并将淘汰策略为volatile-lru或者allkeys-lru。 设置Redis最大占用内存： 打开redis配置文件，设置maxmemory参数，maxmemory是bytes字节类型12345678910# In short... if you have slaves attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for slave# output buffers (but this is not needed if the policy is &apos;noeviction&apos;).## maxmemory &lt;bytes&gt;maxmemory 268435456设置过期策略maxmemory-policy volatile-lru]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker简介]]></title>
    <url>%2FDocker%2F</url>
    <content type="text"><![CDATA[一句话概括容器：容器就是将软件打包成标准化单元，以用于开发、交付和部署。 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。 容器化软件适用于基于Linux和Windows的应用，在任何环境中都能够始终如一地运行。 容器赋予了软件独立性 ，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。 拿内存举例，虚拟机是利用Hypervisor去虚拟化内存，整个调用过程是虚拟内存-&gt;虚拟物理内存-&gt;真正物理内存，但是Docker是利用Docker Engine去调用宿主的的资源，这时候过程是虚拟内存-&gt;真正物理内存。 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便.虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker通常用于隔离不同的应用 ，例如前端，后端以及数据库。 镜像（Image）——一个特殊的文件系统Docker 镜像（Image）就是一个只读的模板。例如：一个镜像可以包含一个完整的操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而Docker 镜像（Image），就相当于是一个 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 Docker 设计时，就充分利用 Union FS的技术，将其设计为 分层存储的架构 。 镜像实际是由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。 比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 容器（Container)——镜像运行时的实体镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。 仓库（Repository）——集中存放镜像文件的地方镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。 一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是Docker用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。 这里补充一下Docker Registry 公开服务和私有 Docker Registry的概念： Docker Registry 公开服务 是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 常用命令docker ps列出所有运行中容器。 进入应用内部的系统 4f是编号docker exec -it 4f bash docker网络类型：bridge（和主机独立） host none bridge需要端口映射 docker run -d -p 8888:8080 test/tomcat -d：表示指定容器后台运行 -p：表示宿主机的8080端口对外映射暴露为8888端口 docker rmdocker rm [options “o”&gt;] “o”&gt;[container…]docker rm nginx-01 nginx-02 db-01 db-02sudo docker rm -l /webapp/redis-f 强行移除该容器，即使其正在运行；-l 移除容器间的网络连接，而非容器本身；-v 移除与容器关联的空间。 docker start|stop|restart docker imagesdocker images [options “o”&gt;] [name]列出本地所有镜像。其中 [name] 对镜像名称进行关键词查询。]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列 RabbitMQ基础知识]]></title>
    <url>%2FRabbitMQ%2F</url>
    <content type="text"><![CDATA[消息队列使用消息队列主要有两点好处： 通过异步处理提高系统性能（削峰、减少响应所需时间）; 降低系统耦合性。 通过异步处理提高系统性能（削峰、减少响应所需时间） 在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 降低系统耦合性 对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的。除了发布-订阅模式，还有点对点订阅模式（一个消息只有一个消费者），我们比较常用的是发布-订阅模式。 另外，这两种消息模型是 JMS 提供的，AMQP 协议还提供了 5 种消息模型 使用消息队列带来的一些问题 系统可用性降低： 系统可用性在某种程度上降低，为什么这样说呢？在加入MQ之前，你不用考虑消息丢失或者说MQ挂掉等等的情况，但是，引入MQ之后你就需要去考虑了！ 系统复杂性提高： 加入MQ之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消- 息传递的顺序性等等问题！ 一致性问题： 我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了! JMSJMS（JAVA Message Service,java消息服务）是java的消息服务，JMS的客户端之间可以通过JMS服务进行异步的消息传输。JMS（JAVA Message Service,Java消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 ActiveMQ 就是基于 JMS 规范实现的。 JMS两种消息模型①点到点（P2P）模型 （Queue）作为消息通信载体；满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。比如：我们生产者发送100条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。） ② 发布/订阅（Pub/Sub）模型 （Topic）作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。 JMS 五种不同的消息正文格式JMS定义了五种不同的消息正文格式，以及调用的消息类型，允许你发送并接收以一些不同形式的数据，提供现有消息格式的一些级别的兼容性。 StreamMessage – Java原始值的数据流 MapMessage–一套名称-值对 TextMessage–一个字符串对象 ObjectMessage–一个序列化的 Java对象 BytesMessage–一个字节的数据流 RabbitMQAMQP，即 Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。消息中间件主要用于组件之间的解耦和通讯。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。RabbitMQ是一个开源的AMQP实现，服务器端用 Erlang 语言编写，支持多种客户端，如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持AJAX。用于在分布式系统中存储转发消息，具有很高的易用性和可用性。 ConnectionFactory、Connection、Channel 都是RabbitMQ对外提供的API中最基本的对象。 ConnectionFactory：ConnectionFactory为Connection的制造工厂。 Connection：Connection是RabbitMQ的socket链接，它封装了socket协议相关部分逻辑。 Channel(信道)：信道是建立在“真实的”TCP连接上的虚拟连接，在一条TCP链接上创建多少条信道是没有限制的，把他想象成光纤就是可以了。它是我们与RabbitMQ打交道的最重要的一个接口，我们大部分的业务操作是在Channel这个接口中完成的，包括定义Queue、定义Exchange、绑定Queue与Exchange、发布消息等。 queue队列生产者Send Message “A”被传送到Queue中，消费者发现消息队列Queue中有订阅的消息，就会将这条消息A读取出来进行一些列的业务操作。这里只是一个消费正对应一个队列Queue，也可以多个消费者订阅同一个队列Queue，当然这里就会将Queue里面的消息平分给其他的消费者，但是会存在一个一个问题就是如果每个消息的处理时间不同，就会导致某些消费者一直在忙碌中，而有的消费者处理完了消息后一直处于空闲状态，因为前面已经提及到了Queue会平分这些消息给相应的消费者。这里我们就可以使用prefetchCount来限制每次发送给消费者消息的个数。详情见下图所示：生产者在将消息发送给 Exchange 的时候，一般会指定一个 routing key，来指定这个消息的路由规则。 Exchange 会根据 routing key 和 Exchange Type（交换器类型） 以及 Binding key 的匹配情况来决定把消息路由到哪个 Queue。RabbitMQ为routing key设定的长度限制为255 bytes。 RabbitMQ常用的Exchange Type有 Fanout、 Direct、 Topic、 Headers 这四种。 Fanout 这种类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中，这时 Routing key 不起作用。 Direct 这种类型的Exchange路由规则也很简单，它会把消息路由到那些 binding key 与 routing key完全匹配的Queue中。 当生产者（P）发送消息时Rotuing key=booking时，这时候将消息传送给Exchange，Exchange获取到生产者发送过来消息后，会根据自身的规则进行与匹配相应的Queue，这时发现Queue1和Queue2都符合，就会将消息传送给这两个队列，如果我们以Rotuing key=create和Rotuing key=confirm发送消息时，这时消息只会被推送到Queue2队列中，其他Routing Key的消息将会被丢弃。 Topic 这种类型的Exchange的路由规则支持 binding key 和 routing key 的模糊匹配，会把消息路由到满足条件的Queue。 binding key 中可以存在两种特殊字符 *与 #，用于做模糊匹配，其中 *用于匹配一个单词，# 用于匹配0个或多个单词，单词以符号.为分隔符。 Headers 这种类型的Exchange不依赖于 routing key 与 binding key 的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配,对比其中的键值对是否完全匹配Queue与Exchange绑定时指定的键值对；如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。 Message durability（消息的持久化） 如果我们希望即使在RabbitMQ服务重启的情况下，也不会丢失消息，我们可以将Queue与Message都设置为可持久化的（durable），这样可以保证绝大部分情况下我们的RabbitMQ消息不会丢失。但依然解决不了小概率丢失事件的发生（比如RabbitMQ服务器已经接收到生产者的消息，但还没来得及持久化该消息时RabbitMQ服务器就断电了），如果我们需要对这种小概率事件也要管理起来，那么我们要用到事务。由于这里仅为RabbitMQ的简单介绍，所以这里将不讲解RabbitMQ相关的事务。具体可以参考 RabbitMQ之消息确认机制（事务+Confirm） 接口优化===同步下单改为异步下单收到请求后由redis先预检库存，不足的话直接返回，减少数据库访问如果有库存，则请求放到消息队列。返回一个排队中请求出队。客户端轮询是否秒杀成功 Redis与RabbitMQ作为消息队列的比较可靠消费Redis：没有相应的机制保证消息的消费，当消费者消费失败的时候，消息体丢失，需要手动处理RabbitMQ：具有消息消费确认，即使消费者消费失败，也会自动使消息体返回原队列，同时可全程持久化，保证消息体被正确消费 可靠发布Reids：不提供，需自行实现RabbitMQ：具有发布确认功能，保证消息被发布到服务器 高可用Redis：采用主从模式，读写分离，但是故障转移还没有非常完善的官方解决方案RabbitMQ：集群采用磁盘、内存节点，任意单点故障都不会影响整个队列的操作 持久化Redis：将整个Redis实例持久化到磁盘RabbitMQ：队列，消息，都可以选择是否持久化 消费者负载均衡Redis：不提供，需自行实现RabbitMQ：根据消费者情况，进行消息的均衡分发 队列监控Redis：不提供，需自行实现RabbitMQ：后台可以监控某个队列的所有信息，（内存，磁盘，消费者，生产者，速率等） 流量控制Redis：不提供，需自行实现RabbitMQ：服务器过载的情况，对生产者速率会进行限制，保证服务可靠性 出入队性能对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。注：此数据来源于互联网，部分数据有误，已修正 应用场景分析Redis：轻量级，高并发，延迟敏感即时数据分析、秒杀计数器、缓存等 RabbitMQ：重量级，高并发，异步批量数据异步处理、并行任务串行化，高负载任务的负载均衡等]]></content>
      <tags>
        <tag>系统</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库基础知识]]></title>
    <url>%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[索引事务隔离级别锁SQL joinInnodb和MyISAM引擎MyCAT 索引数据结构： 二叉树：O(logn),深度会深，io多 B树、B+树 Hash：哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快。哈希索引有好几个局限(根据他本质的原理可得)： 哈希索引也没办法利用索引完成排序 不支持最左匹配原则 在有大量重复键值情况下，哈希索引的效率也是极低的—-&gt;哈希碰撞问题。 不支持范围查询 innoDB密集索引：每个搜索码值都对应一个索引值Myisam稀疏索引：只为索引码的某些值建立索引项 如何定位并优化慢查询sql：根据慢日志定位慢查询SQL使用explain等工具分析SQL修改SQL或尽量让SQL走索引 没有索引的查找本集的主题是索引，在正式介绍索引之前，我们需要了解一下没有索引的时候是怎么查找记录的。为了方便大家理解，我们下边先只唠叨搜索条件为对某个列精确匹配的情况，所谓精确匹配，就是搜索条件中用等于=连接起的表达式，比如这样： 1SELECT [列名列表] FROM 表名 WHERE 列名 = xxx; 在一个页中的查找假设目前表中的记录比较少，所有的记录都可以被存放到一个页中，在查找记录的时候可以根据搜索条件的不同分为两种情况： 以主键为搜索条件这个查找过程我们已经很熟悉了，可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录。 以其他列作为搜索条件对非主键列的查找的过程可就不这么幸运了，因为在数据页中并没有对非主键列建立所谓的页目录，所以我们无法通过二分法快速定位相应的槽。这种情况下只能从最小记录开始依次遍历单链表中的每条记录，然后对比每条记录是不是符合搜索条件。很显然，这种查找的效率是非常低的。 在很多页中查找大部分情况下我们表中存放的记录都是非常多的，需要好多的数据页来存储这些记录。在很多页中查找记录的话可以分为两个步骤： 定位到记录所在的页。 从所在的页内中查找相应的记录。 不论是根据主键列或者其他列的值进行查找，由于我们并不能快速的定位到记录所在的页，所以只能从第一个页沿着双向链表一直往下找，在每一个页中根据我们上边已经唠叨过的查找方式去查找指定的记录。因为要遍历所有的数据页，所以这种方式显然是超级耗时的，如果一个表有一亿条记录，使用这种方式去查找记录那要等到猴年马月才能等到查找结果。 为了故事的顺利发展，我们先建一个表： 1234567mysql&gt; CREATE TABLE index_demo( -&gt; c1 INT, -&gt; c2 INT, -&gt; c3 CHAR(1), -&gt; PRIMARY KEY(c1) -&gt; ) ROW_FORMAT = Compact;Query OK, 0 rows affected (0.03 sec)mysql&gt; 这个新建的index_demo表中有2个INT类型的列，1个CHAR(1)类型的列，而且我们规定了c1列为主键，这个表使用Compact行格式来实际存储记录的。为了我们理解上的方便，我们简化了一下index_demo表的行格式示意图： 我们只在示意图里展示记录的这几个部分： record_type：记录头信息的一项属性，表示记录的类型，0表示普通记录、2表示最小记录、3表示最大记录、1我们还没用过，等会再说～ next_type：记录头信息的一项属性，表示下一条地址的偏移量，为了方便大家理解，我们都会用箭头来表明下一条记录是谁。 各个列的值：就是各个数据列的值，其中我们用橘黄色的格子代表c1列，深蓝色的格子代表c2列，红色格子代表c3列。 其他信息：除了上述3种信息以外的所有信息，包括其他隐藏列的值以及记录的额外信息。 为了节省篇幅，我们之后的示意图中会把记录的其他信息这个部分省略掉，因为它占地方并且不会有什么观赏效果。另外，为了方便理解，我们觉得把记录竖着放看起来感觉更好，所以将记录格式示意图的其他信息去掉并把它竖起来的效果就是这样： 一个简单的索引方案回到正题，我们为什么要遍历所有的数据页呢？因为各个页中的记录并没有规律，我们并不知道我们的搜索条件匹配哪些页中的记录，所以 不得不 依次遍历。所以如果我们想快速的定位到需要查找的记录在哪些数据页中该咋办？就像为数据页中的记录建立一个目录一样，我们也可以为所有的数据页建立一个目录呀，建这个目录必须完成下边这些事儿： 下一个数据页的主键值必须大于上一个页中的主键值。 其实这句话的完整表述是这样的：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。为了故事的顺利发展，我们这里需要做一个假设：假设我们的每个数据页最多能存放3条记录（实际上一个数据页非常大，可以存放下好多记录）。有了这个假设之后我们向index_demo表插入3条记录： 1234mysql&gt; INSERT INTO index_demo VALUES(1, 4, &apos;u&apos;), (3, 9, &apos;d&apos;), (5, 3, &apos;y&apos;);Query OK, 3 rows affected (0.01 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; 那么这些记录已经按照主键值的大小串联成一个单向链表了，如图所示： 从途中可以看出来，index_demo表中的3条记录都被插入到了编号为10的数据页中了。此时我们再来插入一条记录： 123mysql&gt; INSERT INTO index_demo VALUES(4, 4, &apos;a&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; 因为页10最多只能放3条记录，所以我们不得不再分配一个新页： 需要注意的一点是，新分配的数据页编号可能并不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着。它们只是通过维护着上一个页和下一个页的编号而建立了链表关系。另外，页10中用户记录最大的主键值是5，而页28中有一条记录的主键值是4，因为5 &gt; 4，所以这就不符合下一个数据页的主键值必须大于上一个页中的主键值的要求，所以在插入主键值为4的记录的时候需要伴随着一次记录移动，也就是把主键值为5的记录移动到页28中，然后再把主键值为4的记录插入到页10中，这个过程的示意图如下： 这个过程表明了在对页中的记录进行增删改操作的过程中，我们必须通过一些诸如记录移动的操作来始终保证这个状态一直成立：下一个数据页的主键值必须大于上一个页中的主键值。 给所有的页建立一个目录项。由于数据页的编号可能并不是连续的，所以在向index_demo表中插入许多条记录后，可能是这样的效果： 因为这些16KB的页在物理存储上并不挨着，所以如果想从这么多页中根据主键值快速定位某些记录所在的页，我们需要给它们做个目录，每个页对应一个目录项，每个目录项包括下边两个部分： 页的用户记录中最小的主键值，我们用key来表示。 页号，我们用page_no表示。 所以我们为上边几个页做好的目录就像这样子： 以页28为例，它对应目录项2，这个目录项中包含着该页的页号28以及该页中用户记录的最小主键值5。我们只需要把几个目录项在物理存储器上连续存储，比如把他们放到一个数组里，就可以实现根据主键值快速查找某条记录的功能了。比方说我们想找主键值为20的记录，具体查找过程分两步： 先从目录项中根据二分法快速确定出主键值为20的记录在目录项3中（因为 12 &lt; 20 &lt; 209），它对应的页是页9。 再根据前边说的在页中查找记录的方式去页9中定位具体的记录。 至此，针对数据页做的简易目录就搞定了。不过忘了说了，这个目录有一个别名，称为索引。 InnoDB中的索引方案上边之所以称为一个简易的索引方案，是因为我们假设所有目录项都可以在物理存储器上连续存储，但是这样做有几个问题： InnoDB是使用页来作为管理存储空间的基本单位，也就是最多能保证16KB的连续存储空间，而随着表中记录数量的增多，需要非常大的连续的存储空间才能把所有的目录项都放下，这对记录数量非常多的表是不现实的。 我们时常会对记录进行增删，假设我们把页28中的记录都删除了，页28也就没有存在的必要了，那意味着目录项2也就没有存在的必要了，这就需要把目录项2后的目录项都向前移动一下，这种牵一发而动全身的设计不是什么好主意。 所以，设计InnoDB的大叔们需要一种可以灵活管理所有目录项的方式。他们灵光乍现，忽然发现这些目录项其实长得跟我们的用户记录差不多，只不过目录项中的两个列是主键和页号而已，所以他们复用了之前存储用户记录的数据页来存储目录项，为了和用户记录做一下区分，我们把这些用来表示目录项的记录称为目录项记录。那InnoDB怎么区分一条记录是普通的用户记录还是目录项记录呢？别忘了记录头信息里的record_type属性，它的各个取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 原来这个值为1的record_type是这个意思呀，我们把前边使用到的目录项放到数据页中的样子就是这样： 从图中可以看出来，我们新分配了一个编号为30的页来专门存储目录项记录。这里再次强调一遍目录项记录和普通的用户记录的不同点： 目录项记录的record_type值是1，而普通用户记录的record_type值是0。 目录项记录只有主键值和页的编号两个列，而普通的用户记录的列是用户自己定义的，可能包含很多列，另外还有InnoDB自己添加的隐藏列。 还记得我们之前在唠叨记录头信息的时候说过一个叫min_rec_mask的属性么，只有在存储目录项记录的页中的主键值最小的目录项记录的min_rec_mask值为1，其他别的记录的min_rec_mask值都是0。 除了上述几点外，这两者就没啥差别了，它们用的是一样的数据页（页面类型都是0x45BF，这个属性在Page Header中，忘了的话可以翻到前边的文章看），页的组成结构也是一样一样的（就是我们前边介绍过的7个部分），都会为主键值生成Page Directory（页目录）以加快在页内的查询速度。所以现在根据某个主键值去查找记录的步骤可以大致拆分成下边两步，以查找主键为20的记录为例（因为都是从一个页中通过主键查某条记录，所以都可以使用Page Directory通过二分法而实现快速查找）： 先到存储目录项记录的页中通过二分法快速定位到对应目录项，因为12 &lt; 20 &lt; 209，所以定位到对应的记录所在的页就是页9. 从页9中根据二分法快速定位到主键值为20的用户记录。 虽然说目录项记录中只存储主键值和对应的页号，比用户记录需要的存储空间小多了，但是不论怎么说一个页只有16KB大小，能存放的目录项记录也是有限的，那如果表中的数据太多，以至于一个数据页不足以存放所有的目录项记录，该咋办呢？ 当然是再多整一个存储目录项记录的页喽～ 为了大家更好的理解如何新分配一个目录项记录页的过程，我们假设一个存储目录项记录的页最多只能存放4条目录项记录（请注意是假设哦，真实情况下可以存放好多条的），所以如果此时我们再向上图中插入一条主键值为320的用户记录的话，那就需要一个分配一个新的存储目录项记录的页： 从图中可以看出，我们插入了一条主键值为320的用户记录之后新生成了2个数据页，以查找主键值为20的记录为例： 为存储该用户记录而新生成了页31。 因为原先存储目录项记录的页30的容量已满（我们前边假设只能存储4条目录项记录），所以不得不新生成了一个页32来存放页31对应的目录项。 因为存储目录项记录的页不止一个，所以如果我们想根据主键值查找一条用户记录大致需要3个步骤： 确定目录项记录页我们现在的存储目录项记录的页有两个，即页30和页32，又因为页30表示的目录项的主键值的范围是[1, 320)，页32表示的目录项的主键值不小于320，所以主键值为20的记录对应的目录项记录在页30中。 通过目录项记录页确定用户记录真实所在的页。在一个存储目录项记录中定位一条目录项记录的方式说过了，不赘述了～ 在真实存储用户记录的页中定位到具体的记录。在一个存储用户记录的页中定位一条真实的用户记录的方式已经说过200遍了，你再不会我就，我就，我就求你到上一篇唠叨数据页结构的文章中多看几遍，求你了～ 那么问题来了，在这个查询步骤的第1步中我们需要定位存储目录项记录的页，但是这些页在存储空间中也可能不挨着，如果我们表中的数据非常多则会产生很多存储目录项记录的页，那我们怎么根据主键值快速定位一个存储目录项记录的页呢？其实也简单，为这些存储目录项记录的页再生成一个更高级的目录，就像是一个多级目录一样，大目录里嵌套小目录，小目录里才是实际的数据，所以现在各个页的示意图就是这样子： 如图，我们生成了一个存储更高级目录项的页33，这个页中的两条记录分别代表页30和页32，如果用户记录的主键值在[1, 320)之间，则到页30中查找更详细的目录项记录，如果主键值不小于320的话，就到页32中查找更详细的目录项记录。不过这张图好漂亮喔，随着表中记录的增加，这个目录的层级会继续增加，如果简化一下，那么我们可以用下边这个图来描述它（B+tree）： 因为我们把数据页都存放到B+树这个数据结构中了，所以我们也把我们的数据页称为节点。从图中可以看出来，我们的实际用户记录其实都存放在B+树的最底层的节点上，这些节点也被称为叶子节点或叶节点，其余的节点都是用来存放目录项的，这些节点统统被称为内节点或者说非叶节点。其中最上边的那个节点也称为根节点。 从图中可以看出来，一个B+树的节点其实可以分成好多层，设计InnoDB的大叔们为了讨论方便，规定最下边的那层，也就是存放我们用户记录的那层为第0层，之后依次往上加。上边我们做了一个非常极端的假设，存放用户记录的页最多存放3条记录，存放目录项记录的页最多存放4条记录，其实真实环境中一个页存放的记录数量是非常大的，假设，假设，假设所有的数据页，包括存储真实用户记录和目录项记录的页，都可以存放1000条记录，那么： 如果B+树只有1层，也就是只有1个用于存放用户记录的节点，最多能存放1000条记录。 如果B+树有2层，最多能存放1000×1000=1000000条记录。 如果B+树有3层，最多能存放1000×1000×1000=1000000000条记录。 如果B+树有4层，最多能存放1000×1000×1000×1000=1000000000000条记录。 你的表里能存放1000000000000条记录么？所以一般情况下，我们用到的B+树都不会超过4层，那我们通过主键去查找某条记录最多只需要做4个页面内的查找，又因为在每个页面内有所谓的Page Directory（页目录），所以在页面内也可以通过二分法实现快速定位记录。 聚簇索引我们上边介绍的B+树本身就是一个目录，或者说本身就是一个索引。它有两个特点： 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的主键大小顺序排成一个双向链表。 各个存放目录项的页也是根据页中记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。所谓完整的用户记录，就是指这个记录中存储了所有列的值。 我们把具有这两种特性的B+树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL语句中显式的去创建，InnoDB存储引擎会自动的为我们创建聚簇索引。另外有趣的一点是，在InnoDB存储引擎中，聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点），也就是所谓的索引即数据。 二级索引大家有木有发现，上边介绍的聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件可以多建几棵B+树，不同的B+树中的数据采用不同的排序规则。比方说我们用c2列的大小作为数据页、页中记录的排序规则，再建一棵B+树，效果如下图所示： 这个B+树与上边介绍的聚簇索引有几处不同： 使用记录c2列的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照c2列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 各个存放目录项的页也是根据页中记录的c2列大小顺序排成一个双向链表。 B+树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。 所以如果我们现在想通过c2列的值查找某些记录的话就可以使用我们刚刚建好的这个B+树了，以查找c2列的值为4的记录为例，查找过程如下： 确定目录项记录页根据根页面，也就是页44，可以快速定位到目录项记录所在的页为页42（因为2 &lt; 4 &lt; 9）。 通过目录项记录页确定用户记录真实所在的页。在页42中可以快速定位到实际存储用户记录的页，但是由于c2列并没有唯一性约束，所以c2列值为4的记录可能分布在多个数据页中，又因为2 &lt; 4 ≤ 4，所以确定实际存储用户记录的页在页34和页35中。 在真实存储用户记录的页中定位到具体的记录。到页34和页35中定位到具体的记录。 但是这个B+树的叶子节点中的记录只存储了c2和c1（也就是主键）两个列，所以我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录。 我们根据这个以c2列大小排序的B+树只能确定我们要查找记录的主键值，所以如果我们想根据c2列的值查找到完整的用户记录的话，仍然需要到聚簇索引中再查一遍，这个过程也被称为回表。也就是根据c2列的值查询一条完整的用户记录需要使用到2`棵B+`树！ 为什么我们还需要一次回表操作呢？直接把完整的用户记录放到叶子节点不就好了么？你说的对，如果把完整的用户记录放到叶子节点是可以不用回表，但是太占地方了呀～相当于每建立一棵B+树都需要把所有的用户记录再都拷贝一遍，这就有点太浪费存储空间了。因为这种按照非主键列建立的B+树需要一次回表操作才可以定位到完整的用户记录，所以这种B+树也被称为二级索引（英文名secondary index），或者辅助索引。由于我们使用的是c2列的大小作为B+树的排序规则，所以我们也称这个B+树为为c2列建立的索引。 联合索引 我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层： 先把各个记录和页按照c2列进行排序。 在记录的c2列相同的情况下，采用c3列进行排序 为c2和c3列建立的索引的示意图如下： 如图所示，我们需要注意一下几点： 每条目录项记录都由c2、c3、页号这三个部分组成，各条记录先按照c2列的值进行排序，如果记录的c2列相同，则按照c3列的值进行排序。 B+树叶子节点处的用户记录由c2、c3和主键c1列组成。 千万要注意一点，以c2和c3列的大小为排序规则建立的B+树称为联合索引，它的意思与分别为c2和c3列建立索引的表述是不同的，不同点如下： 建立联合索引只会建立如上图一样的1棵B+树。 为c2和c3列建立索引会分别以c2和c3列的大小为排序规则建立2棵B+树。 MyISAM中的索引方案至此，我们介绍的都是InnoDB存储引擎中的索引方案，为了内容的完整性，以及各位可能在面试的时候遇到这类的问题，我们有必要再简单介绍一下MyISAM存储引擎中的索引方案。我们知道InnoDB中索引即数据，也就是聚簇索引的那棵B+树的叶子节点中已经把所有完整的用户记录都包含了，而MyISAM的索引方案虽然也使用B+树，但是却将索引和数据分开存储： 将表中的记录按照插入时间顺序的存储在一块存储空间上，我们可以通过行号而快速访问到一条记录（因为index_demo表的记录是定长的，所以可以使用行号来进行快速访问，对于变长的记录MyISAM有不同的处理方案，我们这里就不介绍了），如图所示： 由于在插入数据的时候并没有刻意按照主键大小排序，所以我们并不能在这些数据上使用二分法进行查找。 MyISAM会单独为表的主键创建一个B+树索引，只不过在B+树的叶子节点中存储的不是完整的用户记录，而是主键值 + 行号的组合。也就是先通过索引找到对应的行号，再通过行号去找对应的记录！这一点和InnoDB是完全不相同的，在InnoDB存储引擎中，我们只需要根据主键值对聚簇索引进行一次查找能找到对应的记录，而在MyISAM中却需要进行一次回表操作，意味着MyISAM中建立的索引全部都是二级索引！ 如果有需要的话，我们也可以对其它的列分别建立索引或者建立联合索引，原理和InnoDB中的索引是一样的，只不过在叶子节点处存储的是相应的列 + 行号而已。这些索引也全部都是二级索引。 MySQL中创建和删除索引的语句光顾着唠叨索引的原理了，那我们如何使用MySQL语句去建立这种索引呢？InnoDB和MyISAM会自动为主键或者声明为UNIQUE的列去自动建立B+树索引，但是如果我们想为其他的列建立索引就需要我们显式的去指明。为啥不自动为每个列都建立个索引呢？别忘了，每建立一个索引都会建立一棵B+树，每插入一条记录都要维护各个记录、数据页的排序关系，这是很费性能和存储空间的。 我们可以在创建表的时候指定需要建立索引的单个列或者建立联合索引的多个列： 1CREATE TALBE 表名 ( 各种列的信息 ··· , [KEY|INDEX] 索引名 (需要被索引的单个列或多个列)) 其中的KEY和INDEX是同义词，任意选用一个就可以。我们也可以在修改表结构的时候添加索引： 1ALTER TABLE 表名 ADD [INDEX|KEY] 索引名 (需要被索引的单个列或多个列); 也可以在修改表结构的时候删除索引： 1ALTER TABLE 表名 DROP [INDEX|KEY] 索引名; 比方说我们想在创建index_demo表的时候就为c2和c3列添加一个联合索引，可以这么写建表语句： 123456CREATE TABLE index_demo( c1 INT, c2 INT, c3 CHAR(1), PRIMARY KEY(c1), INDEX idx_c2_c3 (c2, c3)); 在这个建表语句中我们创建的索引名是idx_c2_c3，这个名称可以随便起，不过我们还是建议以idx_为前缀，后边跟着需要建立索引的列名，多个列名之间用下划线_分隔开。 如果我们想删除这个索引，可以这么写： 1ALTER TABLE index_demo DROP INDEX idx_c2_c3; 总结 对于InnoDB存储引擎来说，在单个页中查找某条记录分为两种情况： 以主键为搜索条件，可以使用Page Directory通过二分法快速定位相应的用户记录。 以其他列为搜索条件，需要按照记录组成的单链表依次遍历各条记录。 没有索引的情况下，不论是以主键还是其他列作为搜索条件，只能沿着页的双链表从左到右依次遍历各个页。 InnoDB存储引擎的索引是一棵B+树，完整的用户记录都存储在B+树第0层的叶子节点，其他层次的节点都属于内节点，内节点里存储的是目录项记录。InnoDB的索引分为两大种： 聚簇索引以主键值的大小为页和记录的排序规则，在叶子节点处存储的记录包含了表中所有的列。 二级索引以自定义的列的大小为页和记录的排序规则，在叶子节点处存储的记录内容是列 + 主键。 MyISAM存储引擎的数据和索引分开存储，这种存储引擎的索引全部都是二级索引，在叶子节点处存储的是列 + 页号。 查询的匹配问题1234567CREATE TABLE person_info( name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, KEY idx_name_age_birthday (name, birthday, phone_number)); 全值匹配如果我们的搜索条件中的列和索引列一致的话，这种情况就称为全值匹配，比方说下边这个查找语句： 1SELECT * FROM person_info WHERE name = &apos;Ashburn&apos; AND birthday = &apos;1990-09-27&apos; AND phone_number = &apos;15123983239&apos;; 我们建立的idx_name_age_birthday索引包含的3个列在这个查询语句中都展现出来了，而且搜索条件中列的顺序和定义索引列时的顺序是一致的。如果搜索条件中的列的顺序和索引列的顺序不一致就不太好了，比方说先对birthday列的值进行匹配的话，由于B+树中的数据页和记录是先按name列的值进行排序的，不能直接使用二分法快速定位记录，所以只能扫描所有的记录页。 匹配左边的列比如下边的语句就用不到这个B+树索引么？ 1SELECT * FROM person_info WHERE birthday = &apos;1990-09-27&apos;; 是的，的确用不到，因为B+树的数据页和记录先是按照name列的值排序的，你直接根据birthday的值去查找，臣妾做不到呀～ 那如果我就想在只使用birthday的值去通过B+树索引进行查找咋办呢？这好办，你再对birthday列建一个B+树索引就行了。 但是需要特别注意的一点是，搜索条件中的列的顺序必须和索引列的定义顺序一致，比方说索引列的定义顺序是name、birthday、phone_number，如果我们的搜索条件中只有name和phone_number，而没有birthday，这样只能用到name列的索引，birthday和phone_number的索引就用不上了（因为name值相同的记录先按照birthday进行排序，birthday值相同的记录才按照phone_number值进行排序）。 匹配列前缀对于字符串类型的索引列来说，我们没必要对该列的值进行精确匹配，只匹配它的前缀也是可以的，因为前缀本身就已经是排好序的。比方说我们想查询名字以&#39;As&#39;开头的记录，那就可以这么写查询语句： 1SELECT * FROM person_info WHERE name LIKE &apos;As%&apos;; B+树中的数据页和记录都先是按照name列排序的，只给出前缀也是可以通过二分法快速定位的。但是需要注意的是，如果只给出后缀或者中间的某个字符串，比如这样： 1SELECT * FROM person_info WHERE name LIKE &apos;%As%&apos;; MySQL就无法通过二分法来快速定位记录位置了，因为字符串中间有&#39;As&#39;的字符串并没有排好序，所以只能全表扫描了。 匹配范围值回头看我们idx_name_age_birthday索引的B+树示意图，所有记录都是按照索引列的值从小到大的顺序排好序的，所以这极大的方便我们查找索引列的值在某个范围内的记录。比方说下边这个查询语句： 1SELECT * FROM person_info WHERE name &gt; &apos;Asa&apos; AND name &lt; &apos;Barlow&apos;; 由于B+树中的节点和数据页是先按name列排序的，所以我们上边的查询过程其实是这样的： 找到name值为Asa的记录。 找到name值为Barlow的记录。 由于所有记录都是由连链表连起来的（记录之间用单链表，数据页之间用双链表），所以他们之间的记录都可以很容易的取出来喽～ 找到这些记录的主键值，再到聚簇索引中回表查找完整的记录。 不过进行范围查找的时候需要注意，如果对多个列同时进行范围查找的话，只有对索引最左边的那个列进行范围查找的时候才能用到B+树索引，比方说这样： 1SELECT * FROM person_info WHERE name &gt; &apos;Asa&apos; AND name &lt; &apos;Barlow&apos; AND birthday &gt; &apos;1980-01-01&apos;; 为啥不能呢？因为上边这个查询可以分成两个部分： 通过条件name &gt; &#39;Asa&#39; AND name &lt; &#39;Barlow&#39;来对name进行范围，查找的结果可能有多条name值不同的记录， 对这些name值不同的记录继续通过birthday &gt; &#39;1980-01-01&#39;条件继续过滤。 但是值得注意的是，只有name值相同的情况下才能用birthday列的值进行排序，也就是说通过name进行范围查找的结果并不是按照birthday列进行排序的，所以在搜索条件中继续以birthday列进行查找时是用不到B+树索引的。 精确匹配某一列并范围匹配另外一列对于同一个联合索引来说，虽然对多个列都进行范围查找时只能用到最左边那个索引列，但是如果左边的列是精确查找，则右边的列可以进行范围查找，比方说这样： 1SELECT * FROM person_info WHERE name = &apos;Ashburn&apos; AND birthday &gt; &apos;1980-01-01&apos; AND birthday &lt; &apos;2000-12-31&apos; AND phone_number &gt; &apos;15100000000&apos;; 这个查询的条件可以分为3个部分： name = &#39;Ashburn&#39;，对name列进行精确查找，当然可以使用B+树索引了。 birthday &gt; &#39;1980-01-01&#39; AND birthday &lt; &#39;2000-12-31&#39;，由于name列是精确查找，所以通过name = &#39;Ashburn&#39;条件查找后得到的结果的name值都是相同的，它们会再按照birthday的值进行排序。所以此时对birthday列进行范围查找是可以用到B+树索引的。 phone_number &gt; &#39;15100000000&#39;，通过birthday的范围查找的记录的birthday的值可能不同，所以这个条件无法再利用B+树索引了，只能遍历上一步查询得到的记录。 用于排序我们在写查询语句的时候经常需要对查询出来的记录按照某种规则进行排序，对于不适用B+树索引进行排序的情况，我们只能把记录都加载到内存中，再用一些排序算法，比如快速排序、归并排序、吧啦吧啦排序等等在内存中对这些记录进行排序，然后再把排好序的结果集返回到客户端。但是如果ORDER BY子句里使用到了我们的索引列，就有可能省去在内存中排序的步骤，比如下边这个简单的查询语句： 1SELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10; 这个查询的结果集需要先按照name值排序，如果记录的name值相同，则需要按照birthday来排序，如果birthday的值相同，则需要按照phone_number排序。大家可以回过头去看我们建立的idx_name_age_birthday索引的示意图，因为这个B+树索引本身就是排好序的，所以直接从索引中提取数据就好了。简单吧？是的，索引就是这么diao~ 但是有个问题需要注意，ORDER BY的子句后边的列的顺序也必须按照索引列的顺序给出，如果给出ORDER BY phone_number, birthday, name的顺序，那也是用不了B+树索引，这种颠倒顺序就不能使用索引的原因我们上边详细说过了，这就不赘述了。同理，ORDER BY name、ORDER BY name, birthday这种匹配索引左边的列的形式可以使用部分的B+树索引。 用于分组有时候我们为了方便统计表中的一些信息，会把表中的记录按照某些列进行分组。比如下边这个分组查询： 1SELECT name, birthday, phone_number, COUNT(*) FROM person_info GROUP BY name, birthday, phone_number 这个查询语句相当于做了3次分组操作： 先把记录按照name值进行分组，所有name值相同的记录划分为一组。 将每个name值相同的分组里的记录再按照birthday的值进行分组，将birthday值相同的记录放到一个小分组里，所以看起来就像在一个大分组里又化分了好多小分组。 再将上一步中产生的小分组按照phone_number的值分成更小的分组，所以整体上看起来就像是先把记录分成一个大分组，然后把大分组分成若干个小分组，然后把若干个小分组再细分成更多的小小分组。 然后针对那些小小分组进行统计，比如在我们这个查询语句中就是统计每个小小分组包含的记录条数。如果没有索引的话，这个分组过程全部需要在内存里实现，而如果有了索引的话，恰巧这个分组顺序又和我们的B+树中的索引列的顺序是一致的，而我们的B+树索引又是按照索引列排好序的，这不正好么，所以可以直接使用B+树索引进行分组。 和使用B+树索引进行排序是一个道理，分组列的顺序也需要和索引列的顺序一致，也可以只使用索引列中左边的列进行分组， 索引失效问题 对于创建的多列索引（复合索引），不是使用的第一部分就不会使用索引 123alter table student add index my_index(name, age) // name左边的列， age 右边的列 select * from student where name = &apos;aaa&apos; // 会用到索引select * from student where age = 18 // 不会使用索引 对于使用 like 查询， 查询如果是 ‘%aaa’ 不会使用索引，而 ‘aaa%’ 会使用到索引。 12select * from student where name like &apos;aaa%&apos; // 会用到索引select * from student where name like &apos;%aaa&apos; 或者 &apos;_aaa&apos; // 不会使用索引 如果条件中有 or， 有条件没有使用索引，即使其中有条件带索引也不会使用，换言之， 就是要求使用的所有字段，都必须单独使用时能使用索引。 如果列类型是字符串，那么一定要在条件中使用引号引用起来，否则不使用索引。 如果mysql认为全表扫面要比使用索引快，则不使用索引。如：表里只有一条数据。 事务的隔离级别、锁深入理解乐观锁与悲观锁 事务的隔离级别 锁在关系数据库管理系统里，悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）是一种并发控制的方法。它可以阻止一个事务以影响其他用户的方式来修改数据。如果一个事务执行的操作都某行数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 上面我们提到，使用select…for update会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁把整张表锁住，这点需要注意。 乐观锁（ Optimistic Locking ） 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。只能防止脏读后数据的提交，不能解决脏读。相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 行级锁开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 页级锁开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 在不通过索引条件查询的时候,InnoDB 确实使用的是表锁,而不是行锁。MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。 行锁TODO….gap锁 ANSI/ISO SQL定义的标准隔离级别有四种，从高到底依次为：可序列化(Serializable)、可重复读(Repeatable reads)、提交读(Read committed)、未提交读(Read uncommitted)。 未提交读(Read uncommitted)事务在读数据的时候并未对数据加锁。事务在修改数据的时候只对数据增加行级共享锁。所以，未提交读会导致脏读可以读到其他事务未提交的结果 提交读(Read committed)提交读(READ COMMITTED)也可以翻译成读已提交，通过名字也可以分析出，在一个事务修改数据过程中，如果事务还没提交，其他事务不能读该数据。 事务对当前被读取的数据加 行级共享锁（当读到时才加锁），一旦读完该行，立即释放该行级共享锁；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加 行级排他锁，直到事务结束才释放。简而言之，提交读这种隔离级别保证了读到的任何数据都是提交的数据，避免了脏读(dirty reads)。但是不保证事务重新读的时候能读到相同的数据，因为在每次数据读完之后其他事务可以修改刚才读到的数据。 | 事务一 | 事务二 || /* Query 1 */ SELECT * FROM users WHERE id = 1; | || | /* Query 2 */UPDATE users SET age = 21 WHERE id = 1; COMMIT; /* in multiversion concurrency control, or lock-based READ COMMITTED */ || /* Query 1 */SELECT * FROM users WHERE id = 1; COMMIT;/*lock-based REPEATABLE READ */ | 可重复读(Repeatable reads)事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加 行级共享锁，直到事务结束才释放；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加 行级排他锁，直到事务结束才释放。 事务1在读取某行记录的整个过程中，事务2都可以对该行记录进行读取（因为事务一对该行记录增加行级共享锁的情况下，事务二同样可以对该数据增加共享锁来读数据。）。 事务1在读取某行记录的整个过程中，事务2都不能修改该行数据（事务一在读取的整个过程会对数据增加共享锁，直到事务提交才会释放锁，所以整个过程中，任何其他事务都不能对该行数据增加排他锁。所以，可重复读能够解决不可重复读的读现象） 事务1更新某行记录时，事务2不能对这行记录做更新，直到事务1结束。（事务一在更新数据的时候，会对该行数据增加排他锁，知道事务结束才会释放锁，所以，在事务二没有提交之前，事务一都能不对数据增加共享锁进行数据的读取。所以，提交读可以解决脏读的现象） 可序列化(Serializable)事务在读取数据时，必须先对其加 表级共享锁 ，直到事务结束才释放；事务在更新数据时，必须先对其加 表级排他锁 ，直到事务结束才释放。 脏读 脏读又称无效数据的读出，是指在数据库访问中，事务T1将某一值修改，然后事务T2读取该值，此后T1因为某种原因撤销对该值的修改，这就导致了T2所读取到的数据是无效的。 不可重复读 一种更易理解的说法是：在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 幻读 幻读是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中的“全部数据行”。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入“一行新数据”。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样.一般解决幻读的方法是增加范围锁RangeS，锁定检锁范围为只读，这样就避免了幻读。 幻读(phantom read)”是不可重复读(Non-repeatable reads)的一种特殊场景：当事务没有获取范围锁的情况下执行SELECT … WHERE操作可能会发生“幻影读(phantom read)”。 | 事务一 | 事务二 || /* Query 1 */ SELECT * FROM users WHERE age BETWEEN 10 AND 30; | || | /* Query 2 */INSERT INTO users VALUES ( 3, &#39;Bob&#39;, 27 ); COMMIT; || /* Query 1 */SELECT * FROM users WHERE age BETWEEN 10 AND 30; | 1.事务一的第一次查询条件是age BETWEEN 10 AND 30;如果这是有十条记录符合条件。这时，他会给符合条件的这十条记录增加行级共享锁。任何其他事务无法更改这十条记录。 2.事务二执行一条sql语句，语句的内容是向表中插入一条数据。因为此时没有任何事务对表增加表级锁，所以，该操作可以顺利执行。 3.事务一再次执行SELECT * FROM users WHERE age BETWEEN 10 AND 30;时，结果返回的记录变成了十一条，比刚刚增加了一条，增加的这条正是事务二刚刚插入的那条。 所以，事务一的两次范围查询结果并不相同。这也就是我们提到的幻读。 当前读、快照读 DB_TRX_ID ，，DB_ROLL_PTR ，， DB_ROW_ID字段 undo日志 函数依赖函数依赖定义为：设R(U)是属性集U上的关系模式。X,Y是U的子集，若对于R(U)的任意一个可能的关系r，r中不可能存在两个元组在X上的属性值相等，而在Y上的属性值不等，则成X函数确定Y或者Y函数依赖与X， 记作：X→Y。 平凡的函数依赖：如果X→Y，但Y∈X，则称X→Y是平凡的函数依赖 非平凡的函数依赖：如果X→Y，但Y∉X，则称X→Y是非平凡的函数依赖。通常情况下总是讨论非平凡的函数依赖 完全函数依赖：在R(U)中，如果X→Y，并且对于x的任何一个真子集X’，都有X´不能决定Y，则称Y对X完全函数依赖，记作：X-f-&gt;Y (f即 full) 部分函数依赖：在R(U)中，如果X→Y，但Y不完全函数依赖于X，则称Y对X部分函数依赖，记作：X-p-&gt;Y (p即part) 部分函数依赖也称为局部函数依赖 传递依赖：在R(U,F)中，如果X→Y，Y∉X，Y→Z，Y不完全函数依赖于X，则称Z对X传递依赖 无损连接 SQL 知识if，ifnull把salary表中的女改成男，男改成女: update salary set sex = if( sex = ‘男’,’女’,’男’); if(true,a,b), if(false,a,b) 这个就是第一个如果是true，就等于a，false就等于b，有点像三元表达式 MySQL IFNULL函数是MySQL控制流函数之一，它接受两个参数，如果不是NULL，则返回第一个参数。 否则，IFNULL函数返回第二个参数。 两个参数可以是文字值或表达式。 以下说明了IFNULL函数的语法： IFNULL(expression_1,expression_2);SQL如果expression_1不为NULL，则IFNULL函数返回expression_1; 否则返回expression_2的结果。 IFNULL函数根据使用的上下文返回字符串或数字。 limit ，offset① select * from table limit 2,1;//含义是跳过2条取出1条数据，limit后面是从第2条开始读，读取1条信息，即读取第3条数据 ② select * from table limit 2 offset 1;//含义是从第1条（不包括）数据开始取出2条数据，limit后面跟的是2条数据，offset后面是从第1条开始读取，即读取第2,3条 distinct去掉重复值，如果有多个列，会应用到每个列 on条件与where条件的区别综上得出，ON后面对于左表的过滤条件，在最后结果行数中会被忽略，并不会先去过滤左表数据再连接查询，但是ON后的右表条件会先过滤右表数据再连接左表进行查询。 连接查询时，都是用符合ON后的左右表的过滤条件的数据进行连接查询，只有符合左右表过滤条件的数据才能正确匹配，剩下的左表数据会正常出现在结果集中，但匹配的右表数据是NULL。因此对于左表的过滤条件切记要放到Where后，对于右表的过滤条件要看情况了。如果需要先过滤右表数据就把条件放到ON后面即可。 on、where、having的区别 on、where、having这三个都可以加条件的子句中，on是最先执行，where次之，having最后。有时候如果这先后顺序不影响中间结果的话，那最终结果是相同的。但因为on是先把不符合条件的记录过滤后才进行统计，它就可以减少中间运算要处理的数据，按理说应该速度是最快的。 根据上面的分析，可以知道where也应该比having快点的，因为它过滤数据后才进行sum，所以having是最慢的。但也不是说having没用，因为有时在步骤3还没出来都不知道那个记录才符合要求时，就要用having了。 在两个表联接时才用on的，所以在一个表的时候，就剩下where跟having比较了。在这单表查询统计的情况下，如果要过滤的条件没有涉及到要计算字段，那它们的结果是一样的，只是where可以使用rushmore技术，而having就不能，在速度上后者要慢。 如果要涉及到计算的字段，就表示在没计算之前，这个字段的值是不确定的，根据上篇写的工作流程，where的作用时间是在计算之前就完成的，而having就是在计算后才起作用的，所以在这种情况下，两者的结果会不同。 在多表联接查询时，on比where更早起作用。系统首先根据各个表之间的联接条件，把多个表合成一个临时表后，再由where进行过滤，然后再计算，计算完后再由having进行过滤。由此可见，要想过滤条件起到正确的作用，首先要明白这个条件应该在什么时候起作用，然后再决定放在那里 JOIN123456789101112Id_P LastName FirstName Address City1 Adams John Oxford Street London2 Bush George Fifth Avenue New York3 Carter Thomas Changan Street BeijingId_O OrderNo Id_P1 77895 32 44678 33 22456 14 24562 15 34764 65 1234SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName 结果：12345LastName FirstName OrderNoAdams John 22456Adams John 24562Carter Thomas 77895Carter Thomas 44678 在表中存在至少一个匹配时（来自两个表的所有字段都不为空），INNER JOIN 关键字返回行。与 JOIN 是相同的LEFT JOIN 关键字会从左表 (table_name1) 那里返回所有的行，即使在右表 (table_name2) 中没有匹配的行。LEFT OUTER JOIN。123456789101112SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastNameLastName FirstName OrderNoAdams John 22456Adams John 24562Carter Thomas 77895Carter Thomas 44678Bush George RIGHT JOIN 关键字会右表 (table_name2) 那里返回所有的行，即使在左表 (table_name1) 中没有匹配的行。只要其中某个表存在匹配，FULL JOIN 关键字就会返回行。LastName FirstName OrderNoAdams John 22456Adams John 24562Carter Thomas 77895Carter Thomas 44678Bush George 34764 AVG()、COUNT()、SUM()等函数对NULL值处理AVE()忽略NULL值，而不是将其作为“0”参与计算 COUNT(*)对表中行数进行计数，不管是否有NULL COUNT(字段名)对特定列有数据的行进行计数，忽略NULL值 SUM()可以对单个列求和，也可以对多个列运算后求和。忽略NULL值，且当对多个列运算求和时，如果运算的列中任意一列的值为NULL，则忽略这行的记录。例如： SUM(A+B+C)，A、B、C 为三列，如果某行记录中A列值为NULL，则不统计这行。 max() min()忽略null、 HAVING过滤分组group by 分组之后，可以用having过滤，。where是在分组之前过滤。 inner join…on…case when123456789 --简单case函数case sex when &apos;1&apos; then &apos;男&apos; when &apos;2&apos; then &apos;女&apos;else &apos;其他&apos; end--case搜索函数case when sex = &apos;1&apos; then &apos;男&apos; when sex = &apos;2&apos; then &apos;女&apos;else &apos;其他&apos; end 这两种方式，可以实现相同的功能。简单case函数的写法相对比较简洁，但是和case搜索函数相比，功能方面会有些限制，比如写判断式。还有一个需要注意的问题，case函数只返回第一个符合条件的值，剩下的case部分将会被自动忽略。–比如说，下面这段sql，你永远无法得到“第二类”这个结果123case when col_1 in ( &apos;a&apos;, &apos;b&apos;) then &apos;第一类&apos; when col_1 in (&apos;a&apos;) then &apos;第二类&apos;else&apos;其他&apos; end Innodb与Myisam引擎1. 区别：（1）事务处理：MyISAM是非事务安全型的，而InnoDB是事务安全型的（支持事务处理等高级处理）；（2）锁机制不同：MyISAM是表级锁，而InnoDB是行级锁；（3）select ,update ,insert ,delete 操作：MyISAM：如果执行大量的SELECT，MyISAM是更好的选择InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表（4）查询表的行数不同：MyISAM：select count() from table,MyISAM只要简单的读出保存好的行数，注意的是，当count()语句包含 where条件时，两种表的操作是一样的InnoDB ： InnoDB 中不保存表的具体行数，也就是说，执行select count(*) from table时，InnoDB要扫描一遍整个表来计算有多少行（5）外键支持：mysiam表不支持外键，而InnoDB支持 2. 为什么MyISAM会比Innodb 的查询速度快。INNODB在做SELECT的时候，要维护的东西比MYISAM引擎多很多；1）数据块，INNODB要缓存，MYISAM只缓存索引块， 这中间还有换进换出的减少；2）innodb寻址要映射到块，再到行，MYISAM 记录的直接是文件的OFFSET，定位比INNODB要快3）INNODB还需要维护MVCC一致；虽然你的场景没有，但他还是需要去检查和维护MVCC ( Multi-Version Concurrency Control )多版本并发控制 3. 应用场景MyISAM适合：(1)做很多count 的计算；(2)插入不频繁，查询非常频繁；(3)没有事务。InnoDB适合：(1)可靠性要求比较高，或者要求事务；(2)表更新和查询都相当的频繁，并且行锁定的机会比较大的情况。 MVCC 【mysql】关于innodb中MVCC的一些理解通过加锁，让所有的读者等待写者工作完成，但是这样效率会很差。MVCC 使用了一种不同的手段，每个连接到数据库的读者，在某个瞬间看到的是数据库的一个快照，写者写操作造成的变化在写操作完成之前（或者数据库事务提交之前）对于其他的读者来说是不可见的。 innodb存储的最基本row中包含一些额外的存储信息 DATA_TRX_ID，DATA_ROLL_PTR，DB_ROW_ID，DELETE BIT 6字节的DATA_TRX_ID 标记了最新更新这条行记录的transaction id，每处理一个事务，其值自动+1 7字节的DATA_ROLL_PTR 指向当前记录项的rollback segment的undo log记录，找之前版本的数据就是通过这个指针 6字节的DB_ROW_ID，当由innodb自动产生聚集索引时，聚集索引包括这个DB_ROW_ID的值，否则聚集索引中不包括这个值.，这个用于索引当中 DELETE BIT位用于标识该记录是否被删除，这里的不是真正的删除数据，而是标志出来的删除。真正意义的删除是在commit的时候 InnoDB的MVCC,是通过在每行记录后面保存两个隐藏的列来实现的,这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值,而是系统版本号(可以理解为事务的ID)，每开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID. SELECTInnoDB会根据以下两个条件检查每行记录: a.InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的. b.行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除.只有a,b同时满足的记录，才能返回作为查询结果.DELETEInnoDB会为删除的每一行保存当前系统的版本号(事务的ID)作为删除标识. innodb索引和数据在一起 myisam不在一起 存储程序原文链接 有时候为了完成一个常用的功能需要执行许多条语句，每次都在客户端里一条一条的去输入这么多语句是很烦的，我们希望有一种批处理的形式，让我们以很简单的方式一次性的执行完这些语句，MySQL中的存储程序本质上封装了一些可执行的语句，然后给用户提供一种简单的调用方式来执行这些语句，根据调用方式的不同，我们可以把存储程序分为存储例程、触发器和事件这几种类型。其中，存储例程又可以被细分为存储函数和存储过程。我们画个图表示一下： 自定义变量变量是和常量相对的，一般的程序语言都提供对变量的支持，MySQL中对我们自定义的变量的命名有个要求，那就是变量名称前必须加一个@符号。我们自定义变量的值的类型可以是任意MySQL支持的类型，比方说我们来自定义一个变量： 1234567mysql&gt; //多种设置方式SET @a = 1;SET @b = @a;SET @a = (SELECT m1 FROM t1 LIMIT 1);SELECT m1, n1 FROM t1 LIMIT 1 INTO @a, @bSELECT @a 可以设置结束分隔符 1234567mysql&gt; delimiter $mysql&gt; SELECT * FROM t1 LIMIT 1; -&gt; SELECT * FROM t2 LIMIT 1; -&gt; SELECT * FROM t3 LIMIT 1; -&gt; $ 用完改回去 delimiter ; 存储例程简介存储例程是存储程序的一种类型，本质上也是封装了一些可执行的语句，只不过它的调用方式是：需要手动去调用！存储例程又可以分为存储函数和存储过程，下边我们详细唠叨这两个家伙。 存储函数存储函数其实就是一种函数，只不过在这个函数里可以执行命令语句而已。函数的概念大家都应该不陌生，它可以把处理某个问题的过程封装起来，之后我们直接调用函数就可以去解决同样的问题了，简单方便又环保。MySQL中定义存储函数的语句如下： 1234567CREATE FUNCTION avg_score(s VARCHAR(100))RETURNS DOUBLEBEGIN（declare count integer有需要的话在这里定义变量） RETURN (SELECT AVG(score) FROM student_score WHERE subject = s);END 从这里我们可以看出，定义一个存储函数需要指定函数名称、参数列表、返回值类型以及函数体内容，如果该函数不需要参数，那参数列表可以被省略，函数体内容可以包括一条或多条语句，每条语句都要以分号;结尾。 存储过程存储函数侧重于执行这些语句并返回一个值，而存储过程更侧重于单纯的去执行这些语句。存储函数执行语句并返回一个值，所以常用在表达式中。存储过程偏向于调用那些语句，并不能用在表达式中，我们需要显式的使用CALL语句来调用一个存储过程： 123456CREATE PROCEDURE 存储过程名称([参数列表])BEGIN 需要执行的语句ENDCALL 存储过程([参数列表]); 比存储函数牛逼的一点是，存储过程在定义参数的时候可以选择参数类型（注意！不是数据类型）,如果我们不写明参数类型的话，该参数的类型默认是IN 123456789参数类型 参数名 数据类型mysql&gt; CREATE PROCEDURE p_out (-&gt; OUT a INT-&gt; )-&gt; BEGIN-&gt; SELECT a;//由于a是out，所以外面传进来的值里面读不到，但是可以改变后输出-&gt; SET a = 123;-&gt; END 存储过程和存储函数的不同点 存储函数在定义时需要显式用RETURNS语句标明返回的数据类型，而且在函数体中必须使用RETURN语句来显式指定返回的值，存储过程不需要。 存储函数的参数类型只能是IN，而存储过程支持IN、OUT、INOUT三种参数类型。 存储函数只能返回一个值，而存储过程可以通过设置多个OUT类型的参数来返回多个结果。 存储函数执行过程中产生的结果集并不会被显示到客户端，而存储过程执行过程中产生的结果集会被显示到客户端。 存储函数的调用直接使用在表达式中，而存储过程只能通过CALL语句来显式调用。 游标简介截止到现在为止，我们只能使用INTO语句将一行记录的各个列值赋值到多个变量里，比如在上边的get_score_data存储过程里这么写： 1SELECT MAX(score), MIN(score), AVG(score) FROM student_score WHERE subject = s INTO max_score, min_score, avg_score; 但是如果结果集中有多条记录的话，我们就无法把它们赋值给某些变量了～ 所以为了方便我们去访问这些有多条记录的结果集，MySQL中引入了游标的概念。 我们下边以对t1表的查询为例来介绍一下游标，比如我们有这样一个查询： 12345678910mysql&gt; SELECT m1, n1 FROM t1;+------+------+| m1 | n1 |+------+------+| 1 | a || 2 | b || 3 | c || 4 | d |+------+------+4 rows in set (0.00 sec)mysql&gt; 这个SELECT m1, n1 FROM t1查询语句对应的结果集有4条记录，这个游标其实是用来标记结果集中我们正在访问的某一行记录，初始状态下它标记结果集中的第一条记录，就像这样： 我们可以根据这个游标取出它对应记录的信息，随后再移动游标，让它执向别的记录。游标既可以用在存储函数中，也可以用在存储过程中，我们下边以存储过程为例来说明游标的使用方式，它的使用大致分成这么四个步骤： 创建游标 打开游标 通过游标访问记录 关闭游标 12345678910111213141516171819202122232425CREATE PROCEDURE cursor_demo()BEGIN -- 声明变量 DECLARE m_value INT; DECLARE n_value CHAR(1); DECLARE not_done INT DEFAULT 1; -- 声明游标 DECLARE t1_record_cursor CURSOR FOR SELECT m1, n1 FROM t1; -- 在游标遍历完记录的时候将变量 not_done 的值设置为 0，并且继续执行后边的语句 -- CONTINUE表示在`FETCH`语句获取不到记录的时候仍然会执行之后存储过程的语句，`EXIT`表示在 -- FETCH`语句获取不到记录的时候仍然不会执行之后存储过程的语句 DECLARE CONTINUE HANDLER FOR NOT FOUND SET not_done = 0; -- 使用游标遍历 OPEN t1_record_cursor; WHILE not_done = 1 DO FETCH t1_record_cursor INTO m_value, n_value; SELECT m_value, n_value, not_done; END WHILE; CLOSE t1_record_cursor;END 触发器存储函数与存储过程都是需要我们手动调用的，如果想在执行某条语句之前或者之后自动去调用另外一些语句，比如下边的这些场景： 在向t1表插入数据之前对自动对数据进行校验，要求m1列的值必须在1~10之间，校验规则如下： 如果插入的记录的m1列的值小于1，则按1插入。 如果m1列的值大于10，则按10插入。 在向t1表中插入记录之后自动把这条记录插入到t2表。 那我们就需要考虑一下触发器了。我们看一下定义触发器的语句： 12345678CREATE TRIGGER 触发器名&#123;BEFORE|AFTER&#125; &#123;INSERT|DELETE|UPDATE&#125;ON 表名FOR EACH ROW BEGIN 触发器内容END 需要注意的是，由大括号{}包裹并且内部用竖线|分隔的语句表示必须在给定的选项中选取一个值，比如{BEFORE|AFTER}表示必须在BEFORE、AFTER这两个之间选取一个。因为触发器会对某个语句影响的所有记录依次调用我们自定义的触发器内容，所以我们需要一种访问该记录中的内容的方式，MySQL提供了NEW和OLD两个单词来分别代表新记录和旧记录，它们在不同操作中的含义不同： 对于INSERT语句设置的触发器来说，NEW代表准备插入的记录，不能使用OLD。 对于DELETE语句设置的触发器来说，OLD代表删除前的记录，不能使用NEW。 对于UPDATE语句设置的触发器来说，NEW代表修改后的记录，OLD代表修改前的记录。 触发器内容中不能有输出结果集的语句 事件如果我们想指定某些语句在某个时间点或者每隔一个时间段执行一次的话，可以选择创建一个事件，语法就是这样：SET GLOBAL event_scheduler = ON; 123456789CREATE EVENT 事件名ON SCHEDULE&#123;AT 某个确定的时间点 | EVERY 期望的时间间隔 [STARTS datetime][END datetime]&#125;-- AT DATE_ADD(NOW(), INTERVAL 2 DAY)-- AT &apos;2018-03-10 15:48:54&apos;-- EVERY 1 HOUR STARTS &apos;2018-03-10 15:48:54&apos; ENDS &apos;2018-03-12 15:48:54&apos;DOBEGIN 具体的语句END MyCAThttps://www.jianshu.com/p/21b1e133dd9b分布式数据库系统中间层 应用场景：需要读写分离，需要分库分表，多租户，数据统计系统，HBASE的一种替代方案 支持全局表支持ER的分片策略支持一致性hash分片 使用MySQL客户端管理mycat动态加载配置文件：reload @@config;查看数据节点：show @@datanode;查看后端数据库：show @@datasource; 复制（replication）和集群（cluster）/读写分离Replication Replication的思想是将数据在集群的多个节点同步、备份，以提高集群数据的可用性（HA）；Mysql使用Replication架构来实现上述目的，同时可以提升了集群整体的并发能力。。 Replication具有如下优点： 扩展：将负载分布在多个slaves上以提高性能，所有的writes以及事务中的read操作都将有master处理，其他reads将转发给slaves；对于“读写比”较高的应用，replication可以通过增加slaves节点来提高并发能力；因为write只能在master上提交，因此架构扩展对提升write并发能力并不明显，对于writes密集性应用我们应该考虑其他架构。 数据安全：slave可以中断自己的replication进程，这不会打断master上的数据请求，所以可以在slave上运行backup服务，定期全量backup是保护数据的手段之一。（如果在master上执行backup，需要让master处于readonly状态，这也意味这所有的write请求需要阻塞）。 分析：数据在master上创建，那么数据分析可以在slave上进行，这将不会影响master的性能。利用mysql做数据分析（或者数据分析平台的源数据），通常都是将某个slave作为数据输入端。 远距数据分布：如果master的物理位置较远，你可以在临近需求的地方部署slaves，以便就近使用数据，而不需要总是访问远端的master，这在数据分析、数据备份与容灾等方面有很大帮助。 Sharding和Replication replication机制的优缺点 优点：负载高时可以通过replication机制来提高读写的吞吐和性能。 缺点：首先它的有效很依赖于读操作的比例，Master往往会成为瓶颈所在，写操作需要顺序排队来执行，过载的话Master首先扛不住，Slaves的数据同步的延迟也可能比较大，而且会大大耗费CPU的计算能力，因为write操作在Master上执行以后还是需要在每台slave机器上都跑一次。 sharding技术的优缺点 优点：sharding技术可以弥补replication机制的缺点。因为sharding可以很好的扩展，我们知道每台机器无论配置多么好它都有自身的物理上限，所以当我们应用已经能触及或远远超出单台机器的某个上限的时候，我们惟有寻找别的机器的帮助或者继续升级的我们的硬件，但常见的方案还是横向扩展, 通过添加更多的机器来共同承担压力。我们还得考虑当我们的业务逻辑不断增长，我们的机器能不能通过线性增长就能满足需求？Sharding可以轻松的将计算，存储，I/O并行分发到多台机器上，这样可以充分利用多台机器各种处理能力，同时可以避免单点失败，提供系统的可用性，进行很好的错误隔离。 Mysql主从复制的原理主从复制通过三个过程实现，其一个过程发生在主服务器上，另外两个过程发生在从服务器上。具体情况如下： 主服务器将用户对数据库的写操作以二进制格式保存到Binary Log（二进制日志）文件中， 然后由Binlog Dump线程将二进制日志文件传输给从服务器。 从服务器通过一个 I/O 线程将主服务器的二进制日志文件中的写操作复制到一个叫 Relay Log 的中继日志文件中。 从服务器通过另一个 SQL 线程将 Relay Log 中继日志文件中的写操作依次在本地执行，从而实现主从服务器之间的数据的同步。 BinLog Dump线程 该线程运行在主服务器上，主要工作是把 Binary Log 二进制日志文件的数据发送给从服务器。 使用SHOW PROCESSLIST语句，可查看该线程是否正在运行。 I/O线程 从服务器执行 START SLAVE 语句后，会创建一个 I/O 线程。此线程运行在从服务器上，与主服务器建立连接，然后向主服务器发出同步请求。之后，I/O 线程将主服务器发送的写操作复制到本地 Relay Log 日志文件中。 使用SHOW SLAVE STATUS语句，可查看 I/O 线程状态。 SQL线程 该线程运行在从服务器上，主要工作是读取 Relay Log 日志文件中的更新操作，并将这些操作依次执行，从而使主从服务器的数据保持同步。 复制级别 基于语句的复制： 在主服务器上执行的SQL语句，在从服务器上执行同样的语句。MySQL默认采用基于语句的复制，效率比较高。 一旦发现没法精确复制时，会自动选着基于行的复制。 基于行的复制：把改变的内容复制过去，而不是把命令在从服务器上执行一遍. 从mysql5.0开始支持 混合类型的复制: 默认采用基于语句的复制，一旦发现基于语句的无法精确的复制时，就会采用基于行的复制。 Replication的三种常用架构 Master - Slaves 在实际应用场景中，MySQL复制90%以上都是一个Master复制到一个或者多个Slave的架构模式，主要用于读压力比较大的应用的数据库端廉价扩展解决方案。因为只要Master和Slave的压力不是太大（尤其是Slave端压力）的话，异步复制的延时一般都很少很少。尤其是自从Slave端的复制方式改成两个线程处理之后，更是减小了Slave端的延时问题。而带来的效益是，对于数据实时性要求不是特别严格的应用，只需要通过廉价的pcserver来扩展Slave的数量，将读压力分散到多台Slave的机器上面，即可通过分散单台数据库服务器的读压力来解决数据库端的读性能瓶颈，毕竟在大多数数据库应用系统中的读压力还是要比写压力大很多。这在很大程度上解决了目前很多中小型网站的数据库压力瓶颈问题，甚至有些大型网站也在使用类似方案解决数据库瓶颈。 Master - Master 有些时候，简单的从一个MySQL复制到另外一个MySQL的基本Replication架构，可能还会需要在一些特定的场景下进行Master的切换。如在Master端需要进行一些特别的维护操作的时候，可能需要停MySQL的服务。这时候，为了尽可能减少应用系统写服务的停机时间，最佳的做法就是将我们的Slave节点切换成Master来提供写入的服务。 但是这样一来，我们原来Master节点的数据就会和实际的数据不一致了。为了解决这个问题，我们可以通过搭建DualMaster环境来避免很多的问题。何谓DualMaster环境？实际上就是两个MySQLServer互相将对方作为自己的Master，自己作为对方的Slave来进行复制。这样，任何一方所做的变更，都会通过复制应用到另外一方的数据库中。 可能有些读者朋友会有一个担心，这样搭建复制环境之后，难道不会造成两台MySQL之间的循环复制么？实际上MySQL自己早就想到了这一点，所以在MySQL的BinaryLog中记录了当前MySQL的server-id，而且这个参数也是我们搭建MySQLReplication的时候必须明确指定，而且Master和Slave的server-id参数值比需要不一致才能使MySQLReplication搭建成功。一旦有了server-id的值之后，MySQL就很容易判断某个变更是从哪一个MySQLServer最初产生的，所以就很容易避免出现循环复制的情况。而且，如果我们不打开记录Slave的BinaryLog的选项（–log-slave-update）的时候，MySQL根本就不会记录复制过程中的变更到BinaryLog中，就更不用担心可能会出现循环复制的情形了。下如将更清晰的展示DualMaster复制架构组成： 通过DualMaster复制架构，我们不仅能够避免因为正常的常规维护操作需要的停机所带来的重新搭建Replication环境的操作，因为我们任何一端都记录了自己当前复制到对方的什么位置了，当系统起来之后，就会自动开始从之前的位置重新开始复制，而不需要人为去进行任何干预，大大节省了维护成本。 不仅仅如此，DualMaster复制架构和一些第三方的HA管理软件结合，还可以在我们当前正在使用的Master出现异常无法提供服务之后，非常迅速的自动切换另外一端来提供相应的服务，减少异常情况下带来的停机时间，并且完全不需要人工干预。 当然，我们搭建成一个DualMaster环境，并不是为了让两端都提供写的服务。在正常情况下，我们都只会将其中一端开启写服务，另外一端仅仅只是提供读服务，或者完全不提供任何服务，仅仅只是作为一个备用的机器存在。为什么我们一般都只开启其中的一端来提供写服务呢？主要还是为了避免数据的冲突，防止造成数据的不一致性。因为即使在两边执行的修改有先后顺序，但由于Replication是异步的实现机制（CAP），同样会导致即使晚做的修改也可能会被早做的修改所覆盖。 Master –Slaves - Slaves 在有些应用场景中，可能读写压力差别比较大，读压力特别的大，一个Master可能需要上10台甚至更多的Slave才能够支撑注读的压力。这时候，Master就会比较吃力了，因为仅仅连上来的SlaveIO线程就比较多了，这样写的压力稍微大一点的时候，Master端因为复制就会消耗较多的资源，很容易造成复制的延时。 遇到这种情况如何解决呢？这时候我们就可以利用MySQL可以在Slave端记录复制所产生变更的BinaryLog信息的功能，也就是打开—log-slave-update选项。然后，通过二级（或者是更多级别）复制来减少Master端因为复制所带来的压力。也就是说，我们首先通过少数几台MySQL从Master来进行复制，这几台机器我们姑且称之为第一级Slave集群，然后其他的Slave再从第一级Slave集群来进行复制。从第一级Slave进行复制的Slave，我称之为第二级Slave集群。如果有需要，我们可以继续往下增加更多层次的复制。这样，我们很容易就控制了每一台MySQL上面所附属Slave的数量。这种架构我称之为Master-Slaves-Slaves架构 这种多层级联复制的架构，很容易就解决了Master端因为附属Slave太多而成为瓶颈的风险。下图展示了多层级联复制的Replication架构。 读写分离读写分离，基本的原理是让主数据库处理事务性增、改、删操作（INSERT、UPDATE、DELETE），而从数据库处理SELECT查询操作。数据库复制被用来把事务性操作导致的变更同步到集群中的从数据库。 读写分离的好处： 增加冗余 增加了机器的处理能力 对于读操作为主的应用，使用读写分离是最好的场景，因为可以确保写的服务器压力更小，而读又可以接受点时间上的延迟。 读写分离在实现上一般采用复制的方式，读在Slave端，写和事务在Master端，按照其实现可分为如下两种大类： 程序修改mysql操作类 在数据库操作时直接指定读写库的位置，这种方式 优点：直接和数据库通信，简单快捷的读写分离和随机的方式实现的负载均衡，权限独立分配 缺点：数据段和程序的耦合度太高，自己维护更新，增减服务器在代码处理。 数据库代理 使用mysql官方（mysql proxy）或者第三方数据库代理（如mycat）将数据库连接抽象和屏蔽， 优点：直接实现读写分离和负载均衡，不用修改代码， 缺点：增加额外耗时和性能损耗。 Replication的常用方案 master-master架构 两台服务器装mysql，各自作为对方的从机接受对方发来的数据，做到数据的同步备份，感觉和master-slave基本实现原理是一样的。这样保证了数据的一致性，如何保证其中一台服务器故障，自动切换到另外的一个master上呢，使用MMM(MySQL Master-Master Replication Manager)来管理。 heartbeat+drbd+mysql主从复制 基本原理与1相似，这里需要做一个master库的冗余备份，使用drbd来保证不同服务器中两个master库的数据一致性。利用heartbeat来完成其中一台服务器发生故障后的自动切换。结构如下图： Mysql + keepalived 实现双主热备读写分离 keepalived主要用于实现故障切换和热备，作为mysql的补充。 NDB ClusterMySQL NDB Cluster是一个适用于分布式计算环境的高可用性、高冗余版本的MySQL。NDB集群由一组计算机组成，称为主机，每个计算机运行一个或多个进程。这些进程称为节点，可能包括MySQL服务器（用于访问NDB数据）、数据节点（用于存储数据）、一个或多个管理服务器，以及可能的其他专门的数据访问程序。在NDB集群中这些组件的关系如下所示： 所有这些程序一起工作来形成一个NDB集群。当数据被NDB存储引擎存储时，表（和表数据）存储在数据节点中。这样的表可以直接从集群中的所有MySQL服务器（SQL节点）访问。因此，在一个将数据存储在集群中的工资单应用程序中，如果一个应用程序更新了雇员的工资，那么查询这些数据的所有其他MySQL服务器都可以立即看到这个变化。 作者：季舟1链接：https://www.jianshu.com/p/c989ee86d7cf来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 NDB CLUSTER（也称为NDB）是一个内存存储引擎，提供高可用的数据持久化功能。NDB CLUSTER存储引擎可以配置一系列故障转移和负载平衡。 集群节点集群节点有三种类型，在最小的NDB集群配置中，至少会有三个节点。 Management node这种类型节点的作用是管理NDB集群中的其他节点，执行诸如提供配置数据、启动和停止节点以及运行备份等功能。因为这个节点类型管理其他节点的配置，所以应该首先启动这种类型的节点，在任何其他节点之前。执行ndb_mgmd命令启动该节点。 Data node这种类型节点的作用是存储集群数据。一个副本足以用于数据存储，但不提供冗余;因此，建议使用2（或更多）副本来提供冗余，从而获得高可用性。执行ndbd或ndbmtd（多线程）命令启动该节点。NDB集群表通常存储在内存中，而不是在磁盘上（这就是为什么我们将NDB集群称为内存中的数据库）。然而，一些NDB集群数据可以存储在磁盘上。 SQL node在NDB Cluster中SQL节点是一个使用NDBCLUSTER存储引擎的传统MySQL服务器。 期望在生产环境中使用三个节点的设置是不现实的。这样的配置不提供冗余;为了从NDB集群的高可用性特性中获益，您必须使用多个数据和SQL节点。还强烈推荐使用多个管理节点。 客户端标准MySQL客户端NDB集群可以与用PHP、Perl、C、C++、Java、Python、Ruby等编写的现有MySQL应用程序一起使用。这样的客户端应用程序发送SQL语句并接收来自MySQL服务器的响应，它们充当NDB集群SQL节点，就像它们与独立的MySQL服务器交互一样。例如，使用Connector/J 5.0.6和更高版本的Java客户端可以使用jdbc:mysql:loadbalance://url以透明地实现负载平衡。 NDB客户端客户端程序可以使用NDB API（一个高层次的C++ API，NDBCLUSTER存储引擎）直接访问NDB集群数据，绕过任何可能连接到集群的MySQL服务器。对于NDB集群来说，也可以使用NDB集群连接器来为NDB集群编写Java应用程序。NDB集群连接器包括ClusterJ，这是一种类似于对象关系映射持久性框架的高级数据库API，如Hibernate和JPA，它们直接连接到NDBCLUSTER，因此不需要访问MySQL服务器。在NDB集群中也为ClusterJPA提供了支持，这是一个利用ClusterJ和JDBC的优势的NDB集群的OpenJPA实现。ID查找和其他快速操作是使用ClusterJ（绕过MySQL服务器）执行的，而可以从MySQL查询优化器中获益的更复杂的查询是通过MySQL服务器发送的，使用JDBC。 管理客户端这些客户端连接到管理服务器，并提供启动和停止节点的命令，启动和停止消息追踪（仅调试版本），显示节点版本和状态，启动和停止备份，等等。这种类型的程序的一个例子是ndbmgm管理客户端提供的NDB集群。 检查点 一般来说，当数据保存到磁盘时，据说已到达检查点。 更具体的是NDB Cluster，检查点是所有已提交事务存储在磁盘上的时间点。 关于NDB存储引擎，有两种类型的检查点可以协同工作，以确保维护集群数据的一致视图。 这些显示在以下列表中： 本地检查点（LCP）：这是一个特定于单个节点的检查点; 但是，LCP或多或少同时发生在集群中的所有节点上。 LCP通常每隔几分钟发生一次; 精确的间隔会有所不同，并取决于节点存储的数据量，群集活动的级别以及其他因素。在NDB 7.6.4之前，LCP涉及将所有节点的数据保存到磁盘。 NDB 7.6.4引入了对部分LCP的支持，在某些条件下可以显着提高恢复时间。 有关更多信息，请参见第21.1.4.2节“NDB Cluster 7.6中的新增功能”，以及启用部分LCP并控制其使用的存储量的EnablePartialLcp和RecoveryWork配置参数的说明。 全局检查点（GCP）：每隔几秒就会发生一次GCP，此时所有节点的事务都已同步，并且重做日志被刷新到磁盘。 Nodes, Node Groups, Replicas, and Partitions Data Node。 ndbd或ndbmtd进程，用于存储一个或多个副本，即分配给该节点所属的节点组的分区副本（本节稍后讨论）。 每个数据节点应位于单独的计算机上。 虽然也可以在一台计算机上托管多个数据节点进程，但通常不建议这样的配置。 当引用ndbd或ndbmtd进程时，术语“节点”和“数据节点”通常可互换使用; 如上所述，管理节点（ndb_mgmd进程）和SQL节点（mysqld进程）在本讨论中如此指定。 Node group。节点组由一个或多个节点组成，并存储分区或副本集（请参阅下一项）。 NDB群集中的节点组数量不能直接配置; 它是数据节点数和副本数（NoOfReplicas配置参数）的函数，如下所示：# of node groups = # of data nodes / NoOfReplicas Partition。 这是群集存储的数据的一部分。 每个节点负责保留分配给它的任何分区的至少一个副本（即，至少一个副本）。NDB Cluster默认使用的分区数取决于数据节点的数量和数据节点使用的LDM线程数，如下所示：[# of partitions] = [# of data nodes] * [# of LDM threads] 使用运行ndbmtd的数据节点时，LDM线程的数量由MaxNoOfExecutionThreads的设置控制。 使用ndbd时，只有一个LDM线程，这意味着与参与集群的节点一样多的集群分区。 使用ndbmtd且MaxNoOfExecutionThreads设置为3或更小时也是如此。 （您应该知道LDM线程的数量随着此参数的值而增加，但不是严格线性的，并且设置它有其他限制;有关详细信息，请参阅MaxNoOfExecutionThreads的说明。） NDB and user-defined partitioning。 NDB群集通常会自​​动分区NDBCLUSTER表。但是，也可以使用NDBCLUSTER表进行用户定义的分区。这受到以下限制： 使用NDB表生产时仅支持KEY和LINEAR KEY分区方案。 可以为任何NDB表显式定义的最大分区数是8 MaxNoOfExecutionThreads [节点组数]，NDB群集中的节点组数量正如本节前面所讨论的那样确定。使用ndbd进行数据节点进程时，设置MaxNoOfExecutionThreads无效;在这种情况下，为了执行该计算，可以将其视为等于1。 Replica。 这是群集分区的副本。 节点组中的每个节点都存储一个副本。 有时也称为分区副本。 副本数等于每个节点组的节点数。 副本完全属于单个节点; 节点可以（通常会）存储多个副本。 下图说明了一个NDB集群，其中四个数据节点运行ndbd，分别安排在两个节点组中，每个节点组包含两个节点; 节点1和2属于节点组0，节点3和4属于节点组1。这里只显示数据节点; 虽然有效的NDB群集需要ndb_mgmd进程进行群集管理，并且至少有一个SQL节点需要访问群集存储的数据，但为清楚起见，这些已从图中省略。 群集存储的数据分为四个分区，编号为0,1,2和3.每个分区在同一节点组中存储多个副本。分区存储在备用节点组中，如下所示： 分区0存储在节点组0上;主副本（主副本）存储在节点1上，备份副本（分区的备份副本）存储在节点2上。 分区1存储在另一个节点组（节点组1）上;此分区的主副本位于节点3上，其备份副本位于节点4上。 分区2存储在节点组0上。但是，它的两个副本的放置与分区0的放置相反;对于分区2，主副本存储在节点2上，备份存储在节点1上。 分区3存储在节点组1上，并且其两个副本的放置与分区1的位置相反。即，其主副本位于节点4上，备份在节点3上。 对于NDB集群的持续运行，这意味着：只要参与集群的每个节点组至少有一个节点运行，集群就拥有所有数据的完整副本并且仍然可行。这将在下图中说明。 在此示例中，群集由两个节点组组成，每个节点组由两个数据节点组成。 每个数据节点都运行ndbd的实例。 来自节点组0的至少一个节点和来自节点组1的至少一个节点的任何组合足以使群集保持“活动”。 但是，如果来自单个节点组的两个节点都发生故障，则由另一个节点组中的其余两个节点组成的组合是不够的。 在这种情况下，群集已丢失整个分区，因此无法再提供对所有NDB群集数据的完整集合的访问。 在NDB 7.5.4和更高版本中，单个NDB群集实例支持的最大节点组数为48（Bug＃80845，Bug＃22996305）。 Network communication and latencyNDB Cluster需要数据节点和API节点（包括SQL节点）之间以及数据节点和其他数据节点之间的通信，以执行查询和更新。这些进程之间的通信延迟可直接影响观察到的用户查询的性能和延迟。此外，为了在节点无声故障的情况下保持一致性和服务，NDB Cluster使用心跳和超时机制，将来自节点的通信的延长丢失视为节点故障。这可以减少冗余。回想一下，为了保持数据一致性，当节点组中的最后一个节点发生故障时，NDB集群会关闭。因此，为了避免增加强制关闭的风险，应尽可能避免节点之间的通信中断。 数据或API节点的故障导致涉及故障节点的所有未提交事务的中止。数据节点恢复需要在数据节点恢复服务之前，从幸存的数据节点同步故障节点的数据，并重新建立基于磁盘的重做和检查点日志。此恢复可能需要一些时间，在此期间群集以减少的冗余运行。 心跳依赖于所有节点及时生成心跳信号。如果节点过载，由于与其他程序共享导致机器CPU不足，或者由于交换而出现延迟，则可能无法执行此操作。如果心跳生成被充分延迟，则其他节点会将响应缓慢的节点视为失败。 在某些情况下，将慢节点作为故障节点的这种处理可能是或可能不是合乎需要的，这取决于节点的慢速操作对集群其余部分的影响。为NDB群集设置HeartbeatIntervalDbDb和HeartbeatIntervalDbApi等超时值时，必须注意实现快速检测，故障转移和恢复服务，同时避免可能出现的代价高昂的误报。 如果预期数据节点之间的通信延迟高于LAN环境中预期的通信延迟（大约100μs），则必须增加超时参数以确保任何允许的延迟时段都在配置的超时内。以这种方式增加超时对于检测故障的最坏情况时间以及因此服务恢复的时间具有相应的影响。 LAN环境通常可以配置稳定的低延迟，并且可以通过快速故障转移提供冗余。可以从TCP级别可见的最小和受控延迟（NDB群集正常运行）中恢复单个链路故障。 WAN环境可能会提供一系列延迟，以及冗余和较慢的故障转移时间。单个链路故障可能需要在端到端连接恢复之前传播路由更改。在TCP级别，这可能表现为各个通道上的大延迟。在这些情况下观察到的最坏情况的TCP延迟与IP层在故障周围重新路由的最坏情况时间有关。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++知识]]></title>
    <url>%2FC-C-%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[构造函数 析构函数析构函数和构造函数是一对。构造函数用于创建对象，而析构函数是用来撤销对象。 内联函数inline1.内联函数在运行时可调试，而宏定义不可以;2.编译器会对内联函数的参数类型做安全检查或自动类型转换（同普通函数），而宏定义则不会；3.内联函数可以访问类的成员变量，宏定义则不能；4.在类中声明同时定义的成员函数，自动转化为内联函数。 C语言中switch 的查找实现原理 if…else结果的查找当case语句是小于3句的时候，switch语句底层的实现和if…else的实现方式相同。 线性查找当case语句大于等于4的时候，且每两个case之间产生的间隔之和不超过6时，就按线性结构查找。即，如下图的汇编里面的jmp dword ptr [edx*4+11B1428h]该指令里面的11B1428h地址里面，其存放着各个case语句的首地址。由于内存中下标是从0开始的，因此，通过对其进行减一操作，在判断其是否大于11B1428h地址的数组长度，如果大于直接跳出，否则通过计算直接定位到该数组上的地址进行跳转。 树形查找当最大case值和最小case值之差大于255的情况下，此时，编译器会采用树形查找。即，将数据由小到大排列，并取中间值（如果是偶数，就取中间两个靠右的那一个），在左右两边继续取中间值划分（左右两边划分不需要将中间值算进去），直到小于等于3个数据的时候不在划分。​​ 内存泄露https://blog.csdn.net/u012662731/article/details/78652651]]></content>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程与并发]]></title>
    <url>%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[线程池基本组成1、线程池管理器（ThreadPool）：用于创建并管理线程池，包括 创建线程池，销毁线程池，添加新任务；2、工作线程（PoolWorker）：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务；3、任务接口（Task）：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等；4、任务队列（taskQueue）：用于存放没有处理的任务。提供一种缓冲机制。 降低资源消耗。 通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。 当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。使用线程池可以进行统一的分配，调优和监控，延时执行、定时循环执行的策略等。 java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类 构造参数 1234567891011java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类构造参数public ThreadPoolExecutor(int ==corePoolSize==,该线程池中核心线程数最大值。线程池新建线程的时候，如果当前线程总数小于corePoolSize，则新建的是核心线程，如果超过corePoolSize，则新建的是非核心线程 核心线程默认情况下会一直存活在线程池中，即使这个核心线程啥也不干(闲置状态)。 如果指定ThreadPoolExecutor的allowCoreThreadTimeOut这个属性为true，那么核心线程如果不干活(闲置状态)的话，超过一定时间(时长下面参数决定)，就会被销毁掉 int ==maximumPoolSize==,（线程不够用时能够创建的最大线程数）线程总数 = 核心线程数 + 非核心线程数long ==keepAliveTime==,非核心线程闲置超时时长TimeUnit ==unit==,keepAliveTime的单位，TimeUnit是一个枚举类型BlockingQueue&lt;Runnable&gt; ==workQueue==,任务队列：维护着等待执行的Runnable对象 当所有的核心线程都在干活时，新添加的任务会被添加到这个队列中等待处理，如果队列满了，则新建非核心线程执行任务 ThreadFactory ==threadFactory==, 创建新线程，Executors.defaultThreadFactory()RejectedExecutionHandler ==handler ==线程池的饱和策略) 通过ThreadPoolExecutor.execute(Runnable command)方法即可向线程池内添加一个任务 当一个任务被添加进线程池时： 1、线程数量未达到corePoolSize，则新建一个线程(核心线程)执行任务2、线程数量达到了corePools，则将任务移入队列BlockingQueue等待3、如果无法将任务加入BlockingQueue（队列已满），则在非corePool中创建新的线程来处理任务（注意，执行这一步骤需要获取全局锁）。4、队列已满，总线程数又达到了maximumPoolSize，(RejectedExecutionHandler)抛出异常 四种java实现好的线程池CachedThreadPool() 可缓存线程池：线程数无限制有空闲线程则复用空闲线程，若无空闲线程则新建线程一定程序减少频繁创建/销毁线程，减少系统开销 FixedThreadPool() 定长线程池：可控制线程最大并发数（同时执行的线程数）超出的线程会在队列中等待 ScheduledThreadPool() 定长线程池：支持定时及周期性任务执行。 SingleThreadExecutor() 单线程化的线程池：有且仅有一个工作线程执行任务所有任务按照指定顺序执行，即遵循队列的入队出队规则 123456789101112import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class ThreadPoolExecutorTest &#123;public static void main(String[] args) &#123;ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5);scheduledThreadPool.schedule(new Runnable() &#123;public void run() &#123;System.out.println(&quot;delay 3 seconds&quot;);&#125;&#125;, 3, TimeUnit.SECONDS);&#125;&#125; 线程池的状态 线程池风险：死锁、资源不足、并发错误、 线程泄漏、请求过载 执行execute()方法和submit()方法的区别是什么呢？ execute() 方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit） 方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 使用线程Thread类如果一个类继承Thread，则不适合资源共享。但是如果实现了Runable接口的话，则很容易的实现资源共享。实现Runnable接口比继承Thread类所具有的优势： 适合多个相同的程序代码的线程去处理同一个资源 可以避免java中的单继承的限制 增加程序的健壮性，代码可以被多个线程共享，代码和数据独立 12345//继承 ThreadThread1 mTh1=new Thread1("A");Thread1 mTh2=new Thread1("B");mTh1.start();mTh2.start(); Runnable接口1234567891011先看一下**java.lang.Runnable**吧，它是一个接口，在它里面只声明了一个run()方法：由于run()方法返回值为void类型，所以在执行完任务之后无法返回任何结果。public interface Runnable &#123; public abstract void run();&#125;//实现Runnable接口Thread2 mTh = new Thread2();new Thread(mTh, "C").start();//同一个mTh，但是在Thread中就不可以，如果用同一个实例化对象mt，就会出现异常 new Thread(mTh, "D").start();new Thread(mTh, "E").start(); 这两种方式都有一个缺陷就是：在执行完任务之后无法获取执行结果。如果需要获取执行结果，就必须通过共享变量或者使用线程通信的方式来达到效果，这样使用起来就比较麻烦。 而自从Java 1.5开始，就提供了Callable和Future，通过它们可以在任务执行完毕之后得到任务执行结果。 Callable接口Callable接口位于java.util.concurrent包下，在它里面也只声明了一个方法，只不过这个方法叫做call()。123public interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; 可以看到，这是一个泛型接口，call()函数返回的类型就是传递进来的V类型。Callable接口可以看作是Runnable接口的补充，call方法带有返回值，并且可以抛出异常。 FutureTask类如何获取Callable的返回结果呢？一般是通过FutureTask这个中间媒介来实现的。整体的流程是这样的： 把Callable实例当作参数，生成一个FutureTask的对象 然后把这个对象当作一个Runnable，作为参数另起线程。 由于FutureTask实现了Runnable，因此它既可以通过Thread包装来直接执行，也可以提交给ExecuteService来执行。下面以Thread包装线程方式启动来说明一下。 1234567891011121314151617181920212223242526import java.util.concurrent.Callable;import java.util.concurrent.FutureTask;public class Demo &#123; public static void main(String[] args) throws Exception &#123; Callable&lt;Integer&gt; call = new Callable&lt;Integer&gt;() &#123; public Integer call() throws Exception &#123; System.out.println("计算线程正在计算结果..."); Thread.sleep(3000); return 1; &#125; &#125;; FutureTask&lt;Integer&gt; task = new FutureTask&lt;&gt;(call); Thread thread = new Thread(task); thread.start(); System.out.println("main线程干点别的..."); Integer result = task.get(); System.out.println("从计算线程拿到的结果为：" + result); &#125;&#125; Future接口FutureTask继承体系中的核心接口是Future。 Future的核心思想是：一个方法，计算过程可能非常耗时，等待方法返回，显然不明智。可以在调用方法的时候，立马返回一个Future，可以通过Future这个数据结构去控制方法f的计算过程。这里的控制包括： 12345678910public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning);//还没计算完，可以取消计算过程 boolean isCancelled();///判断计算是否被取消 boolean isDone();//判断是否计算完 V get() throws InterruptedException, ExecutionException; //获取计算结果（如果还没计算完，也是必须等待的） V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 也就是说Future提供了三种功能： 判断任务是否完成； 能够中断任务； 能够获取任务执行结果。 实现Runnable接口和Callable接口的区别如果想让线程池执行任务的话需要实现的Runnable接口或Callable接口。 Runnable接口或Callable接口实现类都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行。两者的区别在于 Runnable 接口不会返回结果但是 Callable 接口可以返回结果。 备注： 工具类 Executors 可以实现 Runnable 对象和 Callable 对象之间的相互转换。（`Executors.callable（Runnable task)也就是说Future提供了三种功能： 1）判断任务是否完成； 2）能够中断任务； 3）能够获取任务执行结果。或Executors.callable（Runnable task，Object resule）` ）。 volatile关键字如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。为了解决缓存不一致性问题，通常来说有以下2种解决方法： 通过在总线加LOCK#锁的方式 通过缓存一致性协议 这2种方式都是硬件层面上提供的方式。 i如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。 缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 Java内存间交互操作JLS定义了线程对主存的操作指令：lock，unlock，read，load，use，assign，store，write。这些行为是不可分解的原子操作，在使用上相互依赖，read-load从主内存复制变量到当前工作内存，use-assign执行代码改变共享变量值，store-write用工作内存数据刷新主存相关内容。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 原子性，可见性，有序性多线程的三个特性。 对于可见性，Java提供了volatile关键字来保证可见性。 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 下面这段话摘自《深入理解Java虚拟机》：“观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令” lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效。 下面列举几个Java中使用volatile的几个场景。 状态标记量 double check防止指令重排序:防止指令重排序导致其他线程获取到未初始化完的对象。instance = new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。 给 instance 分配内存 调用 Singleton 的构造函数来初始化成员变量 将instance对象指向分配的内存空间（执行完这步 instance 就为非 null 了） 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后报错。加了volatile就不会重排序。 当new一个对象的时候，也是被分配在主内存。 synchronized 关键字和 volatile 关键字的区别 volatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字要好。但是volatile关键字只能用于变量，而synchronized关键字可以修饰方法以及代码块。 多线程访问volatile关键字不会发生阻塞，而synchronized关键字可能会发生阻塞。 volatile关键字能保证数据的可见性，但不能保证数据的原子性。synchronized关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized关键字解决的是多个线程之间访问资源的同步性。 基础线程机制Executor管理多个异步任务的执行，而无需程序员显式地管理线程的生命周期。这里的异步是指多个任务的执行互不干扰，不需要进行同步操作。 主要有三种 Executor： CachedThreadPool：一个任务创建一个线程； FixedThreadPool：所有任务只能使用固定大小的线程； SingleThreadExecutor：相当于大小为 1 的 FixedThreadPool。 1234567public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; executorService.execute(new MyRunnable()); &#125; executorService.shutdown();&#125; Daemon守护线程是程序运行时在后台提供服务的线程，不属于程序中不可或缺的部分。当所有非守护线程结束时，程序也就终止，同时会杀死所有守护线程。main() 属于非守护线程。使用 setDaemon() 方法将一个线程设置为守护线程。 1234public static void main(String[] args) &#123;Thread thread = new Thread(new MyRunnable());thread.setDaemon(true);&#125; sleep()Thread.sleep(millisec) 方法会休眠当前正在执行的线程，millisec 单位为毫秒。 sleep() 可能会抛出 InterruptedException，因为异常不能跨线程传播回 main() 中，因此必须在本地进行处理。线程中抛出的其它异常也同样需要在本地进行处理。 1234567public void run() &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; yield()对静态方法 Thread.yield() 的调用声明了当前线程已经完成了生命周期中最重要的部分，可以切换给其它线程来执行。该方法只是对线程调度器的一个建议，而且也只是建议具有相同优先级的其它线程可以运行。 123public void run() &#123; Thread.yield();&#125; 中断一个线程执行完毕之后会自动结束，如果在运行过程中发生异常也会提前结束。 InterruptedException通过调用一个线程的 interrupt() 来中断该线程，如果该线程处于阻塞、限期等待或者无限期等待状态，那么就会抛出 InterruptedException，从而提前结束该线程。但是不能中断 I/O 阻塞和 synchronized 锁阻塞。 对于以下代码，在 main() 中启动一个线程之后再中断它，由于线程中调用了 Thread.sleep() 方法，因此会抛出一个 InterruptedException，从而提前结束线程，不执行之后的语句。 123456789101112131415161718public class InterruptExample &#123; private static class MyThread1 extends Thread &#123; @Override public void run() &#123; try &#123; Thread.sleep(2000); System.out.println(&quot;Thread run&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 123456public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new MyThread1(); thread1.start(); thread1.interrupt(); System.out.println(&quot;Main run&quot;);&#125; 123456Main runjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at InterruptExample.lambda$main$0(InterruptExample.java:5) at InterruptExample$$Lambda$1/713338599.run(Unknown Source) at java.lang.Thread.run(Thread.java:745) interrupted()如果一个线程的 run() 方法执行一个无限循环，并且没有执行 sleep() 等会抛出 InterruptedException 的操作，那么调用线程的 interrupt() 方法就无法使线程提前结束。 但是调用 interrupt() 方法会设置线程的中断标记，此时调用 interrupted() 方法会返回 true。因此可以在循环体中使用 interrupted() 方法来判断线程是否处于中断状态，从而提前结束线程。 1234567891011121314public class InterruptExample &#123; private static class MyThread2 extends Thread &#123; @Override public void run() &#123; while (!interrupted()) &#123; // .. &#125; System.out.println(&quot;Thread end&quot;); &#125; &#125;&#125; 12345public static void main(String[] args) throws InterruptedException &#123; Thread thread2 = new MyThread2(); thread2.start(); thread2.interrupt();&#125; 1Thread end Executor 的中断操作调用 Executor 的 shutdown() 方法会等待线程都执行完毕之后再关闭，但是如果调用的是 shutdownNow() 方法，则相当于调用每个线程的 interrupt() 方法。 以下使用 Lambda 创建线程，相当于创建了一个匿名内部线程。 12345678910111213public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; &#123; try &#123; Thread.sleep(2000); System.out.println(&quot;Thread run&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); executorService.shutdownNow(); System.out.println(&quot;Main run&quot;);&#125; 12345678Main runjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at ExecutorInterruptExample.lambda$main$0(ExecutorInterruptExample.java:9) at ExecutorInterruptExample$$Lambda$1/1160460865.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 如果只想中断 Executor 中的一个线程，可以通过使用 submit() 方法来提交一个线程，它会返回一个 Future&lt;?&gt; 对象，通过调用该对象的 cancel(true) 方法就可以中断线程。 1234Future&lt;?&gt; future = executorService.submit(() -&gt; &#123; // ..&#125;);future.cancel(true); synchronized关键字JVM 实现的 synchronizedJDK 实现的 ReentrantLock synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 主要的三种使用方式 修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁。 修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 。也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份，所以对该类的所有对象都加了锁）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 和 synchronized 方法一样，synchronized(this)代码块也是锁定当前对象的。synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。这里再提一下：synchronized关键字加到非 static 静态方法上是给对象实例上锁。另外需要注意的是：尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓冲功能。 双重校验锁实现对象单例（线程安全）12345678910111213141516171819202122public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; //类对象加锁 synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的，uniqueInstance = new Singleton() 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出先问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 底层原理synchronized 同步语句块的情况123456789public class SynchronizedDemo &#123; public void method() &#123; synchronized (this) &#123; System.out.println("synchronized 代码块"); &#125; &#125;&#125; 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行javac SynchronizedDemo.java命令生成编译后的 .class 文件，然后执行 javap -c -s -v -l SynchronizedDemo.class 从上面我们可以看出： synchronized 同步语句块的实现使用的是 monitorenter和 monitorexit指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权。当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 synchronized 修饰方法的的情况12345public class SynchronizedDemo2 &#123; public synchronized void method() &#123; System.out.println("synchronized 方法"); &#125;&#125; synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED标识，该标识指明了该方法是一个同步方法，JVM 通过该访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 synchronized和ReenTrantLock 的区别 两者都是可重入锁两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 synchronized 依赖于 JVM 而 ReenTrantLock 依赖于 APIsynchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReenTrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ReenTrantLock 比 synchronized 增加了一些高级功能相比synchronized，ReenTrantLock增加了一些高级功能。主要来说主要有三点： 等待可中断； 可实现公平锁； 可实现选择性通知（锁可以绑定多个条件） ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock默认情况是非公平的，可以通过 ReenTrantLock类的ReentrantLock(boolean fair) 构造方法来制定是否是公平的。 synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 锁的不是代码。是对象 对象在内存中的布局：对象头、实例数据、对齐填充 释放的时机总结下使用synchronized同步锁释放的时机。我们知道程序执行进入同步代码块中monitorenter代表尝试获取锁，退出代码块monitorexit代表释放锁。而在程序中，是无法显式释放对同步监视器的锁的，而会在如下4种情况下释放锁。 当前线程的同步方法、代码块执行结束的时候释放 当前线程在同步方法、同步代码块中遇到break 、 return 终于该代码块或者方法的时候释放。 出现未处理的error或者exception导致异常结束的时候释放 程序执行了 同步对象 wait 方法 ，当前线程暂停，释放锁 在以下两种情况不会释放锁。 代码块中使用了 Thread.sleep() Thread.yield() 这些方法暂停线程的执行，不会释放。 线程执行同步代码块时，其他线程调用 suspend 方法将该线程挂起，该线程不会释放锁 ，所以我们应该避免使用 suspend 和 resume 来控制线程 。 JVM 对 synchronized 的锁优化自旋锁锁消除锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除。锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。 对于一些看起来没有加锁的代码，其实隐式的加了很多锁。例如下面的字符串拼接代码就隐式加了锁： 1234public static String concatString(String s1, String s2, String s3) &#123; return s1 + s2 + s3;&#125; String 是一个不可变的类，编译器会对 String 的拼接自动优化。在 JDK 1.5 之前，会转化为 StringBuffer 对象的连续 append() 操作： 12345678public static String concatString(String s1, String s2, String s3) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString();&#125; 每个 append() 方法中都有一个同步块。虚拟机观察变量 sb，很快就会发现它的动态作用域被限制在 concatString() 方法内部。也就是说，sb 的所有引用永远不会逃逸到 concatString() 方法之外，其他线程无法访问到它，因此可以进行消除。 锁粗化如果一系列的连续操作都对同一个对象反复加锁和解锁，频繁的加锁操作就会导致性能损耗。 上一节的示例代码中连续的 append() 方法就属于这类情况。如果虚拟机探测到由这样的一串零碎的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部。对于上一节的示例代码就是扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，这样只需要加锁一次就可以了。 轻量级锁JDK 1.6 引入了偏向锁和轻量级锁，从而让锁拥有了四个状态：无锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。 下图左侧是一个线程的虚拟机栈，其中有一部分称为 Lock Record 的区域，这是在轻量级锁运行过程创建的，用于存放锁对象的 Mark Word。而右侧就是一个锁对象，包含了 Mark Word 和其它信息。 轻量级锁是相对于传统的重量级锁而言，它使用 CAS 操作来避免重量级锁使用互斥量的开销。对于绝大部分的锁，在整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步，如果 CAS 失败了再改用互斥量进行同步。 加锁过程： 在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，官方称之为 Displaced Mark Word。这时候线程堆栈与对象头的状态如图1所示。 拷贝对象头中的Mark Word复制到锁记录中。 拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock record里的owner指针指向object mark word。如果更新成功，则执行步骤（3），否则执行步骤（4）。 如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，即表示此对象处于轻量级锁定状态，这时候线程堆栈与对象头的状态如图2所示。 如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行。否则说明多个线程竞争锁，轻量级锁就要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也要进入阻塞状态。 而当前线程便尝试使用自旋来获取锁，自旋就是为了不让线程阻塞，而采用循环去获取锁的过程。 如果 CAS 操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的虚拟机栈，如果是的话说明当前线程已经拥有了这个锁对象，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁。 解锁过程： 通过CAS操作尝试把线程中复制的Displaced Mark Word对象替换当前的Mark Word。 如果替换成功，整个同步过程就完成了。 如果替换失败，说明有其他线程尝试过获取该锁（此时锁已膨胀），那就要在释放锁的同时，唤醒被挂起的线程。 偏向锁引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗必须小于节省下来的CAS原子指令的性能消耗）。上面说过，轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。 偏向锁获取过程： 访问Mark Word中偏向锁的标识是否设置成1，锁标志位是否为01——确认为可偏向状态。 如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤（5），否则进入步骤（3）。 如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行（5）；如果竞争失败，执行（4）。 如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。 执行同步代码。 偏向锁的释放： 偏向锁的撤销在上述第四步骤中有提到：偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。 偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 重量级锁、轻量级锁和偏向锁之间转换 锁 悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中 synchronized和 ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中 java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁一般会使用版本号机制或CAS算法实现。 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子：假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 12341、操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。2、在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。3、操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。4、操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 CAS算法CAS算法乐观锁的一种表现。 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 输入： 需要读写的内存位置 V 我们认为这个位置现在的值 A 想要写入的新值 B 输出： V 位置以前的值（无论写入操作是否成功）含义： 我们认为 V 处的值应该是 A，如果是，把 V 处的值改为 B，如果不是则不修改，然后把 V 处现在的值返回给我。一般情况下是一个自旋操作，即不断的重试。 自旋锁、自适应自旋锁自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁。 1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125; 可重入的自旋锁123456789101112131415161718192021222324252627282930313233public class ReentrantSpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); private int count; public void lock() &#123; Thread current = Thread.currentThread(); if (current == cas.get()) &#123; // 如果当前线程已经获取到了锁，线程数增加一，然后返回 count++; return; &#125; // 如果没获取到锁，则通过CAS自旋 while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread cur = Thread.currentThread(); if (cur == cas.get()) &#123; if (count &gt; 0) &#123;// 如果大于0，表示当前线程多次获取了该锁，释放锁通过count减一来模拟 count--; &#125; else &#123;// 如果count==0，可以将锁释放，这样就能保证获取锁的次数与释放锁的次数是一致的了。 cas.compareAndSet(cur, null); &#125; &#125; &#125;&#125; 自旋锁与互斥锁 自旋锁与互斥锁都是为了实现保护资源共享的机制。 无论是自旋锁还是互斥锁，在任意时刻，都最多只能有一个保持者。 获取互斥锁的线程，如果锁已经被占用，则该线程将进入睡眠状态；获取自旋锁的线程则不会睡眠，而是一直循环等待锁释放。 乐观锁的缺点1、 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference类就提供了此种能力，其中的 compareAndSet方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 、循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3、 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了 AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用 AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS与synchronized的使用情景简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 Lock接口简介锁是用于通过多个线程控制对共享资源的访问的工具。通常，锁提供对共享资源的独占访问：一次只能有一个线程可以获取锁，并且对共享资源的所有访问都要求首先获取锁。 但是，一些锁可能允许并发访问共享资源，如ReadWriteLock的读写锁。 在Lock接口出现之前，Java程序是靠synchronized关键字实现锁功能的。JDK1.5之后并发包中新增了Lock接口以及相关实现类来实现锁功能。 虽然synchronized方法和语句的范围机制使得使用监视器锁更容易编程，并且有助于避免涉及锁的许多常见编程错误，但是有时您需要以更灵活的方式处理锁。例如，用于遍历并发访问的数据结构的一些算法需要使用“手动”或“链锁定”：您获取节点A的锁定，然后获取节点B，然后释放A并获取C，然后释放B并获得D等。在这种场景中synchronized关键字就不那么容易实现了，使用Lock接口容易很多。 Lock的简单使用Lock接口的实现类：ReentrantLock ， ReentrantReadWriteLock.ReadLock ， ReentrantReadWriteLock.WriteLock 1234567891011121314151617181920212223public class LockExample &#123; private Lock lock = new ReentrantLock(); public void func() &#123; lock.lock(); try &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.print(i + " "); &#125; &#125; finally &#123; lock.unlock(); // 确保释放锁，从而避免发生死锁。 &#125; &#125;&#125;public static void main(String[] args) &#123; LockExample lockExample = new LockExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; lockExample.func()); executorService.execute(() -&gt; lockExample.func()); //0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9&#125; Lock接口特性 特性 描述 尝试非阻塞地获取锁 当前线程尝试获取锁，如果这一时刻锁没有被其他线程获取到，则成功获取并持有锁 能被中断地获取锁 获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常将会被抛出，同时锁会被释放 超时获取锁 在指定的截止时间之前获取锁， 超过截止时间后仍旧无法获取则返回 ReentrantLock类常见方法： 方法名称 描述 ReentrantLock() 创建一个 ReentrantLock的实例。 ReentrantLock(boolean fair) 创建一个特定锁类型（公平锁/非公平锁）的ReentrantLock的实例 Condition接口简介synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。 Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是有JVM选择的，使用ReentrantLock类结合Condition实例可以实现“选择性通知”，这个功能非常重要，而且是Condition接口默认提供的。 而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程 公平锁与非公平锁Lock锁分为：公平锁 和 非公平锁。 公平锁表示线程获取锁的顺序是按照线程加锁的顺序来分配的，即先来先得的FIFO先进先出顺序。 非公平锁就是一种获取锁的抢占机制，是随机获取锁的，和公平锁不一样的就是先来的不一定先的到锁，这样可能造成某些线程一直拿不到锁，结果也就是不公平的了。 线程间的协作join()在线程中调用另一个线程的 join() 方法，会将当前线程挂起，而不是忙等待，直到目标线程结束。 对于以下代码，虽然 b 线程先启动，但是因为在 b 线程中调用了 a 线程的 join() 方法，b 线程会等待 a 线程结束才继续执行，因此最后能够保证 a 线程的输出先于 b 线程的输出。 1234567891011121314151617181920212223242526272829303132333435public class JoinExample &#123; private class A extends Thread &#123; @Override public void run() &#123; System.out.println(&quot;A&quot;); &#125; &#125; private class B extends Thread &#123; private A a; B(A a) &#123; this.a = a; &#125; @Override public void run() &#123; try &#123; a.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125; System.out.println(&quot;B&quot;); &#125; &#125; public void test() &#123; A a = new A(); B b = new B(a); b.start(); a.start(); &#125;&#125; 12345public static void main(String[] args) &#123; JoinExample example = new JoinExample(); example.test(); //A B&#125; wait、notify、notifyAll()调用 wait() 使得线程等待某个条件满足，线程在等待时会被挂起，当其他线程的运行使得这个条件满足时，其它线程会调用 notify() 或者 notifyAll() 来唤醒挂起的线程。 它们都属于 Object 的一部分，而不属于 Thread。 只能用在同步方法或者同步控制块中使用，否则会在运行时抛出 IllegalMonitorStateExeception。 使用 wait() 挂起期间，线程会释放锁。这是因为，如果没有释放锁，那么其它线程就无法进入对象的同步方法或者同步控制块中，那么就无法执行 notify() 或者 notifyAll() 来唤醒挂起的线程，造成死锁。 wait() 是 Object 的方法，而 sleep() 是 Thread 的静态方法； wait() 会释放锁，sleep() 不会，是让出cpu。 await、signal、signalAll1234567891011121314151617线程consumersynchronize(obj)&#123; obj.wait();//没东西了，等待&#125;线程producersynchronize(obj)&#123; obj.notify();//有东西了，唤醒 &#125;lock.lock();condition.await();lock.unlock();lock.lock();condition.signal();lock.unlock; 为了突出区别，省略了若干细节。区别有三点： lock不再用synchronize把同步代码包装起来； 阻塞需要另外一个对象condition； 同步和唤醒的对象是condition而不是lock，对应的方法是await和signal，而不是wait和notify。 为什么需要使用condition呢？简单一句话，lock更灵活。以前的方式只能有一个等待队列，在实际应用时可能需要多个，比如读和写。为了这个灵活性，lock将同步互斥控制和等待队列分离开来，互斥保证在某个时刻只有一个线程访问临界区（lock自己完成），等待队列负责保存被阻塞的线程（condition完成）。 通过查看ReentrantLock的源代码发现，condition其实是等待队列的一个管理者，condition确保阻塞的对象按顺序被唤醒。 在Lock的实现中，LockSupport被用来实现线程状态的改变，后续将更进一步研究LockSupport的实现机制。 线程的状态 使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。cpu会从众多的可执行态里选择，也就是说，当前也就是刚刚的那个线程还是有可能会被再次执行到的，并不是说一定会执行其他线程而该线程在下一次中不会执行到了。 用了yield方法后，该线程就会把CPU时间让掉，让其他或者自己的线程执行（也就是谁先抢到谁执行） ThreadLocalThreadLocal 不继承 Thread，也不实现 Runable 接口， ThreadLocal 类为每一个线程都维护了自己独有的变量拷贝。每个线程都拥有自己独立的变量，其作用在于数据独立。ThreadLocal 采用 hash 表的方式来为每个线程提供一个变量的副本 阻塞队列简介阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。 阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 用优先队列实现最大堆最小堆 12345678Comparator&lt;Integer&gt; mycomparator = new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return o2.compareTo(o1); &#125;&#125;; maxheap = new PriorityQueue&lt;Integer&gt;(20,mycomparator); minheap = new PriorityQueue&lt;Integer&gt;(20); 阻塞队列实现原理如果队列是空的，消费者会一直等待，当生产者添加元素时候，消费者是如何知道当前队列有元素的呢？如果让你来设计阻塞队列你会如何设计，让生产者和消费者能够高效率的进行通讯呢？让我们先来看看JDK是如何实现的。 使用通知模式实现。所谓通知模式，就是当生产者往满的队列里添加元素时会阻塞住生产者，当消费者消费了一个队列中的元素后，会通知生产者当前队列可用。通过查看JDK源码发现ArrayBlockingQueue使用了Condition来实现，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243private final Condition notFull;private final Condition notEmpty;public ArrayBlockingQueue(int capacity, boolean fair) &#123; //省略其他代码 notEmpty = lock.newCondition(); notFull = lock.newCondition();&#125;public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == items.length) notFull.await(); insert(e); &#125;finally &#123; lock.unlock(); &#125;&#125;public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == 0) notEmpty.await(); return extract(); &#125;finally &#123; lock.unlock(); &#125;&#125;private void insert(E x) &#123; items[putIndex] = x; putIndex = inc(putIndex); ++count; notEmpty.signal();&#125; 当我们往队列里插入一个元素时，如果队列不可用，阻塞生产者主要通过LockSupport.park(this);来实现 1234567891011121314151617181920public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 手写生产者消费者使用Object.wait()和Object.notify()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class Test &#123; private int queueSize = 10; private PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;Integer&gt;(queueSize); public static void main(String[] args) &#123; Test test = new Test(); Producer producer = test.new Producer(); Consumer consumer = test.new Consumer(); producer.start(); consumer.start(); &#125; class Consumer extends Thread &#123; @Override public void run() &#123; consume(); &#125; private void consume() &#123; while(true) &#123; synchronized (queue) &#123; while(queue.size() == 0) &#123; try &#123; System.out.println("队列空，等待数据"); queue.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); queue.notify(); &#125; &#125; queue.poll(); //每次移走队首元素 queue.notify(); System.out.println("从队列取走一个元素，队列剩余"+queue.size()+"个元素"); &#125; &#125; &#125; &#125; class Producer extends Thread &#123; @Override public void run() &#123; produce(); &#125; private void produce() &#123; while(true) &#123; synchronized (queue) &#123; while(queue.size() == queueSize) &#123; try &#123; System.out.println("队列满，等待有空余空间"); queue.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); queue.notify(); &#125; &#125; queue.offer(1); //每次插入一个元素 queue.notify(); System.out.println("向队列取中插入一个元素，队列剩余空间："+(queueSize-queue.size())); &#125; &#125; &#125; &#125;&#125; 使用阻塞队列实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class Test &#123; private int queueSize = 10; private ArrayBlockingQueue&lt;Integer&gt; queue = new ArrayBlockingQueue&lt;Integer&gt;(queueSize); public static void main(String[] args) &#123; Test test = new Test(); Producer producer = test.new Producer(); Consumer consumer = test.new Consumer(); producer.start(); consumer.start(); &#125; class Consumer extends Thread &#123; @Override public void run() &#123; consume(); &#125; private void consume() &#123; while(true) &#123; try &#123; queue.take(); System.out.println("从队列取走一个元素，队列剩余"+queue.size()+"个元素"); &#125; catch (InterruptedException e) &#123;e.printStackTrace();&#125; &#125; &#125; &#125; class Producer extends Thread &#123; @Override public void run() &#123; produce(); &#125; private void produce() &#123; while(true) &#123; try &#123; queue.put(1); System.out.println("向队列取中插入一个元素，队列剩余空间："+(queueSize-queue.size())); &#125; catch (InterruptedException e) &#123;e.printStackTrace();&#125; &#125; &#125; &#125;&#125; Atomic 原子类Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。 所以，所谓原子类说简单点就是具有原子/原子操作特征的类。并发包 java.util.concurrent 的原子类都存放在 java.util.concurrent.atomic 下。 JUC 包中的原子类是哪4类 基本类型使用原子的方式更新基本类型 AtomicInteger：整形原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类 数组类型使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray ：引用类型数组原子类 引用类型 AtomicReference：引用类型原子类 AtomicStampedRerence：原子更新引用类型里的字段原子类 AtomicMarkableReference ：原子更新带有标记位的引用类型 对象的属性修改类型 AtomicIntegerFieldUpdater:原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicInteger 类常用方法1234567public final int get() //获取当前的值public final int getAndSet(int newValue)//获取当前的值，并设置新的值public final int getAndIncrement()//获取当前的值，并自增public final int getAndDecrement() //获取当前的值，并自减public final int getAndAdd(int delta) //获取当前的值，并加上预期的值boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update）public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 使用 AtomicInteger 之后，不用对 increment() 方法加锁也可以保证线程安全 AtomicInteger 类的原理1234567891011// setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用）private static final Unsafe unsafe = Unsafe.getUnsafe();private static final long valueOffset;static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField("value")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125; private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址，返回值是 valueOffset。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 AQS（AbstractQueuedSynchronizer)AQS在java.util.concurrent.locks包下面。是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。 并发编程中一些问题多线程就一定好吗？快吗？并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：内存泄漏、上下文切换、死锁还有受限于硬件和软件的资源闲置问题。 多线程就是几乎同时执行多个线程（一个处理器在某一个时间点上永远都只能是一个线程！即使这个处理器是多核的，除非有多个处理器才能实现多个线程同时运行）。CPU通过给每个线程分配CPU时间片来实现伪同时运行，因为CPU时间片一般很短很短，所以给人一种同时运行的感觉。 上下文切换当前任务在执行完CPU时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换会这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。Linux相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 那么我们现在可能会考虑 ：如何减少上下文切换的次数呢？ 减少上下文切换上下文切换又分为2种：让步式上下文切换和抢占式上下文切换。前者是指执行线程主动释放CPU，与锁竞争严重程度成正比，可通过减少锁竞争和使用CAS算法来避免；后者是指线程因分配的时间片用尽而被迫放弃CPU或者被其他优先级更高的线程所抢占，一般由于线程数大于CPU可用核心数引起，可通过适当减少线程数和使用协程来避免。 总结一下： 减少锁的使用。因为多线程竞争锁时会引起上下文切换。 使用CAS算法。这种算法也是为了减少锁的使用。CAS算法是一种无锁算法。 减少线程的使用。人物很少的时候创建大量线程会导致大量线程都处于等待状态。 使用协程（微线程或者说是轻量级的线程，它占用的内存更少并且更灵活）。 我们上面提到了两个名词：“CAS算法” 和 “协程”。可能有些人不是很了解这俩东西，所以这里简单说一下。 避免死锁在操作系统中，死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。 在线程中，如果两个线程同时等待对方释放锁也会产生死锁。 锁是一个好东西，但是使用不当就会造成死锁。一旦死锁产生程序就无法继续运行下去。所以如何避免死锁的产生，在我们使用并发编程时至关重要。根据《Java并发编程的艺术》有下面四种避免死锁的常见方法： 避免一个线程同时获得多个锁 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况 解决资源限制什么是资源限制所谓资源限制就是我们在进行并发编程时，程序的运行速度受限于计算机硬件资源比如CPU,内存等等或软件资源比如软件的质量、性能等等。举个例子：如果说服务器的带宽只有2MB/s，某个资源的下载速度是1MB/s，系统启动10个线程下载该资源并不会导致下载速度编程10MB/s，所以在并发编程时，需要考虑这些资源的限制。硬件资源限制有：带宽的上传和下载速度、硬盘读写速度和CPU处理速度；软件资源限制有数据库的连接数、socket连接数、软件质量和性能等等。 资源限制引发的问题在并发编程中，程序运行加快的原因是运行方式从串行运行变为并发运行，但是如果如果某段程序的并发执行由于资源限制仍然在串行执行的话，这时候程序的运行不仅不会加快，反而会更慢，因为可能增加了上下文切换和资源调度的时间。 如何解决资源限制的问题对于硬件资源限制，可以考虑使用集群并行执行程序。既然单机的资源有限制，那么就让程序在多机上运行。比如使用Hadoop或者自己搭建服务器集群。 对于软件资源的限制，可以考虑使用资源池将资源复用。比如使用连接池将数据库和Socket复用，或者在调用对方webservice接口获取数据时，只建立一个连接。另外还可以考虑使用良好的开源软件。 在资源限制的情况下如何进行并发编程根据不同的资源限制调整程序的并发度，比如下载文件程序依赖于两个资源-带宽和硬盘读写速度。有数据库操作时，设计数据库练连接数，如果SQL语句执行非常快，而线程的数量比数据库连接数大很多，则某些线程会被阻塞，等待数据库连接。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机内存管理机制]]></title>
    <url>%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[整体结构参考原文 程序计数器 当前线程所执行的字节码的行号指示器，字节码解释器工作时就是通过改变这个计数器的值来确定下一条要执行的字节码指令的位置 执行 Java 方法和 native 方法时的区别： 执行 Java 方法时：记录虚拟机正在执行的字节码指令地址； 执行 native 方法时：无定义； 是 5 个区域中唯一不会出现 OOM 的区域。 Java 虚拟机栈 Java 方法执行的内存模型，每个方法执行的过程，就是它所对应的栈帧在虚拟机栈中入栈到出栈的过程； 服务于 Java 方法； 可能抛出的异常： OutOfMemoryError（在虚拟机栈可以动态扩展的情况下，扩展时无法申请到足够的内存）； StackOverflowError（线程请求的栈深度 &gt; 虚拟机所允许的深度）； 虚拟机参数设置：-Xss. 本地方法栈 服务于 native 方法； 可能抛出的异常：与 Java 虚拟机栈一样。 Java堆 唯一的目的：存放对象实例； 垃圾收集器管理的主要区域； 可以处于物理上不连续的内存空间中； 可能抛出的异常： OutOfMemoryError（堆中没有内存可以分配给新创建的实例，并且堆也无法再继续扩展了）。 虚拟机参数设置： 最大值：-Xmx 最小值：-Xms 两个参数设置成相同的值可避免堆自动扩展。 现代的垃圾收集器基本都是采用分代收集算法，该算法的思想是针对不同的对象采取不同的垃圾回收算法，因此虚拟机把 Java 堆分成以下三块： 新生代（Young Generation） 老年代（Old Generation） 永久代（Permanent Generation） 方法区 存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据； 类信息：即 Class 类，如类名、访问修饰符、常量池、字段描述、方法描述等。 垃圾收集行为在此区域很少发生； 不过也不能不清理，对于经常动态生成大量 Class 的应用，如 Spring 等，需要特别注意类的回收状况。 运行时常量池也是方法区的一部分； Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池，用于存放编译器生成的各种字面量（就是代码中定义的 static final 常量）和符号引用，这部分信息就存储在运行时常量池中。 可能抛出的异常： OutOfMemoryError（方法区无法满足内存分配需求时）。 直接内存 JDK 1.4 的 NIO 类可以使用 native 函数库直接分配堆外内存，这是一种基于通道与缓冲区的 I/O 方式，它在 Java 堆中存储一个 DirectByteBuffer 对象作为堆外内存的引用，这样就可以对堆外内存进行操作了。因为可以避免 Java 堆和 Native 堆之间来回复制数据，在一些场景可以带来显著的性能提高。 虚拟机参数设置：-XX:MaxDirectMemorySize 默认等于 Java 堆最大值，即 -Xmx 指定的值。 将直接内存放在这里讲解的原因是它也可能会出现 OutOfMemoryError； 服务器管理员在配置 JVM 参数时，会根据机器的实际内存设置 -Xmx 等信息，但经常会忽略直接内存（默认等于 -Xmx 设置值），这可能会使得各个内存区域的总和大于物理内存限制，从而导致动态扩展时出现 OOM。 Java内存模型中堆和栈的区别元空间metaspace和永久代permgen JMM(java memory model) java内存模型在多线程环境下，线程之间的要通信,就不得不提JMM(java内存模型)在JVM内部使用的java内存模型(JMM)将线程堆栈和堆之间的内存分开 线程堆栈(thread stack): 运行在java虚拟机上的每个线程都有自己的线程堆栈(thread stack) 线程堆栈还包含正在执行的每个方法的所有局部变量,一个线程只能访问它自己的线程堆栈。由线程创建的局部变量对于除创建它的线程之外的所有其他线程都是不可见的。 即使两个线程正在执行完全相同的代码，两个线程仍然会在每个线程堆栈中创建该代码的局部变量,一个线程可能会将一个有限变量的副本传递给另一个线程，但它不能共享原始局部变量本身 堆: 堆包含在Java应用程序中创建的所有对象，而不管是不是由线程创建的该对象。 堆中的对象可以被具有对象引用的所有线程访问。当一个线程访问一个对象时，它也可以访问该对象的成员变量。 如果两个线程同时调用同一个对象上的一个方法，它们都可以访问该对象的成员变量，但每个线程都有自己的局部变量副本 堆中的数据是共享的,线程不安全的 主内存和工作内存 Java内存模型的主要目标是定义程序中各个变量的访问规则，即在JVM中将变量存储到内存和从内存中取出变量这样的底层细节。此处的变量与Java编程里面的变量有所不同步，它包含了实例字段、静态字段和构成数组对象的元素，但不包含局部变量和方法参数，因为后者是线程私有的，不会共享，当然不存在数据竞争问题（如果局部变量是一个reference引用类型，它引用的对象在Java堆中可被各个线程共享，但是reference引用本身在Java栈的局部变量表中，是线程私有的）。为了获得较高的执行效能，Java内存模型并没有限制执行引起使用处理器的特定寄存器或者缓存来和主内存进行交互，也没有限制即时编译器进行调整代码执行顺序这类优化措施。 JMM规定了所有的变量都存储在主内存（Main Memory）中。每个线程还有自己的工作内存（Working Memory）,线程的工作内存中保存了该线程使用到的变量的主内存的副本拷贝，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的变量（volatile变量仍然有工作内存的拷贝，但是由于它特殊的操作顺序性规定，所以看起来如同直接在主内存中读写访问一般）。不同的线程之间也无法直接访问对方工作内存中的变量，线程之间值的传递都需要通过主内存来完成。 线程1和线程2要想进行数据的交换一般要经历下面的步骤： 线程1把工作内存1中的更新过的共享变量刷新到主内存中去。 线程2到主内存中去读取线程1刷新过的共享变量，然后copy一份到工作内存2中去。 原子性、可见性、有序性Java内存模型是围绕着并发编程中原子性、可见性、有序性这三个特征来建立的。 原子性（Atomicity）：一个操作不能被打断，要么全部执行完毕，要么不执行。在这点上有点类似于事务操作，要么全部执行成功，要么回退到执行该操作之前的状态。 可见性：一个线程对共享变量做了修改之后，其他的线程立即能够看到（感知到）该变量这种修改（变化）。Java内存模型是通过将在工作内存中的变量修改后的值同步到主内存，在读取变量前从主内存刷新最新值到工作内存中，这种依赖主内存的方式来实现可见性的。无论是普通变量还是volatile变量都是如此，区别在于： volatile：的特殊规则保证了volatile变量值修改后的新值立刻同步到主内存，每次使用volatile变量前立即从主内存中刷新，因此volatile保证了多线程之间的操作变量的可见性，而普通变量则不能保证这一点。 使用synchronized关键字：在同步方法/同步块开始时（Monitor Enter）,使用共享变量时会从主内存中刷新变量值到工作内存中（即从主内存中读取最新值到线程私有的工作内存中），在同步方法/同步块结束时(Monitor Exit),会将工作内存中的变量值同步到主内存中去（即将线程私有的工作内存中的值写入到主内存进行同步）。 使用Lock接口：的最常用的实现ReentrantLock(重入锁)来实现可见性：当我们在方法的开始位置执行lock.lock()方法，这和synchronized开始位置（Monitor Enter）有相同的语义，即使用共享变量时会从主内存中刷新变量值到工作内存中（即从主内存中读取最新值到线程私有的工作内存中），在方法的最后finally块里执行lock.unlock()方法，和synchronized结束位置（Monitor Exit）有相同的语义,即会将工作内存中的变量值同步到主内存中去（即将线程私有的工作内存中的值写入到主内存进行同步）。 final关键字：被final修饰的变量，在构造函数一旦初始化完成，并且在构造函数中并没有把“this”的引用传递出去（“this”引用逃逸是很危险的，其他的线程很可能通过该引用访问到只“初始化一半”的对象），那么其他线程就可以看到final变量的值。 有序性：对于一个线程的代码而言，我们总是以为代码的执行是从前往后的，依次执行的。这么说不能说完全不对，在单线程程序里，确实会这样执行；但是在多线程并发时，程序的执行就有可能出现乱序。用一句话可以总结为：在本线程内观察，操作都是有序的；如果在一个线程中观察另外一个线程，所有的操作都是无序的。前半句是指“线程内表现为串行语义（WithIn Thread As-if-Serial Semantics）”,后半句是指“指令重排”现象和“工作内存和主内存同步延迟”现象。Java提供了两个关键字volatile和synchronized来保证多线程之间操作的有序性 volatile关键字本身通过加入内存屏障来禁止指令的重排序 synchronized关键字通过一个变量在同一时间只允许有一个线程对其进行加锁的规则来实现，在单线程程序中，不会发生“指令重排”和“工作内存和主内存同步延迟”现象，只在多线程程序中出现。 happens-before原则： Java内存模型中定义的两项操作之间的次序关系，如果说操作A先行发生于操作B，操作A产生的影响能被操作B观察到，“影响”包含了修改了内存中共享变量的值、发送了消息、调用了方法等。 下面是Java内存模型下一些”天然的“happens-before关系，这些happens-before关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来的话，它们就没有顺序性保障，虚拟机可以对它们进行随意地重排序。 程序次序规则(Pragram Order Rule)：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确地说应该是控制流顺序而不是程序代码顺序，因为要考虑分支、循环结构。 管程锁定规则(Monitor Lock Rule)：一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是同一个锁，而”后面“是指时间上的先后顺序。 volatile变量规则(Volatile Variable Rule)：对一个volatile变量的写操作先行发生于后面对这个变量的读取操作，这里的”后面“同样指时间上的先后顺序。 线程启动规则(Thread Start Rule)：Thread对象的start()方法先行发生于此线程的每一个动作。 线程终于规则(Thread Termination Rule)：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.join()方法结束，Thread.isAlive()的返回值等作段检测到线程已经终止执行。 线程中断规则(Thread Interruption Rule)：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测是否有中断发生。 对象终结规则(Finalizer Rule)：一个对象初始化完成(构造方法执行完成)先行发生于它的finalize()方法的开始。 传递性(Transitivity)：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。 一个操作”时间上的先发生“不代表这个操作会是”先行发生“，那如果一个操作”先行发生“是否就能推导出这个操作必定是”时间上的先发生 “呢？也是不成立的，一个典型的例子就是指令重排序。所以时间上的先后顺序与happens-before原则之间基本没有什么关系，所以衡量并发安全问题一切必须以happens-before 原则为准。 HotSpot 虚拟机堆中的对象对象的创建（遇到一条 new 指令时） 检查这个指令的参数能否在常量池中定位到一个类的符号引用，并检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，先把这个类加载进内存； 类加载检查通过后，虚拟机将为新对象分配内存，此时已经可以确定存储这个对象所需的内存大小； 在堆中为新对象分配可用内存； 将分配到的内存初始化； 设置对象头中的数据； 此时，从虚拟机的角度看，对象已经创建好了，但从 Java 程序的角度看，对象创建才刚刚开始，构造函数还没有执行。 第 3 步，在堆中为新对象分配可用内存时，会涉及到以下两个问题： 如何在堆中为新对象划分可用的内存？ 指针碰撞（内存分配规整） 用过的内存放一边，没用过的内存放一边，中间用一个指针分隔； 分配内存的过程就是将指针向没用过的内存那边移动所需的长度； 空闲列表（内存分配不规整） 维护一个列表，记录哪些内存块是可用的； 分配内存时，从列表上选取一块足够大的空间分给对象，并更新列表上的记录； 如何处理多线程创建对象时，划分内存的指针的同步问题？ 对分配内存空间的动作进行同步处理（CAS）； 把内存分配动作按照线程划分在不同的空间之中进行； 每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）； 哪个线程要分配内存就在哪个线程的 TLAB 上分配，TLAB 用完需要分配新的 TLAB 时，才需要同步锁定； 通过 -XX:+/-UseTLAB 参数设定是否使用 TLAB。 对象的内存布局 对象头： 第一部分：存储对象自身运行时的数据，HashCode、GC分代年龄等（Mark Word）； 第二部分：类型指针，指向它的类元数据的指针，虚拟机通过这个指针来判断这个对象是哪个类的实例（HotSpot 采用的是直接指针的方式访问对象的）； 如果是个数组对象，对象头中还有一块用于记录数组长度的数据。 实例数据： 默认分配顺序：longs/doubles、ints、shorts/chars、bytes/booleans、oops (Ordinary Object Pointers)，相同宽度的字段会被分配在一起，除了 oops，其他的长度由长到短； 默认分配顺序下，父类字段会被分配在子类字段前面。注：HotSpot VM要求对象的起始地址必须是8字节的整数倍，所以不够要补齐。 对象的访问Java 程序需要通过虚拟机栈上的 reference 数据来操作堆上的具体对象，reference 数据是一个指向对象的引用，不过如何通过这个引用定位到具体的对象，目前主要有以下两种访问方式：句柄访问和直接指针访问。 句柄访问句柄访问会在 Java 堆中划分一块内存作为句柄池，每一个句柄存放着到对象实例数据和对象类型数据的指针。优势：对象移动的时候（这在垃圾回收时十分常见）只需改变句柄池中对象实例数据的指针，不需要修改reference本身。 直接指针访问直接指针访问方式在 Java 堆对象的实例数据中存放了一个指向对象类型数据的指针，在 HotSpot 中，这个指针会被存放在对象头中。优势：减少了一次指针定位对象实例数据的开销，速度更快。 OOM 异常 (OutOfMemoryError)Java 堆溢出 出现标志：java.lang.OutOfMemoryError: Java heap space 解决方法： 先通过内存映像分析工具分析 Dump 出来的堆转储快照，确认内存中的对象是否是必要的，即分清楚是出现了内存泄漏还是内存溢出； 如果是内存泄漏，通过工具查看泄漏对象到 GC Root 的引用链，定位出泄漏的位置； 如果不存在泄漏，检查虚拟机堆参数（-Xmx 和 -Xms）是否可以调大，检查代码中是否有哪些对象的生命周期过长，尝试减少程序运行期的内存消耗。 虚拟机参数： -XX:HeapDumpOnOutOfMemoryError：让虚拟机在出现内存泄漏异常时 Dump 出当前的内存堆转储快照用于事后分析。 Java 虚拟机栈和本地方法栈溢出 单线程下，栈帧过大、虚拟机容量过小都不会导致 OutOfMemoryError，只会导致 StackOverflowError（栈会比内存先爆掉），一般多线程才会出现 OutOfMemoryError，因为线程本身要占用内存； 如果是多线程导致的 OutOfMemoryError，在不能减少线程数或更换 64 位虚拟机的情况，只能通过减少最大堆和减少栈容量来换取更多的线程； 这个调节思路和 Java 堆出现 OOM 正好相反，Java 堆出现 OOM 要调大堆内存的设置值，而栈出现 OOM 反而要调小。 方法区和运行时常量池溢出 测试思路：产生大量的类去填满方法区，直到溢出； 在经常动态生成大量 Class 的应用中，如 Spring 框架（使用 CGLib 字节码技术），方法区溢出是一种常见的内存溢出，要特别注意类的回收状况。 直接内存溢出 出现特征：Heap Dump 文件中看不见明显异常，程序中直接或间接用了 NIO； 虚拟机参数：-XX:MaxDirectMemorySize，如果不指定，则和 -Xmx 一样。 垃圾收集 (GC)垃圾收集（Garbage Collection，GC），它的任务是解决以下 3 件问题： 哪些内存需要回收？ 什么时候回收？ 如何回收？其中第一个问题很好回答，在 Java 中，GC 主要发生在 Java 堆和方法区中，对于后两个问题，我们将在之后的内容中进行讨论，并介绍 HotSpot 的 7 个垃圾收集器。 判断对象的生死 判断对象是否可用的算法引用计数算法 算法描述： 给对象添加一个引用计数器； 每有一个地方引用它，计数器加 1； 引用失效时，计数器减 1； 计数器值为 0 的对象不再可用。 缺点： 很难解决循环引用的问题。即 objA.instance = objB; objB.instance = objA;，objA 和 objB 都不会再被访问后，它们仍然相互引用着对方，所以它们的引用计数器不为 0，将永远不能被判为不可用。 可达性分析算法（主流） 算法描述： 从 “GC Root” 对象作为起点开始向下搜索，走过的路径称为引用链（Reference Chain）； 从 “GC Root” 开始，不可达的对象被判为不可用。 Java 中可作为 “GC Root” 的对象： 栈中（本地变量表中的reference） 虚拟机栈中，栈帧中的本地变量表引用的对象；（a = new Obj()，a销毁之前，obj就是gc root） 本地方法栈中，JNI 引用的对象（native方法）； 方法区中 类的静态属性引用的对象； 常量引用的对象；（常量保存的是某个对象的地址） 活跃线程引用的对象即便如此，一个对象也不是一旦被判为不可达，就立即死去的，宣告一个的死亡需要经过两次标记过程。 四种引用类型JDK 1.2 后，Java 中才有了后 3 种引用的实现。 强引用： 像 Object obj = new Object() 这种，只要强引用还存在，垃圾收集器就永远不会回收掉被引用的对象。 软引用： 被软引用关联的对象，只有在内存不够的情况下才会被回收。对于软引用对象，在 OOM 前，虚拟机会把这些对象列入回收范围中进行第二次回收，如果这次回收后，内存还是不够用，就 OOM。 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 弱引用： 被弱引用引用的对象只能生存到下一次垃圾收集前，一旦发生垃圾收集，被弱引用所引用的对象就会被清掉。实现类：WeakReference。Tomcat 中的 ConcurrentCache 就使用了 WeakHashMap 来实现缓存功能。 虚引用： 又称为幽灵引用或者幻影引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象实例。它唯一的用途就是：当被一个虚引用引用的对象被回收时，系统会收到这个对象被回收了的通知。实现类：PhantomReference。 宣告对象死亡的两次标记过程finalize的执行过程(生命周期)当对象变成(GC Roots)不可达时，GC会判断该对象是否覆盖了finalize方法，若未覆盖，则直接将其回收。否则，若对象未执行过finalize方法，将其放入F-Queue队列，由一低优先级线程执行该队列中对象的finalize方法。执行finalize方法完毕后，GC会再次判断该对象是否可达，若不可达，则进行回收，否则，对象“复活 finalize()是Object的protected方法，子类可以覆盖该方法以实现资源清理工作，GC在回收对象之前调用该方法。 finalize()与C中的析构函数不是对应的。C中的析构函数调用的时机是确定的（对象离开作用域或delete掉），但Java中的finalize的调用具有不确定性 不建议用finalize方法完成“非内存资源”的清理工作，但建议用于： 清理本地对象(通过JNI创建的对象)； 作为确保某些非内存资源(如Socket、文件等)释放的一个补充：在finalize方法中显式调用其他资源释放方法。 方法区的回收因为方法区主要存放永久代对象，而永久代对象的回收率比新生代差很多，因此在方法区上进行回收性价比不高。主要是对常量池的回收和对类的卸载。永久代的 GC 主要回收：废弃常量 和 无用的类。 废弃常量：例如一个字符串 “abc”，当没有任何引用指向 “abc” 时，它就是废弃常量了。 无用的类：同时满足以下 3 个条件的类。 该类的所有实例已被回收，Java 堆中不存在该类的任何实例； 加载该类的 Classloader 已被回收； 该类的 Class 对象没有被任何地方引用，即无法在任何地方通过反射访问该类的方法。 垃圾收集算法 基础：标记 - 清除算法 算法描述： 先标记出所有需要回收的对象（图中深色区域）； 标记完后，统一回收所有被标记对象（留下狗啃似的可用内存区域……）。 不足： 效率问题：标记和清理两个过程的效率都不高。 空间碎片问题：标记清除后会产生大量不连续的内存碎片，导致以后为较大的对象分配内存时找不到足够的连续内存，会提前触发另一次 GC。 解决效率问题：复制算法 算法描述： 将可用内存分为大小相等的两块，每次只使用其中一块； 当一块内存用完时，将这块内存上还存活的对象复制到另一块内存上去，将这一块内存全部清理掉。 不足： 可用内存缩小为原来的一半，适合GC过后只有少量对象存活的新生代。 节省内存的方法： 新生代中的对象 98% 都是朝生夕死的，所以不需要按照 1:1 的比例对内存进行划分； 把内存划分为： 1 块比较大的 Eden 区； 2 块较小的 Survivor 区； 每次使用 Eden 区和 1 块 Survivor 区； 回收时，将以上 2 部分区域中的存活对象复制到另一块 Survivor 区中，然后将以上两部分区域清空； JVM 参数设置：-XX:SurvivorRatio=8 表示 Eden 区大小 / 1 块 Survivor 区大小 = 8。 解决空间碎片问题：标记 - 整理算法 算法描述： 标记方法与 “标记 - 清除算法” 一样； 标记完后，将所有存活对象向一端移动，然后直接清理掉边界以外的内存。 不足： 存在效率问题，适合老年代。 进化：分代收集算法 新生代： GC 过后只有少量对象存活 —— 复制算法 老年代： GC 过后对象存活率高 —— 标记 - 整理算法 当系统创建一个对象的时候，总是在Eden区操作，当这个区满了，那么就会触发一次Minor GC，也就是年轻代的垃圾回收。一般来说这时候不是所有的对象都没用了，所以就会把还能用的对象复制到From区。 这样整个Eden区就被清理干净了，可以继续创建新的对象，当Eden区再次被用完，就再触发一次Minor GC，然后呢，注意，这个时候跟刚才稍稍有点区别。这次触发Minor GC后，会将Eden区与From区还在被使用的对象复制到To区，再下一次Minor GC的时候，则是将Eden区与To区中的还在被使用的对象复制到From区。经过若干次Minor GC后，有些对象在From与To之间来回游荡，这时候From区与To区亮出了底线（阈值），这些家伙要是到现在还没挂掉，对不起，一起滚到（复制）老年代吧。 经历一定minor次数依然存活的对象 survivor区放不下的对象 新生成的大对象（-XX:+PretenuerSizeThreshold）老年代经过这么几次折腾，也就扛不住了（空间被用完），好，那就来次集体大扫除（Full GC），也就是全量回收，一起滚蛋吧。全量回收就好比我们刚才比作的大扫除，毕竟动做比较大，成本高，不能跟平时的小型值日（Minor GC）相比，所以如果Full GC使用太频繁的话，无疑会对系统性能产生很大的影响。所以要合理设置年轻代与老年代的大小，尽量减少Full GC的操作 Minor GC触发机制： 当年轻代满时就会触发Minor GC，这里的年轻代满指的是Eden代满，Survivor满不会引发GC。 Full GC触发机制： 调用System.gc()时，系统建议执行Full GC，但是不必然执行 老年代空间不足 永久代空间不足 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、survivor space1（From Space）区向survivor space2（To Space）区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 使用RMI来进行RPC或管理的JDK应用，每小时执行一次Full GC CMS GC时出现promotion failed, concurrent mode failure 常用调优参数-XX:SurvivorRatio:Eden和Survivor比值，默认8-XX:NewRatio:老年代和年轻代内存大小比例-XX:MaxTenuringThreshold:对象从年轻代晋升到老年代经过gc次数的最大阈值 HotSpot 中 GC 算法的实现通过前两小节对于判断对象生死和垃圾收集算法的介绍，我们已经对虚拟机进行 GC 的流程有了一个大致的了解。但是，在 HotSpot 虚拟机中，高效的实现这些算法也是一个需要考虑的问题。所以，接下来，我们将研究一下 HotSpot 虚拟机到底是如何高效的实现这些算法的，以及在实现中有哪些需要注意的问题。 通过之前的分析，GC 算法的实现流程简单的来说分为以下两步： 找到死掉的对象； 把它清了。 想要找到死掉的对象，我们就要进行可达性分析，也就是从 GC Root 找到引用链的这个操作。 也就是说，进行可达性分析的第一步，就是要枚举 GC Roots，这就需要虚拟机知道哪些地方存放着对象应用。如果每一次枚举 GC Roots 都需要把整个栈上位置都遍历一遍，那可就费时间了，毕竟并不是所有位置都存放在引用呀。所以为了提高 GC 的效率，HotSpot 使用了一种 OopMap 的数据结构，OopMap 记录了栈上本地变量到堆上对象的引用关系，也就是说，GC 的时候就不用遍历整个栈只遍历每个栈的 OopMap 就行了。 在 OopMap 的帮助下，HotSpot 可以快速准确的完成 GC 枚举了，不过，OopMap 也不是万年不变的，它也是需要被更新的，当内存中的对象间的引用关系发生变化时，就需要改变 OopMap 中的相应内容。可是能导致引用关系发生变化的指令非常之多，如果我们执行完一条指令就改下 OopMap，这 GC 成本实在太高了。 因此，HotSpot 采用了一种在 “安全点” 更新 OopMap 的方法，安全点的选取既不能让 GC 等待的时间过长，也不能过于频繁增加运行负担，也就是说，我们既要让程序运行一段时间，又不能让这个时间太长。我们知道，JVM 中每条指令执行的是很快的，所以一个超级长的指令流也可能很快就执行完了，所以 真正会出现 “长时间执行” 的一般是指令的复用，例如：方法调用、循环跳转、异常跳转等，虚拟机一般会将这些地方设置为安全点更新 OopMap 并判断是否需要进行 GC 操作。 此外，在进行枚举根节点的这个操作时，为了保证准确性，我们需要在一段时间内 “冻结” 整个应用，即 Stop The World（传说中的 GC 停顿），因为如果在我们分析可达性的过程中，对象的引用关系还在变来变去，那是不可能得到正确的分析结果的。即便是在号称几乎不会发生停顿的 CMS 垃圾收集器中，枚举根节点时也是必须要停顿的。这里就涉及到了一个问题： 我们让所有线程跑到最近的安全点再停顿下来进行 GC 操作呢？ 主要有以下两种方式： 抢先式中断： 先中断所有线程； 发现有线程没中断在安全点，恢复它，让它跑到安全点。 主动式中断： (主要使用) 设置一个中断标记； 每个线程到达安全点时，检查这个中断标记，选择是否中断自己。 除此安全点之外，还有一个叫做 “安全区域” 的东西，一个一直在执行的线程可以自己 “走” 到安全点去，可是一个处于 Sleep 或者 Blocked 状态的线程是没办法自己到达安全点中断自己的，我们总不能让 GC 操作一直等着这些个 ”不执行“ 的线程重新被分配资源吧。对于这种情况，我们要依靠安全区域来解决。 安全区域是指在一段代码片段之中，引用关系不会发生变化，因此在这个区域中的任意位置开始 GC 都是安全的。 当线程执行到安全区域时，它会把自己标识为 Safe Region，这样 JVM 发起 GC 时是不会理会这个线程的。当这个线程要离开安全区域时，它会检查系统是否在 GC 中，如果不在，它就继续执行，如果在，它就等 GC 结束再继续执行。 本小节我们主要讲述 HotSpot 虚拟机是如何发起内存回收的，也就是如何找到死掉的对象，至于如何清掉这些个对象，HotSpot 将其交给了一堆叫做 ”GC 收集器“ 的东西，这东西又有好多种，不同的 GC 收集器的处理方式不同，适用的场景也不同，我们将在下一小节进行详细讲述。 JVM 运行模式ServerClient 7 个垃圾收集器垃圾收集器就是内存回收操作的具体实现，HotSpot 里足足有 7 种，为啥要弄这么多，因为它们各有各的适用场景。有的属于新生代收集器，有的属于老年代收集器，所以一般是搭配使用的（除了万能的 G1）。关于它们的简单介绍以及分类请见下图。 Serial / ParNew 搭配 Serial Old 收集器 Serial 收集器是虚拟机在 Client 模式下的默认新生代收集器，它的优势是简单高效，在单 CPU 模式下很牛。ParNew 收集器就是 Serial 收集器的多线程版本，虽然除此之外没什么创新之处，但它却是许多运行在 Server 模式下的虚拟机中的首选新生代收集器，因为除了 Serial 收集器外，只有它能和 CMS 收集器搭配使用。 Parallel 搭配 Parallel Scavenge 收集器首先，这俩货肯定是要搭配使用的，不仅仅如此，它俩还贼特别，它们的关注点与其他收集器不同，其他收集器关注于尽可能缩短垃圾收集时用户线程的停顿时间，而 Parallel Scavenge 收集器的目的是达到一个可控的吞吐量。 吞吐量 = 运行用户代码时间 / ( 运行用户代码时间 + 垃圾收集时间 ) 因此，Parallel Scavenge 收集器不管是新生代还是老年代都是多个线程同时进行垃圾收集，十分适合于应用在注重吞吐量以及 CPU 资源敏感的场合。可调节的虚拟机参数： -XX:+UserParallelGC -XX:MaxGCPauseMillis：最大 GC 停顿的秒数； -XX:GCTimeRatio：吞吐量大小，一个 0 ~ 100 的数，最大 GC 时间占总时间的比率 = 1 / (GCTimeRatio + 1)； -XX:+UseAdaptiveSizePolicy：一个开关参数，打开后就无需手工指定 -Xmn，-XX:SurvivorRatio 等参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，自行调整。 CMS 收集器 参数设置： -XX:+UseCMSCompactAtFullCollection：在 CMS 要进行 Full GC 时进行内存碎片整理（默认开启） -XX:CMSFullGCsBeforeCompaction：在多少次 Full GC 后进行一次空间整理（默认是 0，即每一次 Full GC 后都进行一次空间整理） G1 收集器-XX：UserG1GC 复制+标记-整理算法 GC 日志解读 Java 内存分配策略 优先在 Eden 区分配 Eden 空间不够将会触发一次 Minor GC； 虚拟机参数： -Xmx：Java 堆的最大值； -Xms：Java 堆的最小值； -Xmn：新生代大小； -XX:SurvivorRatio=8：Eden 区 / Survivor 区 = 8 : 1 大对象直接进入老年代 大对象定义： 需要大量连续内存空间的 Java 对象。例如那种很长的字符串或者数组。 设置对象直接进入老年代大小限制： -XX:PretenureSizeThreshold：单位是字节； 只对 Serial 和 ParNew 两款收集器有效。 目的： 因为新生代采用的是复制算法收集垃圾，大对象直接进入老年代可以避免在 Eden 区和 Survivor 区发生大量的内存复制。 长期存活的对象将进入老年代 固定对象年龄判定： 虚拟机给每个对象定义一个年龄计数器，对象每在 Survivor 中熬过一次 Minor GC，年龄 +1，达到 -XX:MaxTenuringThreshold 设定值后，会被晋升到老年代，-XX:MaxTenuringThreshold 默认为 15； 动态对象年龄判定： Survivor 中有相同年龄的对象的空间总和大于 Survivor 空间的一半，那么，年龄大于或等于该年龄的对象直接晋升到老年代。 空间分配担保我们知道，新生代采用的是复制算法清理内存，每一次 Minor GC，虚拟机会将 Eden 区和其中一块 Survivor 区的存活对象复制到另一块 Survivor 区，但当出现大量对象在一次 Minor GC 后仍然存活的情况时，Survivor 区可能容纳不下这么多对象，此时，就需要老年代进行分配担保，即将 Survivor 无法容纳的对象直接进入老年代。 这么做有一个前提，就是老年代得装得下这么多对象。可是在一次 GC 操作前，虚拟机并不知道到底会有多少对象存活，所以空间分配担保有这样一个判断流程： 发生 Minor GC 前，虚拟机先检查老年代的最大可用连续空间是否大于新生代所有对象的总空间； 如果大于，Minor GC 一定是安全的； 如果小于，虚拟机会查看 HandlePromotionFailure 参数，看看是否允许担保失败； 允许失败：尝试着进行一次 Minor GC； 不允许失败：进行一次 Full GC； 不过 JDK 6 Update 24 后，HandlePromotionFailure 参数就没有用了，规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会进行 Minor GC，否则将进行 Full GC。 Metaspace 元空间与 PermGem 永久代元空间是方法区的在HotSpot jvm 中的实现，方法区主要用于存储类的信息、常量池、方法数据、方法代码等。方法区逻辑上属于堆的一部分，但是为了与堆进行区分，通常又叫“非堆”。 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。，理论上取决于32位/64位系统可虚拟的内存大小。可见也不是无限制的，需要配置参数。 ==元空间和永久代都是方法区的实现== Java 8 彻底将永久代 (PermGen) 移除出了 HotSpot JVM，将其原有的数据迁移至 Java Heap 或 Metaspace。 移除 PermGem 的原因： PermGen 内存经常会溢出，引发恼人的 java.lang.OutOfMemoryError: PermGen，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM； 移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。移除 PermGem 后，方法区和字符串常量的位置： 方法区：移至 Metaspace； 字符串常量：移至 Java Heap。Metaspace 的位置： 本地堆内存(native heap)。Metaspace 的优点： 永久代 OOM 问题将不复存在，因为默认的类的元数据分配只受本地内存大小的限制，也就是说本地内存剩余多少，理论上 Metaspace 就可以有多大；JVM参数： -XX:MetaspaceSize：分配给类元数据空间（以字节计）的初始大小，为估计值。MetaspaceSize的值设置的过大会延长垃圾回收时间。垃圾回收过后，引起下一次垃圾回收的类元数据空间的大小可能会变大。 -XX:MaxMetaspaceSize：分配给类元数据空间的最大值，超过此值就会触发Full GC，取决于系统内存的大小。JVM会动态地改变此值。 -XX:MinMetaspaceFreeRatio：一次GC以后，为了避免增加元数据空间的大小，空闲的类元数据的容量的最小比例，不够就会导致垃圾回收。 -XX:MaxMetaspaceFreeRatio：一次GC以后，为了避免增加元数据空间的大小，空闲的类元数据的容量的最大比例，不够就会导致垃圾回收。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式：七大原则]]></title>
    <url>%2F%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%83%E5%A4%A7%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[开闭原则 依赖倒置原则 单一职责原则 接口隔离原则 迪米特法则（最少知道原则） 里氏替换原则 合成/复用原则 UML类图#:protected~:默认，包权限下划线：static斜体：抽象方法 开闭原则类应该对扩展开放，对修改关闭。 扩展就是添加新功能的意思，因此该原则要求在添加新功能时不需要修改代码。 符合开闭原则最典型的设计模式是装饰者模式，它可以动态地将责任附加到对象上，而不用去修改类的代码。 依赖倒置原则高层次模块不应该依赖于低层的 例如扩展的时候是面向ICourse接口的，而不是在Geely类中添加study方法，main函数也不需要调用Geely的对象的具体课程方法。 大概是：不要写很多不同名字的实现方法，而是去写很多类去继承接口，然后传不同的参数来调用不同类，这些类里的方法名字相同，但实现不同。 单一职责原则修改一个类的原因应该只有一个。换句话说就是让一个类只负责一件事，当这个类需要做过多事情的时候，就需要分解这个类。如果一个类承担的职责过多，就等于把这些职责耦合在了一起，一个职责的变化可能会削弱这个类完成其它职责的能力。 接口隔离原则不应该强迫客户依赖于它们不用的方法。因此使用多个专门的接口比使用单一的总接口要好。 迪米特原则对象对其他对象保持最少的了解，所以少用public尽量降低类之间的耦合只和朋友交流（入参出参、成员变量。。其他内部出现的类不是朋友，尽量搞出去） 里氏替换原则 子类能够替换父类。重载时，子类的入参要比父类宽松。返回 时，子类要更严格。 合成/复用原则 黑箱复用。 下面类图中描述的例子。“人”被继承到“学生”、“经理”和“雇员”等子类。而实际上，学生”、“经理”和“雇员”分别描述一种角色，而“人”可以同时有几种不同的角色。比如，一个人既然是“经理”，就必然是“雇员”；而“人”可能同时还参加MBA课程，从而也是一个“学生”。使用继承来实现角色，则只能使每一个“人”具有Is-A角色，而且继承是静态的，这会使得一个“人”在成为“雇员”身份后，就永远为“雇员”，不能成为“学生”和“经理”，而这显然是不合理的。 这一错误的设计源自于把“角色”的等级结构和“人”的等级结构混淆起来，把“Has-A”角色误解为“Is -A”角色。因此要纠正这种错误，关键是区分“人”与“角色”的区别。下图所示的的设计就正确的做到了这一点。]]></content>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
</search>
