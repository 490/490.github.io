<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程(3) START OF DATABASE RECOVERY]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B23%2F</url>
    <content type="text"><![CDATA[LCP：本地检查点，在NDB中，这意味着主内存中的所有数据都写入磁盘，我们还将更改的磁盘页写入磁盘，以确保磁盘上某个点之前的所有更改都可用。 执行REDO日志：这意味着我们一次读取REDO日志一条REDO日志记录，并在需要时执行REDO日志记录中的操作。 Apply the REDO log：执行REDO日志的同义词。 Prepare REDO log record: 这是一个REDO日志记录，包含有关数据库更改的信息（插入/删除/更新/写入）。 COMMIT REDO log：这是一个REDO日志记录，指定要实际执行Prepare REDO日志记录。 COMMIT REDO日志记录包含Prepare REDO日志记录的后向引用。 ABORT REDO log record：与COMMIT REDO日志记录类似，但此处事务已中止，因此无需应用REDO日志记录。 Database:在此上下文中表示当节点重新启动时驻留在集群或节点中的所有数据。 Off-line Database:意味着我们节点中的数据库不在线，因此不能用于读取。 这是恢复LCP后但在应用REDO日志之前的数据库状态。 Off-line Consistent database:这是一个数据库状态，它与最新的更改不是最新的，但它表示先前存在的数据库中的旧状态。 恢复LCP并执行REDO日志后，即可实现此状态。 On-line Database:这是一个最新的数据库状态，任何可用于读取数据的节点都有其数据库在线（实际上片段是逐个联机的）。 On-line Recoverable Database:这是一个也可以恢复的在线数据库。 在节点重启中，我们首先到达状态在线数据库，但是我们需要运行LCP，然后数据库也可以恢复到其当前状态。 可恢复的数据库也是持久的，这意味着当我们达到此状态时，我们将ACID中的D添加到数据库中。 Node:有API节点，数据节点和管理服务器节点。 数据节点是ndbd / ndbmtd进程，它运行所有数据库逻辑并包含数据库数据。 管理服务器节点是运行包含群集配置的ndb_mgmd并且还执行许多管理服务的进程。 API节点是应用程序进程的一部分，在mysqld中。 每个应用程序进程可以有多个API节点。 每个API节点通过套接字（或其他通信介质）连接到每个数据节点和管理服务器节点。 当一个人引用本文中的节点时，我们主要暗示的是我们在谈论一个数据节点。 Node Group:一组数据节点，它们都包含相同的数据。 节点组中的节点数等于我们在集群中使用的副本数。 Fragment:表的一部分，完全存储在一个节点组中。 Partition:Synonym of fragment. Fragment replica:这是一个节点中的一个片段。 一个片段最多可以有4个副本（因此节点组中最多可以有4个节点）。 Distribution information: 这是有关表的分区（片段的同义词）以及它们驻留在哪些节点上的信息，以及有关在每个片段副本上执行的LCP的信息。 Metadata:这是有关表，索引，触发器，外键，哈希映射，文件，日志文件组，表空间的信息。 字典信息：元数据的同义词。 LDM:代表本地数据管理器，这些是执行处理一个数据节点内处理的数据的代码的块。 它包含处理元组存储的块，哈希索引，T树索引，页面缓冲区管理器，表空间管理器，写入LCP的块和恢复LCP的块，磁盘数据的日志管理器。 作为START_COPYREQ的一部分，真正的数据库恢复过程是什么。 这里执行大多数重要的数据库恢复算法以使数据库再次联机。 仍需要早期阶段来恢复元数据和设置通信，设置内存并将起始节点作为数据节点集群中的完整公民。 START_COPYREQ遍历所有分发信息，并将START_FRAGREQ发送到拥有的DBLQH模块实例，以便在节点上恢复每个片段副本。 DBLQH将立即启动以恢复这些片段副本，它将对片段副本进行排队并一次恢复一个。 这发生在两个阶段，首先需要恢复本地检查点的所有片段副本开始这样做。 在发送了所有要恢复的片段副本之后，我们已经从存储在磁盘上的本地检查点恢复了所有片段（或者有时通过从活动节点获取整个片段），然后是运行磁盘数据UNDO日志的时候了。 最后，在运行此UNDO日志之后，我们已准备好通过应用REDO日志将片段副本恢复到最新的磁盘持久状态。 DBDIH将所有片段副本的所有必需信息发送到DBLQH，然后它将START_RECREQ发送到DBLQH以指示现在已发送所有片段副本信息。 START_RECREQ通过DBLQH代理模块发送，并且该部分被并行化，以便所有LDM实例并行执行以下部分。 如果我们正在进行初始节点重启，我们不需要恢复任何本地检查点，因为初始节点重启意味着我们在没有文件系统的情况下启动。 所以这意味着我们必须从节点组中的其他节点恢复所有数据。 在这种情况下，当我们收到START_FRAGREQ时，我们立即开始在DBLQH中应用片段副本的复制。 在这种情况下，我们不需要运行任何撤消或重做日志，因为没有本地检查点来恢复片段。 完成此操作后，DBDIH报告已通过将START_RECREQ发送到DBLQH发送了所有要启动的片段副本，我们将向TSMAN发送START_RECREQ，之后我们将完成数据的恢复。 我们将指定要作为REDO日志执行的一部分进行恢复的所有片段副本。 这是通过信号EXEC_FRAGREQ完成的。 当所有这些信号都被发送后，我们发送EXEC_SRREQ表示我们已经准备好在DBLQH中执行下一个REDO日志执行阶段。 当发送所有这些信号时，我们已经完成了所谓的DBLQH的第2阶段，DBLQH中的阶段1是在NDB_STTOR阶段3中开始准备REDO日志以进行读取。 因此，当这两个阶段都完成时，我们就可以开始在DBLQH中开始所谓的阶段3。 这些DBLQH阶段与启动阶段无关，这些阶段是DBLQH模块中启动的内部阶段。 DBLQH中的第3阶段是读取REDO日志并将其应用于从本地检查点恢复的片段副本。 这是创建在特定全局检查点上同步的数据库状态所必需的。 因此，我们首先为所有片段安装本地检查点，接下来我们应用REDO日志将片段副本与某个全局检查点同步。 在执行REDO日志之前，我们需要通过检查我们将恢复到所需全局检查点的所有片段副本的限制来计算要在REDO日志中应用的起始GCI和最后一个GCI。 DBDIH已存储有关片段副本的每个本地检查点的信息，这些信息是从REDO日志运行所需的全局检查点范围，以使其进入某个全局检查点的状态。 此信息已在START_FRAGREQ信号中发送。 DBLQH会将每个片段副本的所有这些限制合并到全局范围的全局检查点，以便为此LDM实例运行。 因此每个片段副本都有自己的GCP id范围来执行，这意味着所有这些起始范围的最小值和所有结束范围的最大值是我们需要在REDO日志中执行以引入集群的全局GCP ID范围 再次在线。 下一步是使用start和stop全局检查点id计算每个日志部分的REDO日志中的开始和停止兆字节。 计算这个所需的所有信息已经在内存中，所以这是一个纯粹的计算。 当我们执行REDO日志时，实际上我们只在正确的全局检查点范围内应用COMMIT记录。 COMMIT记录和实际更改记录位于REDO日志中的不同位置，因此对于每兆字节的REDO日志，我们记录REDO日志中我们必须返回多长时间才能找到更改记录。 在运行REDO日志时，我们维护一个相当大的REDO日志缓存，以避免在事务运行很长时间的情况下我们必须执行磁盘读取。 这意味着长时间运行和大型事务可能会对重新启动时间产生负面影响。 在所有日志部分完成此计算后，我们现在准备开始执行REDO日志。 在执行REDO日志完成后，我们还会在REDO日志中写入一些内容，以表明我们之前使用的任何信息都不会在以后使用。 我们现在需要等待所有其他日志部分也完成其REDO日志部分的执行。 REDO日志执行的设计使我们可以在多个阶段执行REDO日志，这适用于我们可以从多个活动节点重建节点的情况。 目前绝不应使用此代码。 因此，下一步是检查REDO日志部件的新头部和尾部。 这是通过使用启动和停止全局检查点来计算此数字的相同代码完成的。 代码的这一阶段还通过确保正确的REDO日志文件打开来准备REDO日志部分以编写新的REDO日志记录。 它还涉及一些相当棘手的代码，以确保正确处理已脏的页面。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程 (2) READ_CONFIG_REQ]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B2%2F</url>
    <content type="text"><![CDATA[READ_CONFIG_REQ对所有软件模块或多或少都相同。 它分配软件模块所需的内存并初始化内存（创建各种空闲列表等）。 它还读取模块感兴趣的各种配置参数（这些参数通常会影响我们分配的内存大小）。原文 它从CMVMI开始，分配大部分全局内存池，接下来我们有NDBFS为磁盘数据创建必要的文件目录，它还创建一次可以由一个文件使用的绑定IO线程（初始线程数可配置） 通过InitalNoOpenFiles），然后它创建了一些磁盘数据文件（用于处理磁盘数据的所有文件）使用的空闲线程（它们可通过IOThreadPool配置的数量），每个这样的线程可用于打开/读取/写入/关闭 磁盘数据文件。 最后，NDBFS还会创建从文件系统线程返回到其他线程的通信通道。 所有其他模块遵循相同的标准，它们根据硬编码定义或通过配置变量计算多个大小，它们为这些变量分配内存，最后它们初始化那些分配的内存结构。 STTOR Phase 0执行的第一个STTOR阶段是STTOR阶段0.在此阶段执行任何操作的唯一模块是NDBCNTR，如果启动是初始启动，则清除文件系统，CMVMI创建文件系统目录。 STTOR Phase 1下一阶段执行的是STTOR阶段1，在此阶段，大多数模块初始化一些更多数据，必要时设置对相邻模块的引用。 此外，DBDIH创建了一些特殊的互斥锁，确保一次只能在代码的某些部分中涉及一个进程。 NDBCNTR在阶段2开始初始化与运行NDB_STTOR相关的一些数据。如果配置为这样，CMVMI将锁定内存，此后它会安装正常的监视程序超时，因为现在执行所有大内存分配。 CMVMI还启动定期内存报告。 QMGR是此阶段中最活跃的模块。 它初始化一些数据，它从DBDIH获得重启类型（初始启动或正常启动），它打开与集群中所有节点的通信，它开始检查包含节点处理的节点故障。 最后，它运行协议以将新节点包括在心跳协议中。 这可能需要一段时间，因为节点包含过程一次只能引入一个节点，并且协议包含一些延迟。然后，BACKUP模块启动磁盘速度检查循环，该循环将在节点启动并运行时运行。 STTOR Phase 2下一步是执行STTOR阶段2.在STTOR阶段2中执行任何操作的唯一模块是NDBCNTR，它要求DIH执行重启类型，它从配置中读取节点，它初始化控制多长时间的部分超时变量 在我们执行部分启动之前等待。 NDBCNTR将信号CNTR_START_REQ发送到当前主节点中的NDBCNTR，该信号使主节点能够在必要时由于其他起始节点或某些其他条件而延迟该节点的启动。 对于集群启动/重启，它还使主节点有机会确保在启动节点之前等待足够的节点启动。 主设备一次只接受一个接收CNTR_START_CONF的节点，下一个节点只能在前一个起始节点完成复制元数据并释放元数据锁并锁定DIH信息后接收CNTR_START_CONF，这在STTOR阶段5中发生。 因此，在滚动重启中，第一个节点将获得CNTR_START_CONF，然后在DICT锁定上被阻塞，等待LCP完成，这是很常见的。 并行启动的其他节点将在CNTR_START_CONF上等待，因为一次只有一个节点可以通过它。 收到CNTR_START_CONF后，NDBCNTR继续运行NDB_STTOR阶段1.此处DBLQH初始化节点记录，它启动报告服务。 它还初始化有关REDO日志的数据，这还包括在所有类型的初始启动时初始化磁盘上的REDO日志（可能非常耗时）。 DBDICT初始化模式文件（包含已在集群中创建的表和其他元数据对象）。 DBTUP初始化一个默认值片段，DBTC和DBDIH初始化一些数据变量。 完成NDBCNTR中的NDB_STTOR阶段后，STTOR阶段2中没有更多工作。 STTOR Phase 3下一步是运行STTOR阶段3.大多数需要集群中节点列表的模块在此阶段读取此信息。 DBDIH在此阶段读取节点，DBDICT设置重启类型。 下一个NDBCNTR接收此阶段并启动NDB_STTOR阶段2.在此阶段，DBLQH设置从其操作记录到DBACC和DBTUP中的操作记录的连接。 这是针对所有DBLQH模块实例并行完成的。 DBDIH现在通过锁定元数据来准备节点重启过程。 这意味着我们将等待任何正在进行的元数据操作完成，并且当它完成时，我们将锁定元数据，这样在我们完成复制元数据信息的阶段之前，不能进行元数据更改。 锁定的原因是所有元数据和分发信息都是完全复制的。 因此，在将数据从主节点复制到起始节点时，我们需要锁定此信息。 虽然我们保留了此锁，但我们无法通过元数据事务更改元数据。 在稍后复制元数据之前，我们还需要确保没有运行本地检查点，因为这也会更新分发信息。 锁定后，我们需要请求从主节点启动节点的权限。 启动节点的许可请求由发送START_PERMREQ到主节点的起始节点处理。 如果另一个节点已在处理节点重启，则可能会收到否定答复，如果需要初始启动，则可能会失败。 如果另一个节点已经启动，我们将等待3秒钟再试一次。 这在DBDIH中作为NDB_STTOR阶段2的一部分执行。 在完成NDB_STTOR阶段2之后，STTOR阶段3继续由CMVMI模块激活对扫描和密钥操作使用的发送打包数据的检查。 接下来，BACKUP模块读取配置的节点。 接下来，SUMA模块设置对页面池的引用，以便它可以重用此全局内存池中的页面，然后DBTUX设置重新启动类型。 最后，PGMAN启动一个stats循环和一个清理循环，只要节点启动并运行，它就会运行。 如果我们的节点仍然涉及主节点中正在进行的某些进程，我们可能会崩溃节点。 这是相当正常的，只会触发一次崩溃，然后是天使进程正常的新启动。 权限请求由主服务器向所有节点发送信息来处理。 对于初始启动，权限请求可能非常耗时，因为我们必须使所有节点上元数据中所有表的所有本地检查点无效。 目前没有此失效过程的并行化，因此它将一次使一个表无效。 STTOR Phase 4完成STTOR阶段3后，我们进入STTOR阶段4.此阶段由DBLQH在BACKUP模块中获取备份记录开始，该备份记录将用于本地检查点处理。 下一个NDBCNTR启动NDB_STTOR阶段3.这也在DBLQH中开始，我们在其中读取已配置的节点。 然后我们开始阅读REDO日志以进行设置（我们将在后台设置它，它将通过稍后描述的集群重启/节点重启的另一部分进行同步），对于所有类型的初始启动，我们将等到 REDO日志的初始化已经完成，直到报告此阶段完成为止。 下一个DBDICT将读取已配置的节点，然后DBTC将读取已配置的节点并启动事务计数器报告。 接下来在NDB_STTOR阶段3中，DBDIH初始化重启数据以进行初始启动。 在完成STTOR第4阶段的工作之前，NDBCNTR将设置一个等待点，使所有起始节点在继续之前达到此点。 这仅适用于群集启动/重新启动，因此不适用于节点重新启动。 主节点控制此等待点，并在集群重启的所有节点都达到此点时将信号NDB_STARTREQ发送到DBDIH。 稍后将详细介绍此信号。 STTOR阶段4中发生的最后一件事是DBSPJ读取配置的节点。 STTOR Phase 5我们现在进入STTOR阶段5.这里做的第一件事是运行NDB_STTOR阶段4.只有DBDIH在这里做了一些工作，它只在节点重启时做了一些事情。 在这种情况下，它要求当前主节点通过向其发送START_MEREQ信号来启动它。 START_MEREQ的工作原理是从主DBDIH节点复制分发信息，然后从主DBDICT复制元数据信息。 它一次复制一个分发信息表，这使得该过程有点慢，因为它包括将表写入起始节点中的磁盘。 跟踪此事件的唯一方法是在起始节点中的DBDIH中将每个表的表分布信息写入。 我们可以跟踪在起始节点DBDICT中接收的DICTSTARTREQ的接收。 当复制DBDIH和DBDICT信息时，我们需要阻止全局检查点，以便从现在开始将新节点包含在元数据和分发信息的所有更改中。 这是通过将INCL_NODEREQ发送到所有节点来执行的。 在此之后，我们可以释放由DBDIH在STTOR阶段2中设置的元数据锁。 完成NDB_STTOR阶段4后，NDBCNTR以下列方式再次同步启动： 如果初始集群启动和主节点然后创建系统表如果集群启动/重新启动，则等待所有节点到达此点。 等待集群启动/重启中的节点后，在主节点中运行NDB_STTOR阶段5（仅发送到DBDIH）。 如果节点重新启动，则运行NDB_STTOR阶段5（仅发送到DBDIH）。 DBDIH中的NDB_STTOR阶段5正在等待本地检查点的完成（如果它是主设备）并且我们正在运行集群启动/重启。 对于节点重启，我们将信号START_COPYREQ发送到起始节点，要求将数据复制到我们的节点。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Cluster重启过程 (1) Restart Phases in MySQL Cluster]]></title>
    <url>%2FCluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[在MySQL Cluster中，重新启动是分阶段处理的，节点的重启由一组阶段驱动。此外，节点重启也与已启动的节点以及与我们的节点并行启动的其他节点同步。此注释将描述所使用的各个阶段。 原文启动节点的第一步是创建数据节点运行时环境。数据节点进程通常使用angel进程运行，此angel进程确保数据节点万一失败时能够自动重新启动。因此，再次运行数据节点的唯一原因是在OS崩溃之后或运营商关闭之后或作为软件升级的一部分。 当启动数据节点时，数据节点需要一个节点id，这是通过设置参数–ndb-nodeid在启动datanode时，或者在检索配置时由管理服务器分配。对于数据节点的所有重新启动，angel进程将确保所分配的节点id将相同。 在分配数据节点进程之后，启动进程保持为angel进程并且新进程成为实际的数据节点进程。实际的数据节点进程首先从managementserver检索配置。 在这个阶段我们已经阅读了选项，我们已经分配了一个节点ID，我们从管理服务器加载了配置。我们将在数据节点日志中打印一些关于我们的线程配置和其他一些重要信息。为了确保我们找到正确的文件并在正确的位置创建文件，我们设置了数据节点进程的数据文件夹。 接下来我们必须启动看门狗线程，因为我们现在开始执行操作，我们希望确保我们不要由于某些软件错误而卡住了。 接下来我们将分配全局内存池的内存，这是分配最多内存的地方，我们仍然有相当多的内存分配，作为NDB内核中各种软件模块初始化的部分，但是我们正在逐步使用全局内存池。 分配内存可能是一个相当耗时的过程，其中操作系统可以为每个分配的GByte内存提供长达一秒的时间（自然是OS依赖的并且会随着时间的推移而变化）。 实际上在这里消耗时间实际上是我们还触摸每个页面以确保分配的内存也被映射到真实物理内存以避免在我们运行该过程时页面未命中。为了加快这个过程，我们已经触及了内存多线程。实际上，大多数内存的分配是可配置的，配置变量LateAlloc可用于延迟大多数内存分配到重启的早期阶段。 分配全局内存池后，我们初始化运行时环境使用的所有数据。 这可确保我们准备好在数据节点进程启动后立即在线程之间发送和接收数据。 在这一点上，我们只启动了看门狗进程并且线程在创建进程的过程中启动（如果我们运行ndbmtd，这个线程稍后将被转换为第一个接收线程，如果我们正在运行，则该线程将被转换为唯一的执行线程NDBD）。 下一步是加载所有软件模块并初始化它们，以确保在消息开始到达执行时正确设置它们。 在我们启动运行时环境之前，我们还需要激活发送和接收服务。 这涉及创建一个套接字客户端线程，该线程试图连接到集群中其他节点的套接字服务器部分，并创建一个线程来监听用于我们作为套接字服务器通信的那些数据节点的套接字服务器。 默认行为是nodeid最低的节点是通信设置中的套接字服务器。 这可以在数据节点配置中更改。 在我们继续并启动数据节点环境之前，我们将运行时环境的启动信号放在其正确的作业缓冲区中。 实际上，为了启动系统，需要在作业缓冲区中放置两个相等的信号。 第一个启动信号开始与其他节点的通信，并设置状态以等待下一个信号实际启动系统。 第二个将开始运行启动阶段。 最后，我们启动运行时环境的所有线程。 这些当前可以包括主线程，代表线程，多个tc线程，多个发送线程，多个接收线程和多个ldm线程。 鉴于已预先分配了所有线程的通信缓冲区，我们可以在这些线程启动时立即开始发送信号。 接收线程一旦到达其线程启动代码中的那一点就会开始处理其接收到的信号。 有两个相同的启动信号，第一个启动定期发送的重复信号，以跟踪数据节点中的时间。 只有第二个开始执行各种启动阶段。 数据节点的启动在一组阶段中处理。 第一阶段是将信号READ_CONFIG_REQ发送到内核中的所有软件模块，然后将STTOR类似地发送到256个阶段的所有软件模块，编号从0到255.这些模块的编号从0到255，我们不使用全部 这些阶段，但代码是灵活的，以便任何这些阶段可以现在使用或在将来的某个时间使用。 此外，我们还有6个模块，这些模块涉及另外一组启动阶段。 在这些阶段发送的信号称为NDB_STTOR。 最初的想法是将此消息视为NDB子系统的本地启动。 这些信号由NDBCNTR发送和处理，并作为NDBCNTR中STTOR处理的一部分发送。 这意味着它成为启动阶段的连续部分。 在开始阶段之前，我们确保任何管理节点都可以连接到我们的节点，并且所有其他节点都已断开连接，并且它们只能向QMGR模块发送消息。 管理服务器接收关于数据节点中的各种事件的报告，并且QMGR模块负责将数据节点包括在集群中。 在我们被包含在集群中之前，我们无法以任何方式与其他节点通信。 开始总是从主线程开始，其中每个软件模块至少由所有多线程模块包含的代理模块表示。 代理模块使用一条消息和一条回复，可以轻松地向一组相同类型的模块发送和接收消息。 READ_CONFIG_REQ信号始终以相同的顺序发送。 它首先发送到CMVMI，这是接收启动顺序的块，它执行许多功能，软件模块可以从这些功能影响运行时环境。 它通常会分配进程的大部分内存并触及所有内存。 它是主线程的一部分。 接收READ_CONFIG_REQ的下一个模块是NDBFS，这是控制文件系统线程的模块，该模块位于主线程中。 下一个模块是DBINFO，该模块支持ndbinfo数据库，用于以表格格式获取有关数据节点内部的信息，该模块位于主线程中。 接下来是DBTUP，这是存储实际数据的模块。 下一个DBACC，存储主键和唯一键哈希索引的模块以及我们控制行锁的位置。 这两个块都包含在ldm线程中。 接下来是DBTC，即管理事务协调的模块，该模块是tc线程的一部分。 接下来是DBLQH，该模块通过键操作和扫描控制对数据的操作，并且还处理REDO日志。 这是ldm线程的主要模块。 接下来是DBTUX，它操作有序索引重用页面，用于在DBTUP中存储行，也是ldm线程的一部分。 接下来是DBDICT，这是一个字典模块，用于存储和处理有关表和列，表空间，日志文件等的所有元数据信息。 DICT是主线程的一部分。 接下来是DBDIH，用于存储和处理有关所有表，表分区和每个分区的所有副本的分发信息的模块。 它控制本地检查点进程，全局检查点进程并控制重新启动处理的主要部分。 DIH模块是主线程的一部分。 接下来是控制重启阶段的NDBCNTR，它是主线程的一部分。 接下来是QMGR，它负责处理心跳协议以及包含和排除集群中的节点。 它是主线程的一部分。 接下来是TRIX，它执行与有序索引和其他基于触发器的服务相关的一些服务。 它是tc线程的一部分。 接下来是BACKUP，它用于备份和本地检查点，是ldm线程的一部分。 接下来是DBUTIL，它提供了许多服务，例如代表模块中的代码执行密钥操作。 它是主线程的一部分。 接下来是负责复制事件的SUMA模块，这是由rep线程处理的模块。 接下来是TSMAN，然后是LGMAN，然后是PGMAN，它们都是磁盘数据处理的一部分，负责处理表空间，UNDO日志记录和页面管理。 它们都是ldm线程的一部分。 RESTORE是一个用于在启动时恢复本地检查点的模块。 该模块也是ldm线程的一部分。 最后，我们有DBSPJ模块来处理向下推送到数据节点的连接查询，它作为tc线程的一部分执行。 DBTUP，DBACC，DBLQH，DBTUX，BACKUP，TSMAN，LGMAN，PGMAN，RESTORE都是紧密集成的模块，它们在每个节点中本地处理数据和索引。 这组模块形成一个LDM实例，每个节点可以有多个LDM实例，这些实例可以分布在一组线程上。 每个LDM实例都拥有自己的数据分区。 我们还有两个不属于重启处理的模块，这是TRPMAN模块，它执行许多与传输相关的功能（与其他节点通信）。 它在接收线程中执行。 最后，我们有THRMAN在每个线程中执行并执行一些线程管理功能。 所有模块都接收READ_CONFIG_REQ，所有模块也接收STTOR用于阶段0和阶段1.在阶段1中，他们报告他们希望获得更多信息的起始阶段。 在READ_CONFIG_REQ期间，线程可以在模块中执行很长时间，因为我们可以分配和触摸大尺寸的存储器。 这意味着我们的监视程序线程在此阶段有一个特殊的超时，以确保我们不会因为长时间初始化内存而导致进程崩溃。 在正常操作中，每个信号应仅执行少量微秒。 通过将消息STTOR发送到所有模块来同步启动阶段，逻辑上每个模块从0到255获得每个启动阶段的该信号。然而，响应消息STTORRY包含模块真正感兴趣的启动阶段列表。 处理起始相位信号的NDBCNTR模块可以优化掉不需要的任何信号。 模块接收STTOR消息的顺序对于所有阶段都是相同的： 1) NDBFS2) DBTC3) DBDIH4) DBLQH5) DBACC6) DBTUP7) DBDICT8) NDBCNTR9) CMVMI10) QMGR11) TRIX12) BACKUP13) DBUTIL14) SUMA15) DBTUX16) TSMAN17) LGMAN18) PGMAN19) RESTORE20) DBINFO21) DBSPJ 此外，还有一个由NDBCNTR控制的特殊启动阶段处理，因此当NDBCNTR收到自己的STTOR消息时，它会启动涉及模块的本地启动阶段处理，DBLQH，DBDICT，DBTUP，DBACC，DBTC和DBDIH。 对于阶段2到8，会发生这种情况。在这些启动阶段发送的消息是NDB_STTOR和NDB_STTORRY，它们的处理方式与STTOR和STTORRY类似。 模块还以相同的顺序接收所有阶段的启动阶段，此顺序为： 1) DBLQH2) DBDICT3) DBTUP4) DBACC5) DBTC6) DBDIH 对于那些多线程的模块，STTOR和NDB_STTOR消息始终由在主线程中执行的代理模块接收。 然后，代理模块将STTOR和NDB_STTOR消息发送到模块的每个单独实例（实例数通常与线程数相同，但有时可能不同）。 它并行执行，因此所有实例并行执行STTOR。 因此，有效地，模块的每个实例将在逻辑上首先接收READ_CONFIG_REQ，然后为每个启动阶段接收一组STTOR消息，并且一些模块也将按特定顺序接收NDB_STTOR。 所有这些消息都按特定顺序发送并按顺序发送。 因此，这意味着我们能够通过在正确的启动阶段执行操作来控制何时完成任务。 接下来，我们将逐步描述节点重启（或节点作为集群启动/重启的一部分启动）中发生的情况。 启动目前是一个顺序过程，除非声明它并行发生。 以下描述因此描述了当前实际发生的顺序。]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库中的undo日志、redo日志、检查点]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84undo%E6%97%A5%E5%BF%97%E3%80%81redo%E6%97%A5%E5%BF%97%E3%80%81%E6%A3%80%E6%9F%A5%E7%82%B9%2F</url>
    <content type="text"><![CDATA[数据库存放数据的文件，本文称其为data file。数据库的内容在内存里是有缓存的，这里命名为db buffer。某次操作，我们取了数据库某表格中的数据，这个数据会在内存中缓存一些时间。对这个数据的修改在开始时候也只是修改在内存中的内容。当db buffer已满或者遇到其他的情况，这些数据会写入data file。 undo，redo日志在内存里也是有缓存的，这里将其叫做log buffer。磁盘上的日志文件称为log file。log file一般是追加内容，可以认为是顺序写，顺序写的磁盘IO开销要小于随机写。 Undo日志记录某数据被修改前的值，可以用来在事务失败时进行rollback；Redo日志记录某数据块被修改后的值，可以用来恢复未写入data file的已成功事务更新的数据。 例如某一事务的事务序号为T1，其对数据X进行修改，设X的原值是5，修改后的值为15，那么Undo日志为&lt;T1, X, 5&gt;，Redo日志为&lt;T1, X, 15&gt;。 当用户生成一个数据库事务时，undo log buffer会记录被修改的数据的原始值，redo会记录被修改的数据的更新后的值。 redo日志应首先持久化在磁盘上，然后事务的操作结果才写入db buffer，（此时，内存中的数据和data file对应的数据不同，我们认为内存中的数据是脏数据），db buffer再选择合适的时机将数据持久化到data file中。这种顺序可以保证在需要故障恢复时恢复最后的修改操作。先持久化日志的策略叫做Write Ahead Log，即预写日志。 在很多系统中，undo日志并非存到日志文件中，而是存放在数据库内部的一个特殊段中。本文中就把这些存储行为都泛化为undo日志存储到undo log file中。 对于某事务T，在log file的记录中必须开始于事务开始标记（比如“start T”），结束于事务结束标记（比如“end T”、”commit T”）。在系统恢复时，如果在log file中某个事务没有事务结束标记，那么需要对这个事务进行undo操作，如果有事务结束标记，则redo。 在db buffer中的内容写入磁盘数据库文件之前，应当把log buffer的内容写入磁盘日志文件。 有一个问题，redo log buffer和undo log buffer存储的事务数量是多少，是按照什么规则将日志写入log file？如果存储的事务数量都是1个，也就意味着是将日志立即刷入磁盘，那么数据的一致性很好保证。在执行事T时，突然断电，如果未对磁盘上的redo log file发生追加操作，可以把这个事务T看做未成功。如果redo log file被修改，则认为事务是成功了，重启数据库使用redo log恢复数据到db buffer和 data file即可。 如果存储多个的话，其实也挺好解释的。就是db buffer写入data file之前，先把日志写入log file。这种方式可以减少磁盘IO，增加吞吐量。不过，这种方式适用于一致性要求不高的场合。因为如果出现断电等系统故障，log buffer、db buffer中的完成的事务会丢失。以转账为例，如果用户的转账事务在这种情况下丢失了，这意味着在系统恢复后用户需要重新转账。 检查点checkpointcheckpoint是为了定期将db buffer的内容刷新到data file。当遇到内存不足、db buffer已满等情况时，需要将db buffer中的内容/部分内容（特别是脏数据）转储到data file中。在转储时，会记录checkpoint发生的”时刻“。在故障回复时候，只需要redo/undo最近的一次checkpoint之后的操作。 幂等性问题在日志文件中的操作记录应该具有幂等性。幂等性，就是说同一个操作执行多次和执行一次，结果是一样的。例如，5*1 = 5*1*1*1，所以对5的乘1操作具有幂等性。日志文件在故障恢复中，可能会回放多次（比如第一次回放到一半时系统断电了，不得不再重新回放），如果操作记录不满足幂等性，会造成数据错误。 InnoDB Redo Flush及脏页刷新机制深入分析我们知道InnoDB采用Write Ahead Log策略来防止宕机数据丢失，即事务提交时，先写重做日志，再修改内存数据页，这样就产生了脏页。既然有重做日志保证数据持久性，查询时也可以直接从缓冲池页中取数据，那为什么还要刷新脏页到磁盘呢？如果重做日志可以无限增大，同时缓冲池足够大，能够缓存所有数据，那么是不需要将缓冲池中的脏页刷新到磁盘。但是，通常会有以下几个问题： 服务器内存有限，缓冲池不够用，无法缓存全部数据 重做日志无限增大成本要求太高 宕机时如果重做全部日志恢复时间过长事实上，当数据库宕机时，数据库不需要重做所有的日志，只需要执行上次刷入点之后的日志。这个点就叫做Checkpoint，它解决了以上的问题： 缩短数据库恢复时间 缓冲池不够用时，将脏页刷新到磁盘 重做日志不可用时，刷新脏页 InnoDB引擎通过LSN(Log Sequence Number)来标记版本，LSN是日志空间中每条日志的结束点，用字节偏移量来表示。每个page有LSN，redo log也有LSN，Checkpoint也有LSN。可以通过命令show engine innodb status来观察：]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机性能监控及调优]]></title>
    <url>%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%8F%8A%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[常用虚拟机性能监控工具JDK 命令行工具其中的重中之重是 jstat 命令！而它最常用的参数就是 -gcutil，使用格式如下：1jstat -gcutil [pid] [intervel] [count] 输出如下： S0：堆上 Survivor space 0 区已使用空间的百分比 S1：堆上 Survivor space 1 区已使用空间的百分比 E：堆上 Eden 区已使用空间的百分比 O：堆上 Old space 区已使用空间的百分比 P：堆上 Perm space 区已使用空间的百分比 YGC：从程序启动到采样时发生的 Minor GC 次数 YGCT：从程序启动到采样时 Minor GC 所用的时间 FGC：从程序启动到采样时发生的 Full GC 次数 FGCT：从程序启动到采样时 Full GC 所用的时间 GCT：从程序启动到采样时 GC 的总时间ps 命令 (Linux)对于 jps 命令，其实没必要使用，一般使用 Linux 里的 ps 就够了，ps 为我们提供了当前进程状态的一次性的查看，它所提供的查看结果并不动态连续的，如果想对进程时间监控，应该用 top 工具。 Linux 上进程的 5 种状态 运行 [R, Runnable]：正在运行或者在运行队列中等待； 中断 [S, Sleep]：休眠中, 受阻, 在等待某个条件的形成或接受到信号； 不可中断 [D]：收到信号不唤醒和不可运行, 进程必须等待直到有中断发生； 僵死 [Z, zombie]：进程已终止, 但进程描述符存在, 直到父进程调用 wait4() 系统调用后释放； 停止 [T, Traced or stop]：进程收到 SIGSTOP, SIGSTP, SIGTIN, SIGTOU 信号后停止运行运行。 1234567891011ps -A # 列出所有进程信息（非详细信息）ps aux # 列出所有进程的信息ps aux | grep zshps -ef # 显示所有进程信息，连同命令行ps -ef | grep zsh ps -u root # 显示指定用户信息ps -l # 列出这次登录bash相关信息ps axjf # 同时列出进程树状信息 JVM 常见参数设置内存设置参数 -Xms：初始堆大小，JVM 启动的时候，给定堆空间大小。 -Xmx：最大堆大小，如果初始堆空间不足的时候，最大可以扩展到多少。 -Xmn：设置年轻代大小。整个堆大小 = 年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为 64M，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun 官方推荐配置为整个堆的 3/8。 -Xss： 设置每个线程的 Java 栈大小。JDK 5 后每个线程 Java 栈大小为 1M。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000 左右。 -XX:NewRatio=n：设置年轻代和年老代的比值。如为 3，表示年轻代与年老代比值为 1:3。 -XX:MaxTenuringThreshold：设置垃圾最大年龄。如果设置为 0 的话，则年轻代对象不经过 Survivor 区，直接进入年老代。对于年老代比较多的应用（即 Minor GC 过后有大量对象存活的应用），可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在 Survivor 区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概率。 设置经验 开发过程的测试应用，要求物理内存大于 4G 1234-Xmx3550m-Xms3550m -Xmn2g-Xss128k 高并发本地测试使用，大对象相对较多（如 IO 流） 1234567-Xmx3550m-Xms3550m-Xss128k-XX:NewRatio=4-XX:SurvivorRatio=4-XX:MaxPermSize=160m-XX:MaxTenuringThreshold=0 环境： 16G 物理内存，高并发服务，重量级对象中等（线程池，连接池等），常用对象比例为 40%（即运行过程中产生的对象 40% 是生命周期较长的） 1234567-Xmx10G-Xms10G-Xss1M-XX:NewRatio=3-XX:SurvivorRatio=4 -XX:MaxPermSize=2048m-XX:MaxTenuringThreshold=5 收集器设置参数 收集器设置 -XX:+UseSerialGC：设置串行收集器，年轻带收集器。 -XX:+UseParallelGC：设置并行收集器。 -XX:+UseParNewGC：设置年轻代为并行收集。可与 CMS 收集同时使用。JDK 5.0 以上，JVM 会根据系统配置自行设置，所以无需再设置此值。 -XX:+UseParallelOldGC：设置并行年老代收集器，JDK6.0 支持对年老代并行收集。 -XX:+UseConcMarkSweepGC：设置年老代并发收集器，测试中配置这个以后，-XX:NewRatio 的配置失效，原因不明。所以，此时年轻代大小最好用 -Xmn 设置。 -XX:+UseG1GC：设置 G1 收集器。 并行收集器参数设置 -XX:ParallelGCThreads=n：设置并行收集器收集时最大线程数使用的 CPU 数。并行收集线程数。 -XX:MaxGCPauseMillis=n：设置并行收集最大暂停时间，单位毫秒。 -XX:GCTimeRatio=n：设置垃圾回收时间占程序运行时间的百分比。 -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的 Survivor 区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。 -XX:CMSFullGCsBeforeCompaction=n：由于 CMS 不对内存空间进行压缩、整理，所以运行一段时间以后会产生”碎片”，使得运行效率降低。此值设置运行多少次 GC 以后对内存空间进行压缩、整理。 -XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片。虚拟机调优案例分析高性能硬件上的程序部署策略补充：64 位虚拟机在 Java EE 方面，企业级应用经常需要使用超过 4GB 的内存，此时，32 位虚拟机将无法满足需求，可是 64 位虚拟机虽然可以设置更大的内存，却存在以下缺点： 内存问题： 由于指针膨胀和各种数据类型对齐补白的原因，运行于 64 位系统上的 Java 应用程序需要消耗更多的内存，通常要比 32 位系统额外增加 10% ~ 30% 的内存消耗。 性能问题： 64 位虚拟机的运行速度在各个测试项中几乎全面落后于 32 位虚拟机，两者大概有 15% 左右的性能差距。 服务系统经常出现卡顿（Full GC 时间太长）首先 jstat -gcutil 观察 GC 的耗时，jstat -gccapacity 检查内存用量（也可以加上 -verbose:gc 参数获取 GC 的详细日志），发现卡顿是由于 Full GC 时间太长导致的，然后 jinfo -v pid，查看虚拟机参数设置，发现 -XX:NewRatio=9，这就是原因： 新生代太小，对象提前进入老年代，触发 Full GC 老年代较大，一次 Full GC 时间较长 可以调小 NewRatio 的值，尽肯能让比较少的对象进入老年代。 除了 Java 堆和永久代之外，会占用较多内存的区域 区域 大小调整 / 说明 内存不足时抛出的异常 直接内存 -XX:MaxDirectMemorySize OutOfMemoryError: Direct buffer memory 线程堆栈 -Xss StackOverflowError 或 OutOfMemoryError: unable to create new native thread Socket 缓存区 每个 Socket 连接都有 Receive(37KB) 和 Send(25KB) 两个缓存区 IOException: Too many open files JNI 代码 如果代码中使用 JNI 调用本地库，那本地库使用的内存也不在堆中 虚拟机和 GC 虚拟机、GC 代码执行要消耗一定内存 从 GC 调优角度解决新生代存活大量对象问题（Minor GC 时间太长） 将 Survivor 空间去除，让新生代中存活的对象在第一次 Minor GC 后立刻进入老年代，等到 Full GC 时再清理。 参数调整方法： -XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0 -XX:AlwaysTenure]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络基础知识]]></title>
    <url>%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[一次完整的 HTTP 请求所经历的步骤即在浏览器中输入 URL 地址 &gt;&gt; 显示主页的过程。总体来说分为以下几个过程： DNS 解析 TCP 连接 发送 HTTP 请求 服务器处理请求并返回 HTTP 报文 浏览器解析渲染页面 连接结束 简单的来说，就是先经过下图的过程，将客户端的请求传到服务器，再经过下图过程的逆过程，将客户端请求的数据返回给客户端，然后客户端浏览器对返回的数据进行渲染，最终得到了我们看到的页面。 DNS 解析DNS 解析的过程就是寻找哪台机器上有你需要资源的过程。也就是说，把你输入的 URL 地址转换为 IP 地址。 TCP 连接客户端 (浏览器) 通过 TCP 传输协议建立到服务器的 TCP 连接，详见后面 TCP 是如何传输数据，以及三次握手和四次挥手等。 发送 HTTP 请求发送 HTTP 请求的过程就是构建 HTTP 请求报文并通过 TCP 协议中发送到服务器指定端口 (HTTP 协议 80/8080，HTTPS 协议443)。HTTP 请求报文是由三部分组成： 请求行 、 请求报头 、 请求正文 。 服务器处理请求并返回 HTTP 报文后端从固定的端口接收到 TCP 报文（这一部分对应于编程语言中的 socket），它会对 TCP 连接进行处理，对 HTTP 协议进行解析，并按照报文格式进一步封装成 HTTP Request 对象，供上层使用。这一部分工作一般是由 Web 服务器去进行，我使用过的 Web 服务器有 Tomcat，Jetty。 HTTP 响应报文也是由三部分组成： 状态码 、 响应报头 、 响应报文 。 状态码： 1xx：指示信息–表示请求已接收，继续处理。 2xx：成功–表示请求已被成功接收、理解、接受。 3xx：重定向–要完成请求必须进行更进一步的操作。 4xx：客户端错误–请求有语法错误或请求无法实现。 5xx：服务器端错误–服务器未能实现合法的请求。 浏览器解析渲染页面即浏览器收到 HTML、CSS、JS 文件后，把页面呈现到屏幕上的过程。 DNS 解析DNS 解析的过程就是寻找哪台机器上有你需要资源的过程。也就是说，把你输入的 URL 地址转换为 IP 地址。 DNS 域名解析过程如下图所示，简单来说就是先查自己的本地域名服务器，如果自己就有缓存，直接从缓存里面读就可以，如果缓存里没有，本地域名服务器会发出递归连环问去查找。 DNS 负载均衡 DNS 可以根据每台机器的负载量，该机器离用户地理位置的距离等等，返回一个合适的机器的 IP 给用户，这个过程就是 DNS 负载均衡，又叫做 DNS 重定向。大家耳熟能详的 CDN (Content Delivery Network) 就是利用 DNS 的重定向技术实现的，DNS 服务器会返回一个跟用户最接近的服务器的 IP 地址给用户，CDN 节点的服务器负责响应用户的请求。 CDN（Content Distribute Network）CDN，内容分发网络。最简单的 CDN 网络由一个 DNS 服务器和几台缓存服务器组成： 当用户点击网站页面上的内容 URL，经过本地 DNS 系统解析，DNS 系统会最终将域名的解析权交给 CNAME 指向的 CDN 专用 DNS 服务器。 CDN 的 DNS 服务器将 CDN 的全局负载均衡设备 IP 地址返回用户。 用户向 CDN 的全局负载均衡设备发起内容 URL 访问请求。 CDN 全局负载均衡设备根据用户 IP 地址，以及用户请求的内容 URL，选择一台用户所属区域的区域负载均衡设备，告诉用户这台服务器的 IP 地址，让用户向这台设备发起请求。选择的依据包括： 根据用户 IP 地址，判断哪一台服务器距用户最近； 根据用户所请求的 URL 中携带的内容名称，判断哪一台服务器上有用户所需内容； 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力； 用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端。 如果这台缓存服务器上并没有用户想要的内容，而区域均衡设备依然将它分配给了用户，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。 TCP 是如何传输数据的？TCP (Transmission Control Protocol, TCP)，是一种面向连接、确保数据在端到端间可靠传输的协议。在传输前需要先建立一条可靠的传输链路，然后让数据在这条链路上流动，完成传输。简单来说就是，TCP 在想尽各种办法保证数据传输的可靠性，为了可靠性 TCP 会这样进行数据传输： 三次握手建立连接； 对发出的每一个字节进行编号确认，校验每一个数据包的有效性，在出现超时进行重传； 通过流量控制（通过滑动窗口协议实现）和拥塞控制（慢启动和拥塞避免、快重传和快恢复）等机制，避免网络状况恶化而影响数据传输； 四次挥手断开连接。 TCP 报头结构TCP 头部长度的前 20 字节是固定的，后面部分长度不定，但最多 40 字节 ，因此 TCP 头部一般在 20 ~ 60 字节之间。它的结构图如下： 它的每一字段的说明如下： 0 ~ 32 比特：源端口和目的端口 ，各占 16 比特（2 字节）。 32 ~ 64 比特：序列号 seq ，当前 TCP 数据报数据部分的第一个字节的序号（4 字节）。 64 ~ 96 比特：确认序号 ack ，表示当前主机作为接收端时，下一个希望接收的序列号是多少，确认号 = 当前主机已经正确接收的最后一个字节的序列号 + 1 96 ~ 112 比特：数据报报头长度，保留字段，标识符。 标识符：用于表示 TCP 报文的性质，只能是 0 或 1。TCP 的常用标识符： URG=1：紧急指针有效性标志，表示本数据报的数据部分包含紧急信息，紧急数据一定位于当前数据包数据部分的最前面，后面的紧急指针则标明了紧急数据的尾部。 ACK=1：在连接建立后传送的所有报文段都必须把 ACK 置 1，也就是说 ACK=1 后确认号字段才有效。 PSH=1：接收方应尽快将报文段提交至应用层，不会等到缓冲区满后再提交，一些交互式应用需要这样的功能，降低命令的响应时间。 RST=1：当该值为 1 时，表示当前 TCP 连接出现严重问题，必须要释放重连。 SYN=1：用在建立连接时 SYN=1, ACK=0：当前报文段是一个连接请求报文。 SYN=1, ACK=1：表示当前报文段是一个同意建立连接的应答报文。 FIN=1：表示此报文段是一个释放连接的请求报文。 112 ~ 128 比特：接收窗口大小 ，该字段用于实现 TCP 的流量控制。 它表示当前接收方的接收窗口的剩余大小，发送方收到该值后会将发送窗口调整成该值的大小（收到一个数据报就调整一次）。发送窗口的大小又决定了发送速率，所以接收方通过设置该值就可以控制发送放的发送速率。 128 ~ 144 比特：校验和 ，用于接收端检验整个数据包在传输过程中是否出错。 144 ~ 160 比特：紧急指针 ， 用来标明紧急数据的尾部，和 URG 标识符一起使用。 TCP 三次握手、四次挥手 TCP 的三次握手：为了建立可靠的通信信道，即双方确认自己与对方的发送和接收都是正常的。 TCP 的四次挥手：确保双方都在没有想说的内容之后，释放 TCP 连接。 三次握手三次握手的流程 第一次握手：A 向 B 发送建立连接请求（A 对 B 说：“我们在一起吧！”） 第二次握手：B 收到 A 的建立连接请求后，发给 A 一个同意建立连接的应答报文（B 对 A 说：“好的，同意和你在一起啦”） 第三次握手：A 向 B 发送个报文，表示我已经收到你的应答了（A 对 B 说：“亲爱的，你同意真是太好了，我们可以互相砸数据了”） 为什么 TCP 连接需要三次握手，两次不可以吗？首先，我们要知道，这三次握手是为了让双方确认自己与对方的发送和接收都是正常的。但是，只成功完成两次握手的时候，B 不知道自己的发送能力是否正常，也不知道 A 的收报能力是否正常，所以它们需要第三次握手。 同时，如果没有第三次连接，很有可能导致 B 建立一个脏连接。 脏连接建立的过程： A 发送的第一个建立连接的请求，这个连接好久好久都没有达到 B 那里； 所以，A 又重新发送了一个新的建立连接请求给 B，这个请求成功了，A 和 B 愉快的交换完了数据并且断开了连接； 此时，A 第一个发的建立连接的请求终于穿越 n 个路由器到达了 B，B 以为这时 A 发来的新的建立连接的请求，愉快的返回了同意建立连接的请求； 如果只需要两次握手，此时 B 会单方面的建立起与 A 的连接，而 A 根本就不在 SYN_SENT 状态，它会把 B 的应答请求直接丢掉，不会建立连接。此时，B单方面创建的这个连接就是脏连接。 四次挥手四次挥手的流程 A：我们分手吧。 B：好的，我收到了你的分手请求，再等一会，我把你剩我这的东西打包给你。（此时 A 已经不能再给 B 发东西了） B：好了，东西发完了，分吧。（此时 B 也不能再给 A 发东西了） A：好的，知道你东西都发完了，我在等 2MSL，然后就消失了。 TIME_WAIT 存在的必要性如果 A 发送完最后一个 ACK=1 后，立即进入 CLOSED 状态，可能会导致 B 无法进入 CLOSED 状态。 原因：假设 A 最后的 ACK 在网络传输中丢失了，B 会认为 A 根本没收到自己发的 FIN=1, ACK=1 报文，会导致 B 超时重发 FIN=1, ACK=1 报文。A 第二次收到 FIN=1, ACK=1 报文后，会再发一次 ACK，并重新开始 TIME_WAIT 的计时。如果 A 发完最后一个 ACK 后立即关闭，B 可能会永远接收不到最后一个 ACK，也就无法进入 CLOSED 状态。 在高并发上，可以将 TIME_WAIT 调到小于 30s 为宜。 设置方法：改变服务器的配置文件 /etc/sysctl.conf 中的 net.ipv4.tcp_fin_timeout=30 注：seq 表示序列号，ack 表示确认号，2MSL 是报文在网络中生存的最长时间。 TCP 流量控制、拥塞控制流量控制产生原因： 如果发送方数据发送的过快，接收方可能来不及接收，这会造成数据的丢失。 解决方法： 通过滑动窗口实现，接收端告诉发送发自己的接收窗口有多大，发送端会调整自己的发送窗口不超过接收端的接收窗口大小。 流量控制引发的死锁： 当发送者收到了一个窗口为 0 的应答后，发送者会停止发送，等待接收者的下一个应答。但是如果这个窗口不为 0 的应答在传输过程丢失，发送者一直等待下去，而接收者以为发送者已经收到该应答，等待接收新数据，这样双方就相互等待，从而产生死锁。 拥塞控制慢启动和拥塞避免首先，发送方维持一个叫做 拥塞窗口 cwnd（congestion window） 的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于 min{拥塞窗口, 接收窗口}。 慢启动就是： 不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。具体过程如下： 慢启动的时候，拥塞窗口每次是呈 2 的指数次方增长的，因为开始的时候需要比较快速的将拥塞窗口的大小增长到一个合适值。如果我们一直使用慢启动的方法确认拥塞窗口 cwnd 的大小，cwnd 会飞速增大，而且增长的粒度会越来越粗，一不小心就增的过大了，就会导致网络的拥塞。 为了避免这种情况，我们设定了一个慢开始门限 ssthresh，令 cwnd 大于一定值之后就采用拥塞避免算法，拥塞避免算法和慢启动算法的区别在于：拥塞避免算法每次只将 cwnd 增加 1，也就是呈加法增长的。ssthresh 的用法如下： cwnd &lt; ssthresh 时，使用慢开始算法 cwnd &gt; ssthresh 时，改用拥塞避免算法 cwnd = ssthresh时，慢开始与拥塞避免算法任意 拥塞避免算法会让拥塞窗口缓慢增长，即每经过一个往返时间 RTT 就把发送方的拥塞窗口 cwnd 加 1，而不是加倍。这样拥塞窗口按线性规律缓慢增长。 无论是在 慢启动阶段 还是在 拥塞避免阶段 ，只要发送方判断 网络出现拥塞 （其根据就是没有收到确认，虽然没有收到确认可能是其他原因的分组丢失，但是因为无法判定，所以都当做拥塞来处理）， 就把慢开始门限设置为出现拥塞时的发送窗口大小的一半（乘法减小算法）。然后把拥塞窗口设置为 1，执行慢开始算法。 通过使用慢启动与拥塞避免算法，拥塞窗口的大小变化大致如下图所示： 注：这里只是为了讨论方便而将拥塞窗口大小的单位改为数据报的个数，实际上应当是字节。 快重传和快恢复快重传 要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不等到自己发送数据时捎带确认。 快重传算法规定：发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传时间计时器到期。如下图： 快恢复 要求当发送发连续接收到三个确认时，就执行乘法减小算法，把慢启动开始门限（ssthresh）减半，但是接下来并不执行慢开始算法。而是将 cwnd 设置为 ssthresh 的大小，然后执行拥塞避免算法。 通过使用快重传与快恢复算法，拥塞窗口的大小变化大致如下图所示： TCP 滑动窗口停止等待协议（ARQ 协议，滑动窗口协议的简易版）原理： A 向 B 每发送一个分组，都要停止发送，等待 B 的确认应答；A 只有收到了 B 的确认应答后才能发送下一个分组。 A 发送的分组丢失或出错 丢失：发送者 A 拥有超时计时器。每发送一个分组便会启动超时计时器，等待 B 的应答。若超时仍未收到应答，A 就会重发刚才的分组。 出错：若 B 收到分组，但通过检查和字段发现分组在运输途中出现差错，它会直接丢弃该分组，并且不会有任何其他动作。A 超时后便会重新发送该分组，直到 B 正确接收为止。 B 发送的确认应答丢失或迟到 丢失：A 迟迟收不到 B 的确认应答，会进行超时重传，B 收到重复的分组后会立即补发一个确认应答。 迟到：A 会根据分组号得知该分组已被接收，会直接丢弃该应答。 滑动窗口协议（连续 ARQ 协议）ARQ 协议的缺点： 每次只发送一个分组，在该分组的应答到来前只能等待。为了解决这个问题，我们改成一次发送一堆，也就是我们有个窗口，在发送端没有收到确认应答时，可以继续发送窗口中的分组，而不是干等着。 累计确认： 接收端不用为每一个分组发送一个应答了，改为为一组分组发送一个确认应答。这个应答会通过 TCP 头中的 ack（确认号）来告诉发送端它下一个希望接收的分组号是多少。 发送窗口： 发送端收到接收端发来的一个确认应答后，会根据确认应答的 TCP 头中的各种信息移动 P1、P2、P3 三个指针： 根据 ack 的值移动 P1 指针，确认哪些分组被成功接收了； 然后根据窗口大小移动 P3 = P1 + 窗口大小； 然后 P2 从 P1 开始向 P3 移动，向接收端发送分组数据。 接收窗口： 接收者收到的字节会存入接收窗口，接收者会对已经正确接收的有序字节进行累计确认，发送完确认应答后，接收窗口就可以向前移动指定字节。 如果某些字节并未按序收到，接收者只会确认最后一个有序的字节，从而乱序的字节就会被重新发送。 注意： 同一时刻发送窗口的大小并不一定和接收窗口一样大（因为时延和拥塞窗口）。 TCP 标准并未规定未按序到达的字节的处理方式。但 TCP 一般都会缓存这些字节，等缺少的字节到达后再交给应用层处理（应用层可以对它进行排序）。这比直接丢弃乱序的字节要节约带宽。 TCP 与 UDP 的区别从特点上看： TCP 是面向连接的，UDP 是无连接的，即 TCP 在传输数据前要先通过三次握手建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制建立连接，数据传输结束后要释放连接。 UDP 在传输数据前不需要建立连接，远地主机在接收到报文之后也无需确认。 所以 TCP 是可靠传输，UDP 是不可靠传输。 TCP 因为有连接，所以数据以字节流的形式传输，UDP 则以数据报文段形式传输，而且 TCP 只能是一对一的，而 UDP 可以各种通信。 从性能上看： TCP 传输效率慢，需要资源多，但可靠。UDP 传输效率快，需要资源少，但不可靠。 应用场景： TCP 应用在要求传输数据可靠的情况下，如文件传输、邮件传输等。UDP 应用在要求通信速度但对可靠性要求比较低的场景，如 QQ 语音、视频等。 首部字节： TCP 首部有 20 ~ 60 个字节，UDP 首部由 8 个字节 4 个字段组成。 怎么用 UDP 实现 TCP？ 在传输层 UDP 是不可靠的，所以需要在应用层自己实现一些保证可靠传输的机制，简单来说，就是使用 UDP 来构建可靠的面向连接的数据传输，就是在应用层实现类似于 TCP 的超时重传（定时器），拥塞控制（滑动窗口），有序接收（添加包序号），应答确认（ack 和 seq）等。目前已经有了实现 UDP 可靠运输的机制 —— UDT：主要用于高速广域网海量数据传输，是应用层协议。 HTTP 长连接、短连接短连接在 HTTP/1.0 中默认使用 短连接：客户端和服务器每进行一次 HTTP 操作，就建立一次连接，任务结束就中断连接。 当客户端浏览器访问的某个 HTML 或其他类型的 Web 页中包含有其他的 Web 资源（如：JavaScript 文件、图像文件、CSS 文件等）时，浏览器就会重新建立一个 HTTP 会话。 应用： WEB 网站的 http 服务一般都用短链接，因为并发量大，但每个用户无需频繁操作。 长连接而从 HTTP/1.1 起，默认使用长连接。使用长连接的HTTP协议，会在响应头加入这行代码： 1Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。 Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接，有关 TCP 长连接、短连接的介绍请看下一小节。 应用： 适用于操作频繁的点对点通讯，而且连接数不能太多。 TCP 长连接、短连接短链接短连接过程： client 向 server 发起连接请求，server 接到请求，然后双方建立连接。client 向 server 发送消息，server 回应 client，然后一次请求就完成了。这时候双方任意都可以发起 close 操作，不过一般都是 client 先发起 close 操作。也就是说，短连接一般只会在 client 和 server 间进行一次请求操作。 优点： 管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。 缺点： 如果客户请求频繁，将在 TCP 的建立和关闭操作上浪费时间和带宽。 长连接长连接过程： client 向 server 发起连接，server 接受 client 连接，双方建立连接，client 与 server完成一次请求后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 TCP 的保活功能主要为服务器应用提供。如果客户端已经消失而连接未断开，则会使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，此时服务器将永远等待客户端的数据。保活功能就是试图在服务端器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何动作，服务器就向客户发送一个探测报文段，根据客户端主机响应探测 4 个客户端状态： 客户主机依然正常运行，且服务器可达。此时客户的TCP响应正常，服务器将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。上述情况下客户端都不能响应 TCP。服务端将无法收到客户端对探测的响应。服务器总共发送 10 个这样的探测，每个间隔 75 秒。若服务器没有收到任何一个响应，它就认为客户端已经关闭并终止连接。 客户端崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户机正常运行，但是服务器不可达。这种情况与第二种状态类似。 优点： 对于请求比较频繁客户来说，可以节省在 TCP 的建立和关闭操作上浪费时间和带宽。 缺点： 存活探测周期太长，而且 client 端一般不会主动关闭它与服务器之间的连接，如果 client 与 server 之间的连接一直不关闭的话，随着客户端连接越来越多，server 早晚有扛不住的时候 ，这时候 server 端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，以避免一些恶意连接导致 server 端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。 HTTP、HTTPS 区别HTTPS 的全称为：HTTP over SSL，简单理解就是在之前的 HTTP 传输上增加了 SSL 协议加密。 1234567|------| | HTTP ||------| &lt;-- HTTPS 在 HTTP 和 TCP 间加了一层 SSL or TLS| TCP ||------|| IP ||------| HTTP 通信存在的问题 容易被监听： http 通信都是明文，数据在客户端与服务器通信过程中，任何一点都可能被劫持，如果明文保存的密码被截取了是很危险的。 被伪装： http 通信时无法保证双方是合法的。比如你请求 www.taobao.com， 你无法知道返回的数据就是来自淘宝，还是中间人伪装的淘宝。 被篡改： 中间人将发给你的信息篡改了你也不知道。 因为 http 不安全，所以 https 出现了！ 区别 HTTPS 需要到 CA 申请证书，HTTP 不需要 HTTPS 密文传输，HTTP 明文传输 连接方式不同，HTTPS 默认使用 443 端口，HTTP 使用 80 端口 HTTPS = HTTP + 加密 + 认证 + 完整性保护，比 HTTP 安全 HTTP 1.1 与 HTTP 1.0 的区别 1.0 需要设置 keep-alive 参数来告知服务器建立长连接，1.1 默认建立长连接。 1.1 支持只发 header 不带 body，如果服务器认为客户端有权利访问，返回 100，否则返回 401，客户端可以接到 100 后再把 body 发过去，接到 401 就不发了，这样比较节省带宽。 1.1 有 host 域，1.0 没有。 host 域用于处理一个IP地址对应多个域名的情况，假设我的虚拟机服务器 IP 是 111.111.111.111，我们可以把 www.qiniu.com，www.taobao.com 和 www.jd.com 这些网站都架设那台虚拟机上面，但是这样会有一个问题，我们每次访问这些域名其实都是解析到服务器 IP 111.111.111.111，那么如何来区分每次根据域名显示出不同的网站的内容呢？就是通过 Host 域的设置，可以在 Tomcat 的 conf 目录下的 server.xml 进行配置。 1.1 会进行带宽优化。1.0 存在浪费带宽的现象，而且不支持断点续传，1.1 在请求头中引入了 range 域，允许只请求某个资源的某个部分，即返回状态码 206。 如果客户端不断的发送请求连接会怎样服务器端会为每个请求创建一个链接，然后向 client 端发送创建连接时的回复，然后进行等待客户端发送第三次握手数据包，这样会白白浪费资源。DDos 攻击就是基于这一点达到的。 DDos 攻击简单的说就是不停的向服务器发送建立连接请求，但不发送第三次握手的数据包。 客户端向服务器端发送连接请求数据包 服务器向客户端回复连接请求数据包，然后服务器等待客户端发送tcp/ip链接的第三步数据包 如果客户端不向服务器端发送最后一个数据包，服务器须等待 30s 到 2min 才能将此连接关闭。当大量的请求只进行到第二步，而不进行第三步，服务器将有大量的资源在等待第三个数据包，造成DDos攻击。 DDos 预防 DDoS清洗：对用户请求数据进行实时监控，及时发现异常流量，封掉异常流量的 IP，使用的命令：iptables。 用 iptables 屏蔽单个 IP 的命令：iptables -I INPUT -s ***.***.***.*** -j DROP 用 iptables 屏蔽整个 IP 段命令：iptables -I INPUT -s 121.0.0.0/8 -j DROP 用 iptables 解禁 IP 命令：iptables -D INPUT -s ***.***.***.*** -j DROP 相当于在 /etc/iptables.conf 配置文件中写入：-A INPUT -s ***.***.***.*** -j DROP CDN 加速：在现实中，CDN 服务将网站访问流量分配到了各个节点中，这样一方面隐藏网站的真实 IP，另一方面即使遭遇 DDoS 攻击，也可以将流量分散到各个节点中，防止源站崩溃。 GET 和 POST 区别 Http 报文层面：GET 将请求信息放在 URL 中，POST 方法报文中，所以 POST 方法更安全，毕竟数据在地址栏上不可见。 数据库层面：GET 符合幂等性，POST 不符合。 缓存层面：GET 可以被缓存、被存储（书签），而 POST 不行。 301、302 重定向HTTP 返回码中 301 和 302 的区别： 301，302 都是 HTTP 的状态码，都代表着某个 URL 发生了转移，不同之处在于： 301 代表永久性转移 (Permanently Moved)。 302 代表暂时性转移(Temporarily Moved )。搜索引擎会抓取新的内容而保留旧的网址，因为服务器返回 302 代码，搜索引擎认为新的网址只是暂时的。 使用场景： 域名到期不想续费； 在搜索引擎的搜索结果中出现了不带 www 的域名，而带 www 的域名却没有收录，这个时候可以用 301 重定向来告诉搜索引擎我们目标的域名是哪一个； 空间服务器不稳定，换空间的时候。 注意：尽量使用 301 跳转！原因：网址劫持！ 从网址 A 做一个 302 重定向到网址 B 时，主机服务器的隐含意思是网址 A 随时有可能改主意，重新显示本身的内容或转向其他的地方。大部分的搜索引擎在大部分情况下，当收到 302 重定向时，一般只要去抓取目标网址就可以了，也就是说网址 B。如果搜索引擎在遇到 302 转向时，百分之百的都抓取目标网址 B 的话，就不用担心网址 URL 劫持了。问题就在于，有的时候搜索引擎，尤其是 Google，并不能总是抓取目标网址。比如说，有的时候 A 网址很短，但是它做了一个 302 重定向到 B 网址，而 B 网址是一个很长的乱七八糟的 URL 网址，甚至还有可能包含一些问号之类的参数。很自然的，A 网址更加用户友好，而 B 网址既难看，又不用户友好。这时 Google 很有可能会仍然显示网址A。 这就造成了网址 URL 劫持的可能性。也就是说，一个不道德的人在他自己的网址 A 做一个 302 重定向到你的网址 B，出于某种原因， Google 搜索结果所显示的仍然是网址 A，但是所用的网页内容却是你的网址 B 上的内容，这种情况就叫做网址 URL 劫持。你辛辛苦苦所写的内容就这样被别人偷走了。 302 重定向很容易被搜索引擎误认为是利用多个域名指向同一网站，那么你的网站就会被封掉，罪名是 “利用重复的内容来干扰 Google 搜索结果的网站排名”。 URL 和 URIURI 叫做统一资源标志符，就是在某一规则下能把一个资源独一无二地标识出来。 URL 叫做统一资源定位符，是以描述资源的位置来唯一确定一个资源的。所以URL 是 URI 的子集。 NAT网络地址转换常用于私有地址与公有地址的转换，以解决 IP 地址匮乏的问题。 NAT 的基本工作原理是：当私有网主机和公共网主机通信的 IP 包经过 NAT 网关时，将 IP 包中的源 IP 或目的 IP 在私有 IP 和 NAT 的公共 IP 之间进行转换。]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的类加载器]]></title>
    <url>%2FJava%E7%9A%84%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[JAVA类装载方式，有两种:1.隐式装载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中。 2.显式装载， 通过class.forname()等方法，显式加载需要的类类加载的动态性体现:一个应用程序总是由n多个类组成，Java程序启动时，并不是一次把所有的类全部加载后再运行，它总是先把保证程序运行的基础类一次性加载到jvm中，其它类等到jvm用到的时候再加载，这样的好处是节省了内存的开销，因为java最早就是为嵌入式系统而设计的，内存宝贵，这是一种可以理解的机制，而用到时再加载这也是java动态性的一种体现. ClassLoaderJDK 默认提供了如下几种ClassLoader Bootstrp loaderBootstrp加载器是用C++语言写的，它是在Java虚拟机启动后初始化的，它主要负责加载%JAVA_HOME%/jre/lib,-Xbootclasspath参数指定的路径以及%JAVA_HOME%/jre/classes中的类。 ExtClassLoaderBootstrp loader加载ExtClassLoader,并且将ExtClassLoader的父加载器设置为Bootstrp loader.ExtClassLoader是用Java写的，具体来说就是 sun.misc.Launcher$ExtClassLoader，ExtClassLoader主要加载%JAVA_HOME%/jre/lib/ext，此路径下的所有classes目录以及java.ext.dirs系统变量指定的路径中类库。 AppClassLoaderBootstrp loader加载完ExtClassLoader后，就会加载AppClassLoader,并且将AppClassLoader的父加载器指定为 ExtClassLoader。AppClassLoader也是用Java写成的，它的实现类是 sun.misc.Launcher$AppClassLoader，另外我们知道ClassLoader中有个getSystemClassLoader方法,此方法返回的正是AppclassLoader.AppClassLoader主要负责加载classpath所指定的位置的类或者是jar文档，它也是Java程序默认的类加载器。负责加载用户类路径（classpath）上的指定类库，我们可以直接使用这个类加载器。一般情况，如果我们没有自定义类加载器默认就是用这个加载器。class.forname得到的class是已经初始化完成的classloder.loadclass得到的class是还没有链接的 类加载器的双亲委派机制双亲委派模型工作过程是：如果一个类加载器收到类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器完成。每个类加载器都是如此，只有当父加载器在自己的搜索范围内找不到指定的类时（即ClassNotFoundException），子加载器才会尝试自己去加载。 采用双亲委派模式的是好处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。其次是考虑到安全因素，java核心api中定义类型不会被随意替换，假设通过网络传递一个名为java.lang.Integer的类，通过双亲委托模式传递到启动类加载器，而启动类加载器在核心Java API发现这个名字的类，发现该类已被加载，并不会重新加载网络传递的过来的java.lang.Integer，而直接返回已加载过的Integer.class，这样便可以防止核心API库被随意篡改。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet的生命周期与工作原理]]></title>
    <url>%2FServlet%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Servlet运行在Servlet容器中，其生命周期由容器来管理。Servlet的生命周期通过javax.servlet.Servlet接口中的init()、service()和destroy()方法来表示,Servlet的生命周期包含了下面4个阶段： 加载和实例化 初始化 请求处理 服务终止 Web Client 向Servlet容器（Tomcat）发出Http请求 Servlet容器接收Web Client的请求 Servlet容器创建一个HttpRequest对象，将Web Client请求的信息封装到这个对象中。 Servlet容器创建一个HttpResponse对象 Servlet容器调用HttpServlet对象的service方法，把HttpRequest对象与HttpResponse对象作为参数传给HttpServlet 对象。 HttpServlet调用HttpRequest对象的有关方法，获取Http请求信息。 HttpServlet调用HttpResponse对象的有关方法，生成响应数据。 Servlet容器把HttpServlet的响应结果传给Web Client。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的异常体系]]></title>
    <url>%2FJava%E7%9A%84%E5%BC%82%E5%B8%B8%E4%BD%93%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[受检查的(checked)异常和不受检查的(unchecked)异常Throwable下面有两个子类一个是Error，一个是Exception.. 在Exception下面，又分出了两个分支，一个是RuntimeException，一个是非RuntimeException。这两个分支是什么意思呢？根据前面错误类型分析，我们其实也可以将错误分为两种，一种就是你自己犯了错，就是你的代码有问题，一种不是你犯了错。对于你自己犯的错，我们就称为RuntimeException，我们也叫做unchecked Exception。对于不是你犯的错，我们统称为非RuntimeException，也叫checked Exception。 对于未检查异常也叫RuntimeException(运行时异常). 对未检查的异常(unchecked exception )的几种处理方式： 1、捕获2、继续抛出3、不处理 对检查的异常(checked exception，除了RuntimeException，其他的异常都是checked exception )的几种处理方式： 1、继续抛出，消极的方法，一直可以抛到java虚拟机来处理2、用try…catch捕获 注意，对于检查的异常必须处理，或者必须捕获或者必须抛出 异常处理完成以后，Exception对象会在下一个垃圾回收过程中被回收掉。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的IO]]></title>
    <url>%2FJava%E7%9A%84IO%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Java容器]]></title>
    <url>%2FJava%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Java反射、泛型]]></title>
    <url>%2FJava%E5%8F%8D%E5%B0%84%E3%80%81%E6%B3%9B%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[泛型泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。 泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。List arrayList = new ArrayList();arrayList.add(“aaaa”); List arrayList = new ArrayList();//arrayList.add(100); 在编译阶段，编译器就会报错 泛型只在编译阶段有效。见“反射”，用方法的反射绕过编译 泛型类、泛型接口、泛型方法 泛型擦除： Java 编译器生成的字节码文件不包含有泛型信息，泛型信息将在编译时被擦除，这个过程称为泛型擦除。其主要过程为 1）将所有泛型参数用其最左边界（最顶级的父类型）类型替换； 2）移除 all 的类型参数。 反射javap 原生的 看class文件 类是java.lang.class类的实例对象 12345678910111213141516171819202122232425262728//第一种表示方式---&gt;实际在告诉我们任何一个类都有一个隐含的静态成员变量class Class c1 = Foo.class; //第二中表达方式 已经知道该类的对象通过getClass方法 Class c2 = foo1.getClass(); /*官网 c1 ,c2 表示了Foo类的类类型(class type) 万事万物皆对象，类也是对象，是Class类的实例对象 * 这个对象我们称为该类的类类型 */ //不管c1 or c2都代表了Foo类的类类型，一个类只可能是Class类的一个实例对象 System.out.println(c1 == c2);相等 //第三种表达方式 Class c3 = null; try &#123; c3 = Class.forName("com.imooc.reflect.Foo"); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println(c2==c3);相等 //我们完全可以通过类的类类型创建该类的对象实例----&gt;通过c1 or c2 or c3创建Foo的实例对象 try &#123; Foo foo = (Foo)c1.newInstance();//需要有无参数的构造方法 foo.print(); &#125; catch (InstantiationException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; new创建对象是静态加载类，编译时刻就需要加载所有可能用到的类 12345678Class c = obj.getClass();/** 成员变量也是对象* java.lang.reflect.Field* Field类封装了关于成员变量的操作* getFields()方法获取的是所有的public的成员变量的信息* getDeclaredFields获取的是该类自己声明的成员变量的信息 *///Field[] fs = c.getFields();Field[] fs = c.getDeclaredFields(); 要获取一个方法就是获取类的信息，获取类的信息首先要获取类的类类型. 获取方法 名称和参数列表来决定 getMethod获取的是public的方法、getDelcaredMethod自己声明的方法 123456"Method m = c.getMethod(""print"", int.class,int.class);//方法的反射操作 //a1.print(10, 20);方法的反射操作是用m对象来进行方法调用 和a1.print调用的效果完全相同//方法如果没有返回值返回null,有返回值返回具体的返回值//Object o = m.invoke(a1,new Object[]&#123;10,20&#125;);Object o = m.invoke(a1, 10,20);]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的深拷贝与浅拷贝]]></title>
    <url>%2FJava%E7%9A%84%E6%B7%B1%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B5%85%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[Java深拷贝浅拷贝将一个对象的引用复制给另外一个对象，一共有三种方式。第一种方式是直接赋值，第二种方式是浅拷贝，第三种是深拷贝。浅：属性不一样，对象（方法）什么的一样深：都不一样new Integer(123) 与 Integer.valueOf(123) 的区别在于，new Integer(123) 每次都会新建一个对象，而 Integer.valueOf(123) 可能会使用缓存对象，因此多次使用 Integer.valueOf(123) 会取得同一个对象的引用。]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot实现基于Token的web后台认证机制]]></title>
    <url>%2FSpringBoot%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EToken%E7%9A%84web%E5%90%8E%E5%8F%B0%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[几种常用的认证机制HTTP Basic Auth、OAuth、Cookie Auth、Token Auth HTTP Basic AuthHTTP Basic Auth简单点说明就是每次请求API时都提供用户的username和password，简言之，Basic Auth是配合RESTful API 使用的最简单的认证方式，只需提供用户名密码即可，但由于有把用户名密码暴露给第三方客户端的风险，在生产环境下被使用的越来越少。因此，在开发对外开放的RESTful API时，尽量避免采用HTTP Basic Auth OAuth2.0OAuth（开放授权）是一个开放的授权标准，允许用户让第三方应用访问该用户在某一web服务上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和密码提供给第三方应用。 Cookie AuthCookie认证机制就是为一次请求认证在服务端创建一个Session对象，同时在客户端的浏览器端创建了一个Cookie对象；通过客户端带上来Cookie对象来与服务器端的session对象匹配来实现状态管理的。默认的，当我们关闭浏览器的时候，cookie会被删除。但可以通过修改cookie 的expire time使cookie在一定时间内有效； Token AuthToken机制相对于Cookie机制又有什么好处呢？ 支持跨域访问: Cookie是不允许垮域访问的，这一点对Token机制是不存在的，前提是传输的用户认证信息通过HTTP头传输. 无状态(也称：服务端可扩展行):Token机制在服务端不需要存储session信息，因为Token 自身包含了所有登录用户的信息，只需要在客户端的cookie或本地介质存储状态信息. 更适用CDN: 可以通过内容分发网络请求你服务端的所有资料（如：javascript，HTML,图片等），而你的服务端只要提供API即可. 去耦: 不需要绑定到一个特定的身份验证方案。Token可以在任何地方生成，只要在你的API被调用的时候，你可以进行Token生成调用即可. 更适用于移动应用: 当你的客户端是一个原生平台（iOS, Android，Windows 8等）时，Cookie是不被支持的（你需要通过Cookie容器进行处理），这时采用Token认证机制就会简单得多。 CSRF:因为不再依赖于Cookie，所以你就不需要考虑对CSRF（跨站请求伪造）的防范。 性能: 一次网络往返时间（通过数据库查询session信息）总比做一次HMACSHA256计算 的Token验证和解析要费时得多. 不需要为登录页面做特殊处理: 如果你使用Protractor 做功能测试的时候，不再需要为登录页面做特殊处理. 基于标准化:你的API可以采用标准化的 JSON Web Token (JWT). 这个标准已经存在多个后端库（.NET, Ruby, Java,Python, PHP）和多家公司的支持（如：Firebase,Google, Microsoft）. 基于Token的身份验证流程如下 客户端使用用户名跟密码请求登录 服务端收到请求，去验证用户名与密码 验证成功后，服务端会签发一个 Token，再把这个 Token 发送给客户端 客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里或者 Local Storage 里 客户端每次向服务端请求资源的时候需要带着服务端签发的 Token 服务端收到请求，然后去验证客户端请求里面带着的 Token，如果验证成功，就向客户端返回请求的数据 Spring Boot 实现实现见UserService，LoginController，PassportInterceptor12345678public class LoginTicket&#123; private int id; private int userId; private Date expired; private int status;//0有效 private String ticket;&#125; 用户先去请求注册或者是登陆，然后服务器去验证他的用户名和密码 验证成功后userservice会生成一个Token，这里是ticket，客户端收到ticket之后呢会把ticket存在Cookie中，登录成功之后会有一个与当前用户对应的ticket 每次访问服务器资源的时候需要带着这个ticket，然后怎么判断是否有呢？就要用拦截器来实现过滤，用拦截器去判断这个ticket当前的状态是什么样的？有没有过期？身份状态是不是有效的？然后根据这个来判断应该赋予什么样的权限？当验证成功之后就把ticket对应的用户的通过下面一段发送给freemaker的上下文，实现页面的正常的渲染 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.zhaole.interceptor;import com.zhaole.dao.LoginTicketDAO;import com.zhaole.dao.UserDAO;import com.zhaole.model.HostHolder;import com.zhaole.model.LoginTicket;import com.zhaole.model.User;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.Date;/** * 拦截器 * @ 用来判断用户的 *1. 当preHandle方法返回false时，从当前拦截器往回执行所有拦截器的afterCompletion方法，再退出拦截器链。也就是说，请求不继续往下传了，直接沿着来的链往回跑。 2.当preHandle方法全为true时，执行下一个拦截器,直到所有拦截器执行完。再运行被拦截的Controller。然后进入拦截器链，运 行所有拦截器的postHandle方法,完后从最后一个拦截器往回执行所有拦截器的afterCompletion方法. */@Componentpublic class PasswordInterceptor implements HandlerInterceptor&#123; @Autowired private LoginTicketDAO loginTicketDAO; @Autowired private UserDAO userDAO; @Autowired private HostHolder hostHolder; @Override public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception &#123; String ticket = null; if(httpServletRequest.getCookies()!=null) &#123; for(Cookie cookie:httpServletRequest.getCookies()) &#123; if(cookie.getName().equals("ticket")) &#123; ticket = cookie.getValue(); break; &#125; &#125; &#125; if(ticket!=null)//说明cookie不空，且name是ticket &#123; //去数据库里把ticket找出来看看是否有效 LoginTicket loginTicket = loginTicketDAO.selectByTicket(ticket); if(loginTicket==null || loginTicket.getExpired().before(new Date()) || loginTicket.getStatus()!=0) &#123; return true; &#125; //有效的话，查这个ticket对应的用户，把这个用户设置到threadlocal里。 User user = userDAO.selectById(loginTicket.getUserId()); hostHolder.setUser(user); &#125; return true; &#125; @Override public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception &#123; if (modelAndView != null &amp;&amp; hostHolder.getUser() != null) &#123; modelAndView.addObject("user", hostHolder.getUser()); &#125; &#125; @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception &#123; hostHolder.clear(); &#125;&#125; 当用户登出的时候就把ticket的身份状态置位为无效状态即可]]></content>
      <tags>
        <tag>网络</tag>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你如何迅速秒杀掉：99%的海量数据处理面试题]]></title>
    <url>%2F%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E8%BF%85%E9%80%9F%E7%A7%92%E6%9D%80%E6%8E%89%EF%BC%9A99-%E7%9A%84%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[原文地址 分而治之/hash映射 + hash统计 + 堆/快速/归并排序； 双层桶划分 Bloom filter/Bitmap； Trie树/数据库/倒排索引； 外排序； 分布式处理之Hadoop/Mapreduce。 一、从set/map谈到hashtable/hash_map/hash_set稍后本文第二部分中将多次提到hash_map/hash_set，下面稍稍介绍下这些容器，以作为基础准备。一般来说，STL容器分两种， 序列式容器(vector/list/deque/stack/queue/heap)， 关联式容器。关联式容器又分为set(集合)和map(映射表)两大类，以及这两大类的衍生体multiset(多键集合)和multimap(多键映射表)，这些容器均以RB-tree完成。此外，还有第3类关联式容器，如hashtable(散列表)，以及以hashtable为底层机制完成的hash_set(散列集合)/hash_map(散列映射表)/hash_multiset(散列多键集合)/hash_multimap(散列多键映射表)。也就是说，set/map/multiset/multimap都内含一个RB-tree，而hash_set/hash_map/hash_multiset/hash_multimap都内含一个hashtable。 所谓关联式容器，类似关联式数据库，每笔数据或每个元素都有一个键值(key)和一个实值(value)，即所谓的Key-Value(键-值对)。当元素被插入到关联式容器中时，容器内部结构(RB-tree/hashtable)便依照其键值大小，以某种特定规则将这个元素放置于适当位置。 包括在非关联式数据库中，比如，在MongoDB内，文档(document)是最基本的数据组织形式，每个文档也是以Key-Value（键-值对）的方式组织起来。一个文档可以有多个Key-Value组合，每个Value可以是不同的类型，比如String、Integer、List等等。123&#123; &quot;name&quot; : &quot;July&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 23 &#125; set/map/multiset/multimap set，同map一样，所有元素都会根据元素的键值自动被排序，因为set/map两者的所有各种操作，都只是转而调用RB-tree的操作行为，不过，值得注意的是，两者都不允许两个元素有相同的键值。 不同的是：set的元素不像map那样可以同时拥有实值(value)和键值(key)，set元素的键值就是实值，实值就是键值，而map的所有元素都是pair，同时拥有实值(value)和键值(key)，pair的第一个元素被视为键值，第二个元素被视为实值。 至于multiset/multimap，他们的特性及用法和set/map完全相同，唯一的差别就在于它们允许键值重复，即所有的插入操作基于RB-tree的insert_equal()而非insert_unique()。 hash_set/hash_map/hash_multiset/hash_multimap hash_set/hash_map，两者的一切操作都是基于hashtable之上。不同的是，hash_set同set一样，同时拥有实值和键值，且实质就是键值，键值就是实值，而hash_map同map一样，每一个元素同时拥有一个实值(value)和一个键值(key)，所以其使用方式，和上面的map基本相同。但由于hash_set/hash_map都是基于hashtable之上，所以不具备自动排序功能。为什么?因为hashtable没有自动排序功能。 至于hash_multiset/hash_multimap的特性与上面的multiset/multimap完全相同，唯一的差别就是它们hash_multiset/hash_multimap的底层实现机制是hashtable（而multiset/multimap，上面说了，底层实现机制是RB-tree），所以它们的元素都不会被自动排序，不过也都允许键值重复。 所以，综上，说白了，什么样的结构决定其什么样的性质，因为set/map/multiset/multimap都是基于RB-tree之上，所以有自动排序功能，而hash_set/hash_map/hash_multiset/hash_multimap都是基于hashtable之上，所以不含有自动排序功能，至于加个前缀multi_无非就是允许键值重复而已。如下图所示： 二、处理海量数据问题之六把密匙分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序1、海量日志数据，提取出某日访问百度次数最多的那个IP。既然是海量数据处理，那么可想而知，给我们的数据那就一定是海量的。针对这个数据的海量，我们如何着手呢?对的，无非就是分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序： 分而治之/hash映射：针对数据太大，内存受限，只能是：把大文件化成(取模映射)小文件，即16字方针：大而化小，各个击破，缩小规模，逐个解决hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。 具体而论，则是： “首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如%1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map对那1000个文件中的所有IP进行频率统计，然后依次找出各个文件中频率最大的那个IP）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。” 关于本题，还有几个问题，如下： Hash取模是一种等价映射，不会存在同一个元素分散到不同小文件中的情况，即这里采用的是mod1000算法，那么相同的IP在hash取模后，只可能落在同一个文件中，不可能被分散的。因为如果两个IP相等，那么经过Hash(IP)之后的哈希值是相同的，将此哈希值取模（如模1000），必定仍然相等。 那到底什么是hash映射呢？简单来说，就是为了便于计算机在有限的内存中处理big数据，从而通过一种映射散列的方式让数据均匀分布在对应的内存位置(如大数据通过取余的方式映射成小树存放在内存中，或大文件映射成多个小文件)，而这个映射散列方式便是我们通常所说的hash函数，设计的好的hash函数能让数据均匀分布而减少冲突。尽管数据映射到了另外一些不同的位置，但数据还是原来的数据，只是代替和表示这些原始数据的形式发生了变化而已。 一致性hash算法，见此文第五部分 2、寻找热门查询，300万个查询字符串中统计最热门的10个查询原题：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。 解答：由上面第1题，我们知道，数据大则划为小的，如如一亿个Ip求Top 10，可先%1000将ip分到1000个小文件中去，并保证一种ip只出现在一个文件中，再对每个小文件中的ip进行hashmap计数统计并按数量排序，最后归并或者最小堆依次处理每个小文件的top10以得到最后的结。 但如果数据规模比较小，能一次性装入内存呢?比如这第2题，虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把他们都放进内存中去（300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理），而现在只是需要一个合适的数据结构，在这里，HashTable绝对是我们优先的选择。 所以我们放弃分而治之/hash映射的步骤，直接上hash统计，然后排序。So，针对此类典型的TOP K问题，采取的对策往往是：hashmap + 堆。如下所示： hash_map统计：先对这批海量数据预处理。具体方法是：维护一个Key为Query字串，Value为该Query出现次数的HashTable，即hash_map(Query，Value)，每次读取一个Query，如果该字串不在Table中，那么加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加一即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计； 堆排序：第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。即借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O（N） + N’ * O（logK），（N为1000万，N’为300万）。 别忘了这篇文章中所述的堆排序思路：“维护k个元素的最小堆，即用容量为k的最小堆存储最先遍历到的k个数，并假设它们即是最大的k个数，建堆费时O（k），并调整堆(费时O（logk）)后，有k1&gt;k2&gt;…kmin（kmin设为小顶堆中最小元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，若x&gt;kmin，则更新堆（x入堆，用时logk），否则不更新堆。这样下来，总费时O（klogk+（n-k）logk）=O（n*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。”–第三章续、Top K算法问题的实现。 当然，你也可以采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。 3、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词由上面那两个例题，分而治之 + hash统计 + 堆/快速排序这个套路，我们已经开始有了屡试不爽的感觉。下面，再拿几道再多多验证下。请看此第3题：又是文件很大，又是内存受限，咋办?还能怎么办呢?无非还是： 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。 堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。4、海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素： 堆排序：在每台电脑上求出TOP10，可以采用包含10个元素的堆完成（TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大）。 求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。 但如果同一个元素重复出现在不同的电脑中呢，如下例子所述： 这个时候，你可以有两种方法： 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。 或者，暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP10。5、有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。 方案1：直接上： hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。 堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件（记为）。最后，对这10个文件进行归并排序（内排序与外排序相结合）。根据此方案1，这里有一份实现。 方案2：一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。 方案3：与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。6、 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 分而治之/hash映射：遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为，这里漏写个了a1）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为）。这样处理后，所有可能相同的url都在对应的小文件（）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。 OK，此第一种方法：分而治之/hash映射 + hash统计 + 堆/快速/归并排序，再看最后4道题，如下：7、怎么在海量数据中找出重复次数最多的一个？方案：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。8、上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。方案：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后利用堆取出前N个出现次数最多的数据。9、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 方案1：如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。 方案2：通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，也可以用trie树统计每个词出现的次数，时间复杂度是O(nle)（le表示单词的平准长度），最终同样找出出现最频繁的前10个词（可用堆来实现），时间复杂度是O(nlg10)。10、 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？ 方案1：这题用trie树比较合适，hash_map也行。 方案2：from xjbzju:，1000w的数据规模插入操作完全不现实，以前试过在stl下100w元素插入set中已经慢得不能忍受，觉得基于hash的实现不会比红黑树好太多，使用vector+sort+unique都要可行许多，建议还是先hash成小文件分开处理再综合。11、 一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。 方案1：首先根据用hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中10个最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词。12、 100w个数中找出最大的100个数。 方案1：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。 方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。 方案3：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。多层划分适用范围：第k大，中位数，不重复或重复的数字基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。13、2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。14、5亿个int找它们的中位数。 思路一：这个例子比上面那个更明显。首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用direct addr table进行统计了。 思路二@绿色夹克衫：同样需要做两遍统计，如果数据存在硬盘上，就需要读取2次。方法同基数排序有些像，开一个大小为65536的Int数组，第一遍读取，统计Int32的高16位的情况，也就是0-65535，都算作0,65536 - 131071都算作1。就相当于用该数除以65536。Int32 除以 65536的结果不会超过65536种情况，因此开一个长度为65536的数组计数就可以。每读取一个数，数组中对应的计数+1，考虑有负数的情况，需要将结果加32768后，记录在相应的数组内。第一遍统计之后，遍历数组，逐个累加统计，看中位数处于哪个区间，比如处于区间k，那么0- k-1的区间里数字的数量sum应该&lt;n/2（2.5亿）。而k+1 - 65535的计数和也&lt;n/2，第二遍统计同上面的方法类似，但这次只统计处于区间k的情况，也就是说(x / 65536) + 32768 = k。统计只统计低16位的情况。并且利用刚才统计的sum，比如sum = 2.49亿，那么现在就是要在低16位里面找100万个数(2.5亿-2.49亿)。这次计数之后，再统计一下，看中位数所处的区间，最后将高位和低位组合一下就是结果了。Bloom filter/Bitmap海量数据处理之Bloom Filter详解适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集基本原理及要点：对于原理来说很简单，位数组+k个独立hash函数。将hash函数对应的值的位数组置1，查找时如果发现所有hash函数对应位都是1说明存在，很明显这个过程并不保证查找的结果是100%正确的。同时也不支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是 counting Bloom filter，用一个counter数组代替位数组，就可以支持删除了。 还有一个比较重要的问题，如何根据输入元素个数n，确定位数组m的大小及hash函数个数。当hash函数个数k=(ln2)(m/n)时错误率最小。在错误率不大于E的情况下，m至少要等于nlg(1/E)才能表示任意n个元素的集合。但m还应该更大些，因为还要保证bit数组里至少一半为0，则m应该&gt;=nlg(1/E)*lge 大概就是nlg(1/E)1.44倍(lg表示以2为底的对数)。 举个例子我们假设错误率为0.01，则此时m应大概是n的13倍。这样k大概是8个。注意这里m与n的单位不同，m是bit为单位，而n则是以元素个数为单位(准确的说是不同元素的个数)。通常单个元素的长度都是有很多bit的。所以使用bloom filter内存上通常都是节省的。 扩展：Bloom filter将集合中的元素映射到位数组中，用k（k为哈希函数个数）个映射位是否全1表示元素在不在这个集合中。Counting bloom filter（CBF）将位数组中的每一位扩展为一个counter，从而支持了元素的删除操作。Spectral Bloom Filter（SBF）将其与集合元素的出现次数关联。SBF采用counter中的最小值来近似表示元素的出现频率。 可以看下上文中的第6题： 6、给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？ 根据这个问题我们来计算下内存的占用，4G=2^32大概是40亿*8大概是340亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些urlip是一一对应的，就可以转换成ip，则大大简单了。 同时，上文的第5题：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。” Bitmap13、在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。 方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。 方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。” 15、给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？ 方案1：frome oo，用位图/Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。Trie树/数据库/倒排索引Trie树 适用范围：数据量大，重复多，但是数据种类小可以放入内存基本原理及要点：实现方式，节点孩子的表示方式扩展：压缩实现。 问题实例： 上面的第2题：寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。 上面的第5题：有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？ 上面的第8题：一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词。其解决方法是：用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词。 数据库索引 适用范围：大数据量的增删改查基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。 关于数据库索引及其优化，更多可参见此文关于MySQL索引背后的数据结构及算法原理关于B 树、B+ 树、B* 树及R 树 倒排索引(Inverted index) 适用范围：搜索引擎，关键字查询基本原理及要点：为何叫倒排索引？一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。 以英文为例，下面是要被索引的文本： T0 = “it is what it is” T1 = “what is it” T2 = “it is a banana” 我们就能得到下面的反向文件索引： “a”: {2} “banana”: {2} “is”: {0, 1, 2} “it”: {0, 1, 2} “what”: {0, 1} 检索的条件”what”,”is”和”it”将对应集合的交集。 正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。扩展：问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。 外排序适用范围：大数据的排序，去重基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树问题实例：有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1M做hash明显不够，所以可以用来排序。内存可以当输入缓冲区使用。如何给10^7个数据量的磁盘文件排序 分布式处理之Mapreduce MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。但如果你要我再通俗点介绍，那么，说白了，Mapreduce的原理就是一个归并排序。 适用范围：数据量大，但是数据种类小可以放入内存基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。问题实例： The canonical example application of MapReduce is a process to count the appearances of each different word in a set of documents: 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数的中数(median)？ Hadhoop框架与MapReduce模式中谈海量数据处理MapReduce技术的初步了解与学习 其它模式/方法论，结合操作系统知识至此，六种处理海量数据问题的模式/方法已经阐述完毕。据观察，这方面的面试题无外乎以上一种或其变形，然题目为何取为是：秒杀99%的海量数据处理面试题，而不是100%呢。OK，给读者看最后一道题，如下：非常大的文件，装不进内存。每行一个int类型数据，现在要你随机取100个数。我们发现上述这道题，无论是以上任何一种模式/方法都不好做，那有什么好的别的方法呢？我们可以看看：操作系统内存分页系统设计(说白了，就是映射+建索引)。Windows 2000使用基于分页机制的虚拟内存。每个进程有4GB的虚拟地址空间。基于分页机制，这4GB地址空间的一些部分被映射了物理内存，一些部分映射硬盘上的交换文 件，一些部分什么也没有映射。程序中使用的都是4GB地址空间中的虚拟地址。而访问物理内存，需要使用物理地址。 物理地址 (physical address): 放在寻址总线上的地址。放在寻址总线上，如果是读，电路根据这个地址每位的值就将相应地址的物理内存中的数据放到数据总线中传输。如果是写，电路根据这个 地址每位的值就将相应地址的物理内存中放入数据总线上的内容。物理内存是以字节(8位)为单位编址的。 虚拟地址 (virtual address): 4G虚拟地址空间中的地址，程序中使用的都是虚拟地址。 使用了分页机制之后，4G的地址空间被分成了固定大小的页，每一页或者被映射到物理内存，或者被映射到硬盘上的交换文件中，或者没有映射任何东西。对于一 般程序来说，4G的地址空间，只有一小部分映射了物理内存，大片大片的部分是没有映射任何东西。物理内存也被分页，来映射地址空间。对于32bit的 Win2k，页的大小是4K字节。CPU用来把虚拟地址转换成物理地址的信息存放在叫做页目录和页表的结构里。 物理内存分页，一个物理页的大小为4K字节，第0个物理页从物理地址 0x00000000 处开始。由于页的大小为4KB，就是0x1000字节，所以第1页从物理地址 0x00001000 处开始。第2页从物理地址 0x00002000 处开始。可以看到由于页的大小是4KB，所以只需要32bit的地址中高20bit来寻址物理页。 返回上面我们的题目：非常大的文件，装不进内存。每行一个int类型数据，现在要你随机取100个数。针对此题，我们可以借鉴上述操作系统中内存分页的设计方法，做出如下解决方案： 操作系统中的方法，先生成4G的地址表，在把这个表划分为小的4M的小文件做个索引，二级索引。30位前十位表示第几个4M文件，后20位表示在这个4M文件的第几个，等等，基于key value来设计存储，用key来建索引。 更多海里数据处理面试题]]></content>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[freemarker基础知识总结]]></title>
    <url>%2Ffreemarker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Freemaker FTL指令常用标签及语法注意**：使用freemaker，要求所有标签必须闭合，否则会导致freemaker无法解析。 freemaker注释:&lt;#– 注释内容 –&gt;格式部分,不会输出 基础语法1、字符输出12345$&#123;emp.name?if_exists&#125; // 变量存在，输出该变量，否则不输出$&#123;emp.name!&#125; // 变量存在，输出该变量，否则不输出$&#123;emp.name?default(&quot;xxx&quot;)&#125; // 变量不存在，取默认值xxx $&#123;emp.name!&quot;xxx&quot;&#125; // 变量不存在，取默认值xxx 常用内部函数： 12345$&#123;&quot;123&lt;br&gt;456&quot;?html&#125; // 对字符串进行HTML编码，对html中特殊字符进行转义$&#123;&quot;str&quot;?cap_first&#125; // 使字符串第一个字母大写 $&#123;&quot;Str&quot;?lower_case&#125; // 将字符串转换成小写 $&#123;&quot;Str&quot;?upper_case&#125; // 将字符串转换成大写$&#123;&quot;str&quot;?trim&#125; // 去掉字符串前后的空白字符 字符串的两种拼接方式拼接： 12$&#123;&quot;hello$&#123;emp.name!&#125;&quot;&#125; // 输出hello+变量名$&#123;&quot;hello&quot;+emp.name!&#125; // 使用+号来连接，输出hello+变量名 可以通过如下语法来截取子串: 1234567891011&lt;#assign str = &quot;abcdefghijklmn&quot;/&gt;// 方法1$&#123;str?substring(0,4)&#125; // 输出abcd// 方法2$&#123;str[0]&#125;$&#123;str[4]&#125; // 结果是ae$&#123;str[1..4]&#125; // 结果是bcde// 返回指定字符的索引$&#123;str?index_of(&quot;n&quot;)&#125; 2、日期输出1$&#123;emp.date?string(&apos;yyyy-MM-dd&apos;)&#125; //日期格式 3、数字输出(以数字20为例)12345678910111213141516171819202122232425$&#123;emp.name?string.number&#125; // 输出20$&#123;emp.name?string.currency&#125; // ￥20.00 $&#123;emp.name?string.percent&#125; // 20%$&#123;1.222?int&#125; // 将小数转为int，输出1&lt;#setting number_format=&quot;percent&quot;/&gt; // 设置数字默认输出方式(&apos;percent&apos;,百分比)&lt;#assign answer=42/&gt; // 声明变量 answer 42#&#123;answer&#125; // 输出 4,200%$&#123;answer?string&#125; // 输出 4,200%$&#123;answer?string.number&#125; // 输出 42$&#123;answer?string.currency&#125; // 输出 ￥42.00$&#123;answer?string.percent&#125; // 输出 4,200%#&#123;answer&#125; // 输出 42数字格式化插值可采用#&#123;expr;format&#125;形式来格式化数字,其中format可以是:mX:小数部分最小X位MX:小数部分最大X位如下面的例子:&lt;#assign x=2.582/&gt;&lt;#assign y=4/&gt;#&#123;x; M2&#125; // 输出2.58#&#123;y; M2&#125; // 输出4#&#123;x; m2&#125; // 输出2.58#&#123;y; m2&#125; // 输出4.0#&#123;x; m1M2&#125; // 输出2.58#&#123;x; m1M2&#125; // 输出4.0 4、申明变量123456789101112131415&lt;#assign foo=false/&gt; // 声明变量,插入布尔值进行显示,注意不要用引号$&#123;foo?string(&quot;yes&quot;,&quot;no&quot;)&#125; // 当为true时输出&quot;yes&quot;,否则输出&quot;no&quot;申明变量的几种方式&lt;#assign name=value&gt; &lt;#assign name1=value1 name2=value2 ... nameN=valueN&gt; &lt;#assign same as above... in namespacehash&gt;&lt;#assign name&gt; capture this &lt;/#assign&gt;&lt;#assign name in namespacehash&gt; capture this &lt;/#assign&gt; 5、比较运算符1234567表达式中支持的比较运算符有如下几个:= 或 == ：判断两个值是否相等.!= ：判断两个值是否不等.&gt; 或 gt ：判断左边值是否大于右边值&gt;= 或 gte ：判断左边值是否大于等于右边值&lt; 或 lt ：判断左边值是否小于右边值&lt;= 或 lte ：判断左边值是否小于等于右边值 6、算术运算符12345FreeMarker表达式中完全支持算术运算,FreeMarker支持的算术运算符包括:+, - , * , / , %注意：（1）、运算符两边必须是数字（2）、使用+运算符时,如果一边是数字,一边是字符串,就会自动将数字转换为字符串再连接,如:$&#123;3 + “5”&#125;,结果是:35 7、逻辑运算符逻辑运算符有如下几个:逻辑与:&amp;&amp;逻辑或:||逻辑非:!逻辑运算符只能作用于布尔值,否则将产生错误 8、FreeMarker中的运算符优先级如下(由高到低排列):①、一元运算符:!②、内建函数:?③、乘除法:*, / , %④、加减法:- , +⑤、比较:&gt; , &lt; , &gt;= , &lt;= (lt , lte , gt , gte)⑥、相等:== , = , !=⑦、逻辑与:&amp;&amp;⑧、逻辑或:||⑨、数字范围:..实际上,我们在开发过程中应该使用括号来严格区分,这样的可读性好,出错少 9、if 逻辑判断（注意：elseif 不加空格）1234567891011121314151617181920&lt;#if condition&gt;...&lt;#elseif condition2&gt;...&lt;#elseif condition3&gt;...&lt;#else&gt;...&lt;/#if&gt;if 空值判断// 当 photoList 不为空时&lt;#if photoList??&gt;...&lt;/#if&gt; 值得注意的是,$&#123;..&#125;只能用于文本部分,不能用于表达式,下面的代码是错误的:&lt;#if $&#123;isBig&#125;&gt;Wow!&lt;/#if&gt;&lt;#if &quot;$&#123;isBig&#125;&quot;&gt;Wow!&lt;/#if&gt;// 正确写法&lt;#if isBig&gt;Wow!&lt;/#if&gt; 10、switch (条件可为数字，可为字符串)12345678910111213&lt;#switch value&gt; &lt;#case refValue1&gt; ....&lt;#break&gt; &lt;#case refValue2&gt; ....&lt;#break&gt; &lt;#case refValueN&gt; ....&lt;#break&gt; &lt;#default&gt; .... &lt;/#switch&gt; 11、集合 &amp; 循环12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// 遍历集合:&lt;#list empList! as emp&gt; $&#123;emp.name!&#125;&lt;/#list&gt;// 可以这样遍历集合:&lt;#list 0..(empList!?size-1) as i&gt; $&#123;empList[i].name!&#125;&lt;/#list&gt;// 与jstl循环类似,也可以访问循环的状态。empList?size // 取集合的长度emp_index: // int类型，当前对象的索引值 emp_has_next: // boolean类型，是否存在下一个对象// 使用&lt;#break&gt;跳出循环&lt;#if emp_index = 0&gt;&lt;#break&gt;&lt;/#if&gt;// 集合长度判断 &lt;#if empList?size != 0&gt;&lt;/#if&gt; // 判断=的时候,注意只要一个=符号,而不是==&lt;#assign l=0..100/&gt; // 定义一个int区间的0~100的集合，数字范围也支持反递增,如100..2&lt;#list 0..100 as i&gt; // 等效于java for(int i=0; i &lt;= 100; i++) $&#123;i&#125;&lt;/#list&gt;// 截取子集合：empList[3..5] //返回empList集合的子集合,子集合中的元素是empList集合中的第4-6个元素// 创建集合：&lt;#list [&quot;星期一&quot;, &quot;星期二&quot;, &quot;星期三&quot;, &quot;星期四&quot;, &quot;星期五&quot;, &quot;星期六&quot;, &quot;星期天&quot;] as x&gt;// 集合连接运算,将两个集合连接成一个新的集合&lt;#list [&quot;星期一&quot;,&quot;星期二&quot;,&quot;星期三&quot;] + [&quot;星期四&quot;,&quot;星期五&quot;,&quot;星期六&quot;,&quot;星期天&quot;] as x&gt;// 除此之外,集合元素也可以是表达式,例子如下:[2 + 2, [1, 2, 3, 4], &quot;whatnot&quot;]// seq_contains：判断序列中的元素是否存在&lt;#assign x = [&quot;red&quot;, 16, &quot;blue&quot;, &quot;cyan&quot;]&gt; $&#123;x?seq_contains(&quot;blue&quot;)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // yes$&#123;x?seq_contains(&quot;yellow&quot;)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // no$&#123;x?seq_contains(16)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // yes$&#123;x?seq_contains(&quot;16&quot;)?string(&quot;yes&quot;, &quot;no&quot;)&#125; // no// seq_index_of：第一次出现的索引&lt;#assign x = [&quot;red&quot;, 16, &quot;blue&quot;, &quot;cyan&quot;, &quot;blue&quot;]&gt; $&#123;x?seq_index_of(&quot;blue&quot;)&#125; // 2// sort_by：排序（升序）&lt;#list movies?sort_by(&quot;showtime&quot;) as movie&gt;&lt;/#list&gt;// sort_by：排序（降序）&lt;#list movies?sort_by(&quot;showtime&quot;)?reverse as movie&gt;&lt;/#list&gt;// 具体介绍：// 不排序的情况：&lt;#list movies as moive&gt; &lt;a href=&quot;$&#123;moive.url&#125;&quot;&gt;$&#123;moive.name&#125;&lt;/a&gt;&lt;/#list&gt;//要是排序，则用&lt;#list movies?sort as movie&gt; &lt;a href=&quot;$&#123;movie.url&#125;&quot;&gt;$&#123;movie.name&#125;&lt;/a&gt;&lt;/#list&gt;// 这是按元素的首字母排序。若要按list中对象元素的某一属性排序的话，则用&lt;#list moives?sort_by([&quot;name&quot;]) as movie&gt; &lt;a href=&quot;$&#123;movie.url&#125;&quot;&gt;$&#123;movie.name&#125;&lt;/a&gt;&lt;/#list&gt;//这个是按list中对象元素的[name]属性排序的，是升序，如果需要降序的话，如下所示：&lt;#list movies?sort_by([&quot;name&quot;])?reverse as movie&gt; &lt;a href=&quot;$&#123;movie.url&#125;&quot;&gt;$&#123;movie.name&#125;&lt;/a&gt;&lt;/#list&gt; 12、Map对象123456789// 创建map&lt;#assign scores = &#123;&quot;语文&quot;:86,&quot;数学&quot;:78&#125;&gt;// Map连接运算符&lt;#assign scores = &#123;&quot;语文&quot;:86,&quot;数学&quot;:78&#125; + &#123;&quot;数学&quot;:87,&quot;Java&quot;:93&#125;&gt;// Map元素输出emp.name // 全部使用点语法emp[&quot;name&quot;] // 使用方括号 13、FreeMarker支持如下转义字符:1234567891011121314151617\” ：双引号(u0022)\’ ：单引号(u0027)\ ：反斜杠(u005C)\n ：换行(u000A)\r ：回车(u000D)\t ：Tab(u0009)\b ：退格键(u0008)\f ：Form feed(u000C)\l ：&lt;\g ：&gt;\a ：&amp;\&#123; ：&#123;\xCode ：直接通过4位的16进制数来指定Unicode码,输出该unicode码对应的字符.如果某段文本中包含大量的特殊符号,FreeMarker提供了另一种特殊格式:可以在指定字符串内容的引号前增加r标记,在r标记后的文件将会直接输出.看如下代码:r”$foo”//输出&#123;foo&#125;$&#123;r”C:/foo/bar”&#125; // 输出 C:/foo/bar 14、include指令123456// include指令的作用类似于JSP的包含指令:&lt;#include &quot;/test.ftl&quot; encoding=&quot;UTF-8&quot; parse=true&gt;// 在上面的语法格式中,两个参数的解释如下:encoding=&quot;GBK&quot; // 编码格式parse=true // 是否作为ftl语法解析,默认是true，false就是以文本方式引入,注意:在ftl文件里布尔值都是直接赋值的如parse=true,而不是parse=&quot;true&quot; 15、import指令123// 类似于jsp里的import,它导入文件，然后就可以在当前文件里使用被导入文件里的宏组件&lt;#import &quot;/libs/mylib.ftl&quot; as my&gt;// 上面的代码将导入/lib/common.ftl模板文件中的所有变量,交将这些变量放置在一个名为com的Map对象中，&quot;my&quot;在freemarker里被称作namespace 17、compress 压缩12345678// 用来压缩空白空间和空白的行 &lt;#compress&gt; ... &lt;/#compress&gt;&lt;#t&gt; // 去掉左右空白和回车换行 &lt;#lt&gt;// 去掉左边空白和回车换行 &lt;#rt&gt;// 去掉右边空白和回车换行 &lt;#nt&gt;// 取消上面的效果 18、escape,noescape 对字符串进行HTML编码1234567891011// escape指令导致body区的插值都会被自动加上escape表达式,但不会影响字符串内的插值,只会影响到body内出现的插值,使用escape指令的语法格式如下:&lt;#escape x as x?html&gt; First name: $&#123;firstName&#125; &lt;#noescape&gt;Last name: $&#123;lastName&#125;&lt;/#noescape&gt; Maiden name: $&#123;maidenName&#125; &lt;/#escape&gt;// 相同表达式First name: $&#123;firstName?html&#125; Last name: $&#123;lastName&#125; Maiden name: $&#123;maidenName?html&#125;]]></content>
      <tags>
        <tag>前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于前缀树图文详解敏感词过滤]]></title>
    <url>%2F%E5%9F%BA%E4%BA%8E%E5%89%8D%E7%BC%80%E6%A0%91%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3%E6%95%8F%E6%84%9F%E8%AF%8D%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[一般设计网站的时候，会有问题发布或者是内容发布的功能，这些功能的有一个很重要的点在于如何实现敏感词过滤，要不然可能会有不良信息的发布，或者发布的内容中有夹杂可能会有恶意功能的代码片段，敏感词过滤的基本的算法是前缀树算法，前缀树也就是字典树，通过前缀树匹配可以加快敏感词匹配的速度。首先是过滤HTML代码，在Spring中有直接的函数可以使用：1question.setContent(HtmlUtils.htmlEscape(question.getContent())); 实现的功能就是将html的代码进行转义后显示出来，使其失效。举一个具体的例子：如果有一串字符串为xwabfabcff,敏感词为abc、bf、bc，若这个字符串中包含敏感词，则使用***代替敏感词，实现一个算法。1.使用三个指针，指针1指向根节点，指针2指向字符串下标起始值，指针3指向字符串当前下标值。指针1为tempnode=rootnode，指针2为begin=0，指针3为position=0，创建stringbuffer sb来保存结果；2.遍历x，tempnode未找到子节点x，将x保存到sb中，1begin=begin+1;position=begin，tempnode=rootnode; 3.遍历w，tempnode未找到子节点w，将w保存到sb中，1begin=begin+1;position=begin，tempnode=rootnode; 同上4.遍历a，tempnode找到子节点a，tempnode指向a节点，则position++；5.遍历b，tempnode发现a节点下有b这个子节点，所以，tempnode指向b节点，则position++；6.遍历f，tempnode发现b节点下没有f这个子节点，所以，代表以begin开头的字符串，不会有敏感字符，因此，将a存入sb中。1position=begin+1;bigin=position;tempnode=rootnode 7.遍历b,tempnode找到子节点b，tempnode指向b节点，则position++；8.遍历f，tempnode发现b节点下有f这个子节点，而且f值敏感词结尾标记，所以，打码。将写入sb中，同时，begin=position+1;position=begin;tempnode=rootnode;9.遍历a，tempnode找到子节点a，tempnode指向a节点，则position++；10.遍历b，tempnode发现a节点下有b这个子节点，所以，tempnode指向b节点，则position++；11.遍历c，tempnode发现b节点下有c这个子节点，而且c值敏感词结尾标记，所以，打码。将*写入sb中，同时，begin=position+1;position=begin;tempnode=rootnode;12.遍历f，tempnode发现根节点下没有f这个节点，因此，将f存入sb中。position=begin+1;bigin=position;tempnode=rootnode；13.遍历f，tempnode发现根节点下没有f这个节点，因此，将f存入sb中。position=begin+1;bigin=position;tempnode=rootnode；因此，最后sb中为：xwa**ff;这里每次是将position指向的字符挨个的与tempnode的子节点进行比较，因此，代码中的while条件应该是1while (position &lt; text.length())&#123;&#125; 同时，需要思考：如果字符串为xwabfabcfb，则最后，begin指向b下标，position指向b下标，tempnode发现根节点下有b节点，因此position++;然后就退出循环了。而此时，sb中还只保存了xwa**f,所以，我们在循环的最后，还要将最后一串字符串加进来。1result.append(text.substring(begin)); 完整的代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158package com.springboot.springboot.service;import org.apache.commons.lang3.CharUtils;import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Service; import java.io.BufferedReader;import java.io.InputStream;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Map;@Servicepublic class SensitiveService implements InitializingBean &#123; private static final Logger logger = LoggerFactory.getLogger(SensitiveService.class); /** * 默认敏感词替换符 */ private static final String DEFAULT_REPLACEMENT = "敏感词"; private class TrieNode &#123; /** * true 关键词的终结 ； false 继续 */ private boolean end = false; /** * key下一个字符，value是对应的节点 */ private Map&lt;Character, TrieNode&gt; subNodes = new HashMap&lt;&gt;(); /** * 向指定位置添加节点树 */ void addSubNode(Character key, TrieNode node) &#123; subNodes.put(key, node); &#125; /** * 获取下个节点 */ TrieNode getSubNode(Character key) &#123; return subNodes.get(key); &#125; boolean isKeywordEnd() &#123; return end; &#125; void setKeywordEnd(boolean end) &#123; this.end = end; &#125; public int getSubNodeCount() &#123; return subNodes.size(); &#125; &#125; /** * 根节点 */ private TrieNode rootNode = new TrieNode(); /** * 判断是否是一个符号 */ private boolean isSymbol(char c) &#123; int ic = (int) c; // 0x2E80-0x9FFF 东亚文字范围 return !CharUtils.isAsciiAlphanumeric(c) &amp;&amp; (ic &lt; 0x2E80 || ic &gt; 0x9FFF); &#125; /** * 过滤敏感词 */ public String filter(String text) &#123; if (StringUtils.isBlank(text)) &#123; return text; &#125; String replacement = DEFAULT_REPLACEMENT; StringBuilder result = new StringBuilder(); TrieNode tempNode = rootNode; //指向树的根节点 int begin = 0; // 回滚数，指向字符串的指针，与树进行交互的 int position = 0; // 当前比较的位置，指向字符串 while (position &lt; text.length()) &#123; char c = text.charAt(position); // 空格直接跳过 if (isSymbol(c)) &#123; if (tempNode == rootNode) &#123; result.append(c); ++begin; &#125; ++position; continue; &#125; tempNode = tempNode.getSubNode(c); // 当前位置的匹配结束 if (tempNode == null) &#123; // 以begin开始的字符串不存在敏感词 result.append(text.charAt(begin)); // 跳到下一个字符开始测试 position = begin + 1; begin = position; // 回到树初始节点 tempNode = rootNode; &#125; else if (tempNode.isKeywordEnd()) &#123; // 发现敏感词， 从begin到position的位置用replacement替换掉 result.append(replacement); position = position + 1; begin = position; tempNode = rootNode; &#125; else &#123; ++position; &#125; &#125; //将最后一次的比较结果添加进去 result.append(text.substring(begin)); return result.toString(); &#125; private void addWord(String lineTxt) &#123; TrieNode tempNode = rootNode; // 循环每个字节 for (int i = 0; i &lt; lineTxt.length(); ++i) &#123; Character c = lineTxt.charAt(i); // 过滤空格 if (isSymbol(c)) &#123; continue; &#125; TrieNode node = tempNode.getSubNode(c); if (node == null) &#123; // 没初始化 node = new TrieNode(); tempNode.addSubNode(c, node); &#125; tempNode = node; if (i == lineTxt.length() - 1) &#123; // 关键词结束， 设置结束标志 tempNode.setKeywordEnd(true); &#125; &#125; &#125; @Override public void afterPropertiesSet() throws Exception &#123; rootNode = new TrieNode(); try &#123; InputStream is = Thread.currentThread().getContextClassLoader() .getResourceAsStream("SensitiveWords.txt"); InputStreamReader read = new InputStreamReader(is); BufferedReader bufferedReader = new BufferedReader(read); String lineTxt; while ((lineTxt = bufferedReader.readLine()) != null) &#123; lineTxt = lineTxt.trim(); addWord(lineTxt); &#125; read.close(); &#125; catch (Exception e) &#123; logger.error("读取敏感词文件失败" + e.getMessage()); &#125; &#125; public static void main(String[] argv) &#123; SensitiveService s = new SensitiveService(); s.addWord("色情"); s.addWord("赌博"); System.out.print(s.filter("你好赌博")); &#125;&#125;]]></content>
      <tags>
        <tag>Spring Boot</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring]]></title>
    <url>%2FSpring%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Redis基础知识]]></title>
    <url>%2FRedis%2F</url>
    <content type="text"><![CDATA[简单来说 Redis 就是一个数据库，不过与传统数据库不同的是 Redis 的数据是存在内存中的，所以存写速度非常快，因此 Redis 被广泛应用于缓存方向。此外，redis 也经常用来做分布式锁。 为什么要用 Redis高性能 假如用户第一次访问数据库中的某些数据。因为是从硬盘上读取的，所以这个过程会比较慢。我们可以将该用户这次访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发 高并发系统挂掉，多挂在数据库读写处，因为磁盘操作这个慢呀。直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 为什么要用 Redis 而不用 map 做缓存?缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 JVM 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 Redis 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 Redis 服务的高可用，会使整个程序的架构变的较为复杂。 Redis 常见数据结构以及使用场景分析StringString 数据结构是简单的 key-value 类型，value 其实不仅可以是 String，也可以是数字。常规 key-value 缓存应用；常规计数：微博数，粉丝数等。 List双向链表，应用场景：微博的关注列表，粉丝列表，消息列表等。 常用命令1234567891011lpushrpushlpoprpopblpop # 阻塞 pop，bl：blocklrange # LRANGE mylist 0 1，取出 list 0~1 的元素llenlremlindex # 按 index get list 种的元素linsert # LINSERT mylist BEFORE &quot;World&quot; &quot;There&quot;lset # LSET mylist 0 &quot;four&quot;，把第 0 个位置改成 &quot;four&quot; 底层实现链表。 HashHash 是一个 string 类型的 field 和 value 的映射表，又名字典，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以Hash数据结构来存储用户信息，商品信息等等。 比如可以用 hash 类型存放： 1234567key=JavaUser293847value=&#123; "id": 1, "name": "SnailClimb", "age": 22, "location": "Wuhan, Hubei"&#125; 常用命令12345678hset # HSET myhash field1 &quot;Hello&quot;，field1 是 key，&quot;Hello&quot; 是 valuehgethdel # 就 hash 特别，删除叫 del，别人都叫 remhgetAll # 返回所有的 field 和 value，顺序：field1，value1，field2，value2，field3，value3 ...hexistshkeyshvalshsetnx # 字段不存在时才 set，字段存在不 set 底层实现 哈希表，一个字典有两个哈希表（ht[0] &amp; ht[1]），一个是平时用的，一个是 rehash 时用的。 插入一个新的键值对时，会先根据 key 计算出哈希值和索引值，然后把键值对发到对应索引处。 哈希算法：MurmurHash2，该算法即使输入的键是由规律的，也能给出一个很好的随机性，并且速度快。 解决键冲突：链地址法，每个哈希表节点有一个 next 指针，冲突的键会形成一个单链表。 为了让哈希表的负载因子维持在一个合理的范围，当哈希表保存的键值对数量太多或太少时，程序会对哈希表的大小进行相应的扩展或收缩，这个过程叫 rehash。步骤如下： ht[0] 是现在正在用的哈希表，Redis 会根据 ht[0] 中当前包含的键值对个数（ht[0].used）为 ht[1] 分配空间，空间大小取决于： 扩展操作：ht[1].size = 第一个大于 ht[0].used * 2 的 2^n 收缩操作：ht[1].size = 第一个大于 ht[0].used 的 2^n 将 ht[0] 上的键值对 rehash 到 ht[1] 上； 将 ht[1] 设置为 ht[0]，并在 ht[1] 新创建一个空白哈希表用于下一次 rehash。 Set适用于无顺序的集合，点赞点踩，抽奖，已读，共同好友（适合用来去重） 常用命令12345678910sadd # SADD myset one two three，可以一次 add 一坨sinter # 两个集合的交集，SINTER key1 key2，其中 key1 和 key2 是两个 set 名sunion # 两个集合的并集sdiff # 第一个集合 - 交集smemberssismembersrem # 删除元素smove # 把一个set中的元素移动到另一个集合scard # 集合的sizesrandmember # SRANDMEMBER myset n，随机取 n 个，可以用来做抽奖，不写 n 就是随机取一个 底层实现Set 有两种类型，一种是 intset，就是整数集合，另一种是对象集合。 Sorted Set排行榜，优先队列（适合用来排序） 多个节点可以包含相同的 score，不过成员对象必须是唯一的。元素先按照 score 大小进行排序，score 相同时，按照成员对象大小（字典序之类的）进行排序。 常用命令12345678910111213zadd # ZADD myzset 2 &quot;two&quot; 3 &quot;three&quot;zcard # 集合的 sizezcount # 可以计算一个范围内数的 sizezscore # 查询 key 的值zincrby # 做加减法，对不存在的值加分会默认新建该值，并且初始值为 0zrange # ZRANGE myzset 0 -1 WITHSCORES，排行功能，打印排行后的结果zrevrange # 反转 Setzrank # key 的正向排名zrevrank # key 的反向排名# 遍历for (Tuple tuple : jedis.zrangeByScoreWithScores(rankKey, &quot;60&quot;, &quot;100&quot;)) &#123; print(37, tuple.getElement() + &quot;:&quot; + String.valueOf(tuple.getScore()));&#125; 底层实现（跳跃表）传说中的跳跃表。跳跃表（skiplist）是一种随机化的数据，跳跃表以有序的方式在层次化的链表中保存元素， 效率和平衡树媲美 —— 查找、删除、添加等操作都可以在对数期望时间下完成， 并且比起平衡树来说， 跳跃表的实现要简单直观得多。 首先说一下我们的需求，我们要一个有序的列表，因为一个有序的列表搜索起来可以用二分法，快啊！ 所以当我们要插入新元素的时候，就不能直接往表尾一放，我们需要保证把这个节点放进去之后，这个表还是有序的，所以我们的插入操作要分两步来进行： 找：把新节点插哪我们的表还是有序滴 插：把新节点插到我们上一步找到的位置处 这需求一看就是平衡树了，可是红黑树之流实现起来有多复杂，大家也是有目共睹的，所以，Redis 用了一种叫跳跃表的数据结构。我们先来介绍一下 跳跃表（这个小灰讲的超好，我就是在下面挑我关心的重点复述一下）： 首先，本质上来讲，跳跃表还是一个链表的，这样插入和删除就很快啦！不过存链表的查找很慢呀，根本用不了二分法之流，只能从头到尾一个一个的遍历，也就是说，它的查找移动步长只能是 1，那么怎么解决这个问题呢？我们首先想到的方法应该就是想尽一切办法能让查找操作以比较大的步长移动。 那么如何实现大步长移动呢？这就想到了 MySQL 的 InnoDB 的索引的实现方式，我们知道 InnoDB 的一个数据页中的数据和一个链表差不多，不过它有一个叫页目录的东西，这个页目录就是从它所在的页中，以一定的间隔抽出一些节点，作为这个页的目录，这样我们就能大跨度的在单链表中进行查找了。 跳跃表就是基于这个方法实现的大跨度的查找，它从真正的数据中抽出了一些作为一级索引，又从一级索引中抽出一部分作为二级索引，就这样抽抽抽，知道最高层只有两个节点为止（就剩俩了也没有必要继续抽了……）。抽完之后大概是这样的： 那么我们抽索引的时候，抽谁呢？跳跃表是个选择障碍症患者，所以它抛硬币，每插入一个新节点，它都有 50% 的概率被选为索引，所以跳跃表的插入操作的是这样滴，比如我们要在上面的那个表里插个 9： 那么删除呢？删除超简单，比如说我们要删个 5，那么我们从最高层开始找 5： 第一层有 5，删了，然后这层就剩 1，然后干脆把这层都删了…… 第二层有 5，删了 第三层有 5，删了，好了，删光了！ 不过呢，Redis 的实现方式和上面描述的过程还是有区别的，Redis 实现跳跃表用了两种结构体： zskiplistNode：表示跳跃表节点； zskiplist：保存跳跃表的相关信息； 先来看一下 zskiplistNode 的定义： 1234567891011121314151617181920typedef struct zskiplistNode &#123; /* 下面俩货是我们的数据，不多说了 */ robj *obj; // member 对象 double score; // 分值 /* 后退指针，指向当前节点的前一个节点，用于从表尾向表头遍历跳跃表中的所有节点 */ struct zskiplistNode *backward; /* 层数组，实现大幅度跳跃的关键 */ /* 每个节点的层数组长度不一定，是一个 1~32 的随机数，是根据幂次定理随机的，就是说数越大，出现的概率越小 */ /* 每一个节点的同一层组成一个单链表，就是那个前进指针，比如 level[3] 吧，它就指向下一个有 level[3] 的节点 */ struct zskiplistLevel &#123; struct zskiplistNode *forward; // 前进指针 unsigned int span; // 这个层跨越的节点数量 // 不要小看这个东西，这个东西可以拿来计算排位 rank // 在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来， // 得到的结果就是目标节点在跳跃表中的排位 // 要是没有这个东西，鬼知道你在进行大步跨越时，跨越了多少节点啊 &#125; level[];&#125; zskiplistNode; 再来看一下 zskiplist 的定义： 12345typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; // 头节点，尾节点 unsigned long length; // 节点数量 int level; // 目前表内节点的最大层数&#125; zskiplist; 所以 Redis 的表看起来是这样子滴：如果我们想要遍历整个跳跃表，就是把 L1 层的链表从头遍历到尾，过程如下图虚线所示：如果我们想查找一个节点，比如 o2，我们的查找过程如下图虚线所示： Redis 设置过期时间Redis 中有个设置时间过期的功能，即对存储在 Redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 对于这种数据，我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。这样就不需要我们再在应用中手动判断这个 key 是否过期，而是将这个任务交给 Redis 来做。 过期时间设置方式：EXPIRE &lt;key&gt; &lt;ttl&gt;原子性的命令：我们是不管了，那么 Redis 到底是怎么删除这个过期数据的呢？ Redis 删除过期数据的方式： 定期删除 + 惰性删除 。 定期删除： Redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。 为什么要随机呢？你想一想假如 Redis 存了几十万个 key ，如果每隔 100ms 就遍历所有的设置过期时间的 key 的话，CPU怕是要废了…… 惰性删除： 定期删除可能会导致很多过期 key 到了时间并没有被删除掉，所以就有了惰性删除。惰性删除就是：假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，这时候你的系统去查了一下那个 key，Redis 会先检查下数据过期没，如果过期了，就会被 Redis 删除掉。 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？当然会有好多过期的 key 堆积在内存中了，如果大量过期 key 堆积在内存里，是导致 Redis 内存块耗尽的。那么怎么解决这个问题呢？这就要依靠 Redis 的内存淘汰机制了。 Redis 内存淘汰机制即 MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，那么如何保证 Redis 中的数据都是热点数据呢？ Redis 提供了如下 8 种数据淘汰策略： 策略 描述 volatile-lru 从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 allkeys-lru 从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 volatile-lfu 从已设置过期时间的数据集（server.db[i].expires）中挑选使用频率最小的数据淘汰 allkeys-lfu 从数据集（server.db[i].dict）中挑选挑选使用频率最小的数据淘汰 volatile-random 从已设置过期时间的数据集（server.db[i].expires）中挑选任意选择数据淘汰 allkeys-random 从数据集（server.db[i].dict）中挑选任意选择数据淘汰 volatile-ttl 从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 noeviction (Default) 禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错 设置方式：在配置文件 redis.conf 中写入 maxmemory-policy 策略名。 记忆： LRU：Least Recently Used LFU：Least Frequently Used volatile：已设置过期时间的数据集（server.db[i].expires） allkeys：数据集（server.db[i].dict） Redis 持久化机制即怎么保证 Redis 挂掉之后再重启数据可以进行恢复？ Redis 是支持持久化的，而且还支持两种，它们分别是： 快照（snapshotting，RDB） 只追加文件（append-only file，AOF） RDB 持久化Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis 主从结构，主要用来提高 Redis 性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是 Redis 默认采用的持久化方式，在 redis.conf 配置文件中默认有此下配置： 123save 900 1 # 在 900 秒(15分钟)之后，如果至少有 1 个 key 发生变化，Redis 就会自动触发 BGSAVE 命令创建快照save 300 10 # 在 300 秒(5分钟)之后，如果至少有 10 个 key 发生变化，Redis 就会自动触发 BGSAVE 命令创建快照save 60 10000 # 在 60 秒(1分钟)之后，如果至少有 10000 个 key 发生变化，Redis 就会自动触发 BGSAVE 命令创建快照 BGSAVE原理：copy-on-write AOF 持久化与快照持久化相比，AOF 持久化的实时性更好，因此已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF 方式的持久化，需要通过 appendonly 参数开启：appendonly yes开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入硬盘中的 AOF 文件。AOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。 在 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是：123appendfsync always # 每次有数据修改发生时都会写入 AOF 文件，这样会严重降低 Redis 的速度appendfsync everysec # 每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no # 让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis 还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 Redis 事务Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 一个事务从开始到结束会经历以下三个阶段： 事务开始 命令入队 事务执行 流程如下： 123456MULTI # 事务开始SET "name" "xxx" # 放到事务队列的 index 0 处GET "name" # 放到事务队列的 index 1 处SET "author" "Bean" # 放到事务队列的 index 2 处GET "author" # 放到事务队列的 index 3 处EXEC # 从 index 0 到 index 3 执行事务队列中的命令，并将 4 个命令的结果返回客户端 那么 WATCH 命令是干嘛的呢？它是一个乐观锁，可以在 EXEC 命令执行前，监视任意数量的数据库键，在 EXEC 命令执行时，只要有一个被监视的键发生了变化，服务器就会拒绝执行事务，并返回：(nil) 表示事务执行失败。 在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的可靠性和安全性。在 Redis 中，事务总是具有原子性（Atomicity)、一致性（Consistency）和隔离性（Isolation），并且当 Redis 运行在某种特定的持久化模式下时，事务也具有持久性（Durability）。 缓存穿透、缓存雪崩、缓存击穿问题的解决方案缓存穿透问题描述： 缓存穿透是指查询一个一定不存在的数据，由于缓存是命中时被动写的，这个数据不存在，所以缓存肯定没有，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能 DB 就挂掉了，要是有人利用不存在的 key 频繁攻击我们的应用，这就是漏洞。 解决方法的思路： 要能快速的判断出一个 key 在我们的系统到底存不存在，数据不存在，就不去 DB 查了。 解决方法：布隆过滤器。 将所有可能存在的数据哈希到一个足够大的 bitmap 中，要查询一个 key 之前，都先用这个 bitmap 判断一下存不存在，数据不存在就不去 DB 查了，从而避免了对底层存储系统的查询压力。 另外也有一个更为简单粗暴的方法，即如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 缓存雪崩问题描述： 缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到 DB，DB 瞬时压力过重雪崩。 解决方法： 将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如 1-5 分钟的随机时间，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 缓存击穿问题描述： 对于一些设置了过期时间的 key，如果这些 key 可能会在某些时间点被超高并发地访问，是一种非常 “热点” 的数据。但当缓存在某个时间点过期的时候，如果恰好在这个时间点有对这个 key 的大量并发请求过来，这些请求发现缓存过期了，就会选择从后端 DB 加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端 DB 压垮。 有如下 3 种解决方法： 使用互斥锁（mutex key）就是在缓存失效的时候（判断拿出来的值为空），不是立即去 load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如 Redis 的 SETNX）去 set 一个 mutex key，当操作返回成功时，再进行 load db 的操作并回设缓存；否则，就重试整个 get 缓存的方法（此时其他成功的请求可能已经把缓存更新好了，这个请求就可以成功的 get 到了）。 也就是说，我们只放一个请求去 load DB，把其他的请求都拦在了缓存层。 “提前” 使用互斥锁在这个 key 的 value 内部设置 1 个超时值 (timeout1)，timeout1 比实际的 timeout (timeout2) 要小。当从 cache 读取到 timeout1 发现它已经过期时候，马上延长 timeout1 并重新设置到 cache。然后再从数据库加载数据并设置到 cache 中。 也就是说，我们根本不让这个热点数据在缓存中不存在，热点数据快过期了就更新一下它，不让它真正的过期。 永远不过期缓存在 Redis 中的热点数据根本不设置过期时间，把过期时间存在 key 对应的 value 里，如果发现要过期了，通过一个后台的异步线程更新缓存。 如何解决 Redis 的并发竞争 Key 问题所谓 Redis 的并发竞争 key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（但如果不存在 Redis 的并发竞争 key 问题，不要使用分布式锁，会影响性能）。 分布式锁应该是怎么样的： setnx k v 互斥性：可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。 这把锁要是一把可重入锁（避免死锁） 不会发生死锁：有一个客户端在持有锁的过程中崩溃而没有解锁，也能保证其他客户端能够加锁。 获取锁和释放锁的性能要好 基于数据库表基于 Redis 的分布式锁基于 Zookeeper 实现分布式锁基于 Zookeeper 临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在 Zookeeper 上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 如何保证缓存与数据库双写时的数据一致性？你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，就是读的时候肯定不写，写的时候肯定不读，这样就可以保证一定不会出现不一致的情况，但是这会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 最经典的缓存 + 数据库读写的模式：Cache Aside Pattern。 即： 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候， 先更新数据库，然后再删除缓存 。（一个 lazy 计算的思想） 为什么是删除缓存，而不是更新缓存？ 很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值，而可能是多个数据库中的结果计算出来的。比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 数据库频繁更新的数据不一定是被频繁访问的数据，这种情况下，数据库更新一次就修改一次缓存是很不值得的。 Cache Aside Pattern 可能会出现的问题： 1. 先修改数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。 解决思路： 先删除缓存，再修改数据库。如果数据库修改失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中。虽然数据还是旧的，不过至少数据库和缓存是一致的。 2. 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了… 更新数据的时候，根据 数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据 + 更新缓存的操作，根据唯一标识路由之后，也发送同一个 jvm 内部队列中。 一个队列对应一个工作线程，每个工作线程 串行 拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时即使一个读请求过来，读到了空的缓存，它也会先将缓存更新的请求发送到队列中，等待缓存更新完成，而不是拿了旧数据就走，走前还不忘把旧数据放缓存里…… 优化点： 一个队列中，多个更新缓存请求串在一起是没意义的，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值，但不会把这个旧值设置到缓存中去。 用Redis做消息队列列表。rpush推入、lpop取出缺点：没有等待队列里有值就直接消费。弥补：应用层sleep，然后lpop重试 或blpop list seconds 发布订阅：subscribe、publish。但是消息无状态，不保证可达。需要用kafka什么的 参考： 如何保证缓存与数据库双写时的数据一致性？ 分布式锁解决并发的三种实现方式 缓存穿透，缓存击穿，缓存雪崩解决方案分析]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker简介]]></title>
    <url>%2FDocker%2F</url>
    <content type="text"><![CDATA[一句话概括容器：容器就是将软件打包成标准化单元，以用于开发、交付和部署。 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。 容器化软件适用于基于Linux和Windows的应用，在任何环境中都能够始终如一地运行。 容器赋予了软件独立性 ，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。 拿内存举例，虚拟机是利用Hypervisor去虚拟化内存，整个调用过程是虚拟内存-&gt;虚拟物理内存-&gt;真正物理内存，但是Docker是利用Docker Engine去调用宿主的的资源，这时候过程是虚拟内存-&gt;真正物理内存。 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便.虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker通常用于隔离不同的应用 ，例如前端，后端以及数据库。 镜像（Image）——一个特殊的文件系统Docker 镜像（Image）就是一个只读的模板。例如：一个镜像可以包含一个完整的操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而Docker 镜像（Image），就相当于是一个 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 Docker 设计时，就充分利用 Union FS的技术，将其设计为 分层存储的架构 。 镜像实际是由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。 比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 容器（Container)——镜像运行时的实体镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。 仓库（Repository）——集中存放镜像文件的地方镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。 一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是Docker用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。 这里补充一下Docker Registry 公开服务和私有 Docker Registry的概念： Docker Registry 公开服务 是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 常用命令docker ps列出所有运行中容器。 进入应用内部的系统 4f是编号docker exec -it 4f bash docker网络类型：bridge（和主机独立） host none bridge需要端口映射 docker run -d -p 8888:8080 test/tomcat -d：表示指定容器后台运行 -p：表示宿主机的8080端口对外映射暴露为8888端口 docker rmdocker rm [options “o”&gt;] “o”&gt;[container…]docker rm nginx-01 nginx-02 db-01 db-02sudo docker rm -l /webapp/redis-f 强行移除该容器，即使其正在运行；-l 移除容器间的网络连接，而非容器本身；-v 移除与容器关联的空间。 docker start|stop|restart docker imagesdocker images [options “o”&gt;] [name]列出本地所有镜像。其中 [name] 对镜像名称进行关键词查询。]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ基础知识简介]]></title>
    <url>%2FRabbitMQ%2F</url>
    <content type="text"><![CDATA[AMQP，即 Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。消息中间件主要用于组件之间的解耦和通讯。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。RabbitMQ是一个开源的AMQP实现，服务器端用 Erlang 语言编写，支持多种客户端，如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持AJAX。用于在分布式系统中存储转发消息，具有很高的易用性和可用性。 ConnectionFactory、Connection、Channel 都是RabbitMQ对外提供的API中最基本的对象。 ConnectionFactory：ConnectionFactory为Connection的制造工厂。 Connection：Connection是RabbitMQ的socket链接，它封装了socket协议相关部分逻辑。 Channel(信道)：信道是建立在“真实的”TCP连接上的虚拟连接，在一条TCP链接上创建多少条信道是没有限制的，把他想象成光纤就是可以了。它是我们与RabbitMQ打交道的最重要的一个接口，我们大部分的业务操作是在Channel这个接口中完成的，包括定义Queue、定义Exchange、绑定Queue与Exchange、发布消息等。 queue队列生产者Send Message “A”被传送到Queue中，消费者发现消息队列Queue中有订阅的消息，就会将这条消息A读取出来进行一些列的业务操作。这里只是一个消费正对应一个队列Queue，也可以多个消费者订阅同一个队列Queue，当然这里就会将Queue里面的消息平分给其他的消费者，但是会存在一个一个问题就是如果每个消息的处理时间不同，就会导致某些消费者一直在忙碌中，而有的消费者处理完了消息后一直处于空闲状态，因为前面已经提及到了Queue会平分这些消息给相应的消费者。这里我们就可以使用prefetchCount来限制每次发送给消费者消息的个数。详情见下图所示：生产者在将消息发送给 Exchange 的时候，一般会指定一个 routing key，来指定这个消息的路由规则。 Exchange 会根据 routing key 和 Exchange Type（交换器类型） 以及 Binding key 的匹配情况来决定把消息路由到哪个 Queue。RabbitMQ为routing key设定的长度限制为255 bytes。 RabbitMQ常用的Exchange Type有 Fanout、 Direct、 Topic、 Headers 这四种。 Fanout 这种类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中，这时 Routing key 不起作用。 Direct 这种类型的Exchange路由规则也很简单，它会把消息路由到那些 binding key 与 routing key完全匹配的Queue中。 当生产者（P）发送消息时Rotuing key=booking时，这时候将消息传送给Exchange，Exchange获取到生产者发送过来消息后，会根据自身的规则进行与匹配相应的Queue，这时发现Queue1和Queue2都符合，就会将消息传送给这两个队列，如果我们以Rotuing key=create和Rotuing key=confirm发送消息时，这时消息只会被推送到Queue2队列中，其他Routing Key的消息将会被丢弃。 Topic 这种类型的Exchange的路由规则支持 binding key 和 routing key 的模糊匹配，会把消息路由到满足条件的Queue。 binding key 中可以存在两种特殊字符 *与 #，用于做模糊匹配，其中 *用于匹配一个单词，# 用于匹配0个或多个单词，单词以符号.为分隔符。 Headers 这种类型的Exchange不依赖于 routing key 与 binding key 的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配,对比其中的键值对是否完全匹配Queue与Exchange绑定时指定的键值对；如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。 Message durability（消息的持久化） 如果我们希望即使在RabbitMQ服务重启的情况下，也不会丢失消息，我们可以将Queue与Message都设置为可持久化的（durable），这样可以保证绝大部分情况下我们的RabbitMQ消息不会丢失。但依然解决不了小概率丢失事件的发生（比如RabbitMQ服务器已经接收到生产者的消息，但还没来得及持久化该消息时RabbitMQ服务器就断电了），如果我们需要对这种小概率事件也要管理起来，那么我们要用到事务。由于这里仅为RabbitMQ的简单介绍，所以这里将不讲解RabbitMQ相关的事务。具体可以参考 RabbitMQ之消息确认机制（事务+Confirm） 接口优化===同步下单改为异步下单收到请求后由redis先预检库存，不足的话直接返回，减少数据库访问如果有库存，则请求放到消息队列。返回一个排队中请求出队。客户端轮询是否秒杀成功]]></content>
      <tags>
        <tag>消息队列</tag>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库基础知识]]></title>
    <url>%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[索引事务隔离级别锁SQL joinInnodb和MyISAM引擎MyCAT 索引数据结构： 二叉树：O(logn),深度会深，io多B树：B+树：底下的链接可做范围统计。磁盘代价低、查询效率稳定、有利于对数据库扫描Hash： innoDB密集索引：每个搜索码值都对应一个索引值Myisam稀疏索引：只为索引码的某些值建立索引项 如何定位并优化慢查询sql：根据慢日志定位慢查询SQL使用explain等工具分析SQL修改SQL或尽量让SQL走索引 事务的隔离级别、锁深入理解乐观锁与悲观锁 事务的隔离级别 锁在关系数据库管理系统里，悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）是一种并发控制的方法。它可以阻止一个事务以影响其他用户的方式来修改数据。如果一个事务执行的操作都某行数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 上面我们提到，使用select…for update会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁把整张表锁住，这点需要注意。 乐观锁（ Optimistic Locking ） 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。只能防止脏读后数据的提交，不能解决脏读。相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 行级锁开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 页级锁开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 在不通过索引条件查询的时候,InnoDB 确实使用的是表锁,而不是行锁。MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。 行锁TODO….gap锁 ANSI/ISO SQL定义的标准隔离级别有四种，从高到底依次为：可序列化(Serializable)、可重复读(Repeatable reads)、提交读(Read committed)、未提交读(Read uncommitted)。 未提交读(Read uncommitted)事务在读数据的时候并未对数据加锁。事务在修改数据的时候只对数据增加行级共享锁。所以，未提交读会导致脏读可以读到其他事务未提交的结果 提交读(Read committed)提交读(READ COMMITTED)也可以翻译成读已提交，通过名字也可以分析出，在一个事务修改数据过程中，如果事务还没提交，其他事务不能读该数据。提交读的数据库锁情况事务对当前被读取的数据加 行级共享锁（当读到时才加锁），一旦读完该行，立即释放该行级共享锁；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加 行级排他锁，直到事务结束才释放。简而言之，提交读这种隔离级别保证了读到的任何数据都是提交的数据，避免了脏读(dirty reads)。但是不保证事务重新读的时候能读到相同的数据，因为在每次数据读完之后其他事务可以修改刚才读到的数据。 可重复读(Repeatable reads)事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加 行级共享锁，直到事务结束才释放；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加 行级排他锁，直到事务结束才释放。可重复读隔离级别可以解决不可重复读的读现象。但是可重复读这种隔离级别中，还有另外一种读现象他解决不了，那就是幻读。 可序列化(Serializable)事务在读取数据时，必须先对其加 表级共享锁 ，直到事务结束才释放；事务在更新数据时，必须先对其加 表级排他锁 ，直到事务结束才释放。 脏读 脏读又称无效数据的读出，是指在数据库访问中，事务T1将某一值修改，然后事务T2读取该值，此后T1因为某种原因撤销对该值的修改，这就导致了T2所读取到的数据是无效的。 不可重复读 一种更易理解的说法是：在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 幻读 幻读是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中的“全部数据行”。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入“一行新数据”。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样.一般解决幻读的方法是增加范围锁RangeS，锁定检锁范围为只读，这样就避免了幻读。 幻读(phantom read)”是不可重复读(Non-repeatable reads)的一种特殊场景：当事务没有获取范围锁的情况下执行SELECT … WHERE操作可能会发生“幻影读(phantom read)”。 当前读、快照读 DB_TRX_ID ，，DB_ROLL_PTR ，， DB_ROW_ID字段 undo日志 SQL JOIN123456789101112Id_P LastName FirstName Address City1 Adams John Oxford Street London2 Bush George Fifth Avenue New York3 Carter Thomas Changan Street BeijingId_O OrderNo Id_P1 77895 32 44678 33 22456 14 24562 15 34764 65 SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName 结果： LastName FirstName OrderNoAdams John 22456Adams John 24562Carter Thomas 77895Carter Thomas 44678 在表中存在至少一个匹配时（来自两个表的所有字段都不为空），INNER JOIN 关键字返回行。与 JOIN 是相同的LEFT JOIN 关键字会从左表 (table_name1) 那里返回所有的行，即使在右表 (table_name2) 中没有匹配的行。 LEFT OUTER JOIN。SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName LastName FirstName OrderNoAdams John 22456Adams John 24562Carter Thomas 77895Carter Thomas 44678Bush GeorgeRIGHT JOIN 关键字会右表 (table_name2) 那里返回所有的行，即使在左表 (table_name1) 中没有匹配的行。只要其中某个表存在匹配，FULL JOIN 关键字就会返回行。LastName FirstName OrderNoAdams John 22456Adams John 24562Carter Thomas 77895Carter Thomas 44678Bush George 34764 Innodb与Myisam引擎1. 区别：（1）事务处理：MyISAM是非事务安全型的，而InnoDB是事务安全型的（支持事务处理等高级处理）；（2）锁机制不同：MyISAM是表级锁，而InnoDB是行级锁；（3）select ,update ,insert ,delete 操作：MyISAM：如果执行大量的SELECT，MyISAM是更好的选择InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表（4）查询表的行数不同：MyISAM：select count() from table,MyISAM只要简单的读出保存好的行数，注意的是，当count()语句包含 where条件时，两种表的操作是一样的InnoDB ： InnoDB 中不保存表的具体行数，也就是说，执行select count(*) from table时，InnoDB要扫描一遍整个表来计算有多少行（5）外键支持：mysiam表不支持外键，而InnoDB支持 2. 为什么MyISAM会比Innodb 的查询速度快。INNODB在做SELECT的时候，要维护的东西比MYISAM引擎多很多；1）数据块，INNODB要缓存，MYISAM只缓存索引块， 这中间还有换进换出的减少；2）innodb寻址要映射到块，再到行，MYISAM 记录的直接是文件的OFFSET，定位比INNODB要快3）INNODB还需要维护MVCC一致；虽然你的场景没有，但他还是需要去检查和维护MVCC ( Multi-Version Concurrency Control )多版本并发控制 3. 应用场景MyISAM适合：(1)做很多count 的计算；(2)插入不频繁，查询非常频繁；(3)没有事务。InnoDB适合：(1)可靠性要求比较高，或者要求事务；(2)表更新和查询都相当的频繁，并且行锁定的机会比较大的情况。 MVCC 【mysql】关于innodb中MVCC的一些理解通过加锁，让所有的读者等待写者工作完成，但是这样效率会很差。MVCC 使用了一种不同的手段，每个连接到数据库的读者，在某个瞬间看到的是数据库的一个快照，写者写操作造成的变化在写操作完成之前（或者数据库事务提交之前）对于其他的读者来说是不可见的。 innodb存储的最基本row中包含一些额外的存储信息 DATA_TRX_ID，DATA_ROLL_PTR，DB_ROW_ID，DELETE BIT 6字节的DATA_TRX_ID 标记了最新更新这条行记录的transaction id，每处理一个事务，其值自动+1 7字节的DATA_ROLL_PTR 指向当前记录项的rollback segment的undo log记录，找之前版本的数据就是通过这个指针 6字节的DB_ROW_ID，当由innodb自动产生聚集索引时，聚集索引包括这个DB_ROW_ID的值，否则聚集索引中不包括这个值.，这个用于索引当中 DELETE BIT位用于标识该记录是否被删除，这里的不是真正的删除数据，而是标志出来的删除。真正意义的删除是在commit的时候 InnoDB的MVCC,是通过在每行记录后面保存两个隐藏的列来实现的,这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值,而是系统版本号(可以理解为事务的ID)，每开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID. SELECTInnoDB会根据以下两个条件检查每行记录: a.InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的. b.行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除.只有a,b同时满足的记录，才能返回作为查询结果.DELETEInnoDB会为删除的每一行保存当前系统的版本号(事务的ID)作为删除标识. innodb索引和数据在一起 myisam不在一起 MyCAThttps://www.jianshu.com/p/21b1e133dd9b分布式数据库系统中间层 应用场景：需要读写分离，需要分库分表，多租户，数据统计系统，HBASE的一种替代方案 支持全局表支持ER的分片策略支持一致性hash分片 使用MySQL客户端管理mycat动态加载配置文件：reload @@config;查看数据节点：show @@datanode;查看后端数据库：show @@datasource;]]></content>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++知识]]></title>
    <url>%2FC-C-%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[构造函数 析构函数析构函数和构造函数是一对。构造函数用于创建对象，而析构函数是用来撤销对象。 内联函数inline1.内联函数在运行时可调试，而宏定义不可以;2.编译器会对内联函数的参数类型做安全检查或自动类型转换（同普通函数），而宏定义则不会；3.内联函数可以访问类的成员变量，宏定义则不能；4.在类中声明同时定义的成员函数，自动转化为内联函数。 C语言中switch 的查找实现原理 if…else结果的查找当case语句是小于3句的时候，switch语句底层的实现和if…else的实现方式相同。 线性查找当case语句大于等于4的时候，且每两个case之间产生的间隔之和不超过6时，就按线性结构查找。即，如下图的汇编里面的jmp dword ptr [edx*4+11B1428h]该指令里面的11B1428h地址里面，其存放着各个case语句的首地址。由于内存中下标是从0开始的，因此，通过对其进行减一操作，在判断其是否大于11B1428h地址的数组长度，如果大于直接跳出，否则通过计算直接定位到该数组上的地址进行跳转。 树形查找当最大case值和最小case值之差大于255的情况下，此时，编译器会采用树形查找。即，将数据由小到大排列，并取中间值（如果是偶数，就取中间两个靠右的那一个），在左右两边继续取中间值划分（左右两边划分不需要将中间值算进去），直到小于等于3个数据的时候不在划分。​​]]></content>
      <tags>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程与并发]]></title>
    <url>%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[线程池基本组成1、线程池管理器（ThreadPool）：用于创建并管理线程池，包括 创建线程池，销毁线程池，添加新任务；2、工作线程（PoolWorker）：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务；3、任务接口（Task）：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等；4、任务队列（taskQueue）：用于存放没有处理的任务。提供一种缓冲机制。如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，这样频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁线程需要时间。 线程并发数量过多，抢占系统资源从而导致阻塞运用线程池能有效的控制线程最大并发数，避免以上的问题 对线程进行一些简单的管理比如：延时执行、定时循环执行的策略等 java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类 构造参数 1234567891011java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类构造参数public ThreadPoolExecutor(int ==corePoolSize==,该线程池中核心线程数最大值。线程池新建线程的时候，如果当前线程总数小于corePoolSize，则新建的是核心线程，如果超过corePoolSize，则新建的是非核心线程 核心线程默认情况下会一直存活在线程池中，即使这个核心线程啥也不干(闲置状态)。 如果指定ThreadPoolExecutor的allowCoreThreadTimeOut这个属性为true，那么核心线程如果不干活(闲置状态)的话，超过一定时间(时长下面参数决定)，就会被销毁掉 int ==maximumPoolSize==,（线程不够用时能够创建的最大线程数）线程总数 = 核心线程数 + 非核心线程数long ==keepAliveTime==,非核心线程闲置超时时长TimeUnit ==unit==,keepAliveTime的单位，TimeUnit是一个枚举类型BlockingQueue&lt;Runnable&gt; ==workQueue==,任务队列：维护着等待执行的Runnable对象 当所有的核心线程都在干活时，新添加的任务会被添加到这个队列中等待处理，如果队列满了，则新建非核心线程执行任务 ThreadFactory ==threadFactory==, 创建新线程，Executors.defaultThreadFactory()RejectedExecutionHandler ==handler ==线程池的饱和策略) 通过ThreadPoolExecutor.execute(Runnable command)方法即可向线程池内添加一个任务 当一个任务被添加进线程池时： 1、线程数量未达到corePoolSize，则新建一个线程(核心线程)执行任务2、线程数量达到了corePools，则将任务移入队列BlockingQueue等待3、如果无法将任务加入BlockingQueue（队列已满），则在非corePool中创建新的线程来处理任务（注意，执行这一步骤需要获取全局锁）。4、队列已满，总线程数又达到了maximumPoolSize，(RejectedExecutionHandler)抛出异常 四种java实现好的线程池CachedThreadPool() 可缓存线程池：线程数无限制有空闲线程则复用空闲线程，若无空闲线程则新建线程一定程序减少频繁创建/销毁线程，减少系统开销 FixedThreadPool() 定长线程池：可控制线程最大并发数（同时执行的线程数）超出的线程会在队列中等待 ScheduledThreadPool() 定长线程池：支持定时及周期性任务执行。 SingleThreadExecutor() 单线程化的线程池：有且仅有一个工作线程执行任务所有任务按照指定顺序执行，即遵循队列的入队出队规则 123456789101112import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;public class ThreadPoolExecutorTest &#123;public static void main(String[] args) &#123;ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5);scheduledThreadPool.schedule(new Runnable() &#123;public void run() &#123;System.out.println(&quot;delay 3 seconds&quot;);&#125;&#125;, 3, TimeUnit.SECONDS);&#125;&#125; 线程池的状态 线程池风险： 死锁、资源不足、并发错误、 线程泄漏、请求过载 Executor管理多个异步任务的执行，而无需程序员显式地管理线程的生命周期。这里的异步是指多个任务的执行互不干扰，不需要进行同步操作。 主要有三种 Executor： CachedThreadPool：一个任务创建一个线程； FixedThreadPool：所有任务只能使用固定大小的线程； SingleThreadExecutor：相当于大小为 1 的 FixedThreadPool。 1234567public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; executorService.execute(new MyRunnable()); &#125; executorService.shutdown();&#125; volatile关键字解析如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。 为了解决缓存不一致性问题，通常来说有以下2种解决方法：1）通过在总线加LOCK#锁的方式 2）通过缓存一致性协议 。这2种方式都是硬件层面上提供的方式。 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。 缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 原子性问题，可见性问题，有序性问题 对于可见性，Java提供了volatile关键字来保证可见性。 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 下面这段话摘自《深入理解Java虚拟机》： “观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令” lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 2）它会强制将对缓存的修改操作立即写入主存； 3）如果是写操作，它会导致其他CPU中对应的缓存行无效。 下面列举几个Java中使用volatile的几个场景。1.状态标记量2.double check Daemon守护线程是程序运行时在后台提供服务的线程，不属于程序中不可或缺的部分。当所有非守护线程结束时，程序也就终止，同时会杀死所有守护线程。main() 属于非守护线程。使用 setDaemon() 方法将一个线程设置为守护线程。 1234public static void main(String[] args) &#123;Thread thread = new Thread(new MyRunnable());thread.setDaemon(true);&#125; 同步-synchronized、ReentrantLock第一个是 JVM 实现的 synchronized，它只作用于同一个对象，如果调用两个对象上的同步代码块，就不会进行同步.方法也是。但是同步一个类public void func() { synchronized (SynchronizedExample.class) { // … }}作用于整个类，也就是说两个线程调用同一个类的不同对象上的这种同步语句，也会进行同步。同步一个静态方法作用于整个类。 而另一个是 JDK 实现的 ReentrantLock。 synchronized==锁的不是代码。是对象== 对象在内存中的布局：对象头、实例数据、对齐填充 ReentrantLockTODO 锁悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中 synchronized和 ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中 java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁一般会使用版本号机制或CAS算法实现。 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子：假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 12341、操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。2、在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。3、操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。4、操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 CAS算法CAS算法乐观锁的一种表现。 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 自旋锁、自适应自旋锁自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁。 1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125; 乐观锁的缺点1、 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference类就提供了此种能力，其中的 compareAndSet方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 、循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3、 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了 AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用 AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS与synchronized的使用情景简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 线程间的协作在线程中调用另一个线程的 join() 方法，会将当前线程挂起，而不是忙等待，直到目标线程结束。 wait、notify、notifyAll()调用 wait() 使得线程等待某个条件满足，线程在等待时会被挂起，当其他线程的运行使得这个条件满足时，其它线程会调用 notify() 或者 notifyAll() 来唤醒挂起的线程。 它们都属于 Object 的一部分，而不属于 Thread。 只能用在同步方法或者同步控制块中使用，否则会在运行时抛出 IllegalMonitorStateExeception。 使用 wait() 挂起期间，线程会释放锁。这是因为，如果没有释放锁，那么其它线程就无法进入对象的同步方法或者同步控制块中，那么就无法执行 notify() 或者 notifyAll() 来唤醒挂起的线程，造成死锁。 wait() 是 Object 的方法，而 sleep() 是 Thread 的静态方法； wait() 会释放锁，sleep() 不会，是让出cpu。 await、signal、signalAll1234567891011121314151617线程consumersynchronize(obj)&#123; obj.wait();//没东西了，等待&#125;线程producersynchronize(obj)&#123; obj.notify();//有东西了，唤醒 &#125;lock.lock();condition.await();lock.unlock();lock.lock();condition.signal();lock.unlock; 为了突出区别，省略了若干细节。区别有三点： 1. lock不再用synchronize把同步代码包装起来； 2. 阻塞需要另外一个对象condition； 3. 同步和唤醒的对象是condition而不是lock，对应的方法是await和signal，而不是wait和notify。 为什么需要使用condition呢？简单一句话，lock更灵活。以前的方式只能有一个等待队列，在实际应用时可能需要多个，比如读和写。为了这个灵活性，lock将同步互斥控制和等待队列分离开来，互斥保证在某个时刻只有一个线程访问临界区（lock自己完成），等待队列负责保存被阻塞的线程（condition完成）。 通过查看ReentrantLock的源代码发现，condition其实是等待队列的一个管理者，condition确保阻塞的对象按顺序被唤醒。 在Lock的实现中，LockSupport被用来实现线程状态的改变，后续将更进一步研究LockSupport的实现机制。 线程的状态 yield使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。cpu会从众多的可执行态里选择，也就是说，当前也就是刚刚的那个线程还是有可能会被再次执行到的，并不是说一定会执行其他线程而该线程在下一次中不会执行到了。 用了yield方法后，该线程就会把CPU时间让掉，让其他或者自己的线程执行（也就是谁先抢到谁执行） 中断线程 ThreadLocalThreadLocal 不继承 Thread，也不实现 Runable 接口， ThreadLocal 类为每一个线程都维护了自己独有的变量拷贝。每个线程都拥有自己独立的变量，其作用在于数据独立。ThreadLocal 采用 hash 表的方式来为每个线程提供一个变量的副本]]></content>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机内存管理机制]]></title>
    <url>%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[整体结构参考原文 程序计数器 当前线程所执行的字节码的行号指示器，字节码解释器工作时就是通过改变这个计数器的值来确定下一条要执行的字节码指令的位置 执行 Java 方法和 native 方法时的区别： 执行 Java 方法时：记录虚拟机正在执行的字节码指令地址； 执行 native 方法时：无定义； 是 5 个区域中唯一不会出现 OOM 的区域。 Java 虚拟机栈 Java 方法执行的内存模型，每个方法执行的过程，就是它所对应的栈帧在虚拟机栈中入栈到出栈的过程； 服务于 Java 方法； 可能抛出的异常： OutOfMemoryError（在虚拟机栈可以动态扩展的情况下，扩展时无法申请到足够的内存）； StackOverflowError（线程请求的栈深度 &gt; 虚拟机所允许的深度）； 虚拟机参数设置：-Xss. 本地方法栈 服务于 native 方法； 可能抛出的异常：与 Java 虚拟机栈一样。 Java堆 唯一的目的：存放对象实例； 垃圾收集器管理的主要区域； 可以处于物理上不连续的内存空间中； 可能抛出的异常： OutOfMemoryError（堆中没有内存可以分配给新创建的实例，并且堆也无法再继续扩展了）。 虚拟机参数设置： 最大值：-Xmx 最小值：-Xms 两个参数设置成相同的值可避免堆自动扩展。现代的垃圾收集器基本都是采用分代收集算法，该算法的思想是针对不同的对象采取不同的垃圾回收算法，因此虚拟机把 Java 堆分成以下三块： 新生代（Young Generation） 老年代（Old Generation） 永久代（Permanent Generation） 方法区 存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据； 类信息：即 Class 类，如类名、访问修饰符、常量池、字段描述、方法描述等。 垃圾收集行为在此区域很少发生； 不过也不能不清理，对于经常动态生成大量 Class 的应用，如 Spring 等，需要特别注意类的回收状况。 运行时常量池也是方法区的一部分； Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池，用于存放编译器生成的各种字面量（就是代码中定义的 static final 常量）和符号引用，这部分信息就存储在运行时常量池中。 可能抛出的异常： OutOfMemoryError（方法区无法满足内存分配需求时）。 直接内存 JDK 1.4 的 NIO 类可以使用 native 函数库直接分配堆外内存，这是一种基于通道与缓冲区的 I/O 方式，它在 Java 堆中存储一个 DirectByteBuffer 对象作为堆外内存的引用，这样就可以对堆外内存进行操作了。因为可以避免 Java 堆和 Native 堆之间来回复制数据，在一些场景可以带来显著的性能提高。 虚拟机参数设置：-XX:MaxDirectMemorySize 默认等于 Java 堆最大值，即 -Xmx 指定的值。 将直接内存放在这里讲解的原因是它也可能会出现 OutOfMemoryError； 服务器管理员在配置 JVM 参数时，会根据机器的实际内存设置 -Xmx 等信息，但经常会忽略直接内存（默认等于 -Xmx 设置值），这可能会使得各个内存区域的总和大于物理内存限制，从而导致动态扩展时出现 OOM。 Java内存模型中堆和栈的区别元空间metaspace和永久代permgen HotSpot 虚拟机堆中的对象对象的创建（遇到一条 new 指令时） 检查这个指令的参数能否在常量池中定位到一个类的符号引用，并检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，先把这个类加载进内存； 类加载检查通过后，虚拟机将为新对象分配内存，此时已经可以确定存储这个对象所需的内存大小； 在堆中为新对象分配可用内存； 将分配到的内存初始化； 设置对象头中的数据； 此时，从虚拟机的角度看，对象已经创建好了，但从 Java 程序的角度看，对象创建才刚刚开始，构造函数还没有执行。 第 3 步，在堆中为新对象分配可用内存时，会涉及到以下两个问题： 如何在堆中为新对象划分可用的内存？ 指针碰撞（内存分配规整） 用过的内存放一边，没用过的内存放一边，中间用一个指针分隔； 分配内存的过程就是将指针向没用过的内存那边移动所需的长度； 空闲列表（内存分配不规整） 维护一个列表，记录哪些内存块是可用的； 分配内存时，从列表上选取一块足够大的空间分给对象，并更新列表上的记录； 如何处理多线程创建对象时，划分内存的指针的同步问题？ 对分配内存空间的动作进行同步处理（CAS）； 把内存分配动作按照线程划分在不同的空间之中进行； 每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）； 哪个线程要分配内存就在哪个线程的 TLAB 上分配，TLAB 用完需要分配新的 TLAB 时，才需要同步锁定； 通过 -XX:+/-UseTLAB 参数设定是否使用 TLAB。 对象的内存布局 对象头： 第一部分：存储对象自身运行时的数据，HashCode、GC分代年龄等（Mark Word）； 第二部分：类型指针，指向它的类元数据的指针，虚拟机通过这个指针来判断这个对象是哪个类的实例（HotSpot 采用的是直接指针的方式访问对象的）； 如果是个数组对象，对象头中还有一块用于记录数组长度的数据。 实例数据： 默认分配顺序：longs/doubles、ints、shorts/chars、bytes/booleans、oops (Ordinary Object Pointers)，相同宽度的字段会被分配在一起，除了 oops，其他的长度由长到短； 默认分配顺序下，父类字段会被分配在子类字段前面。注：HotSpot VM要求对象的起始地址必须是8字节的整数倍，所以不够要补齐。 对象的访问Java 程序需要通过虚拟机栈上的 reference 数据来操作堆上的具体对象，reference 数据是一个指向对象的引用，不过如何通过这个引用定位到具体的对象，目前主要有以下两种访问方式：句柄访问和直接指针访问。 句柄访问句柄访问会在 Java 堆中划分一块内存作为句柄池，每一个句柄存放着到对象实例数据和对象类型数据的指针。优势：对象移动的时候（这在垃圾回收时十分常见）只需改变句柄池中对象实例数据的指针，不需要修改reference本身。 直接指针访问直接指针访问方式在 Java 堆对象的实例数据中存放了一个指向对象类型数据的指针，在 HotSpot 中，这个指针会被存放在对象头中。优势：减少了一次指针定位对象实例数据的开销，速度更快。 OOM 异常 (OutOfMemoryError)Java 堆溢出 出现标志：java.lang.OutOfMemoryError: Java heap space 解决方法： 先通过内存映像分析工具分析 Dump 出来的堆转储快照，确认内存中的对象是否是必要的，即分清楚是出现了内存泄漏还是内存溢出； 如果是内存泄漏，通过工具查看泄漏对象到 GC Root 的引用链，定位出泄漏的位置； 如果不存在泄漏，检查虚拟机堆参数（-Xmx 和 -Xms）是否可以调大，检查代码中是否有哪些对象的生命周期过长，尝试减少程序运行期的内存消耗。 虚拟机参数： -XX:HeapDumpOnOutOfMemoryError：让虚拟机在出现内存泄漏异常时 Dump 出当前的内存堆转储快照用于事后分析。 Java 虚拟机栈和本地方法栈溢出 单线程下，栈帧过大、虚拟机容量过小都不会导致 OutOfMemoryError，只会导致 StackOverflowError（栈会比内存先爆掉），一般多线程才会出现 OutOfMemoryError，因为线程本身要占用内存； 如果是多线程导致的 OutOfMemoryError，在不能减少线程数或更换 64 位虚拟机的情况，只能通过减少最大堆和减少栈容量来换取更多的线程； 这个调节思路和 Java 堆出现 OOM 正好相反，Java 堆出现 OOM 要调大堆内存的设置值，而栈出现 OOM 反而要调小。 方法区和运行时常量池溢出 测试思路：产生大量的类去填满方法区，直到溢出； 在经常动态生成大量 Class 的应用中，如 Spring 框架（使用 CGLib 字节码技术），方法区溢出是一种常见的内存溢出，要特别注意类的回收状况。 直接内存溢出 出现特征：Heap Dump 文件中看不见明显异常，程序中直接或间接用了 NIO； 虚拟机参数：-XX:MaxDirectMemorySize，如果不指定，则和 -Xmx 一样。 垃圾收集 (GC)垃圾收集（Garbage Collection，GC），它的任务是解决以下 3 件问题： 哪些内存需要回收？ 什么时候回收？ 如何回收？其中第一个问题很好回答，在 Java 中，GC 主要发生在 Java 堆和方法区中，对于后两个问题，我们将在之后的内容中进行讨论，并介绍 HotSpot 的 7 个垃圾收集器。 判断对象的生死 判断对象是否可用的算法引用计数算法 算法描述： 给对象添加一个引用计数器； 每有一个地方引用它，计数器加 1； 引用失效时，计数器减 1； 计数器值为 0 的对象不再可用。 缺点： 很难解决循环引用的问题。即 objA.instance = objB; objB.instance = objA;，objA 和 objB 都不会再被访问后，它们仍然相互引用着对方，所以它们的引用计数器不为 0，将永远不能被判为不可用。 可达性分析算法（主流） 算法描述： 从 “GC Root” 对象作为起点开始向下搜索，走过的路径称为引用链（Reference Chain）； 从 “GC Root” 开始，不可达的对象被判为不可用。 Java 中可作为 “GC Root” 的对象： 栈中（本地变量表中的reference） 虚拟机栈中，栈帧中的本地变量表引用的对象； 本地方法栈中，JNI 引用的对象（native方法）； 方法区中 类的静态属性引用的对象； 常量引用的对象；即便如此，一个对象也不是一旦被判为不可达，就立即死去的，宣告一个的死亡需要经过两次标记过程。 四种引用类型JDK 1.2 后，Java 中才有了后 3 种引用的实现。 强引用： 像 Object obj = new Object() 这种，只要强引用还存在，垃圾收集器就永远不会回收掉被引用的对象。 软引用： 被软引用关联的对象，只有在内存不够的情况下才会被回收。对于软引用对象，在 OOM 前，虚拟机会把这些对象列入回收范围中进行第二次回收，如果这次回收后，内存还是不够用，就 OOM。 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 弱引用： 被弱引用引用的对象只能生存到下一次垃圾收集前，一旦发生垃圾收集，被弱引用所引用的对象就会被清掉。实现类：WeakReference。Tomcat 中的 ConcurrentCache 就使用了 WeakHashMap 来实现缓存功能。 虚引用： 又称为幽灵引用或者幻影引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象实例。它唯一的用途就是：当被一个虚引用引用的对象被回收时，系统会收到这个对象被回收了的通知。实现类：PhantomReference。 宣告对象死亡的两次标记过程 当发现对象不可达后，该对象被第一次标记，并进行是否有必要执行 finalize() 方法的判断； 不需要执行：对象没有覆盖 finalize() 方法，或者 finalize() 方法已被执行过（finalize() 只被执行一次）； 需要执行：将该对象放置在一个队列中，稍后由一个虚拟机自动创建的低优先级线程执行。 finalize() 方法是对象逃脱死亡的最后一次机会，不过虚拟机不保证等待 finalize() 方法执行结束，也就是说，虚拟机只触发 finalize() 方法的执行，如果这个方法要执行超久，那么虚拟机并不等待它执行结束，所以最好不要用这个方法。 finalize() 方法能做的，try-finally 都能做，所以忘了这个方法吧！ 方法区的回收因为方法区主要存放永久代对象，而永久代对象的回收率比新生代差很多，因此在方法区上进行回收性价比不高。主要是对常量池的回收和对类的卸载。永久代的 GC 主要回收：废弃常量 和 无用的类。 废弃常量：例如一个字符串 “abc”，当没有任何引用指向 “abc” 时，它就是废弃常量了。 无用的类：同时满足以下 3 个条件的类。 该类的所有实例已被回收，Java 堆中不存在该类的任何实例； 加载该类的 Classloader 已被回收； 该类的 Class 对象没有被任何地方引用，即无法在任何地方通过反射访问该类的方法。 垃圾收集算法 基础：标记 - 清除算法 算法描述： 先标记出所有需要回收的对象（图中深色区域）； 标记完后，统一回收所有被标记对象（留下狗啃似的可用内存区域……）。 不足： 效率问题：标记和清理两个过程的效率都不高。 空间碎片问题：标记清除后会产生大量不连续的内存碎片，导致以后为较大的对象分配内存时找不到足够的连续内存，会提前触发另一次 GC。 解决效率问题：复制算法 算法描述： 将可用内存分为大小相等的两块，每次只使用其中一块； 当一块内存用完时，将这块内存上还存活的对象复制到另一块内存上去，将这一块内存全部清理掉。 不足： 可用内存缩小为原来的一半，适合GC过后只有少量对象存活的新生代。 节省内存的方法： 新生代中的对象 98% 都是朝生夕死的，所以不需要按照 1:1 的比例对内存进行划分； 把内存划分为： 1 块比较大的 Eden 区； 2 块较小的 Survivor 区； 每次使用 Eden 区和 1 块 Survivor 区； 回收时，将以上 2 部分区域中的存活对象复制到另一块 Survivor 区中，然后将以上两部分区域清空； JVM 参数设置：-XX:SurvivorRatio=8 表示 Eden 区大小 / 1 块 Survivor 区大小 = 8。 解决空间碎片问题：标记 - 整理算法 算法描述： 标记方法与 “标记 - 清除算法” 一样； 标记完后，将所有存活对象向一端移动，然后直接清理掉边界以外的内存。 不足： 存在效率问题，适合老年代。 进化：分代收集算法 新生代： GC 过后只有少量对象存活 —— 复制算法 老年代： GC 过后对象存活率高 —— 标记 - 整理算法 当系统创建一个对象的时候，总是在Eden区操作，当这个区满了，那么就会触发一次Minor GC，也就是年轻代的垃圾回收。一般来说这时候不是所有的对象都没用了，所以就会把还能用的对象复制到From区。 这样整个Eden区就被清理干净了，可以继续创建新的对象，当Eden区再次被用完，就再触发一次Minor GC，然后呢，注意，这个时候跟刚才稍稍有点区别。这次触发Minor GC后，会将Eden区与From区还在被使用的对象复制到To区，再下一次Minor GC的时候，则是将Eden区与To区中的还在被使用的对象复制到From区。经过若干次Minor GC后，有些对象在From与To之间来回游荡，这时候From区与To区亮出了底线（阈值），这些家伙要是到现在还没挂掉，对不起，一起滚到（复制）老年代吧。老年代经过这么几次折腾，也就扛不住了（空间被用完），好，那就来次集体大扫除（Full GC），也就是全量回收，一起滚蛋吧。全量回收就好比我们刚才比作的大扫除，毕竟动做比较大，成本高，不能跟平时的小型值日（Minor GC）相比，所以如果Full GC使用太频繁的话，无疑会对系统性能产生很大的影响。所以要合理设置年轻代与老年代的大小，尽量减少Full GC的操作 Minor GC触发机制： 当年轻代满时就会触发Minor GC，这里的年轻代满指的是Eden代满，Survivor满不会引发GC。 Full GC触发机制： 调用System.gc()时，系统建议执行Full GC，但是不必然执行 老年代空间不足 永久代空间不足 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、survivor space1（From Space）区向survivor space2（To Space）区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 使用RMI来进行RPC或管理的JDK应用，每小时执行一次Full GC CMS GC时出现promotion failed, concurrent mode failure HotSpot 中 GC 算法的实现通过前两小节对于判断对象生死和垃圾收集算法的介绍，我们已经对虚拟机进行 GC 的流程有了一个大致的了解。但是，在 HotSpot 虚拟机中，高效的实现这些算法也是一个需要考虑的问题。所以，接下来，我们将研究一下 HotSpot 虚拟机到底是如何高效的实现这些算法的，以及在实现中有哪些需要注意的问题。 通过之前的分析，GC 算法的实现流程简单的来说分为以下两步： 找到死掉的对象； 把它清了。 想要找到死掉的对象，我们就要进行可达性分析，也就是从 GC Root 找到引用链的这个操作。 也就是说，进行可达性分析的第一步，就是要枚举 GC Roots，这就需要虚拟机知道哪些地方存放着对象应用。如果每一次枚举 GC Roots 都需要把整个栈上位置都遍历一遍，那可就费时间了，毕竟并不是所有位置都存放在引用呀。所以为了提高 GC 的效率，HotSpot 使用了一种 OopMap 的数据结构，OopMap 记录了栈上本地变量到堆上对象的引用关系，也就是说，GC 的时候就不用遍历整个栈只遍历每个栈的 OopMap 就行了。 在 OopMap 的帮助下，HotSpot 可以快速准确的完成 GC 枚举了，不过，OopMap 也不是万年不变的，它也是需要被更新的，当内存中的对象间的引用关系发生变化时，就需要改变 OopMap 中的相应内容。可是能导致引用关系发生变化的指令非常之多，如果我们执行完一条指令就改下 OopMap，这 GC 成本实在太高了。 因此，HotSpot 采用了一种在 “安全点” 更新 OopMap 的方法，安全点的选取既不能让 GC 等待的时间过长，也不能过于频繁增加运行负担，也就是说，我们既要让程序运行一段时间，又不能让这个时间太长。我们知道，JVM 中每条指令执行的是很快的，所以一个超级长的指令流也可能很快就执行完了，所以 真正会出现 “长时间执行” 的一般是指令的复用，例如：方法调用、循环跳转、异常跳转等，虚拟机一般会将这些地方设置为安全点更新 OopMap 并判断是否需要进行 GC 操作。 此外，在进行枚举根节点的这个操作时，为了保证准确性，我们需要在一段时间内 “冻结” 整个应用，即 Stop The World（传说中的 GC 停顿），因为如果在我们分析可达性的过程中，对象的引用关系还在变来变去，那是不可能得到正确的分析结果的。即便是在号称几乎不会发生停顿的 CMS 垃圾收集器中，枚举根节点时也是必须要停顿的。这里就涉及到了一个问题： 我们让所有线程跑到最近的安全点再停顿下来进行 GC 操作呢？ 主要有以下两种方式： 抢先式中断： 先中断所有线程； 发现有线程没中断在安全点，恢复它，让它跑到安全点。 主动式中断： (主要使用) 设置一个中断标记； 每个线程到达安全点时，检查这个中断标记，选择是否中断自己。 除此安全点之外，还有一个叫做 “安全区域” 的东西，一个一直在执行的线程可以自己 “走” 到安全点去，可是一个处于 Sleep 或者 Blocked 状态的线程是没办法自己到达安全点中断自己的，我们总不能让 GC 操作一直等着这些个 ”不执行“ 的线程重新被分配资源吧。对于这种情况，我们要依靠安全区域来解决。 安全区域是指在一段代码片段之中，引用关系不会发生变化，因此在这个区域中的任意位置开始 GC 都是安全的。 当线程执行到安全区域时，它会把自己标识为 Safe Region，这样 JVM 发起 GC 时是不会理会这个线程的。当这个线程要离开安全区域时，它会检查系统是否在 GC 中，如果不在，它就继续执行，如果在，它就等 GC 结束再继续执行。 本小节我们主要讲述 HotSpot 虚拟机是如何发起内存回收的，也就是如何找到死掉的对象，至于如何清掉这些个对象，HotSpot 将其交给了一堆叫做 ”GC 收集器“ 的东西，这东西又有好多种，不同的 GC 收集器的处理方式不同，适用的场景也不同，我们将在下一小节进行详细讲述。 7 个垃圾收集器垃圾收集器就是内存回收操作的具体实现，HotSpot 里足足有 7 种，为啥要弄这么多，因为它们各有各的适用场景。有的属于新生代收集器，有的属于老年代收集器，所以一般是搭配使用的（除了万能的 G1）。关于它们的简单介绍以及分类请见下图。 Serial / ParNew 搭配 Serial Old 收集器 Serial 收集器是虚拟机在 Client 模式下的默认新生代收集器，它的优势是简单高效，在单 CPU 模式下很牛。ParNew 收集器就是 Serial 收集器的多线程版本，虽然除此之外没什么创新之处，但它却是许多运行在 Server 模式下的虚拟机中的首选新生代收集器，因为除了 Serial 收集器外，只有它能和 CMS 收集器搭配使用。 Parallel 搭配 Parallel Scavenge 收集器首先，这俩货肯定是要搭配使用的，不仅仅如此，它俩还贼特别，它们的关注点与其他收集器不同，其他收集器关注于尽可能缩短垃圾收集时用户线程的停顿时间，而 Parallel Scavenge 收集器的目的是达到一个可控的吞吐量。 吞吐量 = 运行用户代码时间 / ( 运行用户代码时间 + 垃圾收集时间 ) 因此，Parallel Scavenge 收集器不管是新生代还是老年代都是多个线程同时进行垃圾收集，十分适合于应用在注重吞吐量以及 CPU 资源敏感的场合。可调节的虚拟机参数： -XX:MaxGCPauseMillis：最大 GC 停顿的秒数； -XX:GCTimeRatio：吞吐量大小，一个 0 ~ 100 的数，最大 GC 时间占总时间的比率 = 1 / (GCTimeRatio + 1)； -XX:+UseAdaptiveSizePolicy：一个开关参数，打开后就无需手工指定 -Xmn，-XX:SurvivorRatio 等参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，自行调整。 CMS 收集器 参数设置： -XX:+UseCMSCompactAtFullCollection：在 CMS 要进行 Full GC 时进行内存碎片整理（默认开启） -XX:CMSFullGCsBeforeCompaction：在多少次 Full GC 后进行一次空间整理（默认是 0，即每一次 Full GC 后都进行一次空间整理） G1 收集器 GC 日志解读 Java 内存分配策略 优先在 Eden 区分配 Eden 空间不够将会触发一次 Minor GC； 虚拟机参数： -Xmx：Java 堆的最大值； -Xms：Java 堆的最小值； -Xmn：新生代大小； -XX:SurvivorRatio=8：Eden 区 / Survivor 区 = 8 : 1 大对象直接进入老年代 大对象定义： 需要大量连续内存空间的 Java 对象。例如那种很长的字符串或者数组。 设置对象直接进入老年代大小限制： -XX:PretenureSizeThreshold：单位是字节； 只对 Serial 和 ParNew 两款收集器有效。 目的： 因为新生代采用的是复制算法收集垃圾，大对象直接进入老年代可以避免在 Eden 区和 Survivor 区发生大量的内存复制。 长期存活的对象将进入老年代 固定对象年龄判定： 虚拟机给每个对象定义一个年龄计数器，对象每在 Survivor 中熬过一次 Minor GC，年龄 +1，达到 -XX:MaxTenuringThreshold 设定值后，会被晋升到老年代，-XX:MaxTenuringThreshold 默认为 15； 动态对象年龄判定： Survivor 中有相同年龄的对象的空间总和大于 Survivor 空间的一半，那么，年龄大于或等于该年龄的对象直接晋升到老年代。 空间分配担保我们知道，新生代采用的是复制算法清理内存，每一次 Minor GC，虚拟机会将 Eden 区和其中一块 Survivor 区的存活对象复制到另一块 Survivor 区，但当出现大量对象在一次 Minor GC 后仍然存活的情况时，Survivor 区可能容纳不下这么多对象，此时，就需要老年代进行分配担保，即将 Survivor 无法容纳的对象直接进入老年代。 这么做有一个前提，就是老年代得装得下这么多对象。可是在一次 GC 操作前，虚拟机并不知道到底会有多少对象存活，所以空间分配担保有这样一个判断流程： 发生 Minor GC 前，虚拟机先检查老年代的最大可用连续空间是否大于新生代所有对象的总空间； 如果大于，Minor GC 一定是安全的； 如果小于，虚拟机会查看 HandlePromotionFailure 参数，看看是否允许担保失败； 允许失败：尝试着进行一次 Minor GC； 不允许失败：进行一次 Full GC； 不过 JDK 6 Update 24 后，HandlePromotionFailure 参数就没有用了，规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会进行 Minor GC，否则将进行 Full GC。 Metaspace 元空间与 PermGem 永久代Java 8 彻底将永久代 (PermGen) 移除出了 HotSpot JVM，将其原有的数据迁移至 Java Heap 或 Metaspace。 移除 PermGem 的原因： PermGen 内存经常会溢出，引发恼人的 java.lang.OutOfMemoryError: PermGen，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM； 移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。移除 PermGem 后，方法区和字符串常量的位置： 方法区：移至 Metaspace； 字符串常量：移至 Java Heap。Metaspace 的位置： 本地堆内存(native heap)。Metaspace 的优点： 永久代 OOM 问题将不复存在，因为默认的类的元数据分配只受本地内存大小的限制，也就是说本地内存剩余多少，理论上 Metaspace 就可以有多大；JVM参数： -XX:MetaspaceSize：分配给类元数据空间（以字节计）的初始大小，为估计值。MetaspaceSize的值设置的过大会延长垃圾回收时间。垃圾回收过后，引起下一次垃圾回收的类元数据空间的大小可能会变大。 -XX:MaxMetaspaceSize：分配给类元数据空间的最大值，超过此值就会触发Full GC，取决于系统内存的大小。JVM会动态地改变此值。 -XX:MinMetaspaceFreeRatio：一次GC以后，为了避免增加元数据空间的大小，空闲的类元数据的容量的最小比例，不够就会导致垃圾回收。 -XX:MaxMetaspaceFreeRatio：一次GC以后，为了避免增加元数据空间的大小，空闲的类元数据的容量的最大比例，不够就会导致垃圾回收。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件设计模式七大原则]]></title>
    <url>%2F%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%83%E5%A4%A7%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[开闭原则 依赖倒置原则 单一职责原则 接口隔离原则 迪米特法则（最少知道原则） 里氏替换原则 合成/复用原则 开闭原则]]></content>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UML类图]]></title>
    <url>%2FUML%E7%B1%BB%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[#:protected~:默认，包权限下划线：static斜体：抽象方法]]></content>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
</search>
