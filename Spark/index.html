<!DOCTYPE html>




<html class="theme-next gemini" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="大数据,">










<meta name="description" content="MapReduce局限性MapReduce的局限性：1）代码繁琐；2）只能够支持map和reduce方法；3）执行效率低下；4）不适合迭代多次、交互式、流式的处理；">
<meta name="keywords" content="大数据">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark">
<meta property="og:url" content="https://490.github.io/Spark/index.html">
<meta property="og:site_name" content="我的笔记">
<meta property="og:description" content="MapReduce局限性MapReduce的局限性：1）代码繁琐；2）只能够支持map和reduce方法；3）执行效率低下；4）不适合迭代多次、交互式、流式的处理；">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://490.github.io/images/20190718_085900.png">
<meta property="og:image" content="http://490.github.io/images/20190718_090117.png">
<meta property="og:image" content="http://490.github.io/images/20190718_090128.png">
<meta property="og:image" content="https://s1.51cto.com/images/blog/201901/17/0c8047532a232e90e8e99b1783cd1a7d.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=">
<meta property="og:image" content="https://s1.51cto.com/images/blog/201901/17/f4ccb4b3d930c7a332c6f1bbc56a39ea.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=">
<meta property="og:image" content="http://490.github.io/images/20190717_101905.png">
<meta property="og:image" content="http://490.github.io/images/20190717_200118.png">
<meta property="og:image" content="http://490.github.io/images/20190717_200258.png">
<meta property="og:updated_time" content="2019-07-25T02:03:14.847Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark">
<meta name="twitter:description" content="MapReduce局限性MapReduce的局限性：1）代码繁琐；2）只能够支持map和reduce方法；3）执行效率低下；4）不适合迭代多次、交互式、流式的处理；">
<meta name="twitter:image" content="http://490.github.io/images/20190718_085900.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://490.github.io/Spark/">





  <title>Spark | 我的笔记</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">我的笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br>
            
            文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-vcard-o"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-14T14:22:32+08:00">
                2019-07-14
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Spark/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/Spark/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  12.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  47
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="MapReduce局限性"><a href="#MapReduce局限性" class="headerlink" title="MapReduce局限性"></a>MapReduce局限性</h1><p>MapReduce的局限性：<br>1）代码繁琐；<br>2）只能够支持map和reduce方法；<br>3）执行效率低下；<br>4）不适合迭代多次、交互式、流式的处理；<br><a id="more"></a></p>
<p>框架多样化：<br>1）批处理（离线）：MapReduce、Hive、Pig<br>2）流式处理（实时）： Storm、JStorm<br>3）交互式计算：Impala</p>
<h1 id="Hive-Hbase-HDFS等之间的关系"><a href="#Hive-Hbase-HDFS等之间的关系" class="headerlink" title="Hive,Hbase,HDFS等之间的关系"></a>Hive,Hbase,HDFS等之间的关系</h1><p>Hive：</p>
<p>Hive不支持更改数据的操作，Hive基于数据仓库，提供静态数据的动态查询。其使用类SQL语言，底层经过编译转为MapReduce程序，在Hadoop上运行，数据存储在HDFS上。</p>
<p>HDFS:</p>
<p>HDFS是GFS的一种实现，他的完整名字是分布式文件系统，类似于FAT32，NTFS，是一种文件格式，是底层的。</p>
<p>Hive与Hbase的数据一般都存储在HDFS上。Hadoop HDFS为他们提供了高可靠性的底层存储支持。</p>
<p>Hbase:</p>
<p>Hbase是Hadoop database，即Hadoop数据库。它是一个适合于非结构化数据存储的数据库，HBase基于列的而不是基于行的模式。</p>
<p>HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据。</p>
<p>Hadoop HDFS为HBase提供了高可靠性的底层存储支持，Hadoop MapReduce为HBase提供了高性能的计算能力，Zookeeper为HBase提供了稳定服务和failover机制。Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变的非常简单。 Sqoop则为HBase提供了方便的RDBMS（关系型数据库）数据导入功能，使得<a href="http://baike.baidu.com/view/1437981.htm" target="_blank" rel="noopener">传统数据库</a>数据向HBase中迁移变的非常方便。</p>
<p>Pig： </p>
<p>Pig的语言层包括一个叫做PigLatin的文本语言,Pig Latin是面向数据流的编程方式。Pig和Hive类似更侧重于数据的查询和分析，底层都是转化成MapReduce程序运行。</p>
<p>区别是Hive是类SQL的查询语言，要求数据存储于表中，而Pig是面向数据流的一个程序语言。</p>
<h1 id="Spark多种运行模式"><a href="#Spark多种运行模式" class="headerlink" title="Spark多种运行模式"></a>Spark多种运行模式</h1><h2 id="测试或实验性质的本地运行模式（单机）"><a href="#测试或实验性质的本地运行模式（单机）" class="headerlink" title="测试或实验性质的本地运行模式（单机）"></a>测试或实验性质的本地运行模式（单机）</h2><ul>
<li>该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。其中N代表可以使用N个线程，每个线程拥有一个core。如果不指定N，则默认是1个线程（该线程有1个core）。</li>
<li><p><strong>指令示例：</strong></p>
<p>1）spark-shell –master local 效果是一样的<br>2）spark-shell –master local[4] 代表会有4个线程（每个线程一个core）来并发执行应用程序。</p>
</li>
<li><p>运行该模式非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用，而不用启动Spark的Master、Worker守护进程( 只有集群的Standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS），这是和其他模式的区别，要记住才能理解。</p>
</li>
</ul>
<h2 id="测试或实现性质的本地伪集群运行模式（单机模拟集群）"><a href="#测试或实现性质的本地伪集群运行模式（单机模拟集群）" class="headerlink" title="测试或实现性质的本地伪集群运行模式（单机模拟集群）"></a>测试或实现性质的本地伪集群运行模式（单机模拟集群）</h2><ul>
<li><p>这种运行模式，和Local[N]很像，不同的是，它会在单机启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程只能在一个进程下委屈求全的共享资源。通常也是用来验证开发出来的应用程序逻辑上有没有问题，或者想使用Spark的计算框架而没有太多资源。</p>
</li>
<li><p><strong>指令示例：</strong></p>
<p>1）spark-shell –master local-cluster[2, 3, 1024]</p>
</li>
<li><p>用法是：提交应用程序时使用local-cluster[x,y,z]参数：x代表要生成的executor数，y和z分别代表每个executor所拥有的core和memory数。</p>
</li>
<li><p>该模式依然非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用。而不用启动Spark的Master、Worker守护进程( 只有集群的standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS），这是和其他模式的区别哦，要记住才能理解。</p>
</li>
</ul>
<h2 id="spark自带Cluster-Manager的standalone-Client模式（集群）"><a href="#spark自带Cluster-Manager的standalone-Client模式（集群）" class="headerlink" title="spark自带Cluster Manager的standalone Client模式（集群）"></a>spark自带Cluster Manager的standalone Client模式（集群）</h2><p>需要在多台机器上同时部署spark环境</p>
<ul>
<li>和单机运行的模式不同，这里必须在执行应用程序前，先启动Spark的Master和Worker守护进程。不用启动Hadoop服务，除非你用到了HDFS的内容。可以在想要做为Master的节点上用start-all.sh一条命令即可，这种运行模式，可以使用Spark的8080 web ui来观察资源和应用程序的执行情况了。用如下命令提交应用程序</li>
<li><p><strong>指令示例：</strong></p>
<p>1）spark-shell –master spark://wl1:7077<br>或者<br>2）spark-shell –master spark://wl1:7077 –deploy-mode client</p>
</li>
<li><p><strong>产生的进程：</strong></p>
<p>①Master进程做为cluster manager，用来对应用程序申请的资源进行管理<br>②SparkSubmit 做为Client端和运行driver程序<br>③CoarseGrainedExecutorBackend 用来并发执行应用程序</p>
</li>
</ul>
<h2 id="spark-自带Cluster-manager-的Standalone-cluster模式（集群）"><a href="#spark-自带Cluster-manager-的Standalone-cluster模式（集群）" class="headerlink" title="spark 自带Cluster manager 的Standalone cluster模式（集群）"></a>spark 自带Cluster manager 的Standalone cluster模式（集群）</h2><ul>
<li>这种运行模式和上面第3个还是有很大的区别的。使用如下命令执行应用程序</li>
<li><strong>指令示例：</strong></li>
</ul>
<p><code>spark-submit --master spark://wl1:6066 --deploy-mode cluster</code></p>
<p><strong>第4种模式和第3种模型的区别：</strong></p>
<p>①客户端的SparkSubmit进程会在应用程序提交给集群之后就退出</p>
<p>②Master会在集群中选择一个Worker进程生成一个子进程DriverWrapper来启动driver程序</p>
<p>③而该DriverWrapper 进程会占用Worker进程的一个core，所以同样的资源下配置下，会比第3种运行模式，少用1个core来参与计算</p>
<p>④应用程序的结果，会在执行driver程序的节点的stdout中输出，而不是打印在屏幕上</p>
<h2 id="基于YARN的Resource-Manager-的Client-模式（集群）"><a href="#基于YARN的Resource-Manager-的Client-模式（集群）" class="headerlink" title="基于YARN的Resource Manager 的Client 模式（集群）"></a>基于YARN的Resource Manager 的Client 模式（集群）</h2><ul>
<li><p>现在越来越多的场景，都是Spark跑在Hadoop集群中，所以为了做到资源能够均衡调度，会使用YARN来做为Spark的Cluster Manager，来为Spark的应用程序分配资源。在执行Spark应用程序前，要启动Hadoop的各种服务。由于已经有了资源管理器，所以不需要启动Spark的Master、Worker守护进程。</p>
<p>使用如下命令执行应用程序：</p>
</li>
<li><p><strong>指令示例：</strong></p>
</li>
</ul>
<p>1）spark-shell –master yarn<br>或者<br>2）spark-shell –master yarn –deploy-mode client</p>
<ul>
<li>提交应用程序后，各节点会启动相关的JVM进程，如下：</li>
</ul>
<ol>
<li>在Resource Manager节点上提交应用程序，会生成SparkSubmit进程，该进程会执行driver程序。</li>
<li>RM会在集群中的某个NodeManager上，启动一个ExecutorLauncher进程，来做为ApplicationMaster。</li>
<li>另外，RM也会在多个NodeManager上生成CoarseGrainedExecutorBackend进程来并发的执行应用程序。</li>
</ol>
<h2 id="基于YARN的Resource-Manager-的-Cluster-模式（集群）"><a href="#基于YARN的Resource-Manager-的-Cluster-模式（集群）" class="headerlink" title="基于YARN的Resource Manager 的 Cluster 模式（集群）"></a>基于YARN的Resource Manager 的 Cluster 模式（集群）</h2><ul>
<li><strong>指令示例：</strong></li>
</ul>
<p><code>spark-shell --master yarn --deploy-mode cluster</code></p>
<p><strong>和第5种运行模式，区别如下：</strong></p>
<ul>
<li>①在Resource Manager端提交应用程序，会生成SparkSubmit进程，该进程只用来做Client端，应用程序提交给集群后，就会删除该进程。</li>
<li>②Resource Manager在集群中的某个NodeManager上运行ApplicationMaster，该AM同时会执行driver程序。紧接着，会在各NodeManager上运行CoarseGrainedExecutorBackend来并发执行应用程序。</li>
<li>③应用程序的结果，会在执行driver程序的节点的stdout中输出，而不是打印在屏幕上。</li>
</ul>
<p><strong>此外，还有**</strong>Spark On  Mesos<strong>**模式 可以参阅：</strong></p>
<p><a href="http://ifeve.com/spark-mesos-spark/" target="_blank" rel="noopener">http://ifeve.com/spark-mesos-spark/</a></p>
<h1 id="spark中cache和persist的区别"><a href="#spark中cache和persist的区别" class="headerlink" title="spark中cache和persist的区别"></a>spark中cache和persist的区别</h1><p>cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。cache()调用了persist(),cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">object StorageLevel &#123;</span><br><span class="line">  val NONE = new StorageLevel(false, false, false, false)</span><br><span class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</span><br><span class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</span><br><span class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</span><br><span class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</span><br><span class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</span><br><span class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</span><br><span class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</span><br><span class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</span><br><span class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</span><br><span class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</span><br><span class="line">  val OFF_HEAP = new StorageLevel(false, false, true, false)</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>useDisk：使用硬盘（外存）<br>useMemory：使用内存<br>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。<br>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象<br>replication：备份数（在多个节点上备份）</p>
<p>使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xxDF.cache()</span><br><span class="line">使用</span><br><span class="line">xxDF.unpersist(true)</span><br></pre></td></tr></table></figure></p>
<h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>Hive: 类似于sql的Hive QL语言， sql==&gt;mapreduce</p>
<ul>
<li>特点：mapreduce</li>
<li>改进：hive on tez、hive on spark、hive on mapreduce</li>
</ul>
<p>Spark: hive on spark ==&gt; shark(hive on spark)</p>
<ul>
<li>shark推出：欢迎， 基于spark、基于内存的列式存储、与hive能够兼容</li>
<li>缺点：hive ql的解析、逻辑执行计划生成、执行计划的优化是依赖于hive的<pre><code>仅仅只是把物理执行计划从mr作业替换成spark作业
</code></pre></li>
</ul>
<p>Shark终止以后，产生了2个分支：<br>1）hive on spark<br>    Hive社区，源码是在Hive中<br>2）Spark SQL<br>    Spark社区，源码是在Spark中<br>    支持多种数据源，多种优化技术，扩展性好很多</p>
<p>SQL on Hadoop</p>
<ul>
<li><p>Hive<br>  sql ==&gt; mapreduce<br>  metastore ： 元数据<br>  sql：database、table、view<br>  facebook</p>
</li>
<li><p>impala<br>  cloudera ： cdh（建议大家在生产上使用的hadoop系列版本）、cm<br>  sql：自己的守护进程执行的，非mr<br>  metastore</p>
</li>
<li><p>presto<br>  facebook<br>  京东<br>  sql</p>
</li>
<li><p>drill<br>  sql<br>  访问：hdfs、rdbms、json、hbase、mongodb、s3、hive</p>
</li>
<li><p>Spark SQL<br>  sql<br>  dataframe/dataset api<br>  metastore<br>  访问：hdfs、rdbms、json、hbase、mongodb、s3、hive  ==&gt; 外部数据源</p>
</li>
</ul>
<p>Spark SQL is Apache Spark’s module for working with structured data. </p>
<p>有见到SQL字样吗？<br>Spark SQL它不仅仅有访问或者操作SQL的功能，还提供了其他的非常丰富的操作：外部数据源、优化</p>
<p><strong>Spark SQL概述小结：</strong></p>
<ul>
<li>Spark SQL的应用并不局限于SQL；</li>
<li>访问hive、json、parquet等文件的数据；</li>
<li>SQL只是Spark SQL的一个功能而已；<br>===&gt; Spark SQL这个名字起的并不恰当</li>
<li>Spark SQL提供了SQL的api、DataFrame和Dataset的API；</li>
</ul>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>即席查询</p>
<p>将分析结果传到streaming</p>
<p>etl 清洗数据</p>
<p>把外部数据源的数据弄成dataFrame</p>
<p>大规模集群的查询</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p><strong>A、容易集成</strong></p>
<p>SparkSQL将SQL查询与Spark程序无缝对接，它允许用户使用SQL或熟悉的DataFrame API查询Spark程序内的结构化数据，可应用于Java、Scala、Python和R。</p>
<p><strong>B、统一的数据访问方式</strong></p>
<p>可使用同样的方式连接任何数据源，DataFrame和SQL提供了访问各种数据源的常用方式，包括Hive、Avro、Parquet、ORC、JSON和JDBC，甚至可以通过这些数据源直接加载数据。</p>
<p><strong>C、Hive集成</strong></p>
<p>能够在现有数据仓库上运行SQL或HiveSQL查询，SparkSQL支持HiveQL语法以及HiveSerDes（序列化和反序列化工具）和UDF（用户自定义函数），允许用户访问现有的Hive仓库。</p>
<p><strong>D、标准的数据连接</strong></p>
<p>通过JDBC或ODBC进行数据库连接，服务器模式为商业智能工具提供行业标准的JDBC和ODBC数据连接。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p><strong>SQL</strong><br>Spark SQL的一种用法是直接执行SQL查询语句，你可使用最基本的SQL语法，也可以选择HiveQL语法。Spark SQL可以从已有的Hive中读取数据。更详细的请参考<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables" target="_blank" rel="noopener">Hive Tables</a> 这一节。如果用其他编程语言运行SQL，Spark SQL将以<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#DataFrames" target="_blank" rel="noopener">DataFrame</a>返回结果。你还可以通过命令行<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-spark-sql-cli" target="_blank" rel="noopener">command-line</a> 或者 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" target="_blank" rel="noopener">JDBC/ODBC</a> 使用Spark SQL。</p>
<p><strong>DataFrames</strong></p>
<p>DataFrame是一种分布式数据集合，每一条数据都由几个命名字段组成。概念上来说，她和关系型数据库的表 或者 R和Python中的data frame等价，只不过在底层，DataFrame采用了更多优化。DataFrame可以从很多数据源（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">sources</a>）加载数据并构造得到，如：结构化数据文件，Hive中的表，外部数据库，或者已有的RDD。</p>
<p>DataFrame API支持<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/DataFrame.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>, and <a href="http://spark.apache.org/docs/latest/api/R/index.html" target="_blank" rel="noopener">R</a>。</p>
<p><strong>Datasets</strong> </p>
<p>Dataset是Spark-1.6新增的一种API，目前还是实验性的。Dataset想要把RDD的优势（强类型，可以使用lambda表达式函数）和Spark SQL的优化执行引擎的优势结合到一起。Dataset可以由JVM对象构建（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets" target="_blank" rel="noopener">constructed</a> ）得到，而后Dataset上可以使用各种transformation算子（map，flatMap，filter 等）。</p>
<p>Dataset API 对 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a> 和 <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a>的支持接口是一致的，但目前还不支持Python，不过Python自身就有语言动态特性优势（例如，你可以使用字段名来访问数据，row.columnName）。对Python的完整支持在未来的版本会增加进来。</p>
<h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>可以加载为RDD、DataFrame、DataSet<br>可以从本地、云端（HDFS，S3）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将数据加载成RDD</span><br><span class="line">val masterLog = sc.textFile(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-hadoop001.out"</span>)</span><br><span class="line">val workerLog = sc.textFile(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-hadoop001.out"</span>)</span><br><span class="line">val allLog = sc.textFile(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/logs/*out*"</span>)</span><br><span class="line"></span><br><span class="line">masterLog.count</span><br><span class="line">workerLog.count</span><br><span class="line">allLog.count</span><br></pre></td></tr></table></figure>
<p>RDD和DataFrame关联</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row</span><br><span class="line">val masterRDD = masterLog.map(x =&gt; Row(x))</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line">val schemaString = <span class="string">"line"</span></span><br><span class="line"></span><br><span class="line">val fields = schemaString.split(<span class="string">" "</span>).map(fieldName =&gt; StructField(fieldName, StringType, nullable = <span class="keyword">true</span>))</span><br><span class="line">val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">val masterDF = spark.createDataFrame(masterRDD, schema)</span><br><span class="line">masterDF.show</span><br></pre></td></tr></table></figure>
<p>如果格式是Json/Parquet等智能一点的，spark会自动推测出来，不用手动指定schema</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.format(<span class="string">"parquet"</span>).load(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet"</span>)</span><br><span class="line">usersDF.show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"select * from  parquet.`file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet`"</span>).show</span><br></pre></td></tr></table></figure>
<p>从Cloud读取数据: HDFS/S3<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">val hdfsRDD = sc.textFile(<span class="string">"hdfs://path/file"</span>)</span><br><span class="line">val s3RDD = sc.textFile(<span class="string">"s3a://bucket/object"</span>)</span><br><span class="line"></span><br><span class="line">spark.read.format(<span class="string">"text"</span>).load(<span class="string">"hdfs://path/file"</span>)</span><br><span class="line">spark.read.format(<span class="string">"text"</span>).load(<span class="string">"s3a://bucket/object"</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="DataFrame和SQL的功能对比"><a href="#DataFrame和SQL的功能对比" class="headerlink" title="DataFrame和SQL的功能对比"></a>DataFrame和SQL的功能对比</h2><p>DataFrame = RDD + Schema<br>（定义case class，然后用反射的方式自动生成schema）<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Define the schema using a case class.</span></span><br><span class="line"><span class="comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span></span><br><span class="line"><span class="comment">// you can use custom classes that implement the Product interface.</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">Create</span> <span class="title">an</span> <span class="title">RDD</span> <span class="title">of</span> <span class="title">Person</span> <span class="title">objects</span> <span class="title">and</span> <span class="title">register</span> <span class="title">it</span> <span class="title">as</span> <span class="title">a</span> <span class="title">table</span>.</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">people</span> </span>= sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>).map(_.split(<span class="string">","</span>)).map(p =&gt; <span class="type">Person</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim.toInt)).toDF()</span><br><span class="line">people.registerTempTable(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span></span><br><span class="line"><span class="keyword">val</span> teenagers = sqlContext.sql(<span class="string">"SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index:</span></span><br><span class="line">teenagers.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name:</span></span><br><span class="line">teenagers.map(t =&gt; <span class="string">"Name: "</span> + t.getAs[<span class="type">String</span>](<span class="string">"name"</span>)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span></span><br><span class="line">teenagers.map(_.getValuesMap[<span class="type">Any</span>](<span class="type">List</span>(<span class="string">"name"</span>, <span class="string">"age"</span>))).collect().foreach(println)</span><br><span class="line"><span class="comment">// Map("name" -&gt; "Justin", "age" -&gt; 19)</span></span><br></pre></td></tr></table></figure></p>
<p>DataFrame catalyst优化</p>
<p>DataFrame可以处理text，json，parquet等</p>
<p>SQL和API在DF里，都经过了catalyst优化，（不管菜鸡把SQL写成什么样，最后执行效率都一样）</p>
<h2 id="savemode"><a href="#savemode" class="headerlink" title="savemode"></a>savemode</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df=spark.read.format(<span class="string">"json"</span>).load(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line">df.show</span><br><span class="line"></span><br><span class="line"><span class="type">Mode</span>可以有err、overwrite、append、ignore</span><br><span class="line"></span><br><span class="line">df.select(<span class="string">"name"</span>).write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"file:///path"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Spark架构"><a href="#Spark架构" class="headerlink" title="Spark架构"></a>Spark架构</h1><p>Apache Spark是继Hadoop之后的新一代大数据分布式计算框架，它与Hadoop MapReduce的不同之处在于Spark Job的中间结果可以保存在内存中，不再需要读写HDFS，因此它能更好地适用于机器学习、数据挖掘等需要迭代的算法。Spark最初的设计目标是可以更加高效低分析数据，不仅要使分析程序运行速度快，也要能高效地编写程序。为此Spark使用Scala语言编写，Scala语言集成了面向对象编程和函数式编程的各种特性。基于Scala，Spark为用户提供了交互式的编程方式。</p>
<h2 id="Spark中的基本概念："><a href="#Spark中的基本概念：" class="headerlink" title="Spark中的基本概念："></a>Spark中的基本概念：</h2><ul>
<li>RDD：Resilient Distributed Dataset（弹性分布式数据集）是一个分布式对象集合，每个RDD可以分成多个分区（一个数据集片段），并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算。</li>
<li>Transformation：RDD的一种操作方式，用于指定RDD之间的相互依赖关系。</li>
<li>Action：RDD的一种操作方式，用于执行计算并指定输出形式。</li>
<li>DAG：Directed Acyclic Graph（有向无环图），反映RDD之间的依赖关系。</li>
<li>Executor：运行在Worker Node（工作节点）上的一个进程，为应用程序存储数据。</li>
<li>任务：Task，运行在Executor上的工作单元。</li>
<li>作业：Job，一个作业包含多个RDD及作用于相应RDD上的各种操作，一个action对应一个job。</li>
<li>阶段：Stage，调度作业的基本单位，一个作业分为多组任务，每组任务被称为“阶段”或 “任务集”。xxxbykey会把一个stage分成两个。</li>
<li>Application：1 driver + n executors</li>
<li>Driver program：运行main的程序</li>
<li>Deploy mode：driver进程运行在哪里。cluster、client</li>
</ul>
<h2 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h2><p><img src="http://490.github.io/images/20190718_085900.png" alt="image"></p>
<p>在Spark中，一个Application由一个任务控制节点（Driver）和若干个作业构成，一个作业由多个阶段构成，一个阶段由多个任务组成。如图2.4所示Spark的基本运行流程如下：</p>
<ul>
<li><p>（1）当一个Spark应用被提交时，首先需要为这个应用建立基本运行环境，即由任务控制节点创建一个SparkContext，由SparkContext负责和资源管理器通信以及申请运行Executor的资源、分配任务、监控等。</p>
</li>
<li><p>（2）资源管理器为Executor分配资源并启动Executor进程，其运行情况将随着“心跳”发送到资源管理器上。</p>
</li>
<li><p>（3）SparkContext根据弹性数据集的依赖关系构建DAG图，DAG调度器将解析DAG图，把其分解成多个“阶段”，并计算出各个阶段之间的依赖关系，然后把每个“任务集”提交给底层的任务调度器（Task Scheduler）进行处理。</p>
</li>
<li><p>（4）Executor向SparkContext申请任务，任务调度器将任务分发给Executor运行，任务执行结果将反馈给任务调度器，随后传递给DAG调度器，程序运行完毕后写入数据库并释放资源。</p>
</li>
</ul>
<h2 id="RDD结构"><a href="#RDD结构" class="headerlink" title="RDD结构"></a>RDD结构</h2><p>RDD是一个抽象类<br>带泛型的，可以支持多种类型：String，Person，User</p>
<p>（1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p>
<p><code>def compute()</code></p>
<p>（2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。<br><code>def getPartitions()</code></p>
<p>（3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p>
<p><code>def getDependencies()</code></p>
<p>（4）（可选）一个Partitioner，即RDD的分片函数，分区策略。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。<strong>只有对于于key-value的RDD，才会有Partitioner</strong>，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p>
<p><code>def getPreferredLocations()</code></p>
<p>（5）（可选）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，数据在哪优先把作业调度到数据所在的节点进行计算。</p>
<p><code>val partitioner:Option[Partitioner] = None</code></p>
<p><strong>RDD的特点</strong> </p>
<ol>
<li>是一个分区的只读记录的集合； </li>
<li>一个具有容错机制的特殊集； </li>
<li>只能通过在稳定的存储器或其他RDD上的确定性操作（转换）来创建； </li>
<li>可以分布在集群的节点上，以函数式操作集合的方式，进行各种并行操作</li>
</ol>
<p><strong>RDD之所以为“弹性”的特点</strong></p>
<ol>
<li>基于Lineage的高效容错（第n个节点出错，会从第n-1个节点恢复，血统容错）； </li>
<li>Task如果失败会自动进行特定次数的重试（默认4次）； </li>
<li>Stage如果失败会自动进行特定次数的重试（可以值运行计算失败的阶段），只计算失败的数据分片； </li>
<li>数据调度弹性：DAG TASK 和资源管理无关； </li>
<li>checkpoint； </li>
<li>自动的进行内存和磁盘数据存储的切换；</li>
</ol>
<p><strong>窄依赖和宽依赖</strong></p>
<ul>
<li>窄依赖（pipeline-able）：一个父RDD的partition至多被子RDD的某个partition使用一次。 </li>
<li>宽依赖（shuffle）：会被子RDD的partition使用多次。</li>
</ul>
<p>首先，从计算过程来看，窄依赖是数据以管道方式经一系列计算操作可以运行在了一个集群节点上，如（map、filter等），宽依赖则可能需要将数据通过跨节点传递后运行（如group<strong>ByKey</strong>），有点类似于MR的shuffle过程。 </p>
<p><strong>shuffle会分成两个stage</strong>。</p>
<p>其次，从失败恢复来看，窄依赖的失败恢复起来更高效，因为它只需找到父RDD的一个对应分区即可，而且可以在不同节点上并行计算做恢复；宽依赖则牵涉到父RDD的多个分区，恢复起来相对复杂些。</p>
<h3 id="创建RDD的三种方式"><a href="#创建RDD的三种方式" class="headerlink" title="创建RDD的三种方式"></a>创建RDD的三种方式</h3><p>在RDD中，通常就代表和包含了Spark应用程序的输入源数据。<br>当我们，在创建了初始的RDD之后，才可以通过Spark Core提供的transformation算子，对该RDD进行transformation(转换)操作，来获取其他的RDD。<br>Spark Core为我们提供了三种创建RDD的方式，包括： </p>
<ol>
<li>使用程序中的集合创建RDD </li>
<li>使用本地文件创建RDD </li>
<li>使用HDFS文件创建RDD</li>
</ol>
<p><strong>应用场景</strong> </p>
<ol>
<li>使用程序中的集合创建RDD，主要用于进行测试，可以在实际部署到集群运行之前，自己使用集合构造测试数据，来测试后面的spark应用的流程 </li>
<li>使用本地文件创建RDD，主要用于的场景为：在本地临时性地处理一些存储了大量数据的文件 </li>
<li>使用HDFS文件创建RDD，应该是最常用的生产环境处理方式，主要可以针对HDFS上存储的大数据，进行离线批处理操作</li>
</ol>
<p><strong>并行化创建RDD</strong></p>
<p>如果要通过并行化集合来创建RDD，需要针对程序中的集合，调用SparkContext中的parallelize()方法。Spark会将集合中的数据拷贝到集群上去，形成一个分布式的数据集合，也就是一个RDD。即：集合中的部分数据会到一个节点上，而另一部分数据会到其它节点上。然后就可以采用并行的方式来操作这个分布式数据集合。</p>
<p>在调用parallelize()方法时，有一个重要的参数可以指定，就是要将集合切分成多少个partition。Spark会为每一个partition运行一个task来进行处理。Spark官方的建议是，为集群中的每个CPU创建2-4个partition。Spark默认会根据集群的情况来设置partition的数量。但是也可以在调用parallelize()方法时，传入第二个参数，来设置RDD的partition数量。比如，parallelize(arr, 10)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 并行化创建RDD部分代码 </span></span><br><span class="line"><span class="comment">// 实现1到5的累加求和</span></span><br><span class="line"><span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(arr)</span><br><span class="line"><span class="keyword">val</span> sum = rdd.reduce(_ + _)</span><br></pre></td></tr></table></figure>
<p><strong>使用textFile方法，通过本地文件或HDFS创建RDD</strong></p>
<p>Spark是支持使用任何Hadoop支持的存储系统上的文件创建RDD的，比如说HDFS、Cassandra、HBase以及本地文件。通过调用SparkContext的textFile()方法，可以针对本地文件或HDFS文件创建RDD。</p>
<p><strong>一些特例的方法来创建RDD</strong></p>
<ul>
<li>SparkContext的wholeTextFiles()方法，可以针对一个目录中的大量小文件，返回由（fileName,fileContent）组成的pair，即pairRDD，而不是普通的RDD。该方法返回的是文件名字和文件中的具体内容；而普通的textFile()方法返回的RDD中，每个元素就是文本中一行文本。</li>
<li>SparkContext的sequenceFileK,V方法，可以针对SequenceFile创建RDD，K和V泛型类型就是SequenceFile的key和value的类型。K和V要求必须是Hadoop的序列化机制，比如IntWritable、Text等。</li>
<li>SparkContext的hadoopRDD()方法，对于Hadoop的自定义输入类型，可以创建RDD。该方法接收JobConf、InputFormatClass、Key和Value的Class。</li>
<li>SparkContext的objectFile()方法，可以针对之前调用的RDD的saveAsObjectFile()创建的对象序列化的文件，反序列化文件中的数据，并创建一个RDD。</li>
</ul>
<h2 id="spark-context"><a href="#spark-context" class="headerlink" title="spark context"></a>spark context</h2><p>连接到Spark“集群”， 通过sparkcontext来创建RDD、广播变量到集群</p>
<p>在创建sparkcontext之前需要创建一个sparkconf对象，</p>
<h1 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h1><p>Spark Streaming是Spark的核心组件之一，它可结合批处理和交互查询，提高了Spark处理大规模流式数据的能力，适合一些需要对历史数据和实时数据结合分析的应用场景。Spark Streaming可使用多种输入数据源，如Apache HDFS、Apache Kafka、Apache Flume，甚至包括Socket套接字。经处理后的数据可以存储至数据库、文件系统或者显式地输出。Spark Streaming的基本原理是把实时输入数据流以秒级时间片为单位进行拆分，然后以类似批处理的方式利用Spark引擎处理每个时间片数据，执行流程如图</p>
<p><img src="http://490.github.io/images/20190718_090117.png" alt="image"></p>
<p>Spark Streaming主要的抽象是DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按照时间片（如1秒）分成一段段的DStream，每一段数据转换为Spark中的RDD，对DStream的操作都最终转变为对相应的RDD的操作。图2.6展示了当进行单词统计时，每个时间片的数据（存储句子的RDD）经过flatMap操作，生成了存储单词的RDD。</p>
<p><img src="http://490.github.io/images/20190718_090128.png" alt="image"></p>
<h1 id="YARN架构"><a href="#YARN架构" class="headerlink" title="YARN架构"></a>YARN架构</h1><p>1 RM(ResourceManager) + N NM(NodeManager)</p>
<p>ResourceManager的职责： 一个集群active状态的RM只有一个，负责整个集群的资源管理和调度</p>
<ul>
<li>1）处理客户端的请求(启动/杀死)</li>
<li>2）启动/监控ApplicationMaster(一个作业对应一个AM)</li>
<li>3）监控NM</li>
<li>4）系统的资源分配和调度</li>
</ul>
<p>NodeManager：整个集群中有N个，负责单个节点的资源管理和使用以及task的运行情况</p>
<ul>
<li>1）定期向RM汇报本节点的资源使用请求和各个Container的运行状态</li>
<li>2）接收并处理RM的container启停的各种命令</li>
<li>3）单个节点的资源管理和任务管理</li>
</ul>
<p>ApplicationMaster：每个应用/作业对应一个，负责应用程序的管理</p>
<ul>
<li>1）数据切分</li>
<li>2）为应用程序向RM申请资源(container)，并分配给内部任务</li>
<li>3）与NM通信以启停task， task是运行在container中的</li>
<li>4）task的监控和容错</li>
</ul>
<p>Container：<br>对任务运行情况的描述：cpu、memory、环境变量</p>
<p>YARN执行流程</p>
<ul>
<li>1）用户向YARN提交作业</li>
<li>2）RM为该作业分配第一个container(AM)</li>
<li>3）RM会与对应的NM通信，要求NM在这个container上启动应用程序的AM</li>
<li>4)  AM首先向RM注册，然后AM将为各个任务申请资源，并监控运行情况</li>
<li>5）AM采用轮训的方式通过RPC协议向RM申请和领取资源</li>
<li>6）AM申请到资源以后，便和相应的NM通信，要求NM启动任务</li>
<li>7）NM启动我们作业对应的task</li>
</ul>
<p>YARN环境搭建<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mapred-site.xml</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">yarn-site.xml</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>启动yarn：<code>sbin/start-yarn.sh</code></p>
<p>验证是否启动成功<br><code>jps</code><br><code>ResourceManager</code><br><code>NodeManager</code></p>
<p>web: <a href="http://hadoop001:8088" target="_blank" rel="noopener">http://hadoop001:8088</a></p>
<p>停止yarn： <code>sbin/stop-yarn.sh</code></p>
<p>提交mr作业到yarn上运行： wc</p>
<p><code>/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar</code></p>
<p><code>hadoop jar /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /input/wc/hello.txt /output/wc/</code></p>
<h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><ul>
<li>Flume是一个分布式的、可靠的、高可用的海量日志采集、聚合和传输的系统</li>
<li>数据流模型：Source-Channel-Sink</li>
<li>事务机制保证消息传递的可靠性</li>
<li>内置丰富插件，轻松与其他系统集成</li>
<li>Java实现，优秀的系统框架设计，模块分明，易于开发</li>
</ul>
<h2 id="Flume基本组件"><a href="#Flume基本组件" class="headerlink" title="Flume基本组件"></a>Flume基本组件</h2><ul>
<li>Event：消息的基本单位，有header和body组成</li>
<li><p>Agent：JVM进程，负责将一端外部来源产生的消息转 发到另一端外部的目的地</p>
<ul>
<li>Source：   从数据发生器接收数据,并将接收的数据以Flume的event格式传递给一个或者多个通道channal,Flume提供多种数据接收的方式,比如Avro,Thrift,twitter1%,log4j等</li>
<li>Channel： channal是一种短暂的存储容器,它将从source处接收到的event格式的数据缓存起来,直到它们被sinks消费掉,它在source和sink间起着一共桥梁的作用,channal是一个完整的事务,这一点保证了数据在收发的时候的一致性. 并且它可以和任意数量的source和sink链接. 支持的类型有: JDBC channel , File System channel , Memort channel等.</li>
<li>Sink：sink将数据存储到集中存储器比如Hbase和HDFS,它从channals消费数据(events)并将其传递给目标地. 目标地可能是另一个source,也可能HDFS,HBase</li>
</ul>
</li>
</ul>
<h2 id="Flume插件"><a href="#Flume插件" class="headerlink" title="Flume插件:"></a>Flume插件:</h2><ol>
<li>Interceptors拦截器： 用于source和channel之间,用来更改或者检查Flume的events数据</li>
<li>管道选择器 channels Selectors： 在多管道是被用来选择使用那一条管道来传递数据(events). 管道选择器又分为如下两种:</li>
</ol>
<ul>
<li>默认管道选择器:  每一个管道传递的都是相同的events</li>
<li>多路复用通道选择器:  依据每一个event的头部header的地址选择管道.</li>
</ul>
<ol start="3">
<li>sink线程：用于激活被选择的sinks群中特定的sink,用于负载均衡.</li>
</ol>
<h2 id="flume典型的使用场景"><a href="#flume典型的使用场景" class="headerlink" title="flume典型的使用场景"></a>flume典型的使用场景</h2><h3 id="多代理流"><a href="#多代理流" class="headerlink" title="多代理流"></a>多代理流</h3><p><img src="https://s1.51cto.com/images/blog/201901/17/0c8047532a232e90e8e99b1783cd1a7d.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt="flume"><br>从第一台机器的flume agent传送到第二台机器的flume agent。<br>例：<br><strong>规划</strong>：<br>hadoop02：tail-avro.properties<br>   使用 exec “tail -F /home/hadoop/testlog/welog.log”获取采集数据<br>   使用 avro sink 数据都下一个 agent<br>hadoop03：avro-hdfs.properties<br>   使用 avro 接收采集数据<br>   使用 hdfs sink 数据到目的地</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#tail-avro.properties</span><br><span class="line">a1.sources = r1 </span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line">#Describe/configure the source </span><br><span class="line">a1.sources.r1.type = exec </span><br><span class="line">a1.sources.r1.command = tail -F /home/hadoop/testlog/date.log </span><br><span class="line">a1.sources.r1.channels = c1 </span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro </span><br><span class="line">a1.sinks.k1.channel = c1 </span><br><span class="line">a1.sinks.k1.hostname = hadoop02 </span><br><span class="line">a1.sinks.k1.port = <span class="number">4141</span> </span><br><span class="line">a1.sinks.k1.batch-size = <span class="number">2</span></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = <span class="number">1000</span></span><br><span class="line">a1.channels.c1.transactionCapacity = <span class="number">100</span></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#avro-hdfs.properties</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line">#Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span>.0.0</span><br><span class="line">a1.sources.r1.port = <span class="number">4141</span></span><br><span class="line">#Describe k1</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path =hdfs:<span class="comment">//myha01/testlog/flume-event/%y-%m-%d/%H-%M</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = date_</span><br><span class="line">a1.sinks.k1.hdfs.maxOpenFiles = <span class="number">5000</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize= <span class="number">100</span></span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = <span class="number">102400</span></span><br><span class="line">a1.sinks.k1.hdfs.rollCount = <span class="number">1000000</span></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.round = <span class="keyword">true</span></span><br><span class="line">a1.sinks.k1.hdfs.roundValue = <span class="number">10</span></span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="keyword">true</span></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = <span class="number">1000</span></span><br><span class="line">a1.channels.c1.transactionCapacity = <span class="number">100</span></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<h3 id="多路复用采集"><a href="#多路复用采集" class="headerlink" title="多路复用采集"></a>多路复用采集</h3><p><img src="https://s1.51cto.com/images/blog/201901/17/f4ccb4b3d930c7a332c6f1bbc56a39ea.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt="flume"></p>
<p>在一份agent中有多个channel和多个sink，然后多个sink输出到不同的文件或者文件系统中。<br>规划：<br>Hadoop02：（tail-hdfsandlogger.properties）<br>   使用 exec “tail -F /home/hadoop/testlog/datalog.log”获取采集数据<br>   使用 sink1 将数据 存储hdfs<br>   使用 sink2 将数据都存储 控制台</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#tail-hdfsandlogger.properties</span><br><span class="line">#2个channel和2个sink的配置文件</span><br><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line">#Describe/configure tail -F source1</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /home/hadoop/logs/catalina.out</span><br><span class="line">#指定source进行扇出到多个channnel的规则</span><br><span class="line">a1.sources.s1.selector.type = replicating</span><br><span class="line">a1.sources.s1.channels = c1 c2</span><br><span class="line"></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">#指定channel c1</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">#指定channel c2</span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line"></span><br><span class="line">#Describe the sink</span><br><span class="line">#指定k1的设置</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs:<span class="comment">//myha01/flume_log/%y-%m-%d/%H-%M</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events</span><br><span class="line">a1.sinks.k1.hdfs.maxOpenFiles = <span class="number">5000</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize= <span class="number">100</span></span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = <span class="number">102400</span></span><br><span class="line">a1.sinks.k1.hdfs.rollCount = <span class="number">1000000</span></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = <span class="number">60</span></span><br><span class="line">a1.sinks.k1.hdfs.round = <span class="keyword">true</span></span><br><span class="line">a1.sinks.k1.hdfs.roundValue = <span class="number">10</span></span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="keyword">true</span></span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">#指定k2的</span><br><span class="line">a1.sinks.k2.type = logger</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>
<h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ol>
<li>kafka作为集群运行在一个或者多个服务器上</li>
<li>kafka集群存储的消息是以topic为类别记录的</li>
<li>kafka存储的消息是k-v键值对，k是offset偏移量，v就是消息的内容</li>
<li>topic：kafka将消息分门别类，每一类的消息称之为topic</li>
<li>broker：已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker). 消费者可以订阅一个或多个主题（topic），并从Broker拉数据，从而消费这些已发布的消息。</li>
<li>消息：kafka会保存消息直到它过期，无论是否被消费了。</li>
<li>producer：发布消息的对象，往某个topic中发布消息，也负责选择发布到topic中的哪个分区</li>
<li>consumer：订阅消息并处理发布的消息的对象</li>
<li>patition：topic是逻辑上的概念，patition是物理概念。每个分区都是一个顺序的，不可变的消息队列，并且可以持续添加，producer生产的消息都会append到队列的末尾，而不是随机读写的。分区中的消息都会被分了一个序列号，这个序列号在分区内是唯一的，也就是分区内的偏移量。</li>
<li>如何消费：<br>kafka的生产者没有保持消息消费的顺序，消费的顺序是通过偏移量交给消费者的，消费者持有的元数据就是消息的offset，消费者通过控制offset的移动来决定读取哪里的消息。正常情况下，当消费者消费消息的时候，偏移量是线性增长的。如果消费者想要重新读取数据的时候，就需要将偏移量向前移动。</li>
<li>为什么说是分布式和冗余备份的：<br>分区被分布到集群中的各个服务器中，每个服务器处理它所拥有的分区。根据配置，每个分区还可以复制到其他服务器作为备份容错。每个分区拥有一个leader，有一个或者多个follower（冗余备份的）。一个broker可以是一个分区的leader,同时也可以是别的分区的follwer，避免了所有的请求只让一个或者几个服务器处理，负载均衡。<br>某个broker如果是一个分区的leader，那么它处理这个分区上的所有读写请求，而follower分区被动的复制数据。如果leader宕机，则follower就可以被推举为leader。</li>
<li>为什么说是持久性的：<br>kafka使用文件存储消息，并且会保存所有消息直到它过期，无论是否消费。</li>
<li><p>consumer和topic的关系<br><img src="http://490.github.io/images/20190717_101905.png" alt="image"><br>这个kafka集群中有两个broker，broker1下有partition0和partition3，broker2下有partition1和partition2。<br>有两个消费者集群，消费者集群A拥有两个消费者C1和C2，消费者集群B拥有四个消费者C3,C4,C5,C6。<br>每个partition只能被一个消费者集群中的一个消费者消费，比如broker1中partition0，只能被Consumer GroupA中的C1消费，只能被Consumer GroupB中的C3消费，kafak会确保这个消费者是这个partition的唯一消费者。<br>因为偏移量的唯一值是基于一个分区内的，producer生产的消息按照一定的算法分配到不同的分区，在各个分区内部，偏移量是线性增长的，所以在一个分区内消费消息是可以保持顺序的。但是如果topic里有多个partition的话，那么不能保证全局的消息是顺序的。<br>一个消费组中有多个消费者可以提高消费消息的并发性，并且当partition的消费者出现故障，那么这个partition可以分配给同组的其他消费者，从而提高他的容错性。<br>因为一个partition只能被一个同组的消费者消费，所以当同组中的消费者数量多于partition的数量时，注定有消费者无法消费partition。<br>每个消费者可以消费一个到多个partition。</p>
</li>
<li><p>发布订阅<br>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。发布到topic的消息会被所有订阅者消费。消费端为拉模型，消费状态和订阅关系由客户端负责维护，消息消费完后不会立即删除，会保留历史消息。</p>
</li>
</ol>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>（1）作为消息队列。因为Kafka具有高吞吐量且拥有消息主题分区、备份、容错等特性，使得它适合使用在大规模、高强度的消息数据处理的系统中。</p>
<p>（2）作为流式计算数据源。Kafka消息数据的生产者为流数据产生系统，将数据流分发给Kafka topic，随后Storm、Spark Streaming等流数据计算系统可以实时消费并计算数据。</p>
<p>（3）作为系统用户行为数据源。此时系统将用户的行为例如访问网站、搜索记录、兴趣标签、网页停留时间等数据实时或者周期性的发布到Kafka topic，作为对接系统数据的来源。</p>
<p>（4）作为事件源。在基于事件驱动的系统中，事件可以设计成合理的格式，作为 Kafka 消息数据存储起来，以便相应系统模块做定期或实时处理。Kafka支持大数据量存储、具有备份和容错机制的特性可以让事件驱动型系统更加高效健壮。</p>
<h2 id="震惊了！原来这才是kafka！"><a href="#震惊了！原来这才是kafka！" class="headerlink" title="震惊了！原来这才是kafka！"></a><a href="https://www.jianshu.com/p/d3e963ff8b70" target="_blank" rel="noopener">震惊了！原来这才是kafka！</a></h2><p>大概用法就是，Producers往Brokers里面的指定Topic中写消息，Consumers从Brokers里面拉去指定Topic的消息，然后进行业务处理。<br>图中有两个topic，topic 0有两个partition，topic 1有一个partition，三副本备份。可以看到consumer gourp 1中的consumer 2没有分到partition处理，这是有可能出现的，下面会讲到。</p>
<p>关于broker、topics、partitions的一些元信息用zk来存，监控和路由啥的也都会用到zk。</p>
<p><img src="http://490.github.io/images/20190717_200118.png" alt="image"></p>
<h3 id="生产过程"><a href="#生产过程" class="headerlink" title="生产过程"></a>生产过程</h3><p><img src="http://490.github.io/images/20190717_200258.png" alt="image"></p>
<p>创建一条记录，记录中一个要指定对应的topic和value，key和partition可选。 先序列化，然后按照topic和partition，放进对应的发送队列中。kafka produce都是批量请求，会积攒一批，然后一起发送，不是调send()就进行立刻进行网络发包。<br>如果partition没填，那么情况会是这样的：</p>
<ol>
<li>key有填<br>按照key进行哈希，相同key去一个partition。（如果扩展了partition的数量那么就不能保证了）</li>
<li>key没填<br>round-robin来选partition</li>
</ol>
<p>这些要发往同一个partition的请求按照配置，攒一波，然后由一个单独的线程一次性发过去。</p>
<h3 id="消费过程"><a href="#消费过程" class="headerlink" title="消费过程"></a>消费过程</h3><p>订阅topic是以一个消费组来订阅的，一个消费组里面可以有多个消费者。同一个消费组中的两个消费者，不会同时消费一个partition。换句话来说，<strong>就是一个partition，只能被消费组里的一个消费者消费</strong>，但是可以同时被多个消费组消费。因此，如果消费组内的消费者如果比partition多的话，那么就会有个别消费者一直空闲。</p>
<h3 id="消息投递语义"><a href="#消息投递语义" class="headerlink" title="消息投递语义"></a>消息投递语义</h3><p>At most once：最多一次，消息可能会丢失，但不会重复<br>At least once：最少一次，消息不会丢失，可能会重复<br>Exactly once：只且一次，消息不丢失不重复，只且消费一次（0.11中实现，仅限于下游也是kafka）</p>
<h1 id="为什么要集成Flume和Kafka"><a href="#为什么要集成Flume和Kafka" class="headerlink" title="为什么要集成Flume和Kafka"></a>为什么要集成Flume和Kafka</h1><p>一般使用Flume+Kafka架构都是希望完成实时流式的日志处理，后面再连接上Flink/Storm/Spark Streaming等流式实时处理技术，从而完成日志实时解析的目标。第一、如果Flume直接对接实时计算框架，当数据采集速度大于数据处理速度，很容易发生数据堆积或者数据丢失，而kafka可以当做一个消息缓存队列，从广义上理解，把它当做一个数据库，可以存放一段时间的数据。第二、Kafka属于中间件，一个明显的优势就是使各层解耦，使得出错时不会干扰其他组件。</p>
<p>因此数据从数据源到flume再到Kafka时，数据一方面可以同步到HDFS做离线计算，另一方面可以做实时计算，可实现数据多分发。</p>
<ol>
<li>Kafka是pull based, 如果你有很多下游的Data Consumer，用Kafka；</li>
<li>Kafka有Replication，Flume没有，如果要求很高的容错性(Data High Availability)，选kafka；</li>
<li>需要更好的Hadoop类产品接口，例如HDFS，HBase等，用Flume。</li>
</ol>
<h1 id="Spark-Streaming-整合FLume"><a href="#Spark-Streaming-整合FLume" class="headerlink" title="Spark Streaming 整合FLume"></a>Spark Streaming 整合FLume</h1><p>采用推模式：推模式的理解就是Flume作为缓存，存有数据。监听对应端口，如果服务可以链接，就将数据push过去。(简单，耦合要低)，缺点是SparkStreaming 程序没有启动的话，Flume端会报错，同时可能会导致Spark Streaming 程序来不及消费的情况。</p>
<p>采用拉模式：拉模式就是自己定义一个sink，SparkStreaming自己去channel里面取数据，根据自身条件去获取数据，稳定性好。</p>
<h2 id="poll方式"><a href="#poll方式" class="headerlink" title="poll方式"></a>poll方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">配置文件</span><br><span class="line">simple-agent.sources = netcat-source</span><br><span class="line">simple-agent.sinks = avro-sink</span><br><span class="line">simple-agent.channels = memory-channel</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.type = netcat</span><br><span class="line">simple-agent.sources.netcat-source.bind = hadoop000</span><br><span class="line">simple-agent.sources.netcat-source.port = 44444</span><br><span class="line"></span><br><span class="line">simple-agent.sinks.avro-sink.type = avro</span><br><span class="line">simple-agent.sinks.avro-sink.hostname = hadoop000</span><br><span class="line">simple-agent.sinks.avro-sink.port = 41414</span><br><span class="line"></span><br><span class="line">simple-agent.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.channels = memory-channel</span><br><span class="line">simple-agent.sinks.avro-sink.channel = memory-channel</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlumePushWordCount</span> </span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(args.length != <span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: FlumePushWordCount &lt;hostname&gt; &lt;port&gt;"</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> <span class="type">Array</span>(hostname, port) = args</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>() <span class="comment">//.setMaster("local[2]").setAppName("FlumePushWordCount")</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO... 如何使用SparkStreaming整合Flume</span></span><br><span class="line">    <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createStream(ssc, hostname, port.toInt)</span><br><span class="line"></span><br><span class="line">    flumeStream.map(x=&gt; <span class="keyword">new</span> <span class="type">String</span>(x.event.getBody.array()).trim)</span><br><span class="line">      .flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="pull方式"><a href="#pull方式" class="headerlink" title="pull方式"></a>pull方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">配置文件</span><br><span class="line">simple-agent.sources = netcat-source</span><br><span class="line">simple-agent.sinks = spark-sink</span><br><span class="line">simple-agent.channels = memory-channel</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.type = netcat</span><br><span class="line">simple-agent.sources.netcat-source.bind = hadoop000</span><br><span class="line">simple-agent.sources.netcat-source.port = 44444</span><br><span class="line"></span><br><span class="line">就这里不一样，需要一个包</span><br><span class="line">simple-agent.sinks.spark-sink.type = org.apache.spark.streaming.flume.sink.SparkSink</span><br><span class="line">simple-agent.sinks.spark-sink.hostname = hadoop000</span><br><span class="line">simple-agent.sinks.spark-sink.port = 41414</span><br><span class="line"></span><br><span class="line">simple-agent.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.channels = memory-channel</span><br><span class="line">simple-agent.sinks.spark-sink.channel = memory-channel</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlumePullWordCount</span> </span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">if</span>(args.length != <span class="number">2</span>) </span><br><span class="line">    &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: FlumePullWordCount &lt;hostname&gt; &lt;port&gt;"</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> <span class="type">Array</span>(hostname, port) = args</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>() <span class="comment">//.setMaster("local[2]").setAppName("FlumePullWordCount")</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO... 如何使用SparkStreaming整合Flume</span></span><br><span class="line">    <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createPollingStream(ssc, hostname, port.toInt)</span><br><span class="line"></span><br><span class="line">    flumeStream.map(x=&gt; <span class="keyword">new</span> <span class="type">String</span>(x.event.getBody.array()).trim)</span><br><span class="line">      .flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Spark-Streaming-整合Kafka"><a href="#Spark-Streaming-整合Kafka" class="headerlink" title="Spark Streaming 整合Kafka"></a>Spark Streaming 整合Kafka</h1><h2 id="Receiver模式-又称kafka高级api模式"><a href="#Receiver模式-又称kafka高级api模式" class="headerlink" title="Receiver模式 又称kafka高级api模式"></a>Receiver模式 又称kafka高级api模式</h2><p>构造函数为<code>KafkaUtils.createDstream(ssc,[zk], [consumer group id], [per-topic,partitions] )</code>使用了receivers来接收数据，利用的是Kafka高层次的消费者api，对于所有的receivers接收到的数据将会保存在Spark executors中，然后通过Spark Streaming启动job来处理这些数据，默认会丢失，可启用WAL日志，它同步将接受到数据保存到分布式文件系统上比如HDFS。 所以数据在出错的情况下可以恢复出来 。</p>
<p>简单的理解就是kafka把消息全部封装好，提供给spark去调用，本来kafka的消息分布在不同的partition上面，相当于做了一步数据合并，在发送给spark，故spark可以设置executor个数去消费这部分数据，效率相对慢一些。</p>
<h2 id="Direct模式-又称kafka低级API模式"><a href="#Direct模式-又称kafka低级API模式" class="headerlink" title="Direct模式 又称kafka低级API模式"></a>Direct模式 又称kafka低级API模式</h2><p>简单的理解就是spark直接从kafka底层中的partition直接获取消息，相对于Receiver模式少了一步，效率更快。但是这样一来spark中的executor的工作的个数就为kafka中的partition一致，设置再多的executor都不工作，同时偏移量也需要自己维护。</p>
<p>不同于Receiver接收数据，这种方式定期地从kafka的topic下对应的partition中查询最新的偏移量，再根据偏移量范围在每个batch里面处理数据，Spark通过调用kafka简单的消费者Api读取一定范围的数据。<br>相比基于Receiver方式有几个优点： </p>
<p><strong>A、简化并行</strong></p>
<p>不需要创建多个kafka输入流，然后union它们，sparkStreaming将会创建和kafka分区一种的rdd的分区数，而且会从kafka中并行读取数据，spark中RDD的分区数和kafka中的分区数据是一一对应的关系。</p>
<p><strong>B、高效</strong></p>
<p>第一种实现数据的零丢失是将数据预先保存在WAL中，会复制一遍数据，会导致数据被拷贝两次，第一次是被kafka复制，另一次是写到WAL中。而没有receiver的这种方式消除了这个问题。 </p>
<p><strong>C、恰好一次语义(Exactly-once-semantics)</strong></p>
<p>Receiver读取kafka数据是通过kafka高层次api把偏移量写入zookeeper中，虽然这种方法可以通过数据保存在WAL中保证数据不丢失，但是可能会因为sparkStreaming和ZK中保存的偏移量不一致而导致数据被消费了多次。EOS通过实现kafka低层次api，偏移量仅仅被ssc保存在checkpoint中，消除了zk和ssc偏移量不一致的问题。缺点是无法使用基于zookeeper的kafka监控工具</p>
<h1 id="map-mapPartition-flatMap-flatMapToPair-方法"><a href="#map-mapPartition-flatMap-flatMapToPair-方法" class="headerlink" title="map, mapPartition, flatMap, flatMapToPair 方法"></a>map, mapPartition, flatMap, flatMapToPair 方法</h1><p><strong>map</strong> 函数会对每一条输入进行指定的操作，然后为每一条输入返回一个对象。</p>
<p><strong>MapPartition </strong> 函数会对每个分区中的一组数据进行相应的操作，并最终返回一个指定对象的迭代器。</p>
<p>建议使用 MapPartition 取代  Map 函数：优点1：对于 一些初始化操作，如果用map 函数可能需要对每一条数据都进行一次调用，而使用  MapPartition  可以一个分区只调用一次初始化操作，资源使用更高效！！优点2：通过mapPartition 可以非常方便的对返回结果进行过滤 （比如错误数据过滤），map 较难实现。</p>
<p> <strong>flatMap</strong> 函数则是两个操作的集合——正是“先映射后扁平化”：</p>
<ul>
<li>同map函数一样：对每一条输入进行指定的操作，然后为每一条输入返回一个对象</li>
<li>最后将所有对象合并为一个对象</li>
</ul>
<p>FlatMap 与 Map 的主要区别 ：</p>
<ul>
<li>Map 主要转换是一条数据 返回 一条数据</li>
<li>FlatMap 将一条数据转换为 一组数据 (迭代器)，主要用于将一条记录转换为多条记录的场景，如对 每行文章中的单词进行切分，返回 每行中所有单词。</li>
</ul>
<p><strong>flatMapToPair</strong>  其实是在FlatMap 函数基础上将返回的数据转换为了 1个Tuple, 即 key-value 格式的数据。方便相同的key 的数据进行后续的统计如统计次数等操作。</p>
<h1 id="Spark-ML"><a href="#Spark-ML" class="headerlink" title="Spark ML"></a>Spark ML</h1><p><a href="机器学习基础">机器学习基础</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/大数据/" rel="tag"># 大数据</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Python/" rel="next" title="Python">
                <i class="fa fa-chevron-left"></i> Python
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/基于开发者关联分析的智能协作/" rel="prev" title="基于开发者关联分析的智能协作关键技术与支撑环境">
                基于开发者关联分析的智能协作关键技术与支撑环境 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="le">
            
              <p class="site-author-name" itemprop="name">le</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/490" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#MapReduce局限性"><span class="nav-number">1.</span> <span class="nav-text">MapReduce局限性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hive-Hbase-HDFS等之间的关系"><span class="nav-number">2.</span> <span class="nav-text">Hive,Hbase,HDFS等之间的关系</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark多种运行模式"><span class="nav-number">3.</span> <span class="nav-text">Spark多种运行模式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#测试或实验性质的本地运行模式（单机）"><span class="nav-number">3.1.</span> <span class="nav-text">测试或实验性质的本地运行模式（单机）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#测试或实现性质的本地伪集群运行模式（单机模拟集群）"><span class="nav-number">3.2.</span> <span class="nav-text">测试或实现性质的本地伪集群运行模式（单机模拟集群）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark自带Cluster-Manager的standalone-Client模式（集群）"><span class="nav-number">3.3.</span> <span class="nav-text">spark自带Cluster Manager的standalone Client模式（集群）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-自带Cluster-manager-的Standalone-cluster模式（集群）"><span class="nav-number">3.4.</span> <span class="nav-text">spark 自带Cluster manager 的Standalone cluster模式（集群）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于YARN的Resource-Manager-的Client-模式（集群）"><span class="nav-number">3.5.</span> <span class="nav-text">基于YARN的Resource Manager 的Client 模式（集群）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于YARN的Resource-Manager-的-Cluster-模式（集群）"><span class="nav-number">3.6.</span> <span class="nav-text">基于YARN的Resource Manager 的 Cluster 模式（集群）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark中cache和persist的区别"><span class="nav-number">4.</span> <span class="nav-text">spark中cache和persist的区别</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">5.</span> <span class="nav-text">Spark SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#起源"><span class="nav-number">5.1.</span> <span class="nav-text">起源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用场景"><span class="nav-number">5.2.</span> <span class="nav-text">使用场景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特点"><span class="nav-number">5.3.</span> <span class="nav-text">特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用"><span class="nav-number">5.4.</span> <span class="nav-text">使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#加载数据"><span class="nav-number">5.5.</span> <span class="nav-text">加载数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame和SQL的功能对比"><span class="nav-number">5.6.</span> <span class="nav-text">DataFrame和SQL的功能对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#savemode"><span class="nav-number">5.7.</span> <span class="nav-text">savemode</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark架构"><span class="nav-number">6.</span> <span class="nav-text">Spark架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark中的基本概念："><span class="nav-number">6.1.</span> <span class="nav-text">Spark中的基本概念：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#运行流程"><span class="nav-number">6.2.</span> <span class="nav-text">运行流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD结构"><span class="nav-number">6.3.</span> <span class="nav-text">RDD结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建RDD的三种方式"><span class="nav-number">6.3.1.</span> <span class="nav-text">创建RDD的三种方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-context"><span class="nav-number">6.4.</span> <span class="nav-text">spark context</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Streaming"><span class="nav-number">7.</span> <span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#YARN架构"><span class="nav-number">8.</span> <span class="nav-text">YARN架构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flume"><span class="nav-number">9.</span> <span class="nav-text">Flume</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Flume基本组件"><span class="nav-number">9.1.</span> <span class="nav-text">Flume基本组件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Flume插件"><span class="nav-number">9.2.</span> <span class="nav-text">Flume插件:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#flume典型的使用场景"><span class="nav-number">9.3.</span> <span class="nav-text">flume典型的使用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#多代理流"><span class="nav-number">9.3.1.</span> <span class="nav-text">多代理流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多路复用采集"><span class="nav-number">9.3.2.</span> <span class="nav-text">多路复用采集</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kafka"><span class="nav-number">10.</span> <span class="nav-text">Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本概念"><span class="nav-number">10.1.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#应用场景"><span class="nav-number">10.2.</span> <span class="nav-text">应用场景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#震惊了！原来这才是kafka！"><span class="nav-number">10.3.</span> <span class="nav-text">震惊了！原来这才是kafka！</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#生产过程"><span class="nav-number">10.3.1.</span> <span class="nav-text">生产过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#消费过程"><span class="nav-number">10.3.2.</span> <span class="nav-text">消费过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#消息投递语义"><span class="nav-number">10.3.3.</span> <span class="nav-text">消息投递语义</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#为什么要集成Flume和Kafka"><span class="nav-number">11.</span> <span class="nav-text">为什么要集成Flume和Kafka</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Streaming-整合FLume"><span class="nav-number">12.</span> <span class="nav-text">Spark Streaming 整合FLume</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#poll方式"><span class="nav-number">12.1.</span> <span class="nav-text">poll方式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pull方式"><span class="nav-number">12.2.</span> <span class="nav-text">pull方式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Streaming-整合Kafka"><span class="nav-number">13.</span> <span class="nav-text">Spark Streaming 整合Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Receiver模式-又称kafka高级api模式"><span class="nav-number">13.1.</span> <span class="nav-text">Receiver模式 又称kafka高级api模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Direct模式-又称kafka低级API模式"><span class="nav-number">13.2.</span> <span class="nav-text">Direct模式 又称kafka低级API模式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#map-mapPartition-flatMap-flatMapToPair-方法"><span class="nav-number">14.</span> <span class="nav-text">map, mapPartition, flatMap, flatMapToPair 方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-ML"><span class="nav-number">15.</span> <span class="nav-text">Spark ML</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">le</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">270.3k</span>
  
</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: window.location.pathname, 
            owner: '490',
            repo: 'https://github.com/490/490.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: 'dd4cdc4ae526b060d983e343530d30ed338af4b2',
            
                client_id: '2340dc349a2e19013ca4'
            }});
        gitment.render('gitment-container');
      }

      
      renderGitment();
      
      </script>
    





  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'Yi80CT04XNkVSJTMAPm5FtWc-gzGzoHsz',
        appKey: 'j3zcBrBI4kxBuazJBe3Gra3B',
        placeholder: '^_^',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  

  

  
  

  
  


  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":80,"height":160},"mobile":{"show":true}});</script></body>
</html>
