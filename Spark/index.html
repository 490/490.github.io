<!DOCTYPE html>




<html class="theme-next gemini" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="大数据,">










<meta name="description" content="Spark多种运行模式测试或实验性质的本地运行模式（单机） 该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。其中N代表可以使用N个线程，每个线程拥有一个core。如果不指定N，则默认是1个线程（该线程有1个core）。 指令示例： 1）spark-shell –master local 效果是一样的2）spark-sh">
<meta name="keywords" content="大数据">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark">
<meta property="og:url" content="https://490.github.io/Spark/index.html">
<meta property="og:site_name" content="我的笔记">
<meta property="og:description" content="Spark多种运行模式测试或实验性质的本地运行模式（单机） 该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。其中N代表可以使用N个线程，每个线程拥有一个core。如果不指定N，则默认是1个线程（该线程有1个core）。 指令示例： 1）spark-shell –master local 效果是一样的2）spark-sh">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-07-14T09:12:26.381Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark">
<meta name="twitter:description" content="Spark多种运行模式测试或实验性质的本地运行模式（单机） 该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。其中N代表可以使用N个线程，每个线程拥有一个core。如果不指定N，则默认是1个线程（该线程有1个core）。 指令示例： 1）spark-shell –master local 效果是一样的2）spark-sh">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://490.github.io/Spark/">





  <title>Spark | 我的笔记</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">我的笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br>
            
            文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-vcard-o"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-14T14:22:32+08:00">
                2019-07-14
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Spark/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/Spark/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  12
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Spark多种运行模式"><a href="#Spark多种运行模式" class="headerlink" title="Spark多种运行模式"></a>Spark多种运行模式</h1><h2 id="测试或实验性质的本地运行模式（单机）"><a href="#测试或实验性质的本地运行模式（单机）" class="headerlink" title="测试或实验性质的本地运行模式（单机）"></a>测试或实验性质的本地运行模式（单机）</h2><ul>
<li>该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。其中N代表可以使用N个线程，每个线程拥有一个core。如果不指定N，则默认是1个线程（该线程有1个core）。</li>
<li><p><strong>指令示例：</strong></p>
<p>1）spark-shell –master local 效果是一样的<br>2）spark-shell –master local[4] 代表会有4个线程（每个线程一个core）来并发执行应用程序。</p>
</li>
<li><p>运行该模式非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用，而不用启动Spark的Master、Worker守护进程( 只有集群的Standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS），这是和其他模式的区别，要记住才能理解。</p>
</li>
</ul>
<h2 id="测试或实现性质的本地伪集群运行模式（单机模拟集群）"><a href="#测试或实现性质的本地伪集群运行模式（单机模拟集群）" class="headerlink" title="测试或实现性质的本地伪集群运行模式（单机模拟集群）"></a>测试或实现性质的本地伪集群运行模式（单机模拟集群）</h2><ul>
<li><p>这种运行模式，和Local[N]很像，不同的是，它会在单机启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程只能在一个进程下委屈求全的共享资源。通常也是用来验证开发出来的应用程序逻辑上有没有问题，或者想使用Spark的计算框架而没有太多资源。</p>
</li>
<li><p><strong>指令示例：</strong></p>
<p>1）spark-shell –master local-cluster[2, 3, 1024]</p>
</li>
<li><p>用法是：提交应用程序时使用local-cluster[x,y,z]参数：x代表要生成的executor数，y和z分别代表每个executor所拥有的core和memory数。</p>
</li>
<li><p>该模式依然非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用。而不用启动Spark的Master、Worker守护进程( 只有集群的standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS），这是和其他模式的区别哦，要记住才能理解。</p>
</li>
</ul>
<h2 id="spark自带Cluster-Manager的standalone-Client模式（集群）"><a href="#spark自带Cluster-Manager的standalone-Client模式（集群）" class="headerlink" title="spark自带Cluster Manager的standalone Client模式（集群）"></a>spark自带Cluster Manager的standalone Client模式（集群）</h2><p>需要在多台机器上同时部署spark环境</p>
<ul>
<li>和单机运行的模式不同，这里必须在执行应用程序前，先启动Spark的Master和Worker守护进程。不用启动Hadoop服务，除非你用到了HDFS的内容。可以在想要做为Master的节点上用start-all.sh一条命令即可，这种运行模式，可以使用Spark的8080 web ui来观察资源和应用程序的执行情况了。用如下命令提交应用程序</li>
<li><p><strong>指令示例：</strong></p>
<p>1）spark-shell –master spark://wl1:7077<br>或者<br>2）spark-shell –master spark://wl1:7077 –deploy-mode client</p>
</li>
<li><p><strong>产生的进程：</strong></p>
<p>①Master进程做为cluster manager，用来对应用程序申请的资源进行管理<br>②SparkSubmit 做为Client端和运行driver程序<br>③CoarseGrainedExecutorBackend 用来并发执行应用程序</p>
</li>
</ul>
<h2 id="spark-自带Cluster-manager-的Standalone-cluster模式（集群）"><a href="#spark-自带Cluster-manager-的Standalone-cluster模式（集群）" class="headerlink" title="spark 自带Cluster manager 的Standalone cluster模式（集群）"></a>spark 自带Cluster manager 的Standalone cluster模式（集群）</h2><ul>
<li>这种运行模式和上面第3个还是有很大的区别的。使用如下命令执行应用程序</li>
<li><strong>指令示例：</strong></li>
</ul>
<p><code>spark-submit --master spark://wl1:6066 --deploy-mode cluster</code></p>
<p><strong>第4种模式和第3种模型的区别：</strong></p>
<p>①客户端的SparkSubmit进程会在应用程序提交给集群之后就退出</p>
<p>②Master会在集群中选择一个Worker进程生成一个子进程DriverWrapper来启动driver程序</p>
<p>③而该DriverWrapper 进程会占用Worker进程的一个core，所以同样的资源下配置下，会比第3种运行模式，少用1个core来参与计算</p>
<p>④应用程序的结果，会在执行driver程序的节点的stdout中输出，而不是打印在屏幕上</p>
<h2 id="基于YARN的Resource-Manager-的Client-模式（集群）"><a href="#基于YARN的Resource-Manager-的Client-模式（集群）" class="headerlink" title="基于YARN的Resource Manager 的Client 模式（集群）"></a>基于YARN的Resource Manager 的Client 模式（集群）</h2><ul>
<li><p>现在越来越多的场景，都是Spark跑在Hadoop集群中，所以为了做到资源能够均衡调度，会使用YARN来做为Spark的Cluster Manager，来为Spark的应用程序分配资源。在执行Spark应用程序前，要启动Hadoop的各种服务。由于已经有了资源管理器，所以不需要启动Spark的Master、Worker守护进程。</p>
<p>使用如下命令执行应用程序：</p>
</li>
<li><p><strong>指令示例：</strong></p>
</li>
</ul>
<p>1）spark-shell –master yarn<br>或者<br>2）spark-shell –master yarn –deploy-mode client</p>
<ul>
<li>提交应用程序后，各节点会启动相关的JVM进程，如下：</li>
</ul>
<ol>
<li>在Resource Manager节点上提交应用程序，会生成SparkSubmit进程，该进程会执行driver程序。</li>
<li>RM会在集群中的某个NodeManager上，启动一个ExecutorLauncher进程，来做为ApplicationMaster。</li>
<li>另外，RM也会在多个NodeManager上生成CoarseGrainedExecutorBackend进程来并发的执行应用程序。</li>
</ol>
<h2 id="基于YARN的Resource-Manager-的-Cluster-模式（集群）"><a href="#基于YARN的Resource-Manager-的-Cluster-模式（集群）" class="headerlink" title="基于YARN的Resource Manager 的 Cluster 模式（集群）"></a>基于YARN的Resource Manager 的 Cluster 模式（集群）</h2><ul>
<li><strong>指令示例：</strong></li>
</ul>
<p><code>spark-shell --master yarn --deploy-mode cluster</code></p>
<p><strong>和第5种运行模式，区别如下：</strong></p>
<ul>
<li>①在Resource Manager端提交应用程序，会生成SparkSubmit进程，该进程只用来做Client端，应用程序提交给集群后，就会删除该进程。</li>
<li>②Resource Manager在集群中的某个NodeManager上运行ApplicationMaster，该AM同时会执行driver程序。紧接着，会在各NodeManager上运行CoarseGrainedExecutorBackend来并发执行应用程序。</li>
<li>③应用程序的结果，会在执行driver程序的节点的stdout中输出，而不是打印在屏幕上。</li>
</ul>
<p><strong>此外，还有**</strong>Spark On  Mesos<strong>**模式 可以参阅：</strong></p>
<p><a href="http://ifeve.com/spark-mesos-spark/" target="_blank" rel="noopener">http://ifeve.com/spark-mesos-spark/</a></p>
<h1 id="spark中cache和persist的区别"><a href="#spark中cache和persist的区别" class="headerlink" title="spark中cache和persist的区别"></a>spark中cache和persist的区别</h1><p>cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。cache()调用了persist(),cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">object StorageLevel &#123;</span><br><span class="line">  val NONE = new StorageLevel(false, false, false, false)</span><br><span class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</span><br><span class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</span><br><span class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</span><br><span class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</span><br><span class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</span><br><span class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</span><br><span class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</span><br><span class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</span><br><span class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</span><br><span class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</span><br><span class="line">  val OFF_HEAP = new StorageLevel(false, false, true, false)</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>useDisk：使用硬盘（外存）<br>useMemory：使用内存<br>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。<br>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象<br>replication：备份数（在多个节点上备份）</p>
<p>使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xxDF.cache()</span><br><span class="line">使用</span><br><span class="line">xxDF.unpersist(true)</span><br></pre></td></tr></table></figure></p>
<h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>即席查询</p>
<p>将分析结果传到streaming</p>
<p>etl 清洗数据</p>
<p>把外部数据源的数据弄成dataFrame</p>
<p>大规模集群的查询</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p><strong>A、容易集成</strong></p>
<p>SparkSQL将SQL查询与Spark程序无缝对接，它允许用户使用SQL或熟悉的DataFrame API查询Spark程序内的结构化数据，可应用于Java、Scala、Python和R。</p>
<p><strong>B、统一的数据访问方式</strong></p>
<p>可使用同样的方式连接任何数据源，DataFrame和SQL提供了访问各种数据源的常用方式，包括Hive、Avro、Parquet、ORC、JSON和JDBC，甚至可以通过这些数据源直接加载数据。</p>
<p><strong>C、Hive集成</strong></p>
<p>能够在现有数据仓库上运行SQL或HiveSQL查询，SparkSQL支持HiveQL语法以及HiveSerDes（序列化和反序列化工具）和UDF（用户自定义函数），允许用户访问现有的Hive仓库。</p>
<p><strong>D、标准的数据连接</strong></p>
<p>通过JDBC或ODBC进行数据库连接，服务器模式为商业智能工具提供行业标准的JDBC和ODBC数据连接。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p><strong>SQL</strong><br>Spark SQL的一种用法是直接执行SQL查询语句，你可使用最基本的SQL语法，也可以选择HiveQL语法。Spark SQL可以从已有的Hive中读取数据。更详细的请参考<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables" target="_blank" rel="noopener">Hive Tables</a> 这一节。如果用其他编程语言运行SQL，Spark SQL将以<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#DataFrames" target="_blank" rel="noopener">DataFrame</a>返回结果。你还可以通过命令行<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-spark-sql-cli" target="_blank" rel="noopener">command-line</a> 或者 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" target="_blank" rel="noopener">JDBC/ODBC</a> 使用Spark SQL。</p>
<p><strong>DataFrames</strong></p>
<p>DataFrame是一种分布式数据集合，每一条数据都由几个命名字段组成。概念上来说，她和关系型数据库的表 或者 R和Python中的data frame等价，只不过在底层，DataFrame采用了更多优化。DataFrame可以从很多数据源（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">sources</a>）加载数据并构造得到，如：结构化数据文件，Hive中的表，外部数据库，或者已有的RDD。</p>
<p>DataFrame API支持<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/DataFrame.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>, and <a href="http://spark.apache.org/docs/latest/api/R/index.html" target="_blank" rel="noopener">R</a>。</p>
<p><strong>Datasets</strong> </p>
<p>Dataset是Spark-1.6新增的一种API，目前还是实验性的。Dataset想要把RDD的优势（强类型，可以使用lambda表达式函数）和Spark SQL的优化执行引擎的优势结合到一起。Dataset可以由JVM对象构建（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets" target="_blank" rel="noopener">constructed</a> ）得到，而后Dataset上可以使用各种transformation算子（map，flatMap，filter 等）。</p>
<p>Dataset API 对 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a> 和 <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a>的支持接口是一致的，但目前还不支持Python，不过Python自身就有语言动态特性优势（例如，你可以使用字段名来访问数据，row.columnName）。对Python的完整支持在未来的版本会增加进来。</p>
<h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>可以加载为RDD、DataFrame、DataSet<br>可以从本地、云端（HDFS，S3）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将数据加载成RDD</span><br><span class="line">val masterLog = sc.textFile(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-hadoop001.out"</span>)</span><br><span class="line">val workerLog = sc.textFile(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-hadoop001.out"</span>)</span><br><span class="line">val allLog = sc.textFile(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/logs/*out*"</span>)</span><br><span class="line"></span><br><span class="line">masterLog.count</span><br><span class="line">workerLog.count</span><br><span class="line">allLog.count</span><br></pre></td></tr></table></figure>
<p>RDD和DataFrame关联</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row</span><br><span class="line">val masterRDD = masterLog.map(x =&gt; Row(x))</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line">val schemaString = <span class="string">"line"</span></span><br><span class="line"></span><br><span class="line">val fields = schemaString.split(<span class="string">" "</span>).map(fieldName =&gt; StructField(fieldName, StringType, nullable = <span class="keyword">true</span>))</span><br><span class="line">val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">val masterDF = spark.createDataFrame(masterRDD, schema)</span><br><span class="line">masterDF.show</span><br></pre></td></tr></table></figure>
<p>如果格式是Json/Parquet等智能一点的，spark会自动推测出来，不用手动指定schema</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.format(<span class="string">"parquet"</span>).load(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet"</span>)</span><br><span class="line">usersDF.show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"select * from  parquet.`file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet`"</span>).show</span><br></pre></td></tr></table></figure>
<p>从Cloud读取数据: HDFS/S3<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">val hdfsRDD = sc.textFile(<span class="string">"hdfs://path/file"</span>)</span><br><span class="line">val s3RDD = sc.textFile(<span class="string">"s3a://bucket/object"</span>)</span><br><span class="line"></span><br><span class="line">spark.read.format(<span class="string">"text"</span>).load(<span class="string">"hdfs://path/file"</span>)</span><br><span class="line">spark.read.format(<span class="string">"text"</span>).load(<span class="string">"s3a://bucket/object"</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="DataFrame和SQL的功能对比"><a href="#DataFrame和SQL的功能对比" class="headerlink" title="DataFrame和SQL的功能对比"></a>DataFrame和SQL的功能对比</h2><p>DataFrame = RDD + Schema<br>（定义case class，然后用反射的方式自动生成schema）<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Define the schema using a case class.</span></span><br><span class="line"><span class="comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span></span><br><span class="line"><span class="comment">// you can use custom classes that implement the Product interface.</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">Create</span> <span class="title">an</span> <span class="title">RDD</span> <span class="title">of</span> <span class="title">Person</span> <span class="title">objects</span> <span class="title">and</span> <span class="title">register</span> <span class="title">it</span> <span class="title">as</span> <span class="title">a</span> <span class="title">table</span>.</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">people</span> </span>= sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>).map(_.split(<span class="string">","</span>)).map(p =&gt; <span class="type">Person</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim.toInt)).toDF()</span><br><span class="line">people.registerTempTable(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span></span><br><span class="line"><span class="keyword">val</span> teenagers = sqlContext.sql(<span class="string">"SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index:</span></span><br><span class="line">teenagers.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name:</span></span><br><span class="line">teenagers.map(t =&gt; <span class="string">"Name: "</span> + t.getAs[<span class="type">String</span>](<span class="string">"name"</span>)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span></span><br><span class="line">teenagers.map(_.getValuesMap[<span class="type">Any</span>](<span class="type">List</span>(<span class="string">"name"</span>, <span class="string">"age"</span>))).collect().foreach(println)</span><br><span class="line"><span class="comment">// Map("name" -&gt; "Justin", "age" -&gt; 19)</span></span><br></pre></td></tr></table></figure></p>
<p>DataFrame catalyst优化</p>
<p>DataFrame可以处理text，json，parquet等</p>
<p>SQL和API在DF里，都经过了catalyst优化，（不管菜鸡把SQL写成什么样，最后执行效率都一样）</p>
<h2 id="savemode"><a href="#savemode" class="headerlink" title="savemode"></a>savemode</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df=spark.read.format(<span class="string">"json"</span>).load(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line">df.show</span><br><span class="line"></span><br><span class="line"><span class="type">Mode</span>可以有err、overwrite、append、ignore</span><br><span class="line"></span><br><span class="line">df.select(<span class="string">"name"</span>).write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"file:///path"</span>)</span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/大数据/" rel="tag"># 大数据</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Python/" rel="next" title="Python">
                <i class="fa fa-chevron-left"></i> Python
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="le">
            
              <p class="site-author-name" itemprop="name">le</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">47</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/490" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark多种运行模式"><span class="nav-number">1.</span> <span class="nav-text">Spark多种运行模式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#测试或实验性质的本地运行模式（单机）"><span class="nav-number">1.1.</span> <span class="nav-text">测试或实验性质的本地运行模式（单机）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#测试或实现性质的本地伪集群运行模式（单机模拟集群）"><span class="nav-number">1.2.</span> <span class="nav-text">测试或实现性质的本地伪集群运行模式（单机模拟集群）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark自带Cluster-Manager的standalone-Client模式（集群）"><span class="nav-number">1.3.</span> <span class="nav-text">spark自带Cluster Manager的standalone Client模式（集群）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-自带Cluster-manager-的Standalone-cluster模式（集群）"><span class="nav-number">1.4.</span> <span class="nav-text">spark 自带Cluster manager 的Standalone cluster模式（集群）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于YARN的Resource-Manager-的Client-模式（集群）"><span class="nav-number">1.5.</span> <span class="nav-text">基于YARN的Resource Manager 的Client 模式（集群）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于YARN的Resource-Manager-的-Cluster-模式（集群）"><span class="nav-number">1.6.</span> <span class="nav-text">基于YARN的Resource Manager 的 Cluster 模式（集群）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark中cache和persist的区别"><span class="nav-number">2.</span> <span class="nav-text">spark中cache和persist的区别</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">3.</span> <span class="nav-text">Spark SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#使用场景"><span class="nav-number">3.1.</span> <span class="nav-text">使用场景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特点"><span class="nav-number">3.2.</span> <span class="nav-text">特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用"><span class="nav-number">3.3.</span> <span class="nav-text">使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#加载数据"><span class="nav-number">3.4.</span> <span class="nav-text">加载数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame和SQL的功能对比"><span class="nav-number">3.5.</span> <span class="nav-text">DataFrame和SQL的功能对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#savemode"><span class="nav-number">3.6.</span> <span class="nav-text">savemode</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">le</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">247.8k</span>
  
</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: window.location.pathname, 
            owner: '490',
            repo: 'https://github.com/490/490.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: 'dd4cdc4ae526b060d983e343530d30ed338af4b2',
            
                client_id: '2340dc349a2e19013ca4'
            }});
        gitment.render('gitment-container');
      }

      
      renderGitment();
      
      </script>
    





  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'Yi80CT04XNkVSJTMAPm5FtWc-gzGzoHsz',
        appKey: 'j3zcBrBI4kxBuazJBe3Gra3B',
        placeholder: '^_^',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  

  

  
  

  
  


  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":80,"height":160},"mobile":{"show":true}});</script></body>
</html>
