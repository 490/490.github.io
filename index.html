<!DOCTYPE html>




<html class="theme-next gemini" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="我的笔记">
<meta property="og:url" content="https://490.github.io/index.html">
<meta property="og:site_name" content="我的笔记">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="我的笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://490.github.io/">





  <title>我的笔记</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">我的笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br>
            
            文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-vcard-o"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Spark/" itemprop="url">Spark</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-14T14:22:32+08:00">
                2019-07-14
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Spark/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Spark/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  8.4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  34
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MapReduce局限性"><a href="#MapReduce局限性" class="headerlink" title="MapReduce局限性"></a>MapReduce局限性</h1><p>MapReduce的局限性：<br>1）代码繁琐；<br>2）只能够支持map和reduce方法；<br>3）执行效率低下；<br>4）不适合迭代多次、交互式、流式的处理；</p>
<p>框架多样化：<br>1）批处理（离线）：MapReduce、Hive、Pig<br>2）流式处理（实时）： Storm、JStorm<br>3）交互式计算：Impala</p>
<h1 id="Spark多种运行模式"><a href="#Spark多种运行模式" class="headerlink" title="Spark多种运行模式"></a>Spark多种运行模式</h1><h2 id="测试或实验性质的本地运行模式（单机）"><a href="#测试或实验性质的本地运行模式（单机）" class="headerlink" title="测试或实验性质的本地运行模式（单机）"></a>测试或实验性质的本地运行模式（单机）</h2><ul>
<li>该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。其中N代表可以使用N个线程，每个线程拥有一个core。如果不指定N，则默认是1个线程（该线程有1个core）。</li>
<li><p><strong>指令示例：</strong></p>
<p>1）spark-shell –master local 效果是一样的<br>2）spark-shell –master local[4] 代表会有4个线程（每个线程一个core）来并发执行应用程序。</p>
</li>
<li><p>运行该模式非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用，而不用启动Spark的Master、Worker守护进程( 只有集群的Standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS），这是和其他模式的区别，要记住才能理解。</p>
</li>
</ul>
<h2 id="测试或实现性质的本地伪集群运行模式（单机模拟集群）"><a href="#测试或实现性质的本地伪集群运行模式（单机模拟集群）" class="headerlink" title="测试或实现性质的本地伪集群运行模式（单机模拟集群）"></a>测试或实现性质的本地伪集群运行模式（单机模拟集群）</h2><ul>
<li><p>这种运行模式，和Local[N]很像，不同的是，它会在单机启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程只能在一个进程下委屈求全的共享资源。通常也是用来验证开发出来的应用程序逻辑上有没有问题，或者想使用Spark的计算框架而没有太多资源。</p>
</li>
<li><p><strong>指令示例：</strong></p>
<p>1）spark-shell –master local-cluster[2, 3, 1024]</p>
</li>
<li><p>用法是：提交应用程序时使用local-cluster[x,y,z]参数：x代表要生成的executor数，y和z分别代表每个executor所拥有的core和memory数。</p>
</li>
<li><p>该模式依然非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用。而不用启动Spark的Master、Worker守护进程( 只有集群的standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS），这是和其他模式的区别哦，要记住才能理解。</p>
</li>
</ul>
<h2 id="spark自带Cluster-Manager的standalone-Client模式（集群）"><a href="#spark自带Cluster-Manager的standalone-Client模式（集群）" class="headerlink" title="spark自带Cluster Manager的standalone Client模式（集群）"></a>spark自带Cluster Manager的standalone Client模式（集群）</h2><p>需要在多台机器上同时部署spark环境</p>
<ul>
<li>和单机运行的模式不同，这里必须在执行应用程序前，先启动Spark的Master和Worker守护进程。不用启动Hadoop服务，除非你用到了HDFS的内容。可以在想要做为Master的节点上用start-all.sh一条命令即可，这种运行模式，可以使用Spark的8080 web ui来观察资源和应用程序的执行情况了。用如下命令提交应用程序</li>
<li><p><strong>指令示例：</strong></p>
<p>1）spark-shell –master spark://wl1:7077<br>或者<br>2）spark-shell –master spark://wl1:7077 –deploy-mode client</p>
</li>
<li><p><strong>产生的进程：</strong></p>
<p>①Master进程做为cluster manager，用来对应用程序申请的资源进行管理<br>②SparkSubmit 做为Client端和运行driver程序<br>③CoarseGrainedExecutorBackend 用来并发执行应用程序</p>
</li>
</ul>
<h2 id="spark-自带Cluster-manager-的Standalone-cluster模式（集群）"><a href="#spark-自带Cluster-manager-的Standalone-cluster模式（集群）" class="headerlink" title="spark 自带Cluster manager 的Standalone cluster模式（集群）"></a>spark 自带Cluster manager 的Standalone cluster模式（集群）</h2><ul>
<li>这种运行模式和上面第3个还是有很大的区别的。使用如下命令执行应用程序</li>
<li><strong>指令示例：</strong></li>
</ul>
<p><code>spark-submit --master spark://wl1:6066 --deploy-mode cluster</code></p>
<p><strong>第4种模式和第3种模型的区别：</strong></p>
<p>①客户端的SparkSubmit进程会在应用程序提交给集群之后就退出</p>
<p>②Master会在集群中选择一个Worker进程生成一个子进程DriverWrapper来启动driver程序</p>
<p>③而该DriverWrapper 进程会占用Worker进程的一个core，所以同样的资源下配置下，会比第3种运行模式，少用1个core来参与计算</p>
<p>④应用程序的结果，会在执行driver程序的节点的stdout中输出，而不是打印在屏幕上</p>
<h2 id="基于YARN的Resource-Manager-的Client-模式（集群）"><a href="#基于YARN的Resource-Manager-的Client-模式（集群）" class="headerlink" title="基于YARN的Resource Manager 的Client 模式（集群）"></a>基于YARN的Resource Manager 的Client 模式（集群）</h2><ul>
<li><p>现在越来越多的场景，都是Spark跑在Hadoop集群中，所以为了做到资源能够均衡调度，会使用YARN来做为Spark的Cluster Manager，来为Spark的应用程序分配资源。在执行Spark应用程序前，要启动Hadoop的各种服务。由于已经有了资源管理器，所以不需要启动Spark的Master、Worker守护进程。</p>
<p>使用如下命令执行应用程序：</p>
</li>
<li><p><strong>指令示例：</strong></p>
</li>
</ul>
<p>1）spark-shell –master yarn<br>或者<br>2）spark-shell –master yarn –deploy-mode client</p>
<ul>
<li>提交应用程序后，各节点会启动相关的JVM进程，如下：</li>
</ul>
<ol>
<li>在Resource Manager节点上提交应用程序，会生成SparkSubmit进程，该进程会执行driver程序。</li>
<li>RM会在集群中的某个NodeManager上，启动一个ExecutorLauncher进程，来做为ApplicationMaster。</li>
<li>另外，RM也会在多个NodeManager上生成CoarseGrainedExecutorBackend进程来并发的执行应用程序。</li>
</ol>
<h2 id="基于YARN的Resource-Manager-的-Cluster-模式（集群）"><a href="#基于YARN的Resource-Manager-的-Cluster-模式（集群）" class="headerlink" title="基于YARN的Resource Manager 的 Cluster 模式（集群）"></a>基于YARN的Resource Manager 的 Cluster 模式（集群）</h2><ul>
<li><strong>指令示例：</strong></li>
</ul>
<p><code>spark-shell --master yarn --deploy-mode cluster</code></p>
<p><strong>和第5种运行模式，区别如下：</strong></p>
<ul>
<li>①在Resource Manager端提交应用程序，会生成SparkSubmit进程，该进程只用来做Client端，应用程序提交给集群后，就会删除该进程。</li>
<li>②Resource Manager在集群中的某个NodeManager上运行ApplicationMaster，该AM同时会执行driver程序。紧接着，会在各NodeManager上运行CoarseGrainedExecutorBackend来并发执行应用程序。</li>
<li>③应用程序的结果，会在执行driver程序的节点的stdout中输出，而不是打印在屏幕上。</li>
</ul>
<p><strong>此外，还有**</strong>Spark On  Mesos<strong>**模式 可以参阅：</strong></p>
<p><a href="http://ifeve.com/spark-mesos-spark/" target="_blank" rel="noopener">http://ifeve.com/spark-mesos-spark/</a></p>
<h1 id="spark中cache和persist的区别"><a href="#spark中cache和persist的区别" class="headerlink" title="spark中cache和persist的区别"></a>spark中cache和persist的区别</h1><p>cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间。cache()调用了persist(),cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">object StorageLevel &#123;</span><br><span class="line">  val NONE = new StorageLevel(false, false, false, false)</span><br><span class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</span><br><span class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</span><br><span class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</span><br><span class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</span><br><span class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</span><br><span class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</span><br><span class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</span><br><span class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</span><br><span class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</span><br><span class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</span><br><span class="line">  val OFF_HEAP = new StorageLevel(false, false, true, false)</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>useDisk：使用硬盘（外存）<br>useMemory：使用内存<br>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。<br>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象<br>replication：备份数（在多个节点上备份）</p>
<p>使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xxDF.cache()</span><br><span class="line">使用</span><br><span class="line">xxDF.unpersist(true)</span><br></pre></td></tr></table></figure></p>
<h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>Hive: 类似于sql的Hive QL语言， sql==&gt;mapreduce</p>
<ul>
<li>特点：mapreduce</li>
<li>改进：hive on tez、hive on spark、hive on mapreduce</li>
</ul>
<p>Spark: hive on spark ==&gt; shark(hive on spark)</p>
<ul>
<li>shark推出：欢迎， 基于spark、基于内存的列式存储、与hive能够兼容</li>
<li>缺点：hive ql的解析、逻辑执行计划生成、执行计划的优化是依赖于hive的<pre><code>仅仅只是把物理执行计划从mr作业替换成spark作业
</code></pre></li>
</ul>
<p>Shark终止以后，产生了2个分支：<br>1）hive on spark<br>    Hive社区，源码是在Hive中<br>2）Spark SQL<br>    Spark社区，源码是在Spark中<br>    支持多种数据源，多种优化技术，扩展性好很多</p>
<p>SQL on Hadoop</p>
<ul>
<li><p>Hive<br>  sql ==&gt; mapreduce<br>  metastore ： 元数据<br>  sql：database、table、view<br>  facebook</p>
</li>
<li><p>impala<br>  cloudera ： cdh（建议大家在生产上使用的hadoop系列版本）、cm<br>  sql：自己的守护进程执行的，非mr<br>  metastore</p>
</li>
<li><p>presto<br>  facebook<br>  京东<br>  sql</p>
</li>
<li><p>drill<br>  sql<br>  访问：hdfs、rdbms、json、hbase、mongodb、s3、hive</p>
</li>
<li><p>Spark SQL<br>  sql<br>  dataframe/dataset api<br>  metastore<br>  访问：hdfs、rdbms、json、hbase、mongodb、s3、hive  ==&gt; 外部数据源</p>
</li>
</ul>
<p>Spark SQL is Apache Spark’s module for working with structured data. </p>
<p>有见到SQL字样吗？<br>Spark SQL它不仅仅有访问或者操作SQL的功能，还提供了其他的非常丰富的操作：外部数据源、优化</p>
<p><strong>Spark SQL概述小结：</strong></p>
<ul>
<li>Spark SQL的应用并不局限于SQL；</li>
<li>访问hive、json、parquet等文件的数据；</li>
<li>SQL只是Spark SQL的一个功能而已；<br>===&gt; Spark SQL这个名字起的并不恰当</li>
<li>Spark SQL提供了SQL的api、DataFrame和Dataset的API；</li>
</ul>
<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><p>即席查询</p>
<p>将分析结果传到streaming</p>
<p>etl 清洗数据</p>
<p>把外部数据源的数据弄成dataFrame</p>
<p>大规模集群的查询</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p><strong>A、容易集成</strong></p>
<p>SparkSQL将SQL查询与Spark程序无缝对接，它允许用户使用SQL或熟悉的DataFrame API查询Spark程序内的结构化数据，可应用于Java、Scala、Python和R。</p>
<p><strong>B、统一的数据访问方式</strong></p>
<p>可使用同样的方式连接任何数据源，DataFrame和SQL提供了访问各种数据源的常用方式，包括Hive、Avro、Parquet、ORC、JSON和JDBC，甚至可以通过这些数据源直接加载数据。</p>
<p><strong>C、Hive集成</strong></p>
<p>能够在现有数据仓库上运行SQL或HiveSQL查询，SparkSQL支持HiveQL语法以及HiveSerDes（序列化和反序列化工具）和UDF（用户自定义函数），允许用户访问现有的Hive仓库。</p>
<p><strong>D、标准的数据连接</strong></p>
<p>通过JDBC或ODBC进行数据库连接，服务器模式为商业智能工具提供行业标准的JDBC和ODBC数据连接。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p><strong>SQL</strong><br>Spark SQL的一种用法是直接执行SQL查询语句，你可使用最基本的SQL语法，也可以选择HiveQL语法。Spark SQL可以从已有的Hive中读取数据。更详细的请参考<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables" target="_blank" rel="noopener">Hive Tables</a> 这一节。如果用其他编程语言运行SQL，Spark SQL将以<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#DataFrames" target="_blank" rel="noopener">DataFrame</a>返回结果。你还可以通过命令行<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-spark-sql-cli" target="_blank" rel="noopener">command-line</a> 或者 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" target="_blank" rel="noopener">JDBC/ODBC</a> 使用Spark SQL。</p>
<p><strong>DataFrames</strong></p>
<p>DataFrame是一种分布式数据集合，每一条数据都由几个命名字段组成。概念上来说，她和关系型数据库的表 或者 R和Python中的data frame等价，只不过在底层，DataFrame采用了更多优化。DataFrame可以从很多数据源（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">sources</a>）加载数据并构造得到，如：结构化数据文件，Hive中的表，外部数据库，或者已有的RDD。</p>
<p>DataFrame API支持<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/DataFrame.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>, and <a href="http://spark.apache.org/docs/latest/api/R/index.html" target="_blank" rel="noopener">R</a>。</p>
<p><strong>Datasets</strong> </p>
<p>Dataset是Spark-1.6新增的一种API，目前还是实验性的。Dataset想要把RDD的优势（强类型，可以使用lambda表达式函数）和Spark SQL的优化执行引擎的优势结合到一起。Dataset可以由JVM对象构建（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets" target="_blank" rel="noopener">constructed</a> ）得到，而后Dataset上可以使用各种transformation算子（map，flatMap，filter 等）。</p>
<p>Dataset API 对 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a> 和 <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a>的支持接口是一致的，但目前还不支持Python，不过Python自身就有语言动态特性优势（例如，你可以使用字段名来访问数据，row.columnName）。对Python的完整支持在未来的版本会增加进来。</p>
<h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>可以加载为RDD、DataFrame、DataSet<br>可以从本地、云端（HDFS，S3）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将数据加载成RDD</span><br><span class="line">val masterLog = sc.textFile(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-hadoop001.out"</span>)</span><br><span class="line">val workerLog = sc.textFile(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-hadoop001.out"</span>)</span><br><span class="line">val allLog = sc.textFile(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/logs/*out*"</span>)</span><br><span class="line"></span><br><span class="line">masterLog.count</span><br><span class="line">workerLog.count</span><br><span class="line">allLog.count</span><br></pre></td></tr></table></figure>
<p>RDD和DataFrame关联</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row</span><br><span class="line">val masterRDD = masterLog.map(x =&gt; Row(x))</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line">val schemaString = <span class="string">"line"</span></span><br><span class="line"></span><br><span class="line">val fields = schemaString.split(<span class="string">" "</span>).map(fieldName =&gt; StructField(fieldName, StringType, nullable = <span class="keyword">true</span>))</span><br><span class="line">val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">val masterDF = spark.createDataFrame(masterRDD, schema)</span><br><span class="line">masterDF.show</span><br></pre></td></tr></table></figure>
<p>如果格式是Json/Parquet等智能一点的，spark会自动推测出来，不用手动指定schema</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.format(<span class="string">"parquet"</span>).load(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet"</span>)</span><br><span class="line">usersDF.show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"select * from  parquet.`file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/users.parquet`"</span>).show</span><br></pre></td></tr></table></figure>
<p>从Cloud读取数据: HDFS/S3<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">val hdfsRDD = sc.textFile(<span class="string">"hdfs://path/file"</span>)</span><br><span class="line">val s3RDD = sc.textFile(<span class="string">"s3a://bucket/object"</span>)</span><br><span class="line"></span><br><span class="line">spark.read.format(<span class="string">"text"</span>).load(<span class="string">"hdfs://path/file"</span>)</span><br><span class="line">spark.read.format(<span class="string">"text"</span>).load(<span class="string">"s3a://bucket/object"</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="DataFrame和SQL的功能对比"><a href="#DataFrame和SQL的功能对比" class="headerlink" title="DataFrame和SQL的功能对比"></a>DataFrame和SQL的功能对比</h2><p>DataFrame = RDD + Schema<br>（定义case class，然后用反射的方式自动生成schema）<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Define the schema using a case class.</span></span><br><span class="line"><span class="comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span></span><br><span class="line"><span class="comment">// you can use custom classes that implement the Product interface.</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">Create</span> <span class="title">an</span> <span class="title">RDD</span> <span class="title">of</span> <span class="title">Person</span> <span class="title">objects</span> <span class="title">and</span> <span class="title">register</span> <span class="title">it</span> <span class="title">as</span> <span class="title">a</span> <span class="title">table</span>.</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">people</span> </span>= sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>).map(_.split(<span class="string">","</span>)).map(p =&gt; <span class="type">Person</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim.toInt)).toDF()</span><br><span class="line">people.registerTempTable(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span></span><br><span class="line"><span class="keyword">val</span> teenagers = sqlContext.sql(<span class="string">"SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index:</span></span><br><span class="line">teenagers.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name:</span></span><br><span class="line">teenagers.map(t =&gt; <span class="string">"Name: "</span> + t.getAs[<span class="type">String</span>](<span class="string">"name"</span>)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span></span><br><span class="line">teenagers.map(_.getValuesMap[<span class="type">Any</span>](<span class="type">List</span>(<span class="string">"name"</span>, <span class="string">"age"</span>))).collect().foreach(println)</span><br><span class="line"><span class="comment">// Map("name" -&gt; "Justin", "age" -&gt; 19)</span></span><br></pre></td></tr></table></figure></p>
<p>DataFrame catalyst优化</p>
<p>DataFrame可以处理text，json，parquet等</p>
<p>SQL和API在DF里，都经过了catalyst优化，（不管菜鸡把SQL写成什么样，最后执行效率都一样）</p>
<h2 id="savemode"><a href="#savemode" class="headerlink" title="savemode"></a>savemode</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df=spark.read.format(<span class="string">"json"</span>).load(<span class="string">"file:///home/hadoop/app/spark-2.1.0-bin-2.6.0-cdh5.7.0/examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line">df.show</span><br><span class="line"></span><br><span class="line"><span class="type">Mode</span>可以有err、overwrite、append、ignore</span><br><span class="line"></span><br><span class="line">df.select(<span class="string">"name"</span>).write.format(<span class="string">"parquet"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"file:///path"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="YARN架构"><a href="#YARN架构" class="headerlink" title="YARN架构"></a>YARN架构</h1><p>1 RM(ResourceManager) + N NM(NodeManager)</p>
<p>ResourceManager的职责： 一个集群active状态的RM只有一个，负责整个集群的资源管理和调度</p>
<ul>
<li>1）处理客户端的请求(启动/杀死)</li>
<li>2）启动/监控ApplicationMaster(一个作业对应一个AM)</li>
<li>3）监控NM</li>
<li>4）系统的资源分配和调度</li>
</ul>
<p>NodeManager：整个集群中有N个，负责单个节点的资源管理和使用以及task的运行情况</p>
<ul>
<li>1）定期向RM汇报本节点的资源使用请求和各个Container的运行状态</li>
<li>2）接收并处理RM的container启停的各种命令</li>
<li>3）单个节点的资源管理和任务管理</li>
</ul>
<p>ApplicationMaster：每个应用/作业对应一个，负责应用程序的管理</p>
<ul>
<li>1）数据切分</li>
<li>2）为应用程序向RM申请资源(container)，并分配给内部任务</li>
<li>3）与NM通信以启停task， task是运行在container中的</li>
<li>4）task的监控和容错</li>
</ul>
<p>Container：<br>对任务运行情况的描述：cpu、memory、环境变量</p>
<p>YARN执行流程</p>
<ul>
<li>1）用户向YARN提交作业</li>
<li>2）RM为该作业分配第一个container(AM)</li>
<li>3）RM会与对应的NM通信，要求NM在这个container上启动应用程序的AM</li>
<li>4) AM首先向RM注册，然后AM将为各个任务申请资源，并监控运行情况</li>
<li>5）AM采用轮训的方式通过RPC协议向RM申请和领取资源</li>
<li>6）AM申请到资源以后，便和相应的NM通信，要求NM启动任务</li>
<li>7）NM启动我们作业对应的task</li>
</ul>
<p>YARN环境搭建<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mapred-site.xml</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">yarn-site.xml</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>启动yarn：<code>sbin/start-yarn.sh</code></p>
<p>验证是否启动成功<br><code>jps</code><br><code>ResourceManager</code><br><code>NodeManager</code></p>
<p>web: <a href="http://hadoop001:8088" target="_blank" rel="noopener">http://hadoop001:8088</a></p>
<p>停止yarn： <code>sbin/stop-yarn.sh</code></p>
<p>提交mr作业到yarn上运行： wc</p>
<p><code>/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar</code></p>
<p><code>hadoop jar /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar wordcount /input/wc/hello.txt /output/wc/</code></p>
<h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><ul>
<li>Flume是一个分布式的、可靠的、高可用的海量日志采集、聚合和传输的系统</li>
<li>数据流模型：Source-Channel-Sink</li>
<li>事务机制保证消息传递的可靠性</li>
<li>内置丰富插件，轻松与其他系统集成</li>
<li>Java实现，优秀的系统框架设计，模块分明，易于开发</li>
</ul>
<h2 id="Flume基本组件"><a href="#Flume基本组件" class="headerlink" title="Flume基本组件"></a>Flume基本组件</h2><ul>
<li>Event：消息的基本单位，有header和body组成</li>
<li><p>Agent：JVM进程，负责将一端外部来源产生的消息转 发到另一端外部的目的地</p>
<ul>
<li>Source：   从数据发生器接收数据,并将接收的数据以Flume的event格式传递给一个或者多个通道channal,Flume提供多种数据接收的方式,比如Avro,Thrift,twitter1%等</li>
<li>Channel： channal是一种短暂的存储容器,它将从source处接收到的event格式的数据缓存起来,直到它们被sinks消费掉,它在source和sink间起着一共桥梁的作用,channal是一个完整的事务,这一点保证了数据在收发的时候的一致性. 并且它可以和任意数量的source和sink链接. 支持的类型有: JDBC channel , File System channel , Memort channel等.</li>
<li>Sink：sink将数据存储到集中存储器比如Hbase和HDFS,它从channals消费数据(events)并将其传递给目标地. 目标地可能是另一个source,也可能HDFS,HBase</li>
</ul>
</li>
</ul>
<h2 id="Flume插件"><a href="#Flume插件" class="headerlink" title="Flume插件:"></a>Flume插件:</h2><ol>
<li>Interceptors拦截器： 用于source和channel之间,用来更改或者检查Flume的events数据</li>
<li>管道选择器 channels Selectors： 在多管道是被用来选择使用那一条管道来传递数据(events). 管道选择器又分为如下两种:</li>
</ol>
<ul>
<li>默认管道选择器:  每一个管道传递的都是相同的events</li>
<li>多路复用通道选择器:  依据每一个event的头部header的地址选择管道.</li>
</ul>
<ol start="3">
<li>sink线程：用于激活被选择的sinks群中特定的sink,用于负载均衡.</li>
</ol>
<h2 id="flume典型的使用场景"><a href="#flume典型的使用场景" class="headerlink" title="flume典型的使用场景"></a>flume典型的使用场景</h2><h3 id="多代理流"><a href="#多代理流" class="headerlink" title="多代理流"></a>多代理流</h3><p><img src="https://s1.51cto.com/images/blog/201901/17/0c8047532a232e90e8e99b1783cd1a7d.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt="flume"><br>从第一台机器的flume agent传送到第二台机器的flume agent。<br>例：<br><strong>规划</strong>：<br>hadoop02：tail-avro.properties<br>   使用 exec “tail -F /home/hadoop/testlog/welog.log”获取采集数据<br>   使用 avro sink 数据都下一个 agent<br>hadoop03：avro-hdfs.properties<br>   使用 avro 接收采集数据<br>   使用 hdfs sink 数据到目的地</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#tail-avro.properties</span><br><span class="line">a1.sources = r1 </span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line">#Describe/configure the source </span><br><span class="line">a1.sources.r1.type = exec </span><br><span class="line">a1.sources.r1.command = tail -F /home/hadoop/testlog/date.log </span><br><span class="line">a1.sources.r1.channels = c1 </span><br><span class="line">#Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro </span><br><span class="line">a1.sinks.k1.channel = c1 </span><br><span class="line">a1.sinks.k1.hostname = hadoop02 </span><br><span class="line">a1.sinks.k1.port = <span class="number">4141</span> </span><br><span class="line">a1.sinks.k1.batch-size = <span class="number">2</span></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = <span class="number">1000</span></span><br><span class="line">a1.channels.c1.transactionCapacity = <span class="number">100</span></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#avro-hdfs.properties</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line">#Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.bind = <span class="number">0.0</span>.0.0</span><br><span class="line">a1.sources.r1.port = <span class="number">4141</span></span><br><span class="line">#Describe k1</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path =hdfs:<span class="comment">//myha01/testlog/flume-event/%y-%m-%d/%H-%M</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = date_</span><br><span class="line">a1.sinks.k1.hdfs.maxOpenFiles = <span class="number">5000</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize= <span class="number">100</span></span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = <span class="number">102400</span></span><br><span class="line">a1.sinks.k1.hdfs.rollCount = <span class="number">1000000</span></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.hdfs.round = <span class="keyword">true</span></span><br><span class="line">a1.sinks.k1.hdfs.roundValue = <span class="number">10</span></span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="keyword">true</span></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = <span class="number">1000</span></span><br><span class="line">a1.channels.c1.transactionCapacity = <span class="number">100</span></span><br><span class="line">#Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<h3 id="多路复用采集"><a href="#多路复用采集" class="headerlink" title="多路复用采集"></a>多路复用采集</h3><p><img src="https://s1.51cto.com/images/blog/201901/17/f4ccb4b3d930c7a332c6f1bbc56a39ea.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=" alt="flume"></p>
<p>在一份agent中有多个channel和多个sink，然后多个sink输出到不同的文件或者文件系统中。<br>规划：<br>Hadoop02：（tail-hdfsandlogger.properties）<br>   使用 exec “tail -F /home/hadoop/testlog/datalog.log”获取采集数据<br>   使用 sink1 将数据 存储hdfs<br>   使用 sink2 将数据都存储 控制台</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#tail-hdfsandlogger.properties</span><br><span class="line">#2个channel和2个sink的配置文件</span><br><span class="line">#Name the components on this agent</span><br><span class="line">a1.sources = s1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line">#Describe/configure tail -F source1</span><br><span class="line">a1.sources.s1.type = exec</span><br><span class="line">a1.sources.s1.command = tail -F /home/hadoop/logs/catalina.out</span><br><span class="line">#指定source进行扇出到多个channnel的规则</span><br><span class="line">a1.sources.s1.selector.type = replicating</span><br><span class="line">a1.sources.s1.channels = c1 c2</span><br><span class="line"></span><br><span class="line">#Use a channel which buffers events in memory</span><br><span class="line">#指定channel c1</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">#指定channel c2</span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line"></span><br><span class="line">#Describe the sink</span><br><span class="line">#指定k1的设置</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs:<span class="comment">//myha01/flume_log/%y-%m-%d/%H-%M</span></span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events</span><br><span class="line">a1.sinks.k1.hdfs.maxOpenFiles = <span class="number">5000</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize= <span class="number">100</span></span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat =Text</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = <span class="number">102400</span></span><br><span class="line">a1.sinks.k1.hdfs.rollCount = <span class="number">1000000</span></span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = <span class="number">60</span></span><br><span class="line">a1.sinks.k1.hdfs.round = <span class="keyword">true</span></span><br><span class="line">a1.sinks.k1.hdfs.roundValue = <span class="number">10</span></span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = <span class="keyword">true</span></span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">#指定k2的</span><br><span class="line">a1.sinks.k2.type = logger</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>
<h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ol>
<li>kafka作为集群运行在一个或者多个服务器上</li>
<li>kafka集群存储的消息是以topic为类别记录的</li>
<li>kafka存储的消息是k-v键值对，k是offset偏移量，v就是消息的内容</li>
<li>topic：kafka将消息分门别类，每一类的消息称之为topic</li>
<li>broker：已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker). 消费者可以订阅一个或多个主题（topic），并从Broker拉数据，从而消费这些已发布的消息。</li>
<li>消息：kafka会保存消息直到它过期，无论是否被消费了。</li>
<li>producer：发布消息的对象，往某个topic中发布消息，也负责选择发布到topic中的哪个分区</li>
<li>consumer：订阅消息并处理发布的消息的对象</li>
<li>patition：topic是逻辑上的概念，patition是物理概念。每个分区都是一个顺序的，不可变的消息队列，并且可以持续添加，producer生产的消息都会append到队列的末尾，而不是随机读写的。分区中的消息都会被分了一个序列号，这个序列号在分区内是唯一的，也就是分区内的偏移量。</li>
<li>如何消费：<br>kafka的生产者没有保持消息消费的顺序，消费的顺序是通过偏移量交给消费者的，消费者持有的元数据就是消息的offset，消费者通过控制offset的移动来决定读取哪里的消息。正常情况下，当消费者消费消息的时候，偏移量是线性增长的。如果消费者想要重新读取数据的时候，就需要将偏移量向前移动。</li>
<li>为什么说是分布式和冗余备份的：<br>分区被分布到集群中的各个服务器中，每个服务器处理它所拥有的分区。根据配置，每个分区还可以复制到其他服务器作为备份容错。每个分区拥有一个leader，有一个或者多个follower（冗余备份的）。一个broker可以是一个分区的leader,同时也可以是别的分区的follwer，避免了所有的请求只让一个或者几个服务器处理，负载均衡。<br>某个broker如果是一个分区的leader，那么它处理这个分区上的所有读写请求，而follower分区被动的复制数据。如果leader宕机，则follower就可以被推举为leader。</li>
<li>为什么说是持久性的：<br>kafka使用文件存储消息，并且会保存所有消息直到它过期，无论是否消费。</li>
<li><p>consumer和topic的关系<br><img src="http://490.github.io/images/20190717_101905.png" alt="image"><br>这个kafka集群中有两个broker，broker1下有partition0和partition3，broker2下有partition1和partition2。<br>有两个消费者集群，消费者集群A拥有两个消费者C1和C2，消费者集群B拥有四个消费者C3,C4,C5,C6。<br>每个partition只能被一个消费者集群中的一个消费者消费，比如broker1中partition0，只能被Consumer GroupA中的C1消费，只能被Consumer GroupB中的C3消费，kafak会确保这个消费者是这个partition的唯一消费者。<br>因为偏移量的唯一值是基于一个分区内的，producer生产的消息按照一定的算法分配到不同的分区，在各个分区内部，偏移量是线性增长的，所以在一个分区内消费消息是可以保持顺序的。但是如果topic里有多个partition的话，那么不能保证全局的消息是顺序的。<br>一个消费组中有多个消费者可以提高消费消息的并发性，并且当partition的消费者出现故障，那么这个partition可以分配给同组的其他消费者，从而提高他的容错性。<br>因为一个partition只能被一个同组的消费者消费，所以当同组中的消费者数量多于partition的数量时，注定有消费者无法消费partition。<br>每个消费者可以消费一个到多个partition。</p>
</li>
<li><p>发布订阅<br>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。发布到topic的消息会被所有订阅者消费。消费端为拉模型，消费状态和订阅关系由客户端负责维护，消息消费完后不会立即删除，会保留历史消息。</p>
</li>
</ol>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><h3 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h3><p>主机通过Kafka发送与系统和应用程序健康相关的指标，然后这些信息会被收集和处理从而创建监控仪表盘并发送警告。</p>
<h3 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h3><p> 应用程度使用Kafka作为传统的消息系统实现标准的队列和消息的发布—订阅，例如搜索和内容提要（Content Feed）。比起大多数的消息系统来说，Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。消息系统 一般吞吐量相对较低，但是需要更小的端到端延时，并尝尝依赖于Kafka提供的强大的持久性保障。在这个领域，Kafka足以媲美传统消息系统，如ActiveMR或RabbitMQ</p>
<h3 id="站点的用户活动追踪"><a href="#站点的用户活动追踪" class="headerlink" title="站点的用户活动追踪"></a>站点的用户活动追踪</h3><p>为了更好地理解用户行为，改善用户体验，将用户查看了哪个页面、点击了哪些内容等信息发送到每个数据中心的Kafka集群上，并通过Hadoop进行分析、生成日常报告。</p>
<h3 id="流处理"><a href="#流处理" class="headerlink" title="流处理"></a>流处理</h3><p>保存收集流数据，以提供之后对接的Storm或其他流式计算框架进行处理。很多用户会将那些从原始topic来的数据进行阶段性处理，汇总，扩充或者以其他的方式转换到新的topic下再继续后面的处理。例如一个文章推荐的处理流程，可能是先从RSS数据源中抓取文章的内容，然后将其丢入一个叫做“文章”的topic中；后续操作可能是需要对这个内容进行清理，比如回复正常数据或者删除重复数据，最后再将内容匹配的结果返 还给用户。这就在一个独立的topic之外，产生了一系列的实时数据处理的流程。</p>
<h3 id="日志聚合"><a href="#日志聚合" class="headerlink" title="日志聚合"></a>日志聚合</h3><p>使用Kafka代替日志聚合（log aggregation）。日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或HDFS）进行处理。然而Kafka忽略掉文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比起以日志为中心的系统比如Scribe或者Flume来说，Kafka提供同样高效的性能和因为复制导致的更高的耐用性保证，以及更低的端到端延迟。</p>
<h3 id="持久性日志"><a href="#持久性日志" class="headerlink" title="持久性日志"></a>持久性日志</h3><p>Kafka可以为一种外部的持久性日志的分布式系统提供服务。这种日志可以在节点间备份数据，并为故障节点数据回复提供一种重新同步的机制。Kafka中日志压缩功能为这种用法提供了条件。在这种用法中，Kafka类似于Apache BookKeeper项目。</p>
<h1 id="为什么要集成Flume和Kafka"><a href="#为什么要集成Flume和Kafka" class="headerlink" title="为什么要集成Flume和Kafka"></a>为什么要集成Flume和Kafka</h1><p>一般使用Flume+Kafka架构都是希望完成实时流式的日志处理，后面再连接上Flink/Storm/Spark Streaming等流式实时处理技术，从而完成日志实时解析的目标。第一、如果Flume直接对接实时计算框架，当数据采集速度大于数据处理速度，很容易发生数据堆积或者数据丢失，而kafka可以当做一个消息缓存队列，从广义上理解，把它当做一个数据库，可以存放一段时间的数据。第二、Kafka属于中间件，一个明显的优势就是使各层解耦，使得出错时不会干扰其他组件。</p>
<p>因此数据从数据源到flume再到Kafka时，数据一方面可以同步到HDFS做离线计算，另一方面可以做实时计算，可实现数据多分发。</p>
<ol>
<li>Kafka是pull based, 如果你有很多下游的Data Consumer，用Kafka；</li>
<li>Kafka有Replication，Flume没有，如果要求很高的容错性(Data High Availability)，选kafka；</li>
<li>需要更好的Hadoop类产品接口，例如HDFS，HBase等，用Flume。</li>
</ol>
<h1 id="Spark-Streaming-整合FLume"><a href="#Spark-Streaming-整合FLume" class="headerlink" title="Spark Streaming 整合FLume"></a>Spark Streaming 整合FLume</h1><p>采用推模式：推模式的理解就是Flume作为缓存，存有数据。监听对应端口，如果服务可以链接，就将数据push过去。(简单，耦合要低)，缺点是SparkStreaming 程序没有启动的话，Flume端会报错，同时可能会导致Spark Streaming 程序来不及消费的情况。</p>
<p>采用拉模式：拉模式就是自己定义一个sink，SparkStreaming自己去channel里面取数据，根据自身条件去获取数据，稳定性好。</p>
<h2 id="poll方式"><a href="#poll方式" class="headerlink" title="poll方式"></a>poll方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">配置文件</span><br><span class="line">simple-agent.sources = netcat-source</span><br><span class="line">simple-agent.sinks = avro-sink</span><br><span class="line">simple-agent.channels = memory-channel</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.type = netcat</span><br><span class="line">simple-agent.sources.netcat-source.bind = hadoop000</span><br><span class="line">simple-agent.sources.netcat-source.port = 44444</span><br><span class="line"></span><br><span class="line">simple-agent.sinks.avro-sink.type = avro</span><br><span class="line">simple-agent.sinks.avro-sink.hostname = hadoop000</span><br><span class="line">simple-agent.sinks.avro-sink.port = 41414</span><br><span class="line"></span><br><span class="line">simple-agent.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.channels = memory-channel</span><br><span class="line">simple-agent.sinks.avro-sink.channel = memory-channel</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlumePushWordCount</span> </span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(args.length != <span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: FlumePushWordCount &lt;hostname&gt; &lt;port&gt;"</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> <span class="type">Array</span>(hostname, port) = args</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>() <span class="comment">//.setMaster("local[2]").setAppName("FlumePushWordCount")</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO... 如何使用SparkStreaming整合Flume</span></span><br><span class="line">    <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createStream(ssc, hostname, port.toInt)</span><br><span class="line"></span><br><span class="line">    flumeStream.map(x=&gt; <span class="keyword">new</span> <span class="type">String</span>(x.event.getBody.array()).trim)</span><br><span class="line">      .flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="pull方式"><a href="#pull方式" class="headerlink" title="pull方式"></a>pull方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">配置文件</span><br><span class="line">simple-agent.sources = netcat-source</span><br><span class="line">simple-agent.sinks = spark-sink</span><br><span class="line">simple-agent.channels = memory-channel</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.type = netcat</span><br><span class="line">simple-agent.sources.netcat-source.bind = hadoop000</span><br><span class="line">simple-agent.sources.netcat-source.port = 44444</span><br><span class="line"></span><br><span class="line">就这里不一样，需要一个包</span><br><span class="line">simple-agent.sinks.spark-sink.type = org.apache.spark.streaming.flume.sink.SparkSink</span><br><span class="line">simple-agent.sinks.spark-sink.hostname = hadoop000</span><br><span class="line">simple-agent.sinks.spark-sink.port = 41414</span><br><span class="line"></span><br><span class="line">simple-agent.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.channels = memory-channel</span><br><span class="line">simple-agent.sinks.spark-sink.channel = memory-channel</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlumePullWordCount</span> </span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">if</span>(args.length != <span class="number">2</span>) </span><br><span class="line">    &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: FlumePullWordCount &lt;hostname&gt; &lt;port&gt;"</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> <span class="type">Array</span>(hostname, port) = args</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>() <span class="comment">//.setMaster("local[2]").setAppName("FlumePullWordCount")</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TODO... 如何使用SparkStreaming整合Flume</span></span><br><span class="line">    <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createPollingStream(ssc, hostname, port.toInt)</span><br><span class="line"></span><br><span class="line">    flumeStream.map(x=&gt; <span class="keyword">new</span> <span class="type">String</span>(x.event.getBody.array()).trim)</span><br><span class="line">      .flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Spark-Streaming-整合Kafka"><a href="#Spark-Streaming-整合Kafka" class="headerlink" title="Spark Streaming 整合Kafka"></a>Spark Streaming 整合Kafka</h1><h2 id="Receiver模式-又称kafka高级api模式"><a href="#Receiver模式-又称kafka高级api模式" class="headerlink" title="Receiver模式 又称kafka高级api模式"></a>Receiver模式 又称kafka高级api模式</h2><p>构造函数为<code>KafkaUtils.createDstream(ssc,[zk], [consumer group id], [per-topic,partitions] )</code>使用了receivers来接收数据，利用的是Kafka高层次的消费者api，对于所有的receivers接收到的数据将会保存在Spark executors中，然后通过Spark Streaming启动job来处理这些数据，默认会丢失，可启用WAL日志，它同步将接受到数据保存到分布式文件系统上比如HDFS。 所以数据在出错的情况下可以恢复出来 。</p>
<p>简单的理解就是kafka把消息全部封装好，提供给spark去调用，本来kafka的消息分布在不同的partition上面，相当于做了一步数据合并，在发送给spark，故spark可以设置executor个数去消费这部分数据，效率相对慢一些。</p>
<h2 id="Direct模式-又称kafka低级API模式"><a href="#Direct模式-又称kafka低级API模式" class="headerlink" title="Direct模式 又称kafka低级API模式"></a>Direct模式 又称kafka低级API模式</h2><p>简单的理解就是spark直接从kafka底层中的partition直接获取消息，相对于Receiver模式少了一步，效率更快。但是这样一来spark中的executor的工作的个数就为kafka中的partition一致，设置再多的executor都不工作，同时偏移量也需要自己维护。</p>
<p>不同于Receiver接收数据，这种方式定期地从kafka的topic下对应的partition中查询最新的偏移量，再根据偏移量范围在每个batch里面处理数据，Spark通过调用kafka简单的消费者Api读取一定范围的数据。<br>相比基于Receiver方式有几个优点： </p>
<p><strong>A、简化并行</strong></p>
<p>不需要创建多个kafka输入流，然后union它们，sparkStreaming将会创建和kafka分区一种的rdd的分区数，而且会从kafka中并行读取数据，spark中RDD的分区数和kafka中的分区数据是一一对应的关系。</p>
<p><strong>B、高效</strong></p>
<p>第一种实现数据的零丢失是将数据预先保存在WAL中，会复制一遍数据，会导致数据被拷贝两次，第一次是被kafka复制，另一次是写到WAL中。而没有receiver的这种方式消除了这个问题。 </p>
<p><strong>C、恰好一次语义(Exactly-once-semantics)</strong></p>
<p>Receiver读取kafka数据是通过kafka高层次api把偏移量写入zookeeper中，虽然这种方法可以通过数据保存在WAL中保证数据不丢失，但是可能会因为sparkStreaming和ZK中保存的偏移量不一致而导致数据被消费了多次。EOS通过实现kafka低层次api，偏移量仅仅被ssc保存在checkpoint中，消除了zk和ssc偏移量不一致的问题。缺点是无法使用基于zookeeper的kafka监控工具</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/Python/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Python/" itemprop="url">Python</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-03T08:35:09+08:00">
                2019-07-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Python/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Python/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  2.9k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  12
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>more<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/Python/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </p></div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/SpringSocial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/SpringSocial/" itemprop="url">SpringSocial</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-20T15:39:17+08:00">
                2019-06-20
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/SpringSocial/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/SpringSocial/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  841
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  2
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>对于一个应用来说，提供对社交网络的支持是很有必要的。应用可以通过与社交网络的集成来迅速积累人气。与社交网络的集成主要有两个方面的功能：第一个是比较简单的用户登录集成，即允许用户使用社交网络网站上的已有账户来登录应用。这样做的好处是可以省去要用户重新注册的流程，同时也可以与用户已有的社交网络建立连接；第二个是深度的集成方式，即允许用户把应用中的相关内容分享到社交网络中。这样做的好处是可以保持用户的粘性，同时推广应用本身。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/SpringSocial/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/Scala入门/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Scala入门/" itemprop="url">Scala入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-19T16:30:21+08:00">
                2019-06-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Scala入门/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Scala入门/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  16
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>Scala 是一门多范式（multi-paradigm）的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。<br>Scala 运行在Java虚拟机上，并兼容现有的Java程序。<br>Scala 源代码被编译成Java字节码，所以它可以运行于JVM之上，并可以调用现有的Java类库。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/Scala入门/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/MongoDB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/MongoDB/" itemprop="url">MongoDB</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-17T20:27:21+08:00">
                2019-06-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/MongoDB/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/MongoDB/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  929
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>吐槽和评论数据特点分析</p>
<p>吐槽和评论两项功能存在以下特点：<br>（1）数据量大<br>（2）写入操作频繁<br>（3）价值较低<br>对于这样的数据，我们更适合使用MongoDB来实现数据的存储</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/MongoDB/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/Elasticsearch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Elasticsearch/" itemprop="url">ELK</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-09T14:35:14+08:00">
                2019-06-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Elasticsearch/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Elasticsearch/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>Elasticsearch是一个实时的分布式搜索和分析引擎。它可以帮助你用前所未有的速度去处理大规模数据。ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/Elasticsearch/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </p></div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/Windows通过xrdp远程连接Linux桌面/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/Windows通过xrdp远程连接Linux桌面/" itemprop="url">Windows通过xrdp远程连接Linux桌面</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-08T10:39:45+08:00">
                2019-06-08
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/Windows通过xrdp远程连接Linux桌面/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/Windows通过xrdp远程连接Linux桌面/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  174
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>一般情况下我们用ssh客户端远程登陆Linux系统，至于图形界面下的linux远程登陆工具，我们一般都会想到vnc，但它的安全性不够，在这里，我将介绍XRDP的安装配置方法。我们可以很方便的通过Windows远程桌面Linux。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/Windows通过xrdp远程连接Linux桌面/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/SpringCloud/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/SpringCloud/" itemprop="url">SpringCloud</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-04T16:47:36+08:00">
                2019-06-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/SpringCloud/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/SpringCloud/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4.5k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  18
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>微服务<br><a href="https://blog.lqdev.cn/categories/SpringCloud/" target="_blank" rel="noopener">https://blog.lqdev.cn/categories/SpringCloud/</a><br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/SpringCloud/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </p></div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/SpringBoot中搭建全文检索引擎Solr/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/SpringBoot中搭建全文检索引擎Solr/" itemprop="url">SpringBoot中搭建全文检索引擎Solr</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-09T21:37:16+08:00">
                2019-05-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/SpringBoot中搭建全文检索引擎Solr/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/SpringBoot中搭建全文检索引擎Solr/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  2.4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  11
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="Solr介绍"><a href="#Solr介绍" class="headerlink" title="Solr介绍"></a>Solr介绍</h1><p>Solr 是Apache下的一个顶级开源项目，采用Java开发，它是基于Lucene的全文搜索服务器。Solr可以独立运行在Jetty、Tomcat等这些Servlet容器中。<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/SpringBoot中搭建全文检索引擎Solr/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </p></div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://490.github.io/优化方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="le">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我的笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/优化方法/" itemprop="url">SQL优化方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-30T16:40:14+08:00">
                2019-04-30
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/优化方法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/优化方法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  6
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="SQL-优化"><a href="#SQL-优化" class="headerlink" title="SQL 优化"></a>SQL 优化</h1><p>1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/优化方法/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </p></div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="le">
            
              <p class="site-author-name" itemprop="name">le</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">47</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/490" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">le</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">255k</span>
  
</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    





  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'Yi80CT04XNkVSJTMAPm5FtWc-gzGzoHsz',
        appKey: 'j3zcBrBI4kxBuazJBe3Gra3B',
        placeholder: '^_^',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":80,"height":160},"mobile":{"show":true}});</script></body>
</html>
